I0322 02:28:51.624490  7661 caffe.cpp:178] Use CPU.
I0322 02:28:51.624822  7661 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 10000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: CPU
net: "ECE595/mnist/train_test/lenet_train_test3.prototxt"
I0322 02:28:51.626047  7661 solver.cpp:91] Creating training net from net file: ECE595/mnist/train_test/lenet_train_test3.prototxt
I0322 02:28:51.627523  7661 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0322 02:28:51.627575  7661 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0322 02:28:51.627759  7661 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0322 02:28:51.628239  7661 layer_factory.hpp:77] Creating layer mnist
I0322 02:28:51.629467  7661 net.cpp:91] Creating Layer mnist
I0322 02:28:51.629503  7661 net.cpp:399] mnist -> data
I0322 02:28:51.629617  7661 net.cpp:399] mnist -> label
I0322 02:28:51.642102  7694 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0322 02:28:51.642500  7661 data_layer.cpp:41] output data size: 64,1,28,28
I0322 02:28:51.642886  7661 base_data_layer.cpp:69] Initializing prefetch
I0322 02:28:51.643314  7661 base_data_layer.cpp:72] Prefetch initialized.
I0322 02:28:51.643331  7661 net.cpp:141] Setting up mnist
I0322 02:28:51.643364  7661 net.cpp:148] Top shape: 64 1 28 28 (50176)
I0322 02:28:51.643378  7661 net.cpp:148] Top shape: 64 (64)
I0322 02:28:51.643399  7661 net.cpp:156] Memory required for data: 200960
I0322 02:28:51.643424  7661 layer_factory.hpp:77] Creating layer conv0
I0322 02:28:51.643486  7661 net.cpp:91] Creating Layer conv0
I0322 02:28:51.643504  7661 net.cpp:425] conv0 <- data
I0322 02:28:51.643553  7661 net.cpp:399] conv0 -> conv0
I0322 02:28:51.643951  7661 net.cpp:141] Setting up conv0
I0322 02:28:51.643971  7661 net.cpp:148] Top shape: 64 50 24 24 (1843200)
I0322 02:28:51.643981  7661 net.cpp:156] Memory required for data: 7573760
I0322 02:28:51.644048  7661 layer_factory.hpp:77] Creating layer pool0
I0322 02:28:51.644078  7661 net.cpp:91] Creating Layer pool0
I0322 02:28:51.644089  7661 net.cpp:425] pool0 <- conv0
I0322 02:28:51.644122  7661 net.cpp:399] pool0 -> pool0
I0322 02:28:51.644173  7661 net.cpp:141] Setting up pool0
I0322 02:28:51.644189  7661 net.cpp:148] Top shape: 64 50 12 12 (460800)
I0322 02:28:51.644196  7661 net.cpp:156] Memory required for data: 9416960
I0322 02:28:51.644207  7661 layer_factory.hpp:77] Creating layer conv1
I0322 02:28:51.644243  7661 net.cpp:91] Creating Layer conv1
I0322 02:28:51.644254  7661 net.cpp:425] conv1 <- pool0
I0322 02:28:51.644284  7661 net.cpp:399] conv1 -> conv1
I0322 02:28:51.649942  7661 net.cpp:141] Setting up conv1
I0322 02:28:51.649973  7661 net.cpp:148] Top shape: 64 50 8 8 (204800)
I0322 02:28:51.649989  7661 net.cpp:156] Memory required for data: 10236160
I0322 02:28:51.650029  7661 layer_factory.hpp:77] Creating layer pool1
I0322 02:28:51.650064  7661 net.cpp:91] Creating Layer pool1
I0322 02:28:51.650085  7661 net.cpp:425] pool1 <- conv1
I0322 02:28:51.650117  7661 net.cpp:399] pool1 -> pool1
I0322 02:28:51.650164  7661 net.cpp:141] Setting up pool1
I0322 02:28:51.650188  7661 net.cpp:148] Top shape: 64 50 4 4 (51200)
I0322 02:28:51.650204  7661 net.cpp:156] Memory required for data: 10440960
I0322 02:28:51.650223  7661 layer_factory.hpp:77] Creating layer conv2
I0322 02:28:51.650259  7661 net.cpp:91] Creating Layer conv2
I0322 02:28:51.650279  7661 net.cpp:425] conv2 <- pool1
I0322 02:28:51.650315  7661 net.cpp:399] conv2 -> conv2
F0322 02:28:51.655879  7661 blob.cpp:115] Check failed: data_ 
*** Check failure stack trace: ***
    @     0x7f7512588b5d  google::LogMessage::Fail()
    @     0x7f751258cb77  google::LogMessage::SendToLog()
    @     0x7f751258a9f9  google::LogMessage::Flush()
I0322 02:28:51.660203  7695 data_layer.cpp:102] Prefetch batch: 2 ms.
I0322 02:28:51.660290  7695 data_layer.cpp:103]      Read time: 0.217 ms.
I0322 02:28:51.660321  7695 data_layer.cpp:104] Transform time: 1.175 ms.
    @     0x7f751258acfd  google::LogMessageFatal::~LogMessageFatal()
I0322 02:28:51.662662  7695 data_layer.cpp:102] Prefetch batch: 2 ms.
I0322 02:28:51.662704  7695 data_layer.cpp:103]      Read time: 0.217 ms.
I0322 02:28:51.662735  7695 data_layer.cpp:104] Transform time: 1.156 ms.
    @     0x7f7512b2df33  caffe::Blob<>::mutable_cpu_data()
I0322 02:28:51.665212  7695 data_layer.cpp:102] Prefetch batch: 2 ms.
I0322 02:28:51.665254  7695 data_layer.cpp:103]      Read time: 0.275 ms.
I0322 02:28:51.665282  7695 data_layer.cpp:104] Transform time: 1.078 ms.
    @     0x7f7512bdbb73  caffe::BaseConvolutionLayer<>::Reshape()
    @     0x7f7512b16d3d  caffe::Layer<>::SetUp()
    @     0x7f7512b0283b  caffe::Net<>::Init()
    @     0x7f7512b006c7  caffe::Net<>::Net()
    @     0x7f7512c3973b  caffe::Solver<>::InitTrainNet()
    @     0x7f7512c38f73  caffe::Solver<>::Init()
    @     0x7f7512c38934  caffe::Solver<>::Solver()
    @     0x7f7512aef281  caffe::SGDSolver<>::SGDSolver()
    @     0x7f7512aff26f  caffe::Creator_SGDSolver<>()
    @           0x41a5f8  caffe::SolverRegistry<>::CreateSolver()
    @           0x415bc8  train()
    @           0x417f17  main
    @       0x3d0741ed1d  (unknown)
    @           0x414a89  (unknown)
