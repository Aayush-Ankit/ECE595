I0408 12:28:42.821707 10695 caffe.cpp:218] Using GPUs 0
I0408 12:28:42.837311 10695 caffe.cpp:223] GPU 0: GeForce GTX 950M
I0408 12:28:43.027652 10695 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 10000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_fc_size/lenet_train_test_fcl2_fcn500.prototxt"
train_state {
  level: 0
  stage: ""
}
I0408 12:28:43.027827 10695 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_fc_size/lenet_train_test_fcl2_fcn500.prototxt
I0408 12:28:43.028074 10695 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0408 12:28:43.028102 10695 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0408 12:28:43.028240 10695 net.cpp:51] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0408 12:28:43.028345 10695 layer_factory.hpp:77] Creating layer mnist
I0408 12:28:43.028434 10695 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0408 12:28:43.028451 10695 net.cpp:84] Creating Layer mnist
I0408 12:28:43.028482 10695 net.cpp:380] mnist -> data
I0408 12:28:43.028528 10695 net.cpp:380] mnist -> label
I0408 12:28:43.029188 10695 data_layer.cpp:45] output data size: 64,1,28,28
I0408 12:28:43.030764 10695 net.cpp:122] Setting up mnist
I0408 12:28:43.030777 10695 net.cpp:129] Top shape: 64 1 28 28 (50176)
I0408 12:28:43.030781 10695 net.cpp:129] Top shape: 64 (64)
I0408 12:28:43.030781 10695 net.cpp:137] Memory required for data: 200960
I0408 12:28:43.030788 10695 layer_factory.hpp:77] Creating layer conv0
I0408 12:28:43.030822 10695 net.cpp:84] Creating Layer conv0
I0408 12:28:43.030839 10695 net.cpp:406] conv0 <- data
I0408 12:28:43.030848 10695 net.cpp:380] conv0 -> conv0
I0408 12:28:43.031898 10695 net.cpp:122] Setting up conv0
I0408 12:28:43.031908 10695 net.cpp:129] Top shape: 64 50 24 24 (1843200)
I0408 12:28:43.031909 10695 net.cpp:137] Memory required for data: 7573760
I0408 12:28:43.031970 10695 layer_factory.hpp:77] Creating layer pool0
I0408 12:28:43.031980 10695 net.cpp:84] Creating Layer pool0
I0408 12:28:43.031982 10695 net.cpp:406] pool0 <- conv0
I0408 12:28:43.032001 10695 net.cpp:380] pool0 -> pool0
I0408 12:28:43.032038 10695 net.cpp:122] Setting up pool0
I0408 12:28:43.032043 10695 net.cpp:129] Top shape: 64 50 12 12 (460800)
I0408 12:28:43.032045 10695 net.cpp:137] Memory required for data: 9416960
I0408 12:28:43.032047 10695 layer_factory.hpp:77] Creating layer conv1
I0408 12:28:43.032057 10695 net.cpp:84] Creating Layer conv1
I0408 12:28:43.032060 10695 net.cpp:406] conv1 <- pool0
I0408 12:28:43.032066 10695 net.cpp:380] conv1 -> conv1
I0408 12:28:43.032909 10695 net.cpp:122] Setting up conv1
I0408 12:28:43.032938 10695 net.cpp:129] Top shape: 64 50 8 8 (204800)
I0408 12:28:43.032940 10695 net.cpp:137] Memory required for data: 10236160
I0408 12:28:43.032946 10695 layer_factory.hpp:77] Creating layer pool1
I0408 12:28:43.032951 10695 net.cpp:84] Creating Layer pool1
I0408 12:28:43.032956 10695 net.cpp:406] pool1 <- conv1
I0408 12:28:43.032961 10695 net.cpp:380] pool1 -> pool1
I0408 12:28:43.033004 10695 net.cpp:122] Setting up pool1
I0408 12:28:43.033010 10695 net.cpp:129] Top shape: 64 50 4 4 (51200)
I0408 12:28:43.033013 10695 net.cpp:137] Memory required for data: 10440960
I0408 12:28:43.033016 10695 layer_factory.hpp:77] Creating layer ip1
I0408 12:28:43.033025 10695 net.cpp:84] Creating Layer ip1
I0408 12:28:43.033030 10695 net.cpp:406] ip1 <- pool1
I0408 12:28:43.033036 10695 net.cpp:380] ip1 -> ip1
I0408 12:28:43.035625 10695 net.cpp:122] Setting up ip1
I0408 12:28:43.035635 10695 net.cpp:129] Top shape: 64 500 (32000)
I0408 12:28:43.035636 10695 net.cpp:137] Memory required for data: 10568960
I0408 12:28:43.035643 10695 layer_factory.hpp:77] Creating layer relu1
I0408 12:28:43.035667 10695 net.cpp:84] Creating Layer relu1
I0408 12:28:43.035670 10695 net.cpp:406] relu1 <- ip1
I0408 12:28:43.035673 10695 net.cpp:367] relu1 -> ip1 (in-place)
I0408 12:28:43.035691 10695 net.cpp:122] Setting up relu1
I0408 12:28:43.035694 10695 net.cpp:129] Top shape: 64 500 (32000)
I0408 12:28:43.035696 10695 net.cpp:137] Memory required for data: 10696960
I0408 12:28:43.035697 10695 layer_factory.hpp:77] Creating layer ip2
I0408 12:28:43.035702 10695 net.cpp:84] Creating Layer ip2
I0408 12:28:43.035724 10695 net.cpp:406] ip2 <- ip1
I0408 12:28:43.035728 10695 net.cpp:380] ip2 -> ip2
I0408 12:28:43.036200 10695 net.cpp:122] Setting up ip2
I0408 12:28:43.036206 10695 net.cpp:129] Top shape: 64 10 (640)
I0408 12:28:43.036208 10695 net.cpp:137] Memory required for data: 10699520
I0408 12:28:43.036212 10695 layer_factory.hpp:77] Creating layer relu2
I0408 12:28:43.036231 10695 net.cpp:84] Creating Layer relu2
I0408 12:28:43.036234 10695 net.cpp:406] relu2 <- ip2
I0408 12:28:43.036237 10695 net.cpp:367] relu2 -> ip2 (in-place)
I0408 12:28:43.036242 10695 net.cpp:122] Setting up relu2
I0408 12:28:43.036245 10695 net.cpp:129] Top shape: 64 10 (640)
I0408 12:28:43.036247 10695 net.cpp:137] Memory required for data: 10702080
I0408 12:28:43.036248 10695 layer_factory.hpp:77] Creating layer loss
I0408 12:28:43.036252 10695 net.cpp:84] Creating Layer loss
I0408 12:28:43.036254 10695 net.cpp:406] loss <- ip2
I0408 12:28:43.036257 10695 net.cpp:406] loss <- label
I0408 12:28:43.036278 10695 net.cpp:380] loss -> loss
I0408 12:28:43.036303 10695 layer_factory.hpp:77] Creating layer loss
I0408 12:28:43.036384 10695 net.cpp:122] Setting up loss
I0408 12:28:43.036388 10695 net.cpp:129] Top shape: (1)
I0408 12:28:43.036391 10695 net.cpp:132]     with loss weight 1
I0408 12:28:43.036409 10695 net.cpp:137] Memory required for data: 10702084
I0408 12:28:43.036412 10695 net.cpp:198] loss needs backward computation.
I0408 12:28:43.036417 10695 net.cpp:198] relu2 needs backward computation.
I0408 12:28:43.036418 10695 net.cpp:198] ip2 needs backward computation.
I0408 12:28:43.036422 10695 net.cpp:198] relu1 needs backward computation.
I0408 12:28:43.036422 10695 net.cpp:198] ip1 needs backward computation.
I0408 12:28:43.036435 10695 net.cpp:198] pool1 needs backward computation.
I0408 12:28:43.036438 10695 net.cpp:198] conv1 needs backward computation.
I0408 12:28:43.036454 10695 net.cpp:198] pool0 needs backward computation.
I0408 12:28:43.036456 10695 net.cpp:198] conv0 needs backward computation.
I0408 12:28:43.036499 10695 net.cpp:200] mnist does not need backward computation.
I0408 12:28:43.036501 10695 net.cpp:242] This network produces output loss
I0408 12:28:43.036510 10695 net.cpp:255] Network initialization done.
I0408 12:28:43.036751 10695 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_fc_size/lenet_train_test_fcl2_fcn500.prototxt
I0408 12:28:43.036767 10695 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0408 12:28:43.036866 10695 net.cpp:51] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0408 12:28:43.036952 10695 layer_factory.hpp:77] Creating layer mnist
I0408 12:28:43.037009 10695 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0408 12:28:43.037020 10695 net.cpp:84] Creating Layer mnist
I0408 12:28:43.037025 10695 net.cpp:380] mnist -> data
I0408 12:28:43.037032 10695 net.cpp:380] mnist -> label
I0408 12:28:43.037129 10695 data_layer.cpp:45] output data size: 100,1,28,28
I0408 12:28:43.039162 10695 net.cpp:122] Setting up mnist
I0408 12:28:43.039180 10695 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0408 12:28:43.039185 10695 net.cpp:129] Top shape: 100 (100)
I0408 12:28:43.039186 10695 net.cpp:137] Memory required for data: 314000
I0408 12:28:43.039191 10695 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0408 12:28:43.039196 10695 net.cpp:84] Creating Layer label_mnist_1_split
I0408 12:28:43.039199 10695 net.cpp:406] label_mnist_1_split <- label
I0408 12:28:43.039204 10695 net.cpp:380] label_mnist_1_split -> label_mnist_1_split_0
I0408 12:28:43.039211 10695 net.cpp:380] label_mnist_1_split -> label_mnist_1_split_1
I0408 12:28:43.039289 10695 net.cpp:122] Setting up label_mnist_1_split
I0408 12:28:43.039296 10695 net.cpp:129] Top shape: 100 (100)
I0408 12:28:43.039301 10695 net.cpp:129] Top shape: 100 (100)
I0408 12:28:43.039304 10695 net.cpp:137] Memory required for data: 314800
I0408 12:28:43.039309 10695 layer_factory.hpp:77] Creating layer conv0
I0408 12:28:43.039324 10695 net.cpp:84] Creating Layer conv0
I0408 12:28:43.039328 10695 net.cpp:406] conv0 <- data
I0408 12:28:43.039335 10695 net.cpp:380] conv0 -> conv0
I0408 12:28:43.039584 10695 net.cpp:122] Setting up conv0
I0408 12:28:43.039592 10695 net.cpp:129] Top shape: 100 50 24 24 (2880000)
I0408 12:28:43.039595 10695 net.cpp:137] Memory required for data: 11834800
I0408 12:28:43.039604 10695 layer_factory.hpp:77] Creating layer pool0
I0408 12:28:43.039613 10695 net.cpp:84] Creating Layer pool0
I0408 12:28:43.039630 10695 net.cpp:406] pool0 <- conv0
I0408 12:28:43.039634 10695 net.cpp:380] pool0 -> pool0
I0408 12:28:43.039669 10695 net.cpp:122] Setting up pool0
I0408 12:28:43.039675 10695 net.cpp:129] Top shape: 100 50 12 12 (720000)
I0408 12:28:43.039679 10695 net.cpp:137] Memory required for data: 14714800
I0408 12:28:43.039681 10695 layer_factory.hpp:77] Creating layer conv1
I0408 12:28:43.039690 10695 net.cpp:84] Creating Layer conv1
I0408 12:28:43.039695 10695 net.cpp:406] conv1 <- pool0
I0408 12:28:43.039701 10695 net.cpp:380] conv1 -> conv1
I0408 12:28:43.040695 10695 net.cpp:122] Setting up conv1
I0408 12:28:43.040704 10695 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0408 12:28:43.040709 10695 net.cpp:137] Memory required for data: 15994800
I0408 12:28:43.040717 10695 layer_factory.hpp:77] Creating layer pool1
I0408 12:28:43.040724 10695 net.cpp:84] Creating Layer pool1
I0408 12:28:43.040729 10695 net.cpp:406] pool1 <- conv1
I0408 12:28:43.040736 10695 net.cpp:380] pool1 -> pool1
I0408 12:28:43.040772 10695 net.cpp:122] Setting up pool1
I0408 12:28:43.040778 10695 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0408 12:28:43.040782 10695 net.cpp:137] Memory required for data: 16314800
I0408 12:28:43.040786 10695 layer_factory.hpp:77] Creating layer ip1
I0408 12:28:43.040812 10695 net.cpp:84] Creating Layer ip1
I0408 12:28:43.040817 10695 net.cpp:406] ip1 <- pool1
I0408 12:28:43.040822 10695 net.cpp:380] ip1 -> ip1
I0408 12:28:43.045198 10695 net.cpp:122] Setting up ip1
I0408 12:28:43.045228 10695 net.cpp:129] Top shape: 100 500 (50000)
I0408 12:28:43.045231 10695 net.cpp:137] Memory required for data: 16514800
I0408 12:28:43.045243 10695 layer_factory.hpp:77] Creating layer relu1
I0408 12:28:43.045250 10695 net.cpp:84] Creating Layer relu1
I0408 12:28:43.045254 10695 net.cpp:406] relu1 <- ip1
I0408 12:28:43.045259 10695 net.cpp:367] relu1 -> ip1 (in-place)
I0408 12:28:43.045267 10695 net.cpp:122] Setting up relu1
I0408 12:28:43.045272 10695 net.cpp:129] Top shape: 100 500 (50000)
I0408 12:28:43.045275 10695 net.cpp:137] Memory required for data: 16714800
I0408 12:28:43.045279 10695 layer_factory.hpp:77] Creating layer ip2
I0408 12:28:43.045307 10695 net.cpp:84] Creating Layer ip2
I0408 12:28:43.045326 10695 net.cpp:406] ip2 <- ip1
I0408 12:28:43.045332 10695 net.cpp:380] ip2 -> ip2
I0408 12:28:43.045454 10695 net.cpp:122] Setting up ip2
I0408 12:28:43.045460 10695 net.cpp:129] Top shape: 100 10 (1000)
I0408 12:28:43.045464 10695 net.cpp:137] Memory required for data: 16718800
I0408 12:28:43.045467 10695 layer_factory.hpp:77] Creating layer relu2
I0408 12:28:43.045471 10695 net.cpp:84] Creating Layer relu2
I0408 12:28:43.045475 10695 net.cpp:406] relu2 <- ip2
I0408 12:28:43.045477 10695 net.cpp:367] relu2 -> ip2 (in-place)
I0408 12:28:43.045482 10695 net.cpp:122] Setting up relu2
I0408 12:28:43.045486 10695 net.cpp:129] Top shape: 100 10 (1000)
I0408 12:28:43.045488 10695 net.cpp:137] Memory required for data: 16722800
I0408 12:28:43.045492 10695 layer_factory.hpp:77] Creating layer ip2_relu2_0_split
I0408 12:28:43.045496 10695 net.cpp:84] Creating Layer ip2_relu2_0_split
I0408 12:28:43.045498 10695 net.cpp:406] ip2_relu2_0_split <- ip2
I0408 12:28:43.045502 10695 net.cpp:380] ip2_relu2_0_split -> ip2_relu2_0_split_0
I0408 12:28:43.045518 10695 net.cpp:380] ip2_relu2_0_split -> ip2_relu2_0_split_1
I0408 12:28:43.045557 10695 net.cpp:122] Setting up ip2_relu2_0_split
I0408 12:28:43.045562 10695 net.cpp:129] Top shape: 100 10 (1000)
I0408 12:28:43.045563 10695 net.cpp:129] Top shape: 100 10 (1000)
I0408 12:28:43.045565 10695 net.cpp:137] Memory required for data: 16730800
I0408 12:28:43.045567 10695 layer_factory.hpp:77] Creating layer accuracy
I0408 12:28:43.045572 10695 net.cpp:84] Creating Layer accuracy
I0408 12:28:43.045575 10695 net.cpp:406] accuracy <- ip2_relu2_0_split_0
I0408 12:28:43.045579 10695 net.cpp:406] accuracy <- label_mnist_1_split_0
I0408 12:28:43.045583 10695 net.cpp:380] accuracy -> accuracy
I0408 12:28:43.045589 10695 net.cpp:122] Setting up accuracy
I0408 12:28:43.045593 10695 net.cpp:129] Top shape: (1)
I0408 12:28:43.045595 10695 net.cpp:137] Memory required for data: 16730804
I0408 12:28:43.045598 10695 layer_factory.hpp:77] Creating layer loss
I0408 12:28:43.045600 10695 net.cpp:84] Creating Layer loss
I0408 12:28:43.045603 10695 net.cpp:406] loss <- ip2_relu2_0_split_1
I0408 12:28:43.045605 10695 net.cpp:406] loss <- label_mnist_1_split_1
I0408 12:28:43.045609 10695 net.cpp:380] loss -> loss
I0408 12:28:43.045615 10695 layer_factory.hpp:77] Creating layer loss
I0408 12:28:43.045691 10695 net.cpp:122] Setting up loss
I0408 12:28:43.045696 10695 net.cpp:129] Top shape: (1)
I0408 12:28:43.045697 10695 net.cpp:132]     with loss weight 1
I0408 12:28:43.045706 10695 net.cpp:137] Memory required for data: 16730808
I0408 12:28:43.045707 10695 net.cpp:198] loss needs backward computation.
I0408 12:28:43.045711 10695 net.cpp:200] accuracy does not need backward computation.
I0408 12:28:43.045713 10695 net.cpp:198] ip2_relu2_0_split needs backward computation.
I0408 12:28:43.045716 10695 net.cpp:198] relu2 needs backward computation.
I0408 12:28:43.045717 10695 net.cpp:198] ip2 needs backward computation.
I0408 12:28:43.045720 10695 net.cpp:198] relu1 needs backward computation.
I0408 12:28:43.045722 10695 net.cpp:198] ip1 needs backward computation.
I0408 12:28:43.045723 10695 net.cpp:198] pool1 needs backward computation.
I0408 12:28:43.045725 10695 net.cpp:198] conv1 needs backward computation.
I0408 12:28:43.045728 10695 net.cpp:198] pool0 needs backward computation.
I0408 12:28:43.045730 10695 net.cpp:198] conv0 needs backward computation.
I0408 12:28:43.045734 10695 net.cpp:200] label_mnist_1_split does not need backward computation.
I0408 12:28:43.045738 10695 net.cpp:200] mnist does not need backward computation.
I0408 12:28:43.045740 10695 net.cpp:242] This network produces output accuracy
I0408 12:28:43.045742 10695 net.cpp:242] This network produces output loss
I0408 12:28:43.045765 10695 net.cpp:255] Network initialization done.
I0408 12:28:43.045799 10695 solver.cpp:56] Solver scaffolding done.
I0408 12:28:43.046036 10695 caffe.cpp:248] Starting Optimization
I0408 12:28:43.046041 10695 solver.cpp:273] Solving LeNet
I0408 12:28:43.046043 10695 solver.cpp:274] Learning Rate Policy: inv
I0408 12:28:43.046896 10695 solver.cpp:331] Iteration 0, Testing net (#0)
I0408 12:28:50.443871 10703 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:28:50.763460 10695 solver.cpp:398]     Test net output #0: accuracy = 0.0879
I0408 12:28:50.763484 10695 solver.cpp:398]     Test net output #1: loss = 2.32495 (* 1 = 2.32495 loss)
I0408 12:28:50.873186 10695 solver.cpp:219] Iteration 0 (0 iter/s, 7.82709s/100 iters), loss = 2.30184
I0408 12:28:50.873217 10695 solver.cpp:238]     Train net output #0: loss = 2.30184 (* 1 = 2.30184 loss)
I0408 12:28:50.873250 10695 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0408 12:29:00.422924 10695 solver.cpp:219] Iteration 100 (10.4716 iter/s, 9.54967s/100 iters), loss = 0.578342
I0408 12:29:00.422960 10695 solver.cpp:238]     Train net output #0: loss = 0.578342 (* 1 = 0.578342 loss)
I0408 12:29:00.422968 10695 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0408 12:29:09.972527 10695 solver.cpp:219] Iteration 200 (10.4717 iter/s, 9.54954s/100 iters), loss = 0.128008
I0408 12:29:09.972574 10695 solver.cpp:238]     Train net output #0: loss = 0.128008 (* 1 = 0.128008 loss)
I0408 12:29:09.972579 10695 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0408 12:29:19.415349 10695 solver.cpp:219] Iteration 300 (10.5901 iter/s, 9.44274s/100 iters), loss = 0.222938
I0408 12:29:19.415413 10695 solver.cpp:238]     Train net output #0: loss = 0.222938 (* 1 = 0.222938 loss)
I0408 12:29:19.415421 10695 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0408 12:29:28.958751 10695 solver.cpp:219] Iteration 400 (10.4785 iter/s, 9.54331s/100 iters), loss = 0.0935887
I0408 12:29:28.958784 10695 solver.cpp:238]     Train net output #0: loss = 0.0935887 (* 1 = 0.0935887 loss)
I0408 12:29:28.958792 10695 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0408 12:29:38.228521 10695 solver.cpp:331] Iteration 500, Testing net (#0)
I0408 12:29:45.588831 10703 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:29:45.904286 10695 solver.cpp:398]     Test net output #0: accuracy = 0.9747
I0408 12:29:45.904309 10695 solver.cpp:398]     Test net output #1: loss = 0.0833043 (* 1 = 0.0833043 loss)
I0408 12:29:46.008102 10695 solver.cpp:219] Iteration 500 (5.86535 iter/s, 17.0493s/100 iters), loss = 0.105199
I0408 12:29:46.008149 10695 solver.cpp:238]     Train net output #0: loss = 0.105199 (* 1 = 0.105199 loss)
I0408 12:29:46.008155 10695 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0408 12:29:55.440408 10695 solver.cpp:219] Iteration 600 (10.6019 iter/s, 9.43223s/100 iters), loss = 0.0731211
I0408 12:29:55.440547 10695 solver.cpp:238]     Train net output #0: loss = 0.073121 (* 1 = 0.073121 loss)
I0408 12:29:55.440569 10695 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0408 12:30:05.186734 10695 solver.cpp:219] Iteration 700 (10.2605 iter/s, 9.74616s/100 iters), loss = 0.126894
I0408 12:30:05.186781 10695 solver.cpp:238]     Train net output #0: loss = 0.126893 (* 1 = 0.126893 loss)
I0408 12:30:05.186786 10695 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0408 12:30:14.862637 10695 solver.cpp:219] Iteration 800 (10.335 iter/s, 9.67583s/100 iters), loss = 0.162664
I0408 12:30:14.862666 10695 solver.cpp:238]     Train net output #0: loss = 0.162664 (* 1 = 0.162664 loss)
I0408 12:30:14.862670 10695 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0408 12:30:24.315196 10695 solver.cpp:219] Iteration 900 (10.5792 iter/s, 9.4525s/100 iters), loss = 0.163352
I0408 12:30:24.315237 10695 solver.cpp:238]     Train net output #0: loss = 0.163351 (* 1 = 0.163351 loss)
I0408 12:30:24.315243 10695 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0408 12:30:27.431071 10702 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:30:33.568285 10695 solver.cpp:331] Iteration 1000, Testing net (#0)
I0408 12:30:40.854957 10703 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:30:41.164713 10695 solver.cpp:398]     Test net output #0: accuracy = 0.9807
I0408 12:30:41.164753 10695 solver.cpp:398]     Test net output #1: loss = 0.0605092 (* 1 = 0.0605092 loss)
I0408 12:30:41.269866 10695 solver.cpp:219] Iteration 1000 (5.8981 iter/s, 16.9546s/100 iters), loss = 0.10153
I0408 12:30:41.269911 10695 solver.cpp:238]     Train net output #0: loss = 0.101529 (* 1 = 0.101529 loss)
I0408 12:30:41.269917 10695 sgd_solver.cpp:105] Iteration 1000, lr = 0.00931012
I0408 12:30:50.623847 10695 solver.cpp:219] Iteration 1100 (10.6907 iter/s, 9.3539s/100 iters), loss = 0.0040604
I0408 12:30:50.623874 10695 solver.cpp:238]     Train net output #0: loss = 0.00406027 (* 1 = 0.00406027 loss)
I0408 12:30:50.623879 10695 sgd_solver.cpp:105] Iteration 1100, lr = 0.00924715
I0408 12:30:59.965382 10695 solver.cpp:219] Iteration 1200 (10.7049 iter/s, 9.34148s/100 iters), loss = 0.0109102
I0408 12:30:59.965585 10695 solver.cpp:238]     Train net output #0: loss = 0.0109101 (* 1 = 0.0109101 loss)
I0408 12:30:59.965591 10695 sgd_solver.cpp:105] Iteration 1200, lr = 0.00918515
I0408 12:31:09.311604 10695 solver.cpp:219] Iteration 1300 (10.6998 iter/s, 9.346s/100 iters), loss = 0.0146682
I0408 12:31:09.311650 10695 solver.cpp:238]     Train net output #0: loss = 0.0146681 (* 1 = 0.0146681 loss)
I0408 12:31:09.311668 10695 sgd_solver.cpp:105] Iteration 1300, lr = 0.00912412
I0408 12:31:18.666865 10695 solver.cpp:219] Iteration 1400 (10.6893 iter/s, 9.35519s/100 iters), loss = 0.00508993
I0408 12:31:18.666909 10695 solver.cpp:238]     Train net output #0: loss = 0.00508979 (* 1 = 0.00508979 loss)
I0408 12:31:18.666914 10695 sgd_solver.cpp:105] Iteration 1400, lr = 0.00906403
I0408 12:31:27.865631 10695 solver.cpp:331] Iteration 1500, Testing net (#0)
I0408 12:31:35.172245 10703 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:31:35.482139 10695 solver.cpp:398]     Test net output #0: accuracy = 0.9832
I0408 12:31:35.482179 10695 solver.cpp:398]     Test net output #1: loss = 0.0495292 (* 1 = 0.0495292 loss)
I0408 12:31:35.587430 10695 solver.cpp:219] Iteration 1500 (5.91 iter/s, 16.9205s/100 iters), loss = 0.086085
I0408 12:31:35.587471 10695 solver.cpp:238]     Train net output #0: loss = 0.0860848 (* 1 = 0.0860848 loss)
I0408 12:31:35.587478 10695 sgd_solver.cpp:105] Iteration 1500, lr = 0.00900485
I0408 12:31:44.944844 10695 solver.cpp:219] Iteration 1600 (10.6868 iter/s, 9.35734s/100 iters), loss = 0.0940193
I0408 12:31:44.944897 10695 solver.cpp:238]     Train net output #0: loss = 0.0940192 (* 1 = 0.0940192 loss)
I0408 12:31:44.944917 10695 sgd_solver.cpp:105] Iteration 1600, lr = 0.00894657
I0408 12:31:54.387836 10695 solver.cpp:219] Iteration 1700 (10.5914 iter/s, 9.44165s/100 iters), loss = 0.0245368
I0408 12:31:54.387864 10695 solver.cpp:238]     Train net output #0: loss = 0.0245367 (* 1 = 0.0245367 loss)
I0408 12:31:54.387869 10695 sgd_solver.cpp:105] Iteration 1700, lr = 0.00888916
I0408 12:32:03.747680 10695 solver.cpp:219] Iteration 1800 (10.684 iter/s, 9.35979s/100 iters), loss = 0.0188338
I0408 12:32:03.747721 10695 solver.cpp:238]     Train net output #0: loss = 0.0188337 (* 1 = 0.0188337 loss)
I0408 12:32:03.747726 10695 sgd_solver.cpp:105] Iteration 1800, lr = 0.0088326
I0408 12:32:10.299193 10702 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:32:13.110496 10695 solver.cpp:219] Iteration 1900 (10.6806 iter/s, 9.36275s/100 iters), loss = 0.1263
I0408 12:32:13.110524 10695 solver.cpp:238]     Train net output #0: loss = 0.1263 (* 1 = 0.1263 loss)
I0408 12:32:13.110549 10695 sgd_solver.cpp:105] Iteration 1900, lr = 0.00877687
I0408 12:32:22.314373 10695 solver.cpp:331] Iteration 2000, Testing net (#0)
I0408 12:32:29.609537 10703 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:32:29.922881 10695 solver.cpp:398]     Test net output #0: accuracy = 0.9864
I0408 12:32:29.922905 10695 solver.cpp:398]     Test net output #1: loss = 0.0391774 (* 1 = 0.0391774 loss)
I0408 12:32:30.027534 10695 solver.cpp:219] Iteration 2000 (5.91122 iter/s, 16.917s/100 iters), loss = 0.0171096
I0408 12:32:30.027566 10695 solver.cpp:238]     Train net output #0: loss = 0.0171094 (* 1 = 0.0171094 loss)
I0408 12:32:30.027592 10695 sgd_solver.cpp:105] Iteration 2000, lr = 0.00872196
I0408 12:32:39.387037 10695 solver.cpp:219] Iteration 2100 (10.6844 iter/s, 9.35945s/100 iters), loss = 0.0359938
I0408 12:32:39.387081 10695 solver.cpp:238]     Train net output #0: loss = 0.0359937 (* 1 = 0.0359937 loss)
I0408 12:32:39.387085 10695 sgd_solver.cpp:105] Iteration 2100, lr = 0.00866784
I0408 12:32:48.743021 10695 solver.cpp:219] Iteration 2200 (10.6884 iter/s, 9.35591s/100 iters), loss = 0.0100213
I0408 12:32:48.743214 10695 solver.cpp:238]     Train net output #0: loss = 0.0100212 (* 1 = 0.0100212 loss)
I0408 12:32:48.743221 10695 sgd_solver.cpp:105] Iteration 2200, lr = 0.0086145
I0408 12:32:58.078526 10695 solver.cpp:219] Iteration 2300 (10.7121 iter/s, 9.33528s/100 iters), loss = 0.0956255
I0408 12:32:58.078572 10695 solver.cpp:238]     Train net output #0: loss = 0.0956253 (* 1 = 0.0956253 loss)
I0408 12:32:58.078577 10695 sgd_solver.cpp:105] Iteration 2300, lr = 0.00856192
I0408 12:33:07.419611 10695 solver.cpp:219] Iteration 2400 (10.7055 iter/s, 9.34101s/100 iters), loss = 0.0114372
I0408 12:33:07.419639 10695 solver.cpp:238]     Train net output #0: loss = 0.011437 (* 1 = 0.011437 loss)
I0408 12:33:07.419657 10695 sgd_solver.cpp:105] Iteration 2400, lr = 0.00851008
I0408 12:33:16.633752 10695 solver.cpp:331] Iteration 2500, Testing net (#0)
I0408 12:33:23.933053 10703 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:33:24.243006 10695 solver.cpp:398]     Test net output #0: accuracy = 0.9853
I0408 12:33:24.243029 10695 solver.cpp:398]     Test net output #1: loss = 0.0473837 (* 1 = 0.0473837 loss)
I0408 12:33:24.348743 10695 solver.cpp:219] Iteration 2500 (5.907 iter/s, 16.9291s/100 iters), loss = 0.0328979
I0408 12:33:24.348791 10695 solver.cpp:238]     Train net output #0: loss = 0.0328977 (* 1 = 0.0328977 loss)
I0408 12:33:24.348798 10695 sgd_solver.cpp:105] Iteration 2500, lr = 0.00845897
I0408 12:33:33.685261 10695 solver.cpp:219] Iteration 2600 (10.7107 iter/s, 9.33644s/100 iters), loss = 0.0345466
I0408 12:33:33.685288 10695 solver.cpp:238]     Train net output #0: loss = 0.0345464 (* 1 = 0.0345464 loss)
I0408 12:33:33.685293 10695 sgd_solver.cpp:105] Iteration 2600, lr = 0.00840857
I0408 12:33:43.028512 10695 solver.cpp:219] Iteration 2700 (10.703 iter/s, 9.3432s/100 iters), loss = 0.055628
I0408 12:33:43.028558 10695 solver.cpp:238]     Train net output #0: loss = 0.0556279 (* 1 = 0.0556279 loss)
I0408 12:33:43.028564 10695 sgd_solver.cpp:105] Iteration 2700, lr = 0.00835886
I0408 12:33:52.348894 10695 solver.cpp:219] Iteration 2800 (10.7293 iter/s, 9.32031s/100 iters), loss = 0.00172927
I0408 12:33:52.348923 10695 solver.cpp:238]     Train net output #0: loss = 0.00172909 (* 1 = 0.00172909 loss)
I0408 12:33:52.348929 10695 sgd_solver.cpp:105] Iteration 2800, lr = 0.00830984
I0408 12:33:53.111927 10702 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:34:01.713472 10695 solver.cpp:219] Iteration 2900 (10.6786 iter/s, 9.36452s/100 iters), loss = 0.0206835
I0408 12:34:01.713635 10695 solver.cpp:238]     Train net output #0: loss = 0.0206833 (* 1 = 0.0206833 loss)
I0408 12:34:01.713661 10695 sgd_solver.cpp:105] Iteration 2900, lr = 0.00826148
I0408 12:34:10.946905 10695 solver.cpp:331] Iteration 3000, Testing net (#0)
I0408 12:34:18.231233 10703 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:34:18.541718 10695 solver.cpp:398]     Test net output #0: accuracy = 0.9873
I0408 12:34:18.541769 10695 solver.cpp:398]     Test net output #1: loss = 0.0368046 (* 1 = 0.0368046 loss)
I0408 12:34:18.647552 10695 solver.cpp:219] Iteration 3000 (5.90532 iter/s, 16.9339s/100 iters), loss = 0.0128371
I0408 12:34:18.647598 10695 solver.cpp:238]     Train net output #0: loss = 0.0128369 (* 1 = 0.0128369 loss)
I0408 12:34:18.647604 10695 sgd_solver.cpp:105] Iteration 3000, lr = 0.00821377
I0408 12:34:28.006228 10695 solver.cpp:219] Iteration 3100 (10.6854 iter/s, 9.35861s/100 iters), loss = 0.0201972
I0408 12:34:28.006256 10695 solver.cpp:238]     Train net output #0: loss = 0.020197 (* 1 = 0.020197 loss)
I0408 12:34:28.006261 10695 sgd_solver.cpp:105] Iteration 3100, lr = 0.0081667
I0408 12:34:37.357029 10695 solver.cpp:219] Iteration 3200 (10.6943 iter/s, 9.35075s/100 iters), loss = 0.00795816
I0408 12:34:37.357091 10695 solver.cpp:238]     Train net output #0: loss = 0.00795799 (* 1 = 0.00795799 loss)
I0408 12:34:37.357100 10695 sgd_solver.cpp:105] Iteration 3200, lr = 0.00812025
I0408 12:34:46.704805 10695 solver.cpp:219] Iteration 3300 (10.6978 iter/s, 9.34769s/100 iters), loss = 0.0178811
I0408 12:34:46.704834 10695 solver.cpp:238]     Train net output #0: loss = 0.017881 (* 1 = 0.017881 loss)
I0408 12:34:46.704839 10695 sgd_solver.cpp:105] Iteration 3300, lr = 0.00807442
I0408 12:34:56.059697 10695 solver.cpp:219] Iteration 3400 (10.6897 iter/s, 9.35484s/100 iters), loss = 0.0106612
I0408 12:34:56.059725 10695 solver.cpp:238]     Train net output #0: loss = 0.0106611 (* 1 = 0.0106611 loss)
I0408 12:34:56.059731 10695 sgd_solver.cpp:105] Iteration 3400, lr = 0.00802918
I0408 12:35:05.277346 10695 solver.cpp:331] Iteration 3500, Testing net (#0)
I0408 12:35:12.569614 10703 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:35:12.885597 10695 solver.cpp:398]     Test net output #0: accuracy = 0.9881
I0408 12:35:12.885638 10695 solver.cpp:398]     Test net output #1: loss = 0.0374568 (* 1 = 0.0374568 loss)
I0408 12:35:12.991387 10695 solver.cpp:219] Iteration 3500 (5.90611 iter/s, 16.9316s/100 iters), loss = 0.00313423
I0408 12:35:12.991439 10695 solver.cpp:238]     Train net output #0: loss = 0.00313408 (* 1 = 0.00313408 loss)
I0408 12:35:12.991446 10695 sgd_solver.cpp:105] Iteration 3500, lr = 0.00798454
I0408 12:35:22.339720 10695 solver.cpp:219] Iteration 3600 (10.6972 iter/s, 9.34826s/100 iters), loss = 0.0248059
I0408 12:35:22.339747 10695 solver.cpp:238]     Train net output #0: loss = 0.0248057 (* 1 = 0.0248057 loss)
I0408 12:35:22.339752 10695 sgd_solver.cpp:105] Iteration 3600, lr = 0.00794046
I0408 12:35:31.676867 10695 solver.cpp:219] Iteration 3700 (10.71 iter/s, 9.3371s/100 iters), loss = 0.0229077
I0408 12:35:31.676894 10695 solver.cpp:238]     Train net output #0: loss = 0.0229075 (* 1 = 0.0229075 loss)
I0408 12:35:31.676899 10695 sgd_solver.cpp:105] Iteration 3700, lr = 0.00789695
I0408 12:35:35.886510 10702 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:35:41.296099 10695 solver.cpp:219] Iteration 3800 (10.3959 iter/s, 9.61918s/100 iters), loss = 0.0102298
I0408 12:35:41.296128 10695 solver.cpp:238]     Train net output #0: loss = 0.0102296 (* 1 = 0.0102296 loss)
I0408 12:35:41.296147 10695 sgd_solver.cpp:105] Iteration 3800, lr = 0.007854
I0408 12:35:50.967447 10695 solver.cpp:219] Iteration 3900 (10.3399 iter/s, 9.6713s/100 iters), loss = 0.0197716
I0408 12:35:50.967561 10695 solver.cpp:238]     Train net output #0: loss = 0.0197714 (* 1 = 0.0197714 loss)
I0408 12:35:50.967566 10695 sgd_solver.cpp:105] Iteration 3900, lr = 0.00781158
I0408 12:36:00.338539 10695 solver.cpp:331] Iteration 4000, Testing net (#0)
I0408 12:36:07.652410 10703 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:36:07.967900 10695 solver.cpp:398]     Test net output #0: accuracy = 0.9893
I0408 12:36:07.967941 10695 solver.cpp:398]     Test net output #1: loss = 0.0310883 (* 1 = 0.0310883 loss)
I0408 12:36:08.076094 10695 solver.cpp:219] Iteration 4000 (5.84505 iter/s, 17.1085s/100 iters), loss = 0.0162308
I0408 12:36:08.076140 10695 solver.cpp:238]     Train net output #0: loss = 0.0162306 (* 1 = 0.0162306 loss)
I0408 12:36:08.076146 10695 sgd_solver.cpp:105] Iteration 4000, lr = 0.0077697
I0408 12:36:17.476156 10695 solver.cpp:219] Iteration 4100 (10.6383 iter/s, 9.4s/100 iters), loss = 0.0178611
I0408 12:36:17.476202 10695 solver.cpp:238]     Train net output #0: loss = 0.017861 (* 1 = 0.017861 loss)
I0408 12:36:17.476207 10695 sgd_solver.cpp:105] Iteration 4100, lr = 0.00772833
I0408 12:36:26.881963 10695 solver.cpp:219] Iteration 4200 (10.6318 iter/s, 9.40576s/100 iters), loss = 0.00846373
I0408 12:36:26.882169 10695 solver.cpp:238]     Train net output #0: loss = 0.00846356 (* 1 = 0.00846356 loss)
I0408 12:36:26.882176 10695 sgd_solver.cpp:105] Iteration 4200, lr = 0.00768748
I0408 12:36:36.312214 10695 solver.cpp:219] Iteration 4300 (10.6044 iter/s, 9.43003s/100 iters), loss = 0.0326165
I0408 12:36:36.312265 10695 solver.cpp:238]     Train net output #0: loss = 0.0326163 (* 1 = 0.0326163 loss)
I0408 12:36:36.312271 10695 sgd_solver.cpp:105] Iteration 4300, lr = 0.00764712
I0408 12:36:45.797369 10695 solver.cpp:219] Iteration 4400 (10.5429 iter/s, 9.48508s/100 iters), loss = 0.0209363
I0408 12:36:45.797415 10695 solver.cpp:238]     Train net output #0: loss = 0.0209362 (* 1 = 0.0209362 loss)
I0408 12:36:45.797420 10695 sgd_solver.cpp:105] Iteration 4400, lr = 0.00760726
I0408 12:36:55.094346 10695 solver.cpp:331] Iteration 4500, Testing net (#0)
I0408 12:37:02.432299 10703 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:37:02.742692 10695 solver.cpp:398]     Test net output #0: accuracy = 0.989
I0408 12:37:02.742735 10695 solver.cpp:398]     Test net output #1: loss = 0.0353394 (* 1 = 0.0353394 loss)
I0408 12:37:02.848825 10695 solver.cpp:219] Iteration 4500 (5.86463 iter/s, 17.0514s/100 iters), loss = 0.00503872
I0408 12:37:02.848878 10695 solver.cpp:238]     Train net output #0: loss = 0.00503857 (* 1 = 0.00503857 loss)
I0408 12:37:02.848886 10695 sgd_solver.cpp:105] Iteration 4500, lr = 0.00756788
I0408 12:37:12.219310 10695 solver.cpp:219] Iteration 4600 (10.6719 iter/s, 9.37041s/100 iters), loss = 0.016537
I0408 12:37:12.219357 10695 solver.cpp:238]     Train net output #0: loss = 0.0165368 (* 1 = 0.0165368 loss)
I0408 12:37:12.219362 10695 sgd_solver.cpp:105] Iteration 4600, lr = 0.00752897
I0408 12:37:20.031260 10702 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:37:21.758289 10695 solver.cpp:219] Iteration 4700 (10.4834 iter/s, 9.53891s/100 iters), loss = 0.00481806
I0408 12:37:21.758342 10695 solver.cpp:238]     Train net output #0: loss = 0.00481793 (* 1 = 0.00481793 loss)
I0408 12:37:21.758360 10695 sgd_solver.cpp:105] Iteration 4700, lr = 0.00749052
I0408 12:37:31.179493 10695 solver.cpp:219] Iteration 4800 (10.6144 iter/s, 9.42113s/100 iters), loss = 0.0101501
I0408 12:37:31.179536 10695 solver.cpp:238]     Train net output #0: loss = 0.01015 (* 1 = 0.01015 loss)
I0408 12:37:31.179541 10695 sgd_solver.cpp:105] Iteration 4800, lr = 0.00745253
I0408 12:37:40.577249 10695 solver.cpp:219] Iteration 4900 (10.6409 iter/s, 9.3977s/100 iters), loss = 0.00380422
I0408 12:37:40.577458 10695 solver.cpp:238]     Train net output #0: loss = 0.00380409 (* 1 = 0.00380409 loss)
I0408 12:37:40.577478 10695 sgd_solver.cpp:105] Iteration 4900, lr = 0.00741498
I0408 12:37:49.831017 10695 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_5000.caffemodel
I0408 12:37:49.892534 10695 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_5000.solverstate
I0408 12:37:49.895535 10695 solver.cpp:331] Iteration 5000, Testing net (#0)
I0408 12:37:57.183908 10703 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:37:57.500728 10695 solver.cpp:398]     Test net output #0: accuracy = 0.9898
I0408 12:37:57.500771 10695 solver.cpp:398]     Test net output #1: loss = 0.0313782 (* 1 = 0.0313782 loss)
I0408 12:37:57.605654 10695 solver.cpp:219] Iteration 5000 (5.87262 iter/s, 17.0282s/100 iters), loss = 0.0604403
I0408 12:37:57.605703 10695 solver.cpp:238]     Train net output #0: loss = 0.0604402 (* 1 = 0.0604402 loss)
I0408 12:37:57.605722 10695 sgd_solver.cpp:105] Iteration 5000, lr = 0.00737788
I0408 12:38:07.009043 10695 solver.cpp:219] Iteration 5100 (10.6345 iter/s, 9.40332s/100 iters), loss = 0.0155165
I0408 12:38:07.009073 10695 solver.cpp:238]     Train net output #0: loss = 0.0155163 (* 1 = 0.0155163 loss)
I0408 12:38:07.009078 10695 sgd_solver.cpp:105] Iteration 5100, lr = 0.0073412
I0408 12:38:16.407146 10695 solver.cpp:219] Iteration 5200 (10.6405 iter/s, 9.39806s/100 iters), loss = 0.00674321
I0408 12:38:16.407313 10695 solver.cpp:238]     Train net output #0: loss = 0.00674306 (* 1 = 0.00674306 loss)
I0408 12:38:16.407337 10695 sgd_solver.cpp:105] Iteration 5200, lr = 0.00730495
I0408 12:38:25.884860 10695 solver.cpp:219] Iteration 5300 (10.5513 iter/s, 9.47753s/100 iters), loss = 0.00194496
I0408 12:38:25.884896 10695 solver.cpp:238]     Train net output #0: loss = 0.0019448 (* 1 = 0.0019448 loss)
I0408 12:38:25.884904 10695 sgd_solver.cpp:105] Iteration 5300, lr = 0.00726911
I0408 12:38:35.429136 10695 solver.cpp:219] Iteration 5400 (10.4775 iter/s, 9.54423s/100 iters), loss = 0.0070753
I0408 12:38:35.429164 10695 solver.cpp:238]     Train net output #0: loss = 0.00707512 (* 1 = 0.00707512 loss)
I0408 12:38:35.429170 10695 sgd_solver.cpp:105] Iteration 5400, lr = 0.00723368
I0408 12:38:44.689612 10695 solver.cpp:331] Iteration 5500, Testing net (#0)
I0408 12:38:52.383321 10703 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:38:52.701257 10695 solver.cpp:398]     Test net output #0: accuracy = 0.9883
I0408 12:38:52.701297 10695 solver.cpp:398]     Test net output #1: loss = 0.0339872 (* 1 = 0.0339872 loss)
I0408 12:38:52.807958 10695 solver.cpp:219] Iteration 5500 (5.75415 iter/s, 17.3788s/100 iters), loss = 0.00932764
I0408 12:38:52.808003 10695 solver.cpp:238]     Train net output #0: loss = 0.00932746 (* 1 = 0.00932746 loss)
I0408 12:38:52.808023 10695 sgd_solver.cpp:105] Iteration 5500, lr = 0.00719865
I0408 12:39:02.459908 10695 solver.cpp:219] Iteration 5600 (10.3607 iter/s, 9.65188s/100 iters), loss = 0.000650298
I0408 12:39:02.459936 10695 solver.cpp:238]     Train net output #0: loss = 0.000650119 (* 1 = 0.000650119 loss)
I0408 12:39:02.459940 10695 sgd_solver.cpp:105] Iteration 5600, lr = 0.00716402
I0408 12:39:04.354735 10702 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:39:11.856626 10695 solver.cpp:219] Iteration 5700 (10.6421 iter/s, 9.39667s/100 iters), loss = 0.0044173
I0408 12:39:11.856655 10695 solver.cpp:238]     Train net output #0: loss = 0.00441712 (* 1 = 0.00441712 loss)
I0408 12:39:11.856660 10695 sgd_solver.cpp:105] Iteration 5700, lr = 0.00712977
I0408 12:39:21.249053 10695 solver.cpp:219] Iteration 5800 (10.6469 iter/s, 9.39238s/100 iters), loss = 0.0419367
I0408 12:39:21.249083 10695 solver.cpp:238]     Train net output #0: loss = 0.0419365 (* 1 = 0.0419365 loss)
I0408 12:39:21.249089 10695 sgd_solver.cpp:105] Iteration 5800, lr = 0.0070959
I0408 12:39:30.626895 10695 solver.cpp:219] Iteration 5900 (10.6635 iter/s, 9.37779s/100 iters), loss = 0.0118347
I0408 12:39:30.627044 10695 solver.cpp:238]     Train net output #0: loss = 0.0118345 (* 1 = 0.0118345 loss)
I0408 12:39:30.627050 10695 sgd_solver.cpp:105] Iteration 5900, lr = 0.0070624
I0408 12:39:39.891427 10695 solver.cpp:331] Iteration 6000, Testing net (#0)
I0408 12:39:47.200739 10703 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:39:47.519181 10695 solver.cpp:398]     Test net output #0: accuracy = 0.9905
I0408 12:39:47.519203 10695 solver.cpp:398]     Test net output #1: loss = 0.0298116 (* 1 = 0.0298116 loss)
I0408 12:39:47.624440 10695 solver.cpp:219] Iteration 6000 (5.88326 iter/s, 16.9974s/100 iters), loss = 0.00311418
I0408 12:39:47.624474 10695 solver.cpp:238]     Train net output #0: loss = 0.003114 (* 1 = 0.003114 loss)
I0408 12:39:47.624480 10695 sgd_solver.cpp:105] Iteration 6000, lr = 0.00702927
I0408 12:39:57.002396 10695 solver.cpp:219] Iteration 6100 (10.6634 iter/s, 9.3779s/100 iters), loss = 0.00273109
I0408 12:39:57.002470 10695 solver.cpp:238]     Train net output #0: loss = 0.00273092 (* 1 = 0.00273092 loss)
I0408 12:39:57.002475 10695 sgd_solver.cpp:105] Iteration 6100, lr = 0.0069965
I0408 12:40:06.419199 10695 solver.cpp:219] Iteration 6200 (10.6194 iter/s, 9.41672s/100 iters), loss = 0.0057857
I0408 12:40:06.419419 10695 solver.cpp:238]     Train net output #0: loss = 0.00578552 (* 1 = 0.00578552 loss)
I0408 12:40:06.419440 10695 sgd_solver.cpp:105] Iteration 6200, lr = 0.00696408
I0408 12:40:15.809039 10695 solver.cpp:219] Iteration 6300 (10.6501 iter/s, 9.38961s/100 iters), loss = 0.00743611
I0408 12:40:15.809067 10695 solver.cpp:238]     Train net output #0: loss = 0.00743592 (* 1 = 0.00743592 loss)
I0408 12:40:15.809072 10695 sgd_solver.cpp:105] Iteration 6300, lr = 0.00693201
I0408 12:40:25.205657 10695 solver.cpp:219] Iteration 6400 (10.6422 iter/s, 9.39658s/100 iters), loss = 0.00636441
I0408 12:40:25.205703 10695 solver.cpp:238]     Train net output #0: loss = 0.00636421 (* 1 = 0.00636421 loss)
I0408 12:40:25.205708 10695 sgd_solver.cpp:105] Iteration 6400, lr = 0.00690029
I0408 12:40:34.530743 10695 solver.cpp:331] Iteration 6500, Testing net (#0)
I0408 12:40:42.047365 10703 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:40:42.391201 10695 solver.cpp:398]     Test net output #0: accuracy = 0.9899
I0408 12:40:42.391245 10695 solver.cpp:398]     Test net output #1: loss = 0.0326058 (* 1 = 0.0326058 loss)
I0408 12:40:42.500473 10695 solver.cpp:219] Iteration 6500 (5.7821 iter/s, 17.2947s/100 iters), loss = 0.00811405
I0408 12:40:42.500521 10695 solver.cpp:238]     Train net output #0: loss = 0.00811384 (* 1 = 0.00811384 loss)
I0408 12:40:42.500545 10695 sgd_solver.cpp:105] Iteration 6500, lr = 0.0068689
I0408 12:40:48.060974 10702 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:40:52.057014 10695 solver.cpp:219] Iteration 6600 (10.4641 iter/s, 9.55647s/100 iters), loss = 0.0325381
I0408 12:40:52.057062 10695 solver.cpp:238]     Train net output #0: loss = 0.0325379 (* 1 = 0.0325379 loss)
I0408 12:40:52.057068 10695 sgd_solver.cpp:105] Iteration 6600, lr = 0.00683784
I0408 12:41:01.460345 10695 solver.cpp:219] Iteration 6700 (10.6346 iter/s, 9.40327s/100 iters), loss = 0.00885387
I0408 12:41:01.460391 10695 solver.cpp:238]     Train net output #0: loss = 0.00885365 (* 1 = 0.00885365 loss)
I0408 12:41:01.460397 10695 sgd_solver.cpp:105] Iteration 6700, lr = 0.00680711
I0408 12:41:10.851641 10695 solver.cpp:219] Iteration 6800 (10.6482 iter/s, 9.39123s/100 iters), loss = 0.00137469
I0408 12:41:10.851670 10695 solver.cpp:238]     Train net output #0: loss = 0.00137448 (* 1 = 0.00137448 loss)
I0408 12:41:10.851675 10695 sgd_solver.cpp:105] Iteration 6800, lr = 0.0067767
I0408 12:41:20.243613 10695 solver.cpp:219] Iteration 6900 (10.6474 iter/s, 9.39193s/100 iters), loss = 0.0058655
I0408 12:41:20.243849 10695 solver.cpp:238]     Train net output #0: loss = 0.00586529 (* 1 = 0.00586529 loss)
I0408 12:41:20.243855 10695 sgd_solver.cpp:105] Iteration 6900, lr = 0.0067466
I0408 12:41:29.506220 10695 solver.cpp:331] Iteration 7000, Testing net (#0)
I0408 12:41:36.849236 10703 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:41:37.161516 10695 solver.cpp:398]     Test net output #0: accuracy = 0.9892
I0408 12:41:37.161558 10695 solver.cpp:398]     Test net output #1: loss = 0.0309603 (* 1 = 0.0309603 loss)
I0408 12:41:37.265192 10695 solver.cpp:219] Iteration 7000 (5.87498 iter/s, 17.0213s/100 iters), loss = 0.00554095
I0408 12:41:37.265245 10695 solver.cpp:238]     Train net output #0: loss = 0.00554073 (* 1 = 0.00554073 loss)
I0408 12:41:37.265265 10695 sgd_solver.cpp:105] Iteration 7000, lr = 0.00671681
I0408 12:41:46.640408 10695 solver.cpp:219] Iteration 7100 (10.6665 iter/s, 9.37515s/100 iters), loss = 0.0122144
I0408 12:41:46.640453 10695 solver.cpp:238]     Train net output #0: loss = 0.0122142 (* 1 = 0.0122142 loss)
I0408 12:41:46.640458 10695 sgd_solver.cpp:105] Iteration 7100, lr = 0.00668733
I0408 12:41:56.044000 10695 solver.cpp:219] Iteration 7200 (10.6343 iter/s, 9.40353s/100 iters), loss = 0.0040373
I0408 12:41:56.044214 10695 solver.cpp:238]     Train net output #0: loss = 0.00403707 (* 1 = 0.00403707 loss)
I0408 12:41:56.044220 10695 sgd_solver.cpp:105] Iteration 7200, lr = 0.00665815
I0408 12:42:05.445981 10695 solver.cpp:219] Iteration 7300 (10.6363 iter/s, 9.40177s/100 iters), loss = 0.0238147
I0408 12:42:05.446030 10695 solver.cpp:238]     Train net output #0: loss = 0.0238144 (* 1 = 0.0238144 loss)
I0408 12:42:05.446036 10695 sgd_solver.cpp:105] Iteration 7300, lr = 0.00662927
I0408 12:42:15.158429 10695 solver.cpp:219] Iteration 7400 (10.2961 iter/s, 9.71238s/100 iters), loss = 0.00441154
I0408 12:42:15.158476 10695 solver.cpp:238]     Train net output #0: loss = 0.0044113 (* 1 = 0.0044113 loss)
I0408 12:42:15.158483 10695 sgd_solver.cpp:105] Iteration 7400, lr = 0.00660067
I0408 12:42:24.224735 10702 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:42:24.570868 10695 solver.cpp:331] Iteration 7500, Testing net (#0)
I0408 12:42:31.961825 10703 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:42:32.277123 10695 solver.cpp:398]     Test net output #0: accuracy = 0.9903
I0408 12:42:32.277151 10695 solver.cpp:398]     Test net output #1: loss = 0.0320062 (* 1 = 0.0320062 loss)
I0408 12:42:32.380566 10695 solver.cpp:219] Iteration 7500 (5.8065 iter/s, 17.2221s/100 iters), loss = 0.00126554
I0408 12:42:32.380614 10695 solver.cpp:238]     Train net output #0: loss = 0.00126532 (* 1 = 0.00126532 loss)
I0408 12:42:32.380620 10695 sgd_solver.cpp:105] Iteration 7500, lr = 0.00657236
I0408 12:42:41.776043 10695 solver.cpp:219] Iteration 7600 (10.6435 iter/s, 9.39541s/100 iters), loss = 0.00772279
I0408 12:42:41.776072 10695 solver.cpp:238]     Train net output #0: loss = 0.00772257 (* 1 = 0.00772257 loss)
I0408 12:42:41.776077 10695 sgd_solver.cpp:105] Iteration 7600, lr = 0.00654433
I0408 12:42:51.291039 10695 solver.cpp:219] Iteration 7700 (10.5098 iter/s, 9.51495s/100 iters), loss = 0.024201
I0408 12:42:51.291087 10695 solver.cpp:238]     Train net output #0: loss = 0.0242008 (* 1 = 0.0242008 loss)
I0408 12:42:51.291093 10695 sgd_solver.cpp:105] Iteration 7700, lr = 0.00651658
I0408 12:43:00.727190 10695 solver.cpp:219] Iteration 7800 (10.5976 iter/s, 9.43609s/100 iters), loss = 0.00328953
I0408 12:43:00.727236 10695 solver.cpp:238]     Train net output #0: loss = 0.00328931 (* 1 = 0.00328931 loss)
I0408 12:43:00.727241 10695 sgd_solver.cpp:105] Iteration 7800, lr = 0.00648911
I0408 12:43:10.288678 10695 solver.cpp:219] Iteration 7900 (10.4587 iter/s, 9.56143s/100 iters), loss = 0.00246123
I0408 12:43:10.288869 10695 solver.cpp:238]     Train net output #0: loss = 0.00246102 (* 1 = 0.00246102 loss)
I0408 12:43:10.288877 10695 sgd_solver.cpp:105] Iteration 7900, lr = 0.0064619
I0408 12:43:19.579445 10695 solver.cpp:331] Iteration 8000, Testing net (#0)
I0408 12:43:27.201349 10703 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:43:27.519343 10695 solver.cpp:398]     Test net output #0: accuracy = 0.9901
I0408 12:43:27.519367 10695 solver.cpp:398]     Test net output #1: loss = 0.0308078 (* 1 = 0.0308078 loss)
I0408 12:43:27.628191 10695 solver.cpp:219] Iteration 8000 (5.76724 iter/s, 17.3393s/100 iters), loss = 0.00562554
I0408 12:43:27.628240 10695 solver.cpp:238]     Train net output #0: loss = 0.00562533 (* 1 = 0.00562533 loss)
I0408 12:43:27.628247 10695 sgd_solver.cpp:105] Iteration 8000, lr = 0.00643496
I0408 12:43:37.114572 10695 solver.cpp:219] Iteration 8100 (10.5424 iter/s, 9.48553s/100 iters), loss = 0.0117597
I0408 12:43:37.114617 10695 solver.cpp:238]     Train net output #0: loss = 0.0117595 (* 1 = 0.0117595 loss)
I0408 12:43:37.114622 10695 sgd_solver.cpp:105] Iteration 8100, lr = 0.00640827
I0408 12:43:46.531559 10695 solver.cpp:219] Iteration 8200 (10.6192 iter/s, 9.41692s/100 iters), loss = 0.00782479
I0408 12:43:46.531747 10695 solver.cpp:238]     Train net output #0: loss = 0.00782458 (* 1 = 0.00782458 loss)
I0408 12:43:46.531755 10695 sgd_solver.cpp:105] Iteration 8200, lr = 0.00638185
I0408 12:43:56.024529 10695 solver.cpp:219] Iteration 8300 (10.5343 iter/s, 9.49276s/100 iters), loss = 0.0238921
I0408 12:43:56.024569 10695 solver.cpp:238]     Train net output #0: loss = 0.0238919 (* 1 = 0.0238919 loss)
I0408 12:43:56.024575 10695 sgd_solver.cpp:105] Iteration 8300, lr = 0.00635567
I0408 12:44:05.594439 10695 solver.cpp:219] Iteration 8400 (10.4505 iter/s, 9.56892s/100 iters), loss = 0.00754677
I0408 12:44:05.594470 10695 solver.cpp:238]     Train net output #0: loss = 0.00754657 (* 1 = 0.00754657 loss)
I0408 12:44:05.594475 10695 sgd_solver.cpp:105] Iteration 8400, lr = 0.00632975
I0408 12:44:08.709794 10702 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:44:14.876241 10695 solver.cpp:331] Iteration 8500, Testing net (#0)
I0408 12:44:22.199219 10703 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:44:22.513820 10695 solver.cpp:398]     Test net output #0: accuracy = 0.9904
I0408 12:44:22.513845 10695 solver.cpp:398]     Test net output #1: loss = 0.029738 (* 1 = 0.029738 loss)
I0408 12:44:22.618649 10695 solver.cpp:219] Iteration 8500 (5.874 iter/s, 17.0242s/100 iters), loss = 0.00502261
I0408 12:44:22.618675 10695 solver.cpp:238]     Train net output #0: loss = 0.00502239 (* 1 = 0.00502239 loss)
I0408 12:44:22.618681 10695 sgd_solver.cpp:105] Iteration 8500, lr = 0.00630407
I0408 12:44:32.010136 10695 solver.cpp:219] Iteration 8600 (10.648 iter/s, 9.39145s/100 iters), loss = 0.00079975
I0408 12:44:32.010184 10695 solver.cpp:238]     Train net output #0: loss = 0.000799534 (* 1 = 0.000799534 loss)
I0408 12:44:32.010188 10695 sgd_solver.cpp:105] Iteration 8600, lr = 0.00627864
I0408 12:44:41.402556 10695 solver.cpp:219] Iteration 8700 (10.647 iter/s, 9.39235s/100 iters), loss = 0.00285678
I0408 12:44:41.402621 10695 solver.cpp:238]     Train net output #0: loss = 0.00285656 (* 1 = 0.00285656 loss)
I0408 12:44:41.402642 10695 sgd_solver.cpp:105] Iteration 8700, lr = 0.00625344
I0408 12:44:50.809530 10695 solver.cpp:219] Iteration 8800 (10.6309 iter/s, 9.40656s/100 iters), loss = 0.000952589
I0408 12:44:50.809574 10695 solver.cpp:238]     Train net output #0: loss = 0.000952372 (* 1 = 0.000952372 loss)
I0408 12:44:50.809592 10695 sgd_solver.cpp:105] Iteration 8800, lr = 0.00622847
I0408 12:45:00.227175 10695 solver.cpp:219] Iteration 8900 (10.6184 iter/s, 9.41758s/100 iters), loss = 0.000747634
I0408 12:45:00.227332 10695 solver.cpp:238]     Train net output #0: loss = 0.000747425 (* 1 = 0.000747425 loss)
I0408 12:45:00.227339 10695 sgd_solver.cpp:105] Iteration 8900, lr = 0.00620374
I0408 12:45:09.508949 10695 solver.cpp:331] Iteration 9000, Testing net (#0)
I0408 12:45:16.825100 10703 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:45:17.135134 10695 solver.cpp:398]     Test net output #0: accuracy = 0.9895
I0408 12:45:17.135174 10695 solver.cpp:398]     Test net output #1: loss = 0.0304828 (* 1 = 0.0304828 loss)
I0408 12:45:17.242774 10695 solver.cpp:219] Iteration 9000 (5.87702 iter/s, 17.0154s/100 iters), loss = 0.0179535
I0408 12:45:17.242820 10695 solver.cpp:238]     Train net output #0: loss = 0.0179533 (* 1 = 0.0179533 loss)
I0408 12:45:17.242826 10695 sgd_solver.cpp:105] Iteration 9000, lr = 0.00617924
I0408 12:45:26.657424 10695 solver.cpp:219] Iteration 9100 (10.6218 iter/s, 9.41459s/100 iters), loss = 0.00782704
I0408 12:45:26.657471 10695 solver.cpp:238]     Train net output #0: loss = 0.00782682 (* 1 = 0.00782682 loss)
I0408 12:45:26.657476 10695 sgd_solver.cpp:105] Iteration 9100, lr = 0.00615496
I0408 12:45:36.078794 10695 solver.cpp:219] Iteration 9200 (10.6142 iter/s, 9.42131s/100 iters), loss = 0.00263843
I0408 12:45:36.078922 10695 solver.cpp:238]     Train net output #0: loss = 0.00263822 (* 1 = 0.00263822 loss)
I0408 12:45:36.078928 10695 sgd_solver.cpp:105] Iteration 9200, lr = 0.0061309
I0408 12:45:45.481750 10695 solver.cpp:219] Iteration 9300 (10.6351 iter/s, 9.40281s/100 iters), loss = 0.00406113
I0408 12:45:45.481782 10695 solver.cpp:238]     Train net output #0: loss = 0.00406092 (* 1 = 0.00406092 loss)
I0408 12:45:45.481788 10695 sgd_solver.cpp:105] Iteration 9300, lr = 0.00610706
I0408 12:45:52.064421 10702 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:45:54.891332 10695 solver.cpp:219] Iteration 9400 (10.6275 iter/s, 9.40952s/100 iters), loss = 0.0190097
I0408 12:45:54.891361 10695 solver.cpp:238]     Train net output #0: loss = 0.0190095 (* 1 = 0.0190095 loss)
I0408 12:45:54.891368 10695 sgd_solver.cpp:105] Iteration 9400, lr = 0.00608343
I0408 12:46:04.167675 10695 solver.cpp:331] Iteration 9500, Testing net (#0)
I0408 12:46:11.495103 10703 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:46:11.813226 10695 solver.cpp:398]     Test net output #0: accuracy = 0.9884
I0408 12:46:11.813264 10695 solver.cpp:398]     Test net output #1: loss = 0.0357901 (* 1 = 0.0357901 loss)
I0408 12:46:11.918941 10695 solver.cpp:219] Iteration 9500 (5.87283 iter/s, 17.0276s/100 iters), loss = 0.00394151
I0408 12:46:11.918987 10695 solver.cpp:238]     Train net output #0: loss = 0.00394128 (* 1 = 0.00394128 loss)
I0408 12:46:11.918994 10695 sgd_solver.cpp:105] Iteration 9500, lr = 0.00606002
I0408 12:46:21.327577 10695 solver.cpp:219] Iteration 9600 (10.6286 iter/s, 9.40857s/100 iters), loss = 0.0029237
I0408 12:46:21.327625 10695 solver.cpp:238]     Train net output #0: loss = 0.00292348 (* 1 = 0.00292348 loss)
I0408 12:46:21.327630 10695 sgd_solver.cpp:105] Iteration 9600, lr = 0.00603682
I0408 12:46:30.707293 10695 solver.cpp:219] Iteration 9700 (10.6614 iter/s, 9.37965s/100 iters), loss = 0.00213113
I0408 12:46:30.707340 10695 solver.cpp:238]     Train net output #0: loss = 0.0021309 (* 1 = 0.0021309 loss)
I0408 12:46:30.707357 10695 sgd_solver.cpp:105] Iteration 9700, lr = 0.00601382
I0408 12:46:40.100903 10695 solver.cpp:219] Iteration 9800 (10.6456 iter/s, 9.39355s/100 iters), loss = 0.0106163
I0408 12:46:40.100949 10695 solver.cpp:238]     Train net output #0: loss = 0.0106161 (* 1 = 0.0106161 loss)
I0408 12:46:40.100953 10695 sgd_solver.cpp:105] Iteration 9800, lr = 0.00599102
I0408 12:46:49.579682 10695 solver.cpp:219] Iteration 9900 (10.5499 iter/s, 9.47872s/100 iters), loss = 0.00641821
I0408 12:46:49.579944 10695 solver.cpp:238]     Train net output #0: loss = 0.00641798 (* 1 = 0.00641798 loss)
I0408 12:46:49.579957 10695 sgd_solver.cpp:105] Iteration 9900, lr = 0.00596843
I0408 12:46:59.774206 10695 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_10000.caffemodel
I0408 12:46:59.837203 10695 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_10000.solverstate
I0408 12:46:59.908509 10695 solver.cpp:311] Iteration 10000, loss = 0.00211503
I0408 12:46:59.908530 10695 solver.cpp:331] Iteration 10000, Testing net (#0)
I0408 12:47:07.357278 10703 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:47:07.682463 10695 solver.cpp:398]     Test net output #0: accuracy = 0.9912
I0408 12:47:07.682495 10695 solver.cpp:398]     Test net output #1: loss = 0.0278266 (* 1 = 0.0278266 loss)
I0408 12:47:07.682502 10695 solver.cpp:316] Optimization Done.
I0408 12:47:07.682504 10695 caffe.cpp:259] Optimization Done.
