I0408 12:47:07.763731 10854 caffe.cpp:218] Using GPUs 0
I0408 12:47:07.780282 10854 caffe.cpp:223] GPU 0: GeForce GTX 950M
I0408 12:47:07.972743 10854 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 10000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_fc_size/lenet_train_test_fcl3_fcn100.prototxt"
train_state {
  level: 0
  stage: ""
}
I0408 12:47:07.972923 10854 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_fc_size/lenet_train_test_fcl3_fcn100.prototxt
I0408 12:47:07.973137 10854 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0408 12:47:07.973147 10854 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0408 12:47:07.973248 10854 net.cpp:51] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0408 12:47:07.973333 10854 layer_factory.hpp:77] Creating layer mnist
I0408 12:47:07.973423 10854 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0408 12:47:07.973443 10854 net.cpp:84] Creating Layer mnist
I0408 12:47:07.973449 10854 net.cpp:380] mnist -> data
I0408 12:47:07.973495 10854 net.cpp:380] mnist -> label
I0408 12:47:07.974014 10854 data_layer.cpp:45] output data size: 64,1,28,28
I0408 12:47:07.975298 10854 net.cpp:122] Setting up mnist
I0408 12:47:07.975308 10854 net.cpp:129] Top shape: 64 1 28 28 (50176)
I0408 12:47:07.975311 10854 net.cpp:129] Top shape: 64 (64)
I0408 12:47:07.975313 10854 net.cpp:137] Memory required for data: 200960
I0408 12:47:07.975318 10854 layer_factory.hpp:77] Creating layer conv0
I0408 12:47:07.975370 10854 net.cpp:84] Creating Layer conv0
I0408 12:47:07.975376 10854 net.cpp:406] conv0 <- data
I0408 12:47:07.975400 10854 net.cpp:380] conv0 -> conv0
I0408 12:47:07.976729 10854 net.cpp:122] Setting up conv0
I0408 12:47:07.976739 10854 net.cpp:129] Top shape: 64 50 24 24 (1843200)
I0408 12:47:07.976742 10854 net.cpp:137] Memory required for data: 7573760
I0408 12:47:07.976783 10854 layer_factory.hpp:77] Creating layer pool0
I0408 12:47:07.976793 10854 net.cpp:84] Creating Layer pool0
I0408 12:47:07.976810 10854 net.cpp:406] pool0 <- conv0
I0408 12:47:07.976814 10854 net.cpp:380] pool0 -> pool0
I0408 12:47:07.976882 10854 net.cpp:122] Setting up pool0
I0408 12:47:07.976886 10854 net.cpp:129] Top shape: 64 50 12 12 (460800)
I0408 12:47:07.976908 10854 net.cpp:137] Memory required for data: 9416960
I0408 12:47:07.976909 10854 layer_factory.hpp:77] Creating layer conv1
I0408 12:47:07.976917 10854 net.cpp:84] Creating Layer conv1
I0408 12:47:07.976918 10854 net.cpp:406] conv1 <- pool0
I0408 12:47:07.976922 10854 net.cpp:380] conv1 -> conv1
I0408 12:47:07.977680 10854 net.cpp:122] Setting up conv1
I0408 12:47:07.977689 10854 net.cpp:129] Top shape: 64 50 8 8 (204800)
I0408 12:47:07.977710 10854 net.cpp:137] Memory required for data: 10236160
I0408 12:47:07.977716 10854 layer_factory.hpp:77] Creating layer pool1
I0408 12:47:07.977720 10854 net.cpp:84] Creating Layer pool1
I0408 12:47:07.977723 10854 net.cpp:406] pool1 <- conv1
I0408 12:47:07.977726 10854 net.cpp:380] pool1 -> pool1
I0408 12:47:07.977784 10854 net.cpp:122] Setting up pool1
I0408 12:47:07.977805 10854 net.cpp:129] Top shape: 64 50 4 4 (51200)
I0408 12:47:07.977807 10854 net.cpp:137] Memory required for data: 10440960
I0408 12:47:07.977809 10854 layer_factory.hpp:77] Creating layer ip1
I0408 12:47:07.977814 10854 net.cpp:84] Creating Layer ip1
I0408 12:47:07.977816 10854 net.cpp:406] ip1 <- pool1
I0408 12:47:07.977819 10854 net.cpp:380] ip1 -> ip1
I0408 12:47:07.978925 10854 net.cpp:122] Setting up ip1
I0408 12:47:07.978934 10854 net.cpp:129] Top shape: 64 100 (6400)
I0408 12:47:07.978950 10854 net.cpp:137] Memory required for data: 10466560
I0408 12:47:07.978955 10854 layer_factory.hpp:77] Creating layer relu1
I0408 12:47:07.978960 10854 net.cpp:84] Creating Layer relu1
I0408 12:47:07.978962 10854 net.cpp:406] relu1 <- ip1
I0408 12:47:07.978967 10854 net.cpp:367] relu1 -> ip1 (in-place)
I0408 12:47:07.978991 10854 net.cpp:122] Setting up relu1
I0408 12:47:07.978996 10854 net.cpp:129] Top shape: 64 100 (6400)
I0408 12:47:07.978997 10854 net.cpp:137] Memory required for data: 10492160
I0408 12:47:07.978999 10854 layer_factory.hpp:77] Creating layer ip2
I0408 12:47:07.979017 10854 net.cpp:84] Creating Layer ip2
I0408 12:47:07.979018 10854 net.cpp:406] ip2 <- ip1
I0408 12:47:07.979024 10854 net.cpp:380] ip2 -> ip2
I0408 12:47:07.979555 10854 net.cpp:122] Setting up ip2
I0408 12:47:07.979562 10854 net.cpp:129] Top shape: 64 100 (6400)
I0408 12:47:07.979564 10854 net.cpp:137] Memory required for data: 10517760
I0408 12:47:07.979569 10854 layer_factory.hpp:77] Creating layer relu2
I0408 12:47:07.979575 10854 net.cpp:84] Creating Layer relu2
I0408 12:47:07.979578 10854 net.cpp:406] relu2 <- ip2
I0408 12:47:07.979581 10854 net.cpp:367] relu2 -> ip2 (in-place)
I0408 12:47:07.979588 10854 net.cpp:122] Setting up relu2
I0408 12:47:07.979590 10854 net.cpp:129] Top shape: 64 100 (6400)
I0408 12:47:07.979593 10854 net.cpp:137] Memory required for data: 10543360
I0408 12:47:07.979594 10854 layer_factory.hpp:77] Creating layer ip3
I0408 12:47:07.979612 10854 net.cpp:84] Creating Layer ip3
I0408 12:47:07.979614 10854 net.cpp:406] ip3 <- ip2
I0408 12:47:07.979617 10854 net.cpp:380] ip3 -> ip3
I0408 12:47:07.979708 10854 net.cpp:122] Setting up ip3
I0408 12:47:07.979713 10854 net.cpp:129] Top shape: 64 10 (640)
I0408 12:47:07.979715 10854 net.cpp:137] Memory required for data: 10545920
I0408 12:47:07.979722 10854 layer_factory.hpp:77] Creating layer relu3
I0408 12:47:07.979727 10854 net.cpp:84] Creating Layer relu3
I0408 12:47:07.979728 10854 net.cpp:406] relu3 <- ip3
I0408 12:47:07.979773 10854 net.cpp:367] relu3 -> ip3 (in-place)
I0408 12:47:07.979780 10854 net.cpp:122] Setting up relu3
I0408 12:47:07.979785 10854 net.cpp:129] Top shape: 64 10 (640)
I0408 12:47:07.979790 10854 net.cpp:137] Memory required for data: 10548480
I0408 12:47:07.979792 10854 layer_factory.hpp:77] Creating layer loss
I0408 12:47:07.979797 10854 net.cpp:84] Creating Layer loss
I0408 12:47:07.979801 10854 net.cpp:406] loss <- ip3
I0408 12:47:07.979807 10854 net.cpp:406] loss <- label
I0408 12:47:07.979815 10854 net.cpp:380] loss -> loss
I0408 12:47:07.979826 10854 layer_factory.hpp:77] Creating layer loss
I0408 12:47:07.979907 10854 net.cpp:122] Setting up loss
I0408 12:47:07.979912 10854 net.cpp:129] Top shape: (1)
I0408 12:47:07.979917 10854 net.cpp:132]     with loss weight 1
I0408 12:47:07.979930 10854 net.cpp:137] Memory required for data: 10548484
I0408 12:47:07.979933 10854 net.cpp:198] loss needs backward computation.
I0408 12:47:07.979941 10854 net.cpp:198] relu3 needs backward computation.
I0408 12:47:07.979944 10854 net.cpp:198] ip3 needs backward computation.
I0408 12:47:07.979948 10854 net.cpp:198] relu2 needs backward computation.
I0408 12:47:07.979951 10854 net.cpp:198] ip2 needs backward computation.
I0408 12:47:07.979956 10854 net.cpp:198] relu1 needs backward computation.
I0408 12:47:07.979959 10854 net.cpp:198] ip1 needs backward computation.
I0408 12:47:07.979961 10854 net.cpp:198] pool1 needs backward computation.
I0408 12:47:07.979964 10854 net.cpp:198] conv1 needs backward computation.
I0408 12:47:07.979969 10854 net.cpp:198] pool0 needs backward computation.
I0408 12:47:07.979972 10854 net.cpp:198] conv0 needs backward computation.
I0408 12:47:07.979991 10854 net.cpp:200] mnist does not need backward computation.
I0408 12:47:07.979995 10854 net.cpp:242] This network produces output loss
I0408 12:47:07.980007 10854 net.cpp:255] Network initialization done.
I0408 12:47:07.980161 10854 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_fc_size/lenet_train_test_fcl3_fcn100.prototxt
I0408 12:47:07.980180 10854 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0408 12:47:07.980254 10854 net.cpp:51] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0408 12:47:07.980363 10854 layer_factory.hpp:77] Creating layer mnist
I0408 12:47:07.980427 10854 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0408 12:47:07.980438 10854 net.cpp:84] Creating Layer mnist
I0408 12:47:07.980444 10854 net.cpp:380] mnist -> data
I0408 12:47:07.980450 10854 net.cpp:380] mnist -> label
I0408 12:47:07.980614 10854 data_layer.cpp:45] output data size: 100,1,28,28
I0408 12:47:07.981860 10854 net.cpp:122] Setting up mnist
I0408 12:47:07.981871 10854 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0408 12:47:07.981878 10854 net.cpp:129] Top shape: 100 (100)
I0408 12:47:07.981894 10854 net.cpp:137] Memory required for data: 314000
I0408 12:47:07.981899 10854 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0408 12:47:07.981907 10854 net.cpp:84] Creating Layer label_mnist_1_split
I0408 12:47:07.981911 10854 net.cpp:406] label_mnist_1_split <- label
I0408 12:47:07.981920 10854 net.cpp:380] label_mnist_1_split -> label_mnist_1_split_0
I0408 12:47:07.981927 10854 net.cpp:380] label_mnist_1_split -> label_mnist_1_split_1
I0408 12:47:07.982003 10854 net.cpp:122] Setting up label_mnist_1_split
I0408 12:47:07.982025 10854 net.cpp:129] Top shape: 100 (100)
I0408 12:47:07.982029 10854 net.cpp:129] Top shape: 100 (100)
I0408 12:47:07.982033 10854 net.cpp:137] Memory required for data: 314800
I0408 12:47:07.982036 10854 layer_factory.hpp:77] Creating layer conv0
I0408 12:47:07.982046 10854 net.cpp:84] Creating Layer conv0
I0408 12:47:07.982049 10854 net.cpp:406] conv0 <- data
I0408 12:47:07.982055 10854 net.cpp:380] conv0 -> conv0
I0408 12:47:07.982233 10854 net.cpp:122] Setting up conv0
I0408 12:47:07.982239 10854 net.cpp:129] Top shape: 100 50 24 24 (2880000)
I0408 12:47:07.982242 10854 net.cpp:137] Memory required for data: 11834800
I0408 12:47:07.982252 10854 layer_factory.hpp:77] Creating layer pool0
I0408 12:47:07.982259 10854 net.cpp:84] Creating Layer pool0
I0408 12:47:07.982262 10854 net.cpp:406] pool0 <- conv0
I0408 12:47:07.982267 10854 net.cpp:380] pool0 -> pool0
I0408 12:47:07.982326 10854 net.cpp:122] Setting up pool0
I0408 12:47:07.982331 10854 net.cpp:129] Top shape: 100 50 12 12 (720000)
I0408 12:47:07.982334 10854 net.cpp:137] Memory required for data: 14714800
I0408 12:47:07.982337 10854 layer_factory.hpp:77] Creating layer conv1
I0408 12:47:07.982355 10854 net.cpp:84] Creating Layer conv1
I0408 12:47:07.982358 10854 net.cpp:406] conv1 <- pool0
I0408 12:47:07.982364 10854 net.cpp:380] conv1 -> conv1
I0408 12:47:07.984210 10854 net.cpp:122] Setting up conv1
I0408 12:47:07.984218 10854 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0408 12:47:07.984221 10854 net.cpp:137] Memory required for data: 15994800
I0408 12:47:07.984228 10854 layer_factory.hpp:77] Creating layer pool1
I0408 12:47:07.984261 10854 net.cpp:84] Creating Layer pool1
I0408 12:47:07.984266 10854 net.cpp:406] pool1 <- conv1
I0408 12:47:07.984271 10854 net.cpp:380] pool1 -> pool1
I0408 12:47:07.984297 10854 net.cpp:122] Setting up pool1
I0408 12:47:07.984315 10854 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0408 12:47:07.984318 10854 net.cpp:137] Memory required for data: 16314800
I0408 12:47:07.984320 10854 layer_factory.hpp:77] Creating layer ip1
I0408 12:47:07.984325 10854 net.cpp:84] Creating Layer ip1
I0408 12:47:07.984329 10854 net.cpp:406] ip1 <- pool1
I0408 12:47:07.984334 10854 net.cpp:380] ip1 -> ip1
I0408 12:47:07.984815 10854 net.cpp:122] Setting up ip1
I0408 12:47:07.984833 10854 net.cpp:129] Top shape: 100 100 (10000)
I0408 12:47:07.984834 10854 net.cpp:137] Memory required for data: 16354800
I0408 12:47:07.984840 10854 layer_factory.hpp:77] Creating layer relu1
I0408 12:47:07.984845 10854 net.cpp:84] Creating Layer relu1
I0408 12:47:07.984849 10854 net.cpp:406] relu1 <- ip1
I0408 12:47:07.984853 10854 net.cpp:367] relu1 -> ip1 (in-place)
I0408 12:47:07.984858 10854 net.cpp:122] Setting up relu1
I0408 12:47:07.984860 10854 net.cpp:129] Top shape: 100 100 (10000)
I0408 12:47:07.984863 10854 net.cpp:137] Memory required for data: 16394800
I0408 12:47:07.984865 10854 layer_factory.hpp:77] Creating layer ip2
I0408 12:47:07.984870 10854 net.cpp:84] Creating Layer ip2
I0408 12:47:07.984874 10854 net.cpp:406] ip2 <- ip1
I0408 12:47:07.984879 10854 net.cpp:380] ip2 -> ip2
I0408 12:47:07.984998 10854 net.cpp:122] Setting up ip2
I0408 12:47:07.985002 10854 net.cpp:129] Top shape: 100 100 (10000)
I0408 12:47:07.985004 10854 net.cpp:137] Memory required for data: 16434800
I0408 12:47:07.985008 10854 layer_factory.hpp:77] Creating layer relu2
I0408 12:47:07.985011 10854 net.cpp:84] Creating Layer relu2
I0408 12:47:07.985014 10854 net.cpp:406] relu2 <- ip2
I0408 12:47:07.985018 10854 net.cpp:367] relu2 -> ip2 (in-place)
I0408 12:47:07.985023 10854 net.cpp:122] Setting up relu2
I0408 12:47:07.985025 10854 net.cpp:129] Top shape: 100 100 (10000)
I0408 12:47:07.985026 10854 net.cpp:137] Memory required for data: 16474800
I0408 12:47:07.985029 10854 layer_factory.hpp:77] Creating layer ip3
I0408 12:47:07.985034 10854 net.cpp:84] Creating Layer ip3
I0408 12:47:07.985036 10854 net.cpp:406] ip3 <- ip2
I0408 12:47:07.985039 10854 net.cpp:380] ip3 -> ip3
I0408 12:47:07.985131 10854 net.cpp:122] Setting up ip3
I0408 12:47:07.985136 10854 net.cpp:129] Top shape: 100 10 (1000)
I0408 12:47:07.985137 10854 net.cpp:137] Memory required for data: 16478800
I0408 12:47:07.985142 10854 layer_factory.hpp:77] Creating layer relu3
I0408 12:47:07.985146 10854 net.cpp:84] Creating Layer relu3
I0408 12:47:07.985148 10854 net.cpp:406] relu3 <- ip3
I0408 12:47:07.985152 10854 net.cpp:367] relu3 -> ip3 (in-place)
I0408 12:47:07.985155 10854 net.cpp:122] Setting up relu3
I0408 12:47:07.985158 10854 net.cpp:129] Top shape: 100 10 (1000)
I0408 12:47:07.985160 10854 net.cpp:137] Memory required for data: 16482800
I0408 12:47:07.985162 10854 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0408 12:47:07.985165 10854 net.cpp:84] Creating Layer ip3_relu3_0_split
I0408 12:47:07.985168 10854 net.cpp:406] ip3_relu3_0_split <- ip3
I0408 12:47:07.985172 10854 net.cpp:380] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0408 12:47:07.985177 10854 net.cpp:380] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0408 12:47:07.985239 10854 net.cpp:122] Setting up ip3_relu3_0_split
I0408 12:47:07.985242 10854 net.cpp:129] Top shape: 100 10 (1000)
I0408 12:47:07.985260 10854 net.cpp:129] Top shape: 100 10 (1000)
I0408 12:47:07.985261 10854 net.cpp:137] Memory required for data: 16490800
I0408 12:47:07.985263 10854 layer_factory.hpp:77] Creating layer accuracy
I0408 12:47:07.985268 10854 net.cpp:84] Creating Layer accuracy
I0408 12:47:07.985272 10854 net.cpp:406] accuracy <- ip3_relu3_0_split_0
I0408 12:47:07.985275 10854 net.cpp:406] accuracy <- label_mnist_1_split_0
I0408 12:47:07.985280 10854 net.cpp:380] accuracy -> accuracy
I0408 12:47:07.985287 10854 net.cpp:122] Setting up accuracy
I0408 12:47:07.985291 10854 net.cpp:129] Top shape: (1)
I0408 12:47:07.985292 10854 net.cpp:137] Memory required for data: 16490804
I0408 12:47:07.985294 10854 layer_factory.hpp:77] Creating layer loss
I0408 12:47:07.985298 10854 net.cpp:84] Creating Layer loss
I0408 12:47:07.985301 10854 net.cpp:406] loss <- ip3_relu3_0_split_1
I0408 12:47:07.985302 10854 net.cpp:406] loss <- label_mnist_1_split_1
I0408 12:47:07.985306 10854 net.cpp:380] loss -> loss
I0408 12:47:07.985317 10854 layer_factory.hpp:77] Creating layer loss
I0408 12:47:07.985386 10854 net.cpp:122] Setting up loss
I0408 12:47:07.985390 10854 net.cpp:129] Top shape: (1)
I0408 12:47:07.985393 10854 net.cpp:132]     with loss weight 1
I0408 12:47:07.985406 10854 net.cpp:137] Memory required for data: 16490808
I0408 12:47:07.985409 10854 net.cpp:198] loss needs backward computation.
I0408 12:47:07.985412 10854 net.cpp:200] accuracy does not need backward computation.
I0408 12:47:07.985415 10854 net.cpp:198] ip3_relu3_0_split needs backward computation.
I0408 12:47:07.985417 10854 net.cpp:198] relu3 needs backward computation.
I0408 12:47:07.985420 10854 net.cpp:198] ip3 needs backward computation.
I0408 12:47:07.985422 10854 net.cpp:198] relu2 needs backward computation.
I0408 12:47:07.985424 10854 net.cpp:198] ip2 needs backward computation.
I0408 12:47:07.985440 10854 net.cpp:198] relu1 needs backward computation.
I0408 12:47:07.985442 10854 net.cpp:198] ip1 needs backward computation.
I0408 12:47:07.985445 10854 net.cpp:198] pool1 needs backward computation.
I0408 12:47:07.985447 10854 net.cpp:198] conv1 needs backward computation.
I0408 12:47:07.985467 10854 net.cpp:198] pool0 needs backward computation.
I0408 12:47:07.985471 10854 net.cpp:198] conv0 needs backward computation.
I0408 12:47:07.985472 10854 net.cpp:200] label_mnist_1_split does not need backward computation.
I0408 12:47:07.985476 10854 net.cpp:200] mnist does not need backward computation.
I0408 12:47:07.985477 10854 net.cpp:242] This network produces output accuracy
I0408 12:47:07.985481 10854 net.cpp:242] This network produces output loss
I0408 12:47:07.985489 10854 net.cpp:255] Network initialization done.
I0408 12:47:07.985524 10854 solver.cpp:56] Solver scaffolding done.
I0408 12:47:07.985860 10854 caffe.cpp:248] Starting Optimization
I0408 12:47:07.985864 10854 solver.cpp:273] Solving LeNet
I0408 12:47:07.985867 10854 solver.cpp:274] Learning Rate Policy: inv
I0408 12:47:07.986836 10854 solver.cpp:331] Iteration 0, Testing net (#0)
I0408 12:47:15.621093 10862 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:47:15.934228 10854 solver.cpp:398]     Test net output #0: accuracy = 0.07
I0408 12:47:15.934252 10854 solver.cpp:398]     Test net output #1: loss = 2.30164 (* 1 = 2.30164 loss)
I0408 12:47:16.040230 10854 solver.cpp:219] Iteration 0 (0 iter/s, 8.05433s/100 iters), loss = 2.31738
I0408 12:47:16.040274 10854 solver.cpp:238]     Train net output #0: loss = 2.31738 (* 1 = 2.31738 loss)
I0408 12:47:16.040285 10854 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0408 12:47:25.608772 10854 solver.cpp:219] Iteration 100 (10.451 iter/s, 9.56848s/100 iters), loss = 0.388257
I0408 12:47:25.608819 10854 solver.cpp:238]     Train net output #0: loss = 0.388257 (* 1 = 0.388257 loss)
I0408 12:47:25.608825 10854 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0408 12:47:34.909253 10854 solver.cpp:219] Iteration 200 (10.7522 iter/s, 9.30041s/100 iters), loss = 0.304425
I0408 12:47:34.909303 10854 solver.cpp:238]     Train net output #0: loss = 0.304425 (* 1 = 0.304425 loss)
I0408 12:47:34.909310 10854 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0408 12:47:44.345913 10854 solver.cpp:219] Iteration 300 (10.597 iter/s, 9.4366s/100 iters), loss = 0.382647
I0408 12:47:44.345985 10854 solver.cpp:238]     Train net output #0: loss = 0.382647 (* 1 = 0.382647 loss)
I0408 12:47:44.345991 10854 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0408 12:47:53.709782 10854 solver.cpp:219] Iteration 400 (10.6794 iter/s, 9.36379s/100 iters), loss = 0.101796
I0408 12:47:53.709828 10854 solver.cpp:238]     Train net output #0: loss = 0.101796 (* 1 = 0.101796 loss)
I0408 12:47:53.709833 10854 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0408 12:48:02.895855 10854 solver.cpp:331] Iteration 500, Testing net (#0)
I0408 12:48:10.205320 10862 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:48:10.519770 10854 solver.cpp:398]     Test net output #0: accuracy = 0.9648
I0408 12:48:10.519811 10854 solver.cpp:398]     Test net output #1: loss = 0.113963 (* 1 = 0.113963 loss)
I0408 12:48:10.626229 10854 solver.cpp:219] Iteration 500 (5.91143 iter/s, 16.9164s/100 iters), loss = 0.138769
I0408 12:48:10.626278 10854 solver.cpp:238]     Train net output #0: loss = 0.138769 (* 1 = 0.138769 loss)
I0408 12:48:10.626284 10854 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0408 12:48:19.928726 10854 solver.cpp:219] Iteration 600 (10.7499 iter/s, 9.30243s/100 iters), loss = 0.0779363
I0408 12:48:19.928869 10854 solver.cpp:238]     Train net output #0: loss = 0.0779362 (* 1 = 0.0779362 loss)
I0408 12:48:19.928894 10854 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0408 12:48:29.383251 10854 solver.cpp:219] Iteration 700 (10.5771 iter/s, 9.45436s/100 iters), loss = 0.135009
I0408 12:48:29.383347 10854 solver.cpp:238]     Train net output #0: loss = 0.135008 (* 1 = 0.135008 loss)
I0408 12:48:29.383355 10854 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0408 12:48:38.745385 10854 solver.cpp:219] Iteration 800 (10.6814 iter/s, 9.36204s/100 iters), loss = 0.204295
I0408 12:48:38.745416 10854 solver.cpp:238]     Train net output #0: loss = 0.204294 (* 1 = 0.204294 loss)
I0408 12:48:38.745424 10854 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0408 12:48:48.203434 10854 solver.cpp:219] Iteration 900 (10.5731 iter/s, 9.458s/100 iters), loss = 0.159864
I0408 12:48:48.203464 10854 solver.cpp:238]     Train net output #0: loss = 0.159864 (* 1 = 0.159864 loss)
I0408 12:48:48.203469 10854 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0408 12:48:51.298357 10861 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:48:57.545058 10854 solver.cpp:331] Iteration 1000, Testing net (#0)
I0408 12:49:04.895692 10862 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:49:05.212668 10854 solver.cpp:398]     Test net output #0: accuracy = 0.9795
I0408 12:49:05.212709 10854 solver.cpp:398]     Test net output #1: loss = 0.0643148 (* 1 = 0.0643148 loss)
I0408 12:49:05.318168 10854 solver.cpp:219] Iteration 1000 (5.84293 iter/s, 17.1147s/100 iters), loss = 0.0728624
I0408 12:49:05.318217 10854 solver.cpp:238]     Train net output #0: loss = 0.0728622 (* 1 = 0.0728622 loss)
I0408 12:49:05.318223 10854 sgd_solver.cpp:105] Iteration 1000, lr = 0.00931012
I0408 12:49:14.828843 10854 solver.cpp:219] Iteration 1100 (10.5146 iter/s, 9.51061s/100 iters), loss = 0.00442975
I0408 12:49:14.828873 10854 solver.cpp:238]     Train net output #0: loss = 0.00442955 (* 1 = 0.00442955 loss)
I0408 12:49:14.828892 10854 sgd_solver.cpp:105] Iteration 1100, lr = 0.00924715
I0408 12:49:24.412562 10854 solver.cpp:219] Iteration 1200 (10.4344 iter/s, 9.58366s/100 iters), loss = 0.0137717
I0408 12:49:24.412726 10854 solver.cpp:238]     Train net output #0: loss = 0.0137715 (* 1 = 0.0137715 loss)
I0408 12:49:24.412753 10854 sgd_solver.cpp:105] Iteration 1200, lr = 0.00918515
I0408 12:49:33.807124 10854 solver.cpp:219] Iteration 1300 (10.6447 iter/s, 9.39436s/100 iters), loss = 0.0135501
I0408 12:49:33.807157 10854 solver.cpp:238]     Train net output #0: loss = 0.0135498 (* 1 = 0.0135498 loss)
I0408 12:49:33.807165 10854 sgd_solver.cpp:105] Iteration 1300, lr = 0.00912412
I0408 12:49:43.207063 10854 solver.cpp:219] Iteration 1400 (10.6385 iter/s, 9.39986s/100 iters), loss = 0.0115408
I0408 12:49:43.207118 10854 solver.cpp:238]     Train net output #0: loss = 0.0115407 (* 1 = 0.0115407 loss)
I0408 12:49:43.207123 10854 sgd_solver.cpp:105] Iteration 1400, lr = 0.00906403
I0408 12:49:52.464823 10854 solver.cpp:331] Iteration 1500, Testing net (#0)
I0408 12:49:59.778982 10862 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:50:00.094580 10854 solver.cpp:398]     Test net output #0: accuracy = 0.9829
I0408 12:50:00.094621 10854 solver.cpp:398]     Test net output #1: loss = 0.0534268 (* 1 = 0.0534268 loss)
I0408 12:50:00.202136 10854 solver.cpp:219] Iteration 1500 (5.8841 iter/s, 16.995s/100 iters), loss = 0.11567
I0408 12:50:00.202183 10854 solver.cpp:238]     Train net output #0: loss = 0.11567 (* 1 = 0.11567 loss)
I0408 12:50:00.202189 10854 sgd_solver.cpp:105] Iteration 1500, lr = 0.00900485
I0408 12:50:09.504014 10854 solver.cpp:219] Iteration 1600 (10.7506 iter/s, 9.30179s/100 iters), loss = 0.190461
I0408 12:50:09.504062 10854 solver.cpp:238]     Train net output #0: loss = 0.19046 (* 1 = 0.19046 loss)
I0408 12:50:09.504068 10854 sgd_solver.cpp:105] Iteration 1600, lr = 0.00894657
I0408 12:50:18.834765 10854 solver.cpp:219] Iteration 1700 (10.7174 iter/s, 9.33066s/100 iters), loss = 0.0762394
I0408 12:50:18.834810 10854 solver.cpp:238]     Train net output #0: loss = 0.0762392 (* 1 = 0.0762392 loss)
I0408 12:50:18.834815 10854 sgd_solver.cpp:105] Iteration 1700, lr = 0.00888916
I0408 12:50:28.136981 10854 solver.cpp:219] Iteration 1800 (10.7502 iter/s, 9.30213s/100 iters), loss = 0.0118906
I0408 12:50:28.137027 10854 solver.cpp:238]     Train net output #0: loss = 0.0118904 (* 1 = 0.0118904 loss)
I0408 12:50:28.137043 10854 sgd_solver.cpp:105] Iteration 1800, lr = 0.0088326
I0408 12:50:34.648921 10861 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:50:37.440829 10854 solver.cpp:219] Iteration 1900 (10.7483 iter/s, 9.30376s/100 iters), loss = 0.153951
I0408 12:50:37.440874 10854 solver.cpp:238]     Train net output #0: loss = 0.153951 (* 1 = 0.153951 loss)
I0408 12:50:37.440879 10854 sgd_solver.cpp:105] Iteration 1900, lr = 0.00877687
I0408 12:50:46.716738 10854 solver.cpp:331] Iteration 2000, Testing net (#0)
I0408 12:50:54.055408 10862 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:50:54.369925 10854 solver.cpp:398]     Test net output #0: accuracy = 0.986
I0408 12:50:54.369966 10854 solver.cpp:398]     Test net output #1: loss = 0.0443455 (* 1 = 0.0443455 loss)
I0408 12:50:54.475291 10854 solver.cpp:219] Iteration 2000 (5.87049 iter/s, 17.0344s/100 iters), loss = 0.0156593
I0408 12:50:54.475337 10854 solver.cpp:238]     Train net output #0: loss = 0.0156591 (* 1 = 0.0156591 loss)
I0408 12:50:54.475356 10854 sgd_solver.cpp:105] Iteration 2000, lr = 0.00872196
I0408 12:51:04.068667 10854 solver.cpp:219] Iteration 2100 (10.424 iter/s, 9.59329s/100 iters), loss = 0.0138378
I0408 12:51:04.068699 10854 solver.cpp:238]     Train net output #0: loss = 0.0138377 (* 1 = 0.0138377 loss)
I0408 12:51:04.068706 10854 sgd_solver.cpp:105] Iteration 2100, lr = 0.00866784
I0408 12:51:13.743813 10854 solver.cpp:219] Iteration 2200 (10.3358 iter/s, 9.67508s/100 iters), loss = 0.0383051
I0408 12:51:13.743937 10854 solver.cpp:238]     Train net output #0: loss = 0.038305 (* 1 = 0.038305 loss)
I0408 12:51:13.743942 10854 sgd_solver.cpp:105] Iteration 2200, lr = 0.0086145
I0408 12:51:23.289118 10854 solver.cpp:219] Iteration 2300 (10.4765 iter/s, 9.54514s/100 iters), loss = 0.115309
I0408 12:51:23.289172 10854 solver.cpp:238]     Train net output #0: loss = 0.115309 (* 1 = 0.115309 loss)
I0408 12:51:23.289177 10854 sgd_solver.cpp:105] Iteration 2300, lr = 0.00856192
I0408 12:51:32.861426 10854 solver.cpp:219] Iteration 2400 (10.4469 iter/s, 9.57222s/100 iters), loss = 0.00710735
I0408 12:51:32.861488 10854 solver.cpp:238]     Train net output #0: loss = 0.00710716 (* 1 = 0.00710716 loss)
I0408 12:51:32.861493 10854 sgd_solver.cpp:105] Iteration 2400, lr = 0.00851008
I0408 12:51:42.362727 10854 solver.cpp:331] Iteration 2500, Testing net (#0)
I0408 12:51:49.643615 10862 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:51:49.957665 10854 solver.cpp:398]     Test net output #0: accuracy = 0.9813
I0408 12:51:49.957706 10854 solver.cpp:398]     Test net output #1: loss = 0.0590398 (* 1 = 0.0590398 loss)
I0408 12:51:50.060484 10854 solver.cpp:219] Iteration 2500 (5.81431 iter/s, 17.199s/100 iters), loss = 0.0274782
I0408 12:51:50.060533 10854 solver.cpp:238]     Train net output #0: loss = 0.027478 (* 1 = 0.027478 loss)
I0408 12:51:50.060539 10854 sgd_solver.cpp:105] Iteration 2500, lr = 0.00845897
I0408 12:51:59.350425 10854 solver.cpp:219] Iteration 2600 (10.7644 iter/s, 9.28986s/100 iters), loss = 0.0630477
I0408 12:51:59.350471 10854 solver.cpp:238]     Train net output #0: loss = 0.0630475 (* 1 = 0.0630475 loss)
I0408 12:51:59.350476 10854 sgd_solver.cpp:105] Iteration 2600, lr = 0.00840857
I0408 12:52:08.654415 10854 solver.cpp:219] Iteration 2700 (10.7482 iter/s, 9.30391s/100 iters), loss = 0.0609124
I0408 12:52:08.654461 10854 solver.cpp:238]     Train net output #0: loss = 0.0609122 (* 1 = 0.0609122 loss)
I0408 12:52:08.654479 10854 sgd_solver.cpp:105] Iteration 2700, lr = 0.00835886
I0408 12:52:17.964356 10854 solver.cpp:219] Iteration 2800 (10.7413 iter/s, 9.30986s/100 iters), loss = 0.00191192
I0408 12:52:17.964403 10854 solver.cpp:238]     Train net output #0: loss = 0.00191173 (* 1 = 0.00191173 loss)
I0408 12:52:17.964408 10854 sgd_solver.cpp:105] Iteration 2800, lr = 0.00830984
I0408 12:52:18.725257 10861 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:52:27.288987 10854 solver.cpp:219] Iteration 2900 (10.7244 iter/s, 9.32455s/100 iters), loss = 0.0115662
I0408 12:52:27.289098 10854 solver.cpp:238]     Train net output #0: loss = 0.011566 (* 1 = 0.011566 loss)
I0408 12:52:27.289124 10854 sgd_solver.cpp:105] Iteration 2900, lr = 0.00826148
I0408 12:52:36.457164 10854 solver.cpp:331] Iteration 3000, Testing net (#0)
I0408 12:52:43.786803 10862 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:52:44.101573 10854 solver.cpp:398]     Test net output #0: accuracy = 0.9871
I0408 12:52:44.101613 10854 solver.cpp:398]     Test net output #1: loss = 0.0407873 (* 1 = 0.0407873 loss)
I0408 12:52:44.206512 10854 solver.cpp:219] Iteration 3000 (5.91109 iter/s, 16.9174s/100 iters), loss = 0.0112259
I0408 12:52:44.206559 10854 solver.cpp:238]     Train net output #0: loss = 0.0112257 (* 1 = 0.0112257 loss)
I0408 12:52:44.206565 10854 sgd_solver.cpp:105] Iteration 3000, lr = 0.00821377
I0408 12:52:53.516034 10854 solver.cpp:219] Iteration 3100 (10.7418 iter/s, 9.30944s/100 iters), loss = 0.0492224
I0408 12:52:53.516063 10854 solver.cpp:238]     Train net output #0: loss = 0.0492222 (* 1 = 0.0492222 loss)
I0408 12:52:53.516068 10854 sgd_solver.cpp:105] Iteration 3100, lr = 0.0081667
I0408 12:53:02.820227 10854 solver.cpp:219] Iteration 3200 (10.7479 iter/s, 9.30413s/100 iters), loss = 0.004439
I0408 12:53:02.820420 10854 solver.cpp:238]     Train net output #0: loss = 0.00443878 (* 1 = 0.00443878 loss)
I0408 12:53:02.820426 10854 sgd_solver.cpp:105] Iteration 3200, lr = 0.00812025
I0408 12:53:12.127122 10854 solver.cpp:219] Iteration 3300 (10.745 iter/s, 9.30668s/100 iters), loss = 0.017118
I0408 12:53:12.127149 10854 solver.cpp:238]     Train net output #0: loss = 0.0171178 (* 1 = 0.0171178 loss)
I0408 12:53:12.127154 10854 sgd_solver.cpp:105] Iteration 3300, lr = 0.00807442
I0408 12:53:21.438102 10854 solver.cpp:219] Iteration 3400 (10.7401 iter/s, 9.31092s/100 iters), loss = 0.0124256
I0408 12:53:21.438170 10854 solver.cpp:238]     Train net output #0: loss = 0.0124253 (* 1 = 0.0124253 loss)
I0408 12:53:21.438175 10854 sgd_solver.cpp:105] Iteration 3400, lr = 0.00802918
I0408 12:53:30.613055 10854 solver.cpp:331] Iteration 3500, Testing net (#0)
I0408 12:53:38.170564 10862 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:53:38.482023 10854 solver.cpp:398]     Test net output #0: accuracy = 0.9851
I0408 12:53:38.482048 10854 solver.cpp:398]     Test net output #1: loss = 0.0452399 (* 1 = 0.0452399 loss)
I0408 12:53:38.584997 10854 solver.cpp:219] Iteration 3500 (5.83199 iter/s, 17.1468s/100 iters), loss = 0.00619003
I0408 12:53:38.585041 10854 solver.cpp:238]     Train net output #0: loss = 0.00618978 (* 1 = 0.00618978 loss)
I0408 12:53:38.585047 10854 sgd_solver.cpp:105] Iteration 3500, lr = 0.00798454
I0408 12:53:48.061864 10854 solver.cpp:219] Iteration 3600 (10.5521 iter/s, 9.47679s/100 iters), loss = 0.0263257
I0408 12:53:48.061892 10854 solver.cpp:238]     Train net output #0: loss = 0.0263254 (* 1 = 0.0263254 loss)
I0408 12:53:48.061898 10854 sgd_solver.cpp:105] Iteration 3600, lr = 0.00794046
I0408 12:53:57.388759 10854 solver.cpp:219] Iteration 3700 (10.7218 iter/s, 9.32683s/100 iters), loss = 0.0133043
I0408 12:53:57.388793 10854 solver.cpp:238]     Train net output #0: loss = 0.013304 (* 1 = 0.013304 loss)
I0408 12:53:57.388800 10854 sgd_solver.cpp:105] Iteration 3700, lr = 0.00789695
I0408 12:54:01.576723 10861 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:54:06.711547 10854 solver.cpp:219] Iteration 3800 (10.7265 iter/s, 9.32273s/100 iters), loss = 0.0426518
I0408 12:54:06.711591 10854 solver.cpp:238]     Train net output #0: loss = 0.0426516 (* 1 = 0.0426516 loss)
I0408 12:54:06.711597 10854 sgd_solver.cpp:105] Iteration 3800, lr = 0.007854
I0408 12:54:16.031471 10854 solver.cpp:219] Iteration 3900 (10.7298 iter/s, 9.31986s/100 iters), loss = 0.0456175
I0408 12:54:16.031682 10854 solver.cpp:238]     Train net output #0: loss = 0.0456173 (* 1 = 0.0456173 loss)
I0408 12:54:16.031688 10854 sgd_solver.cpp:105] Iteration 3900, lr = 0.00781158
I0408 12:54:25.249914 10854 solver.cpp:331] Iteration 4000, Testing net (#0)
I0408 12:54:32.601272 10862 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:54:32.919005 10854 solver.cpp:398]     Test net output #0: accuracy = 0.9893
I0408 12:54:32.919049 10854 solver.cpp:398]     Test net output #1: loss = 0.0328704 (* 1 = 0.0328704 loss)
I0408 12:54:33.023324 10854 solver.cpp:219] Iteration 4000 (5.88526 iter/s, 16.9916s/100 iters), loss = 0.0250743
I0408 12:54:33.023370 10854 solver.cpp:238]     Train net output #0: loss = 0.0250741 (* 1 = 0.0250741 loss)
I0408 12:54:33.023376 10854 sgd_solver.cpp:105] Iteration 4000, lr = 0.0077697
I0408 12:54:42.357312 10854 solver.cpp:219] Iteration 4100 (10.7136 iter/s, 9.33391s/100 iters), loss = 0.0219423
I0408 12:54:42.357343 10854 solver.cpp:238]     Train net output #0: loss = 0.021942 (* 1 = 0.021942 loss)
I0408 12:54:42.357348 10854 sgd_solver.cpp:105] Iteration 4100, lr = 0.00772833
I0408 12:54:51.916962 10854 solver.cpp:219] Iteration 4200 (10.4607 iter/s, 9.55959s/100 iters), loss = 0.00568176
I0408 12:54:51.917099 10854 solver.cpp:238]     Train net output #0: loss = 0.00568155 (* 1 = 0.00568155 loss)
I0408 12:54:51.917107 10854 sgd_solver.cpp:105] Iteration 4200, lr = 0.00768748
I0408 12:55:01.245847 10854 solver.cpp:219] Iteration 4300 (10.7196 iter/s, 9.32872s/100 iters), loss = 0.0487614
I0408 12:55:01.245895 10854 solver.cpp:238]     Train net output #0: loss = 0.0487612 (* 1 = 0.0487612 loss)
I0408 12:55:01.245900 10854 sgd_solver.cpp:105] Iteration 4300, lr = 0.00764712
I0408 12:55:10.654397 10854 solver.cpp:219] Iteration 4400 (10.6287 iter/s, 9.40847s/100 iters), loss = 0.0122888
I0408 12:55:10.654434 10854 solver.cpp:238]     Train net output #0: loss = 0.0122886 (* 1 = 0.0122886 loss)
I0408 12:55:10.654444 10854 sgd_solver.cpp:105] Iteration 4400, lr = 0.00760726
I0408 12:55:19.928459 10854 solver.cpp:331] Iteration 4500, Testing net (#0)
I0408 12:55:27.381631 10862 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:55:27.695137 10854 solver.cpp:398]     Test net output #0: accuracy = 0.9875
I0408 12:55:27.695179 10854 solver.cpp:398]     Test net output #1: loss = 0.0379204 (* 1 = 0.0379204 loss)
I0408 12:55:27.798712 10854 solver.cpp:219] Iteration 4500 (5.83296 iter/s, 17.1439s/100 iters), loss = 0.00997575
I0408 12:55:27.798759 10854 solver.cpp:238]     Train net output #0: loss = 0.00997557 (* 1 = 0.00997557 loss)
I0408 12:55:27.798766 10854 sgd_solver.cpp:105] Iteration 4500, lr = 0.00756788
I0408 12:55:37.209421 10854 solver.cpp:219] Iteration 4600 (10.6263 iter/s, 9.41063s/100 iters), loss = 0.00748762
I0408 12:55:37.209506 10854 solver.cpp:238]     Train net output #0: loss = 0.00748743 (* 1 = 0.00748743 loss)
I0408 12:55:37.209513 10854 sgd_solver.cpp:105] Iteration 4600, lr = 0.00752897
I0408 12:55:44.986614 10861 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:55:46.585294 10854 solver.cpp:219] Iteration 4700 (10.6658 iter/s, 9.37576s/100 iters), loss = 0.00328556
I0408 12:55:46.585340 10854 solver.cpp:238]     Train net output #0: loss = 0.00328537 (* 1 = 0.00328537 loss)
I0408 12:55:46.585345 10854 sgd_solver.cpp:105] Iteration 4700, lr = 0.00749052
I0408 12:55:55.924150 10854 solver.cpp:219] Iteration 4800 (10.708 iter/s, 9.33878s/100 iters), loss = 0.0124446
I0408 12:55:55.924196 10854 solver.cpp:238]     Train net output #0: loss = 0.0124444 (* 1 = 0.0124444 loss)
I0408 12:55:55.924201 10854 sgd_solver.cpp:105] Iteration 4800, lr = 0.00745253
I0408 12:56:05.349695 10854 solver.cpp:219] Iteration 4900 (10.6095 iter/s, 9.42548s/100 iters), loss = 0.00452691
I0408 12:56:05.349918 10854 solver.cpp:238]     Train net output #0: loss = 0.00452672 (* 1 = 0.00452672 loss)
I0408 12:56:05.349938 10854 sgd_solver.cpp:105] Iteration 4900, lr = 0.00741498
I0408 12:56:14.542960 10854 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_5000.caffemodel
I0408 12:56:14.598793 10854 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_5000.solverstate
I0408 12:56:14.599894 10854 solver.cpp:331] Iteration 5000, Testing net (#0)
I0408 12:56:21.910848 10862 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:56:22.229193 10854 solver.cpp:398]     Test net output #0: accuracy = 0.9895
I0408 12:56:22.229218 10854 solver.cpp:398]     Test net output #1: loss = 0.0329236 (* 1 = 0.0329236 loss)
I0408 12:56:22.336833 10854 solver.cpp:219] Iteration 5000 (5.8869 iter/s, 16.9869s/100 iters), loss = 0.0551172
I0408 12:56:22.336871 10854 solver.cpp:238]     Train net output #0: loss = 0.055117 (* 1 = 0.055117 loss)
I0408 12:56:22.336879 10854 sgd_solver.cpp:105] Iteration 5000, lr = 0.00737788
I0408 12:56:31.662773 10854 solver.cpp:219] Iteration 5100 (10.7229 iter/s, 9.32587s/100 iters), loss = 0.0249161
I0408 12:56:31.662804 10854 solver.cpp:238]     Train net output #0: loss = 0.0249159 (* 1 = 0.0249159 loss)
I0408 12:56:31.662811 10854 sgd_solver.cpp:105] Iteration 5100, lr = 0.0073412
I0408 12:56:41.236840 10854 solver.cpp:219] Iteration 5200 (10.4449 iter/s, 9.57401s/100 iters), loss = 0.0135875
I0408 12:56:41.237030 10854 solver.cpp:238]     Train net output #0: loss = 0.0135873 (* 1 = 0.0135873 loss)
I0408 12:56:41.237037 10854 sgd_solver.cpp:105] Iteration 5200, lr = 0.00730495
I0408 12:56:50.590471 10854 solver.cpp:219] Iteration 5300 (10.6913 iter/s, 9.35342s/100 iters), loss = 0.00219742
I0408 12:56:50.590518 10854 solver.cpp:238]     Train net output #0: loss = 0.00219724 (* 1 = 0.00219724 loss)
I0408 12:56:50.590550 10854 sgd_solver.cpp:105] Iteration 5300, lr = 0.00726911
I0408 12:57:00.080562 10854 solver.cpp:219] Iteration 5400 (10.5374 iter/s, 9.49002s/100 iters), loss = 0.0121515
I0408 12:57:00.080610 10854 solver.cpp:238]     Train net output #0: loss = 0.0121513 (* 1 = 0.0121513 loss)
I0408 12:57:00.080615 10854 sgd_solver.cpp:105] Iteration 5400, lr = 0.00723368
I0408 12:57:09.516378 10854 solver.cpp:331] Iteration 5500, Testing net (#0)
I0408 12:57:17.124279 10862 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:57:17.450670 10854 solver.cpp:398]     Test net output #0: accuracy = 0.9883
I0408 12:57:17.450713 10854 solver.cpp:398]     Test net output #1: loss = 0.0342572 (* 1 = 0.0342572 loss)
I0408 12:57:17.558272 10854 solver.cpp:219] Iteration 5500 (5.7216 iter/s, 17.4776s/100 iters), loss = 0.00679451
I0408 12:57:17.558317 10854 solver.cpp:238]     Train net output #0: loss = 0.00679431 (* 1 = 0.00679431 loss)
I0408 12:57:17.558323 10854 sgd_solver.cpp:105] Iteration 5500, lr = 0.00719865
I0408 12:57:27.105978 10854 solver.cpp:219] Iteration 5600 (10.4738 iter/s, 9.54763s/100 iters), loss = 0.00280294
I0408 12:57:27.106024 10854 solver.cpp:238]     Train net output #0: loss = 0.00280274 (* 1 = 0.00280274 loss)
I0408 12:57:27.106029 10854 sgd_solver.cpp:105] Iteration 5600, lr = 0.00716402
I0408 12:57:29.018100 10861 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:57:36.634104 10854 solver.cpp:219] Iteration 5700 (10.4953 iter/s, 9.52805s/100 iters), loss = 0.00357749
I0408 12:57:36.634151 10854 solver.cpp:238]     Train net output #0: loss = 0.0035773 (* 1 = 0.0035773 loss)
I0408 12:57:36.634156 10854 sgd_solver.cpp:105] Iteration 5700, lr = 0.00712977
I0408 12:57:46.267052 10854 solver.cpp:219] Iteration 5800 (10.3811 iter/s, 9.63287s/100 iters), loss = 0.0494026
I0408 12:57:46.267086 10854 solver.cpp:238]     Train net output #0: loss = 0.0494024 (* 1 = 0.0494024 loss)
I0408 12:57:46.267092 10854 sgd_solver.cpp:105] Iteration 5800, lr = 0.0070959
I0408 12:57:55.683756 10854 solver.cpp:219] Iteration 5900 (10.6195 iter/s, 9.41664s/100 iters), loss = 0.00402478
I0408 12:57:55.683868 10854 solver.cpp:238]     Train net output #0: loss = 0.00402457 (* 1 = 0.00402457 loss)
I0408 12:57:55.683873 10854 sgd_solver.cpp:105] Iteration 5900, lr = 0.0070624
I0408 12:58:04.850275 10854 solver.cpp:331] Iteration 6000, Testing net (#0)
I0408 12:58:12.175195 10862 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:58:12.494118 10854 solver.cpp:398]     Test net output #0: accuracy = 0.9899
I0408 12:58:12.494158 10854 solver.cpp:398]     Test net output #1: loss = 0.0309928 (* 1 = 0.0309928 loss)
I0408 12:58:12.599618 10854 solver.cpp:219] Iteration 6000 (5.91166 iter/s, 16.9157s/100 iters), loss = 0.00523991
I0408 12:58:12.599644 10854 solver.cpp:238]     Train net output #0: loss = 0.0052397 (* 1 = 0.0052397 loss)
I0408 12:58:12.599650 10854 sgd_solver.cpp:105] Iteration 6000, lr = 0.00702927
I0408 12:58:21.908437 10854 solver.cpp:219] Iteration 6100 (10.7426 iter/s, 9.30877s/100 iters), loss = 0.00299861
I0408 12:58:21.908474 10854 solver.cpp:238]     Train net output #0: loss = 0.00299839 (* 1 = 0.00299839 loss)
I0408 12:58:21.908499 10854 sgd_solver.cpp:105] Iteration 6100, lr = 0.0069965
I0408 12:58:31.236551 10854 solver.cpp:219] Iteration 6200 (10.7204 iter/s, 9.32805s/100 iters), loss = 0.00863271
I0408 12:58:31.236765 10854 solver.cpp:238]     Train net output #0: loss = 0.0086325 (* 1 = 0.0086325 loss)
I0408 12:58:31.236784 10854 sgd_solver.cpp:105] Iteration 6200, lr = 0.00696408
I0408 12:58:40.531744 10854 solver.cpp:219] Iteration 6300 (10.7585 iter/s, 9.29498s/100 iters), loss = 0.00818392
I0408 12:58:40.531772 10854 solver.cpp:238]     Train net output #0: loss = 0.00818368 (* 1 = 0.00818368 loss)
I0408 12:58:40.531795 10854 sgd_solver.cpp:105] Iteration 6300, lr = 0.00693201
I0408 12:58:49.843423 10854 solver.cpp:219] Iteration 6400 (10.7393 iter/s, 9.31163s/100 iters), loss = 0.0088486
I0408 12:58:49.843469 10854 solver.cpp:238]     Train net output #0: loss = 0.00884835 (* 1 = 0.00884835 loss)
I0408 12:58:49.843475 10854 sgd_solver.cpp:105] Iteration 6400, lr = 0.00690029
I0408 12:58:59.401526 10854 solver.cpp:331] Iteration 6500, Testing net (#0)
I0408 12:59:07.030259 10862 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:59:07.342813 10854 solver.cpp:398]     Test net output #0: accuracy = 0.9896
I0408 12:59:07.342856 10854 solver.cpp:398]     Test net output #1: loss = 0.0335645 (* 1 = 0.0335645 loss)
I0408 12:59:07.446012 10854 solver.cpp:219] Iteration 6500 (5.68101 iter/s, 17.6025s/100 iters), loss = 0.0138433
I0408 12:59:07.446060 10854 solver.cpp:238]     Train net output #0: loss = 0.0138431 (* 1 = 0.0138431 loss)
I0408 12:59:07.446066 10854 sgd_solver.cpp:105] Iteration 6500, lr = 0.0068689
I0408 12:59:13.232962 10861 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:59:17.263864 10854 solver.cpp:219] Iteration 6600 (10.1856 iter/s, 9.81778s/100 iters), loss = 0.0318124
I0408 12:59:17.263898 10854 solver.cpp:238]     Train net output #0: loss = 0.0318122 (* 1 = 0.0318122 loss)
I0408 12:59:17.263905 10854 sgd_solver.cpp:105] Iteration 6600, lr = 0.00683784
I0408 12:59:26.997148 10854 solver.cpp:219] Iteration 6700 (10.2741 iter/s, 9.73323s/100 iters), loss = 0.00975228
I0408 12:59:26.997195 10854 solver.cpp:238]     Train net output #0: loss = 0.00975207 (* 1 = 0.00975207 loss)
I0408 12:59:26.997200 10854 sgd_solver.cpp:105] Iteration 6700, lr = 0.00680711
I0408 12:59:37.019256 10854 solver.cpp:219] Iteration 6800 (9.97801 iter/s, 10.022s/100 iters), loss = 0.00338604
I0408 12:59:37.019302 10854 solver.cpp:238]     Train net output #0: loss = 0.00338582 (* 1 = 0.00338582 loss)
I0408 12:59:37.019307 10854 sgd_solver.cpp:105] Iteration 6800, lr = 0.0067767
I0408 12:59:46.342442 10854 solver.cpp:219] Iteration 6900 (10.726 iter/s, 9.32312s/100 iters), loss = 0.00933675
I0408 12:59:46.342648 10854 solver.cpp:238]     Train net output #0: loss = 0.00933653 (* 1 = 0.00933653 loss)
I0408 12:59:46.342675 10854 sgd_solver.cpp:105] Iteration 6900, lr = 0.0067466
I0408 12:59:55.585587 10854 solver.cpp:331] Iteration 7000, Testing net (#0)
I0408 13:00:02.950302 10862 data_layer.cpp:73] Restarting data prefetching from start.
I0408 13:00:03.270691 10854 solver.cpp:398]     Test net output #0: accuracy = 0.9894
I0408 13:00:03.270732 10854 solver.cpp:398]     Test net output #1: loss = 0.0343422 (* 1 = 0.0343422 loss)
I0408 13:00:03.373759 10854 solver.cpp:219] Iteration 7000 (5.87162 iter/s, 17.0311s/100 iters), loss = 0.00975801
I0408 13:00:03.373790 10854 solver.cpp:238]     Train net output #0: loss = 0.00975779 (* 1 = 0.00975779 loss)
I0408 13:00:03.373812 10854 sgd_solver.cpp:105] Iteration 7000, lr = 0.00671681
I0408 13:00:12.823576 10854 solver.cpp:219] Iteration 7100 (10.5823 iter/s, 9.44976s/100 iters), loss = 0.00388127
I0408 13:00:12.823606 10854 solver.cpp:238]     Train net output #0: loss = 0.00388104 (* 1 = 0.00388104 loss)
I0408 13:00:12.823612 10854 sgd_solver.cpp:105] Iteration 7100, lr = 0.00668733
I0408 13:00:22.290297 10854 solver.cpp:219] Iteration 7200 (10.5634 iter/s, 9.46666s/100 iters), loss = 0.00821483
I0408 13:00:22.290391 10854 solver.cpp:238]     Train net output #0: loss = 0.0082146 (* 1 = 0.0082146 loss)
I0408 13:00:22.290396 10854 sgd_solver.cpp:105] Iteration 7200, lr = 0.00665815
I0408 13:00:31.801333 10854 solver.cpp:219] Iteration 7300 (10.5142 iter/s, 9.51091s/100 iters), loss = 0.0267844
I0408 13:00:31.801388 10854 solver.cpp:238]     Train net output #0: loss = 0.0267842 (* 1 = 0.0267842 loss)
I0408 13:00:31.801393 10854 sgd_solver.cpp:105] Iteration 7300, lr = 0.00662927
I0408 13:00:41.209128 10854 solver.cpp:219] Iteration 7400 (10.6296 iter/s, 9.40772s/100 iters), loss = 0.00816641
I0408 13:00:41.209158 10854 solver.cpp:238]     Train net output #0: loss = 0.0081662 (* 1 = 0.0081662 loss)
I0408 13:00:41.209164 10854 sgd_solver.cpp:105] Iteration 7400, lr = 0.00660067
I0408 13:00:50.299274 10861 data_layer.cpp:73] Restarting data prefetching from start.
I0408 13:00:50.643865 10854 solver.cpp:331] Iteration 7500, Testing net (#0)
I0408 13:00:58.508021 10862 data_layer.cpp:73] Restarting data prefetching from start.
I0408 13:00:58.831451 10854 solver.cpp:398]     Test net output #0: accuracy = 0.9895
I0408 13:00:58.831476 10854 solver.cpp:398]     Test net output #1: loss = 0.0345753 (* 1 = 0.0345753 loss)
I0408 13:00:58.938006 10854 solver.cpp:219] Iteration 7500 (5.64054 iter/s, 17.7288s/100 iters), loss = 0.00139054
I0408 13:00:58.938037 10854 solver.cpp:238]     Train net output #0: loss = 0.00139033 (* 1 = 0.00139033 loss)
I0408 13:00:58.938043 10854 sgd_solver.cpp:105] Iteration 7500, lr = 0.00657236
I0408 13:01:08.351539 10854 solver.cpp:219] Iteration 7600 (10.6231 iter/s, 9.41348s/100 iters), loss = 0.0108218
I0408 13:01:08.351569 10854 solver.cpp:238]     Train net output #0: loss = 0.0108216 (* 1 = 0.0108216 loss)
I0408 13:01:08.351577 10854 sgd_solver.cpp:105] Iteration 7600, lr = 0.00654433
I0408 13:01:17.835832 10854 solver.cpp:219] Iteration 7700 (10.5438 iter/s, 9.48424s/100 iters), loss = 0.0237995
I0408 13:01:17.835883 10854 solver.cpp:238]     Train net output #0: loss = 0.0237993 (* 1 = 0.0237993 loss)
I0408 13:01:17.835888 10854 sgd_solver.cpp:105] Iteration 7700, lr = 0.00651658
I0408 13:01:27.269697 10854 solver.cpp:219] Iteration 7800 (10.6002 iter/s, 9.43379s/100 iters), loss = 0.0041731
I0408 13:01:27.269742 10854 solver.cpp:238]     Train net output #0: loss = 0.00417292 (* 1 = 0.00417292 loss)
I0408 13:01:27.269748 10854 sgd_solver.cpp:105] Iteration 7800, lr = 0.00648911
I0408 13:01:36.630144 10854 solver.cpp:219] Iteration 7900 (10.6833 iter/s, 9.3604s/100 iters), loss = 0.00329725
I0408 13:01:36.630309 10854 solver.cpp:238]     Train net output #0: loss = 0.00329707 (* 1 = 0.00329707 loss)
I0408 13:01:36.630314 10854 sgd_solver.cpp:105] Iteration 7900, lr = 0.0064619
I0408 13:01:45.901793 10854 solver.cpp:331] Iteration 8000, Testing net (#0)
I0408 13:01:53.322754 10862 data_layer.cpp:73] Restarting data prefetching from start.
I0408 13:01:53.656038 10854 solver.cpp:398]     Test net output #0: accuracy = 0.9898
I0408 13:01:53.656080 10854 solver.cpp:398]     Test net output #1: loss = 0.0320964 (* 1 = 0.0320964 loss)
I0408 13:01:53.770138 10854 solver.cpp:219] Iteration 8000 (5.83437 iter/s, 17.1398s/100 iters), loss = 0.0113262
I0408 13:01:53.770205 10854 solver.cpp:238]     Train net output #0: loss = 0.011326 (* 1 = 0.011326 loss)
I0408 13:01:53.770211 10854 sgd_solver.cpp:105] Iteration 8000, lr = 0.00643496
I0408 13:02:03.182112 10854 solver.cpp:219] Iteration 8100 (10.6249 iter/s, 9.41188s/100 iters), loss = 0.0234799
I0408 13:02:03.182140 10854 solver.cpp:238]     Train net output #0: loss = 0.0234797 (* 1 = 0.0234797 loss)
I0408 13:02:03.182163 10854 sgd_solver.cpp:105] Iteration 8100, lr = 0.00640827
I0408 13:02:12.509516 10854 solver.cpp:219] Iteration 8200 (10.7212 iter/s, 9.32735s/100 iters), loss = 0.0088781
I0408 13:02:12.509735 10854 solver.cpp:238]     Train net output #0: loss = 0.00887791 (* 1 = 0.00887791 loss)
I0408 13:02:12.509760 10854 sgd_solver.cpp:105] Iteration 8200, lr = 0.00638185
I0408 13:02:21.837662 10854 solver.cpp:219] Iteration 8300 (10.7205 iter/s, 9.32793s/100 iters), loss = 0.0403654
I0408 13:02:21.837692 10854 solver.cpp:238]     Train net output #0: loss = 0.0403652 (* 1 = 0.0403652 loss)
I0408 13:02:21.837697 10854 sgd_solver.cpp:105] Iteration 8300, lr = 0.00635567
I0408 13:02:31.314941 10854 solver.cpp:219] Iteration 8400 (10.5516 iter/s, 9.47723s/100 iters), loss = 0.0119715
I0408 13:02:31.314985 10854 solver.cpp:238]     Train net output #0: loss = 0.0119713 (* 1 = 0.0119713 loss)
I0408 13:02:31.314991 10854 sgd_solver.cpp:105] Iteration 8400, lr = 0.00632975
I0408 13:02:34.421437 10861 data_layer.cpp:73] Restarting data prefetching from start.
I0408 13:02:40.725692 10854 solver.cpp:331] Iteration 8500, Testing net (#0)
I0408 13:02:48.048425 10862 data_layer.cpp:73] Restarting data prefetching from start.
I0408 13:02:48.362545 10854 solver.cpp:398]     Test net output #0: accuracy = 0.9885
I0408 13:02:48.362586 10854 solver.cpp:398]     Test net output #1: loss = 0.0329722 (* 1 = 0.0329722 loss)
I0408 13:02:48.465472 10854 solver.cpp:219] Iteration 8500 (5.83074 iter/s, 17.1505s/100 iters), loss = 0.00674628
I0408 13:02:48.465517 10854 solver.cpp:238]     Train net output #0: loss = 0.0067461 (* 1 = 0.0067461 loss)
I0408 13:02:48.465523 10854 sgd_solver.cpp:105] Iteration 8500, lr = 0.00630407
I0408 13:02:58.729671 10854 solver.cpp:219] Iteration 8600 (9.74267 iter/s, 10.2641s/100 iters), loss = 0.00255405
I0408 13:02:58.729719 10854 solver.cpp:238]     Train net output #0: loss = 0.00255387 (* 1 = 0.00255387 loss)
I0408 13:02:58.729738 10854 sgd_solver.cpp:105] Iteration 8600, lr = 0.00627864
I0408 13:03:08.501667 10854 solver.cpp:219] Iteration 8700 (10.2334 iter/s, 9.77192s/100 iters), loss = 0.0024229
I0408 13:03:08.501700 10854 solver.cpp:238]     Train net output #0: loss = 0.00242273 (* 1 = 0.00242273 loss)
I0408 13:03:08.501724 10854 sgd_solver.cpp:105] Iteration 8700, lr = 0.00625344
I0408 13:03:17.950670 10854 solver.cpp:219] Iteration 8800 (10.5841 iter/s, 9.44815s/100 iters), loss = 0.000967271
I0408 13:03:17.950716 10854 solver.cpp:238]     Train net output #0: loss = 0.0009671 (* 1 = 0.0009671 loss)
I0408 13:03:17.950733 10854 sgd_solver.cpp:105] Iteration 8800, lr = 0.00622847
I0408 13:03:27.611196 10854 solver.cpp:219] Iteration 8900 (10.3515 iter/s, 9.66046s/100 iters), loss = 0.000406998
I0408 13:03:27.611366 10854 solver.cpp:238]     Train net output #0: loss = 0.000406821 (* 1 = 0.000406821 loss)
I0408 13:03:27.611392 10854 sgd_solver.cpp:105] Iteration 8900, lr = 0.00620374
I0408 13:03:37.358870 10854 solver.cpp:331] Iteration 9000, Testing net (#0)
I0408 13:03:44.986410 10862 data_layer.cpp:73] Restarting data prefetching from start.
I0408 13:03:45.365370 10854 solver.cpp:398]     Test net output #0: accuracy = 0.9913
I0408 13:03:45.365414 10854 solver.cpp:398]     Test net output #1: loss = 0.0299292 (* 1 = 0.0299292 loss)
I0408 13:03:45.470974 10854 solver.cpp:219] Iteration 9000 (5.59924 iter/s, 17.8596s/100 iters), loss = 0.0168967
I0408 13:03:45.471021 10854 solver.cpp:238]     Train net output #0: loss = 0.0168965 (* 1 = 0.0168965 loss)
I0408 13:03:45.471029 10854 sgd_solver.cpp:105] Iteration 9000, lr = 0.00617924
I0408 13:03:54.769940 10854 solver.cpp:219] Iteration 9100 (10.754 iter/s, 9.2989s/100 iters), loss = 0.00494775
I0408 13:03:54.769987 10854 solver.cpp:238]     Train net output #0: loss = 0.00494758 (* 1 = 0.00494758 loss)
I0408 13:03:54.769991 10854 sgd_solver.cpp:105] Iteration 9100, lr = 0.00615496
I0408 13:04:04.235105 10854 solver.cpp:219] Iteration 9200 (10.5651 iter/s, 9.4651s/100 iters), loss = 0.00362648
I0408 13:04:04.235306 10854 solver.cpp:238]     Train net output #0: loss = 0.0036263 (* 1 = 0.0036263 loss)
I0408 13:04:04.235328 10854 sgd_solver.cpp:105] Iteration 9200, lr = 0.0061309
I0408 13:04:13.586642 10854 solver.cpp:219] Iteration 9300 (10.6936 iter/s, 9.35135s/100 iters), loss = 0.0050007
I0408 13:04:13.586693 10854 solver.cpp:238]     Train net output #0: loss = 0.00500052 (* 1 = 0.00500052 loss)
I0408 13:04:13.586697 10854 sgd_solver.cpp:105] Iteration 9300, lr = 0.00610706
I0408 13:04:20.100284 10861 data_layer.cpp:73] Restarting data prefetching from start.
I0408 13:04:22.912796 10854 solver.cpp:219] Iteration 9400 (10.7226 iter/s, 9.32608s/100 iters), loss = 0.0538755
I0408 13:04:22.912837 10854 solver.cpp:238]     Train net output #0: loss = 0.0538753 (* 1 = 0.0538753 loss)
I0408 13:04:22.912842 10854 sgd_solver.cpp:105] Iteration 9400, lr = 0.00608343
I0408 13:04:32.332156 10854 solver.cpp:331] Iteration 9500, Testing net (#0)
I0408 13:04:39.627007 10862 data_layer.cpp:73] Restarting data prefetching from start.
I0408 13:04:39.939215 10854 solver.cpp:398]     Test net output #0: accuracy = 0.9895
I0408 13:04:39.939256 10854 solver.cpp:398]     Test net output #1: loss = 0.0352542 (* 1 = 0.0352542 loss)
I0408 13:04:40.045677 10854 solver.cpp:219] Iteration 9500 (5.83675 iter/s, 17.1328s/100 iters), loss = 0.00377928
I0408 13:04:40.045738 10854 solver.cpp:238]     Train net output #0: loss = 0.0037791 (* 1 = 0.0037791 loss)
I0408 13:04:40.045758 10854 sgd_solver.cpp:105] Iteration 9500, lr = 0.00606002
I0408 13:04:49.593277 10854 solver.cpp:219] Iteration 9600 (10.4748 iter/s, 9.54673s/100 iters), loss = 0.00423106
I0408 13:04:49.593323 10854 solver.cpp:238]     Train net output #0: loss = 0.00423087 (* 1 = 0.00423087 loss)
I0408 13:04:49.593328 10854 sgd_solver.cpp:105] Iteration 9600, lr = 0.00603682
I0408 13:04:59.281491 10854 solver.cpp:219] Iteration 9700 (10.3219 iter/s, 9.68814s/100 iters), loss = 0.00564942
I0408 13:04:59.281541 10854 solver.cpp:238]     Train net output #0: loss = 0.00564923 (* 1 = 0.00564923 loss)
I0408 13:04:59.281548 10854 sgd_solver.cpp:105] Iteration 9700, lr = 0.00601382
I0408 13:05:08.777622 10854 solver.cpp:219] Iteration 9800 (10.5307 iter/s, 9.49606s/100 iters), loss = 0.0166943
I0408 13:05:08.777684 10854 solver.cpp:238]     Train net output #0: loss = 0.0166941 (* 1 = 0.0166941 loss)
I0408 13:05:08.777689 10854 sgd_solver.cpp:105] Iteration 9800, lr = 0.00599102
I0408 13:05:18.125097 10854 solver.cpp:219] Iteration 9900 (10.6982 iter/s, 9.34739s/100 iters), loss = 0.00596401
I0408 13:05:18.125210 10854 solver.cpp:238]     Train net output #0: loss = 0.00596382 (* 1 = 0.00596382 loss)
I0408 13:05:18.125216 10854 sgd_solver.cpp:105] Iteration 9900, lr = 0.00596843
I0408 13:05:27.349584 10854 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_10000.caffemodel
I0408 13:05:27.404821 10854 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_10000.solverstate
I0408 13:05:27.468389 10854 solver.cpp:311] Iteration 10000, loss = 0.00227272
I0408 13:05:27.468410 10854 solver.cpp:331] Iteration 10000, Testing net (#0)
I0408 13:05:34.762866 10862 data_layer.cpp:73] Restarting data prefetching from start.
I0408 13:05:35.080703 10854 solver.cpp:398]     Test net output #0: accuracy = 0.9905
I0408 13:05:35.080745 10854 solver.cpp:398]     Test net output #1: loss = 0.0306559 (* 1 = 0.0306559 loss)
I0408 13:05:35.080750 10854 solver.cpp:316] Optimization Done.
I0408 13:05:35.080752 10854 caffe.cpp:259] Optimization Done.
