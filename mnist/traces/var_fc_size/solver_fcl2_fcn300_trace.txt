I0408 12:10:11.152755 10414 caffe.cpp:218] Using GPUs 0
I0408 12:10:11.167469 10414 caffe.cpp:223] GPU 0: GeForce GTX 950M
I0408 12:10:11.346477 10414 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 10000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_fc_size/lenet_train_test_fcl2_fcn300.prototxt"
train_state {
  level: 0
  stage: ""
}
I0408 12:10:11.346657 10414 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_fc_size/lenet_train_test_fcl2_fcn300.prototxt
I0408 12:10:11.346853 10414 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0408 12:10:11.346863 10414 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0408 12:10:11.346957 10414 net.cpp:51] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0408 12:10:11.347051 10414 layer_factory.hpp:77] Creating layer mnist
I0408 12:10:11.347157 10414 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0408 12:10:11.347193 10414 net.cpp:84] Creating Layer mnist
I0408 12:10:11.347240 10414 net.cpp:380] mnist -> data
I0408 12:10:11.347256 10414 net.cpp:380] mnist -> label
I0408 12:10:11.347790 10414 data_layer.cpp:45] output data size: 64,1,28,28
I0408 12:10:11.349278 10414 net.cpp:122] Setting up mnist
I0408 12:10:11.349289 10414 net.cpp:129] Top shape: 64 1 28 28 (50176)
I0408 12:10:11.349292 10414 net.cpp:129] Top shape: 64 (64)
I0408 12:10:11.349293 10414 net.cpp:137] Memory required for data: 200960
I0408 12:10:11.349299 10414 layer_factory.hpp:77] Creating layer conv0
I0408 12:10:11.349310 10414 net.cpp:84] Creating Layer conv0
I0408 12:10:11.349344 10414 net.cpp:406] conv0 <- data
I0408 12:10:11.349354 10414 net.cpp:380] conv0 -> conv0
I0408 12:10:11.350354 10414 net.cpp:122] Setting up conv0
I0408 12:10:11.350381 10414 net.cpp:129] Top shape: 64 50 24 24 (1843200)
I0408 12:10:11.350384 10414 net.cpp:137] Memory required for data: 7573760
I0408 12:10:11.350430 10414 layer_factory.hpp:77] Creating layer pool0
I0408 12:10:11.350451 10414 net.cpp:84] Creating Layer pool0
I0408 12:10:11.350453 10414 net.cpp:406] pool0 <- conv0
I0408 12:10:11.350474 10414 net.cpp:380] pool0 -> pool0
I0408 12:10:11.350548 10414 net.cpp:122] Setting up pool0
I0408 12:10:11.350553 10414 net.cpp:129] Top shape: 64 50 12 12 (460800)
I0408 12:10:11.350570 10414 net.cpp:137] Memory required for data: 9416960
I0408 12:10:11.350571 10414 layer_factory.hpp:77] Creating layer conv1
I0408 12:10:11.350597 10414 net.cpp:84] Creating Layer conv1
I0408 12:10:11.350600 10414 net.cpp:406] conv1 <- pool0
I0408 12:10:11.350603 10414 net.cpp:380] conv1 -> conv1
I0408 12:10:11.351425 10414 net.cpp:122] Setting up conv1
I0408 12:10:11.351433 10414 net.cpp:129] Top shape: 64 50 8 8 (204800)
I0408 12:10:11.351436 10414 net.cpp:137] Memory required for data: 10236160
I0408 12:10:11.351460 10414 layer_factory.hpp:77] Creating layer pool1
I0408 12:10:11.351470 10414 net.cpp:84] Creating Layer pool1
I0408 12:10:11.351472 10414 net.cpp:406] pool1 <- conv1
I0408 12:10:11.351478 10414 net.cpp:380] pool1 -> pool1
I0408 12:10:11.351557 10414 net.cpp:122] Setting up pool1
I0408 12:10:11.351562 10414 net.cpp:129] Top shape: 64 50 4 4 (51200)
I0408 12:10:11.351563 10414 net.cpp:137] Memory required for data: 10440960
I0408 12:10:11.351567 10414 layer_factory.hpp:77] Creating layer ip1
I0408 12:10:11.351572 10414 net.cpp:84] Creating Layer ip1
I0408 12:10:11.351573 10414 net.cpp:406] ip1 <- pool1
I0408 12:10:11.351577 10414 net.cpp:380] ip1 -> ip1
I0408 12:10:11.353202 10414 net.cpp:122] Setting up ip1
I0408 12:10:11.353210 10414 net.cpp:129] Top shape: 64 300 (19200)
I0408 12:10:11.353230 10414 net.cpp:137] Memory required for data: 10517760
I0408 12:10:11.353237 10414 layer_factory.hpp:77] Creating layer relu1
I0408 12:10:11.353241 10414 net.cpp:84] Creating Layer relu1
I0408 12:10:11.353245 10414 net.cpp:406] relu1 <- ip1
I0408 12:10:11.353248 10414 net.cpp:367] relu1 -> ip1 (in-place)
I0408 12:10:11.353272 10414 net.cpp:122] Setting up relu1
I0408 12:10:11.353276 10414 net.cpp:129] Top shape: 64 300 (19200)
I0408 12:10:11.353278 10414 net.cpp:137] Memory required for data: 10594560
I0408 12:10:11.353279 10414 layer_factory.hpp:77] Creating layer ip2
I0408 12:10:11.353283 10414 net.cpp:84] Creating Layer ip2
I0408 12:10:11.353286 10414 net.cpp:406] ip2 <- ip1
I0408 12:10:11.353291 10414 net.cpp:380] ip2 -> ip2
I0408 12:10:11.353418 10414 net.cpp:122] Setting up ip2
I0408 12:10:11.353422 10414 net.cpp:129] Top shape: 64 10 (640)
I0408 12:10:11.353425 10414 net.cpp:137] Memory required for data: 10597120
I0408 12:10:11.353428 10414 layer_factory.hpp:77] Creating layer relu2
I0408 12:10:11.353451 10414 net.cpp:84] Creating Layer relu2
I0408 12:10:11.353453 10414 net.cpp:406] relu2 <- ip2
I0408 12:10:11.353474 10414 net.cpp:367] relu2 -> ip2 (in-place)
I0408 12:10:11.353477 10414 net.cpp:122] Setting up relu2
I0408 12:10:11.353480 10414 net.cpp:129] Top shape: 64 10 (640)
I0408 12:10:11.353482 10414 net.cpp:137] Memory required for data: 10599680
I0408 12:10:11.353483 10414 layer_factory.hpp:77] Creating layer loss
I0408 12:10:11.353487 10414 net.cpp:84] Creating Layer loss
I0408 12:10:11.353489 10414 net.cpp:406] loss <- ip2
I0408 12:10:11.353492 10414 net.cpp:406] loss <- label
I0408 12:10:11.353497 10414 net.cpp:380] loss -> loss
I0408 12:10:11.353507 10414 layer_factory.hpp:77] Creating layer loss
I0408 12:10:11.353619 10414 net.cpp:122] Setting up loss
I0408 12:10:11.353624 10414 net.cpp:129] Top shape: (1)
I0408 12:10:11.353626 10414 net.cpp:132]     with loss weight 1
I0408 12:10:11.353659 10414 net.cpp:137] Memory required for data: 10599684
I0408 12:10:11.353662 10414 net.cpp:198] loss needs backward computation.
I0408 12:10:11.353667 10414 net.cpp:198] relu2 needs backward computation.
I0408 12:10:11.353669 10414 net.cpp:198] ip2 needs backward computation.
I0408 12:10:11.353672 10414 net.cpp:198] relu1 needs backward computation.
I0408 12:10:11.353672 10414 net.cpp:198] ip1 needs backward computation.
I0408 12:10:11.353698 10414 net.cpp:198] pool1 needs backward computation.
I0408 12:10:11.353701 10414 net.cpp:198] conv1 needs backward computation.
I0408 12:10:11.353703 10414 net.cpp:198] pool0 needs backward computation.
I0408 12:10:11.353705 10414 net.cpp:198] conv0 needs backward computation.
I0408 12:10:11.353708 10414 net.cpp:200] mnist does not need backward computation.
I0408 12:10:11.353709 10414 net.cpp:242] This network produces output loss
I0408 12:10:11.353718 10414 net.cpp:255] Network initialization done.
I0408 12:10:11.353873 10414 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_fc_size/lenet_train_test_fcl2_fcn300.prototxt
I0408 12:10:11.353888 10414 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0408 12:10:11.353988 10414 net.cpp:51] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0408 12:10:11.354056 10414 layer_factory.hpp:77] Creating layer mnist
I0408 12:10:11.354110 10414 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0408 12:10:11.354120 10414 net.cpp:84] Creating Layer mnist
I0408 12:10:11.354123 10414 net.cpp:380] mnist -> data
I0408 12:10:11.354130 10414 net.cpp:380] mnist -> label
I0408 12:10:11.354207 10414 data_layer.cpp:45] output data size: 100,1,28,28
I0408 12:10:11.355741 10414 net.cpp:122] Setting up mnist
I0408 12:10:11.355753 10414 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0408 12:10:11.355756 10414 net.cpp:129] Top shape: 100 (100)
I0408 12:10:11.355757 10414 net.cpp:137] Memory required for data: 314000
I0408 12:10:11.355761 10414 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0408 12:10:11.355784 10414 net.cpp:84] Creating Layer label_mnist_1_split
I0408 12:10:11.355787 10414 net.cpp:406] label_mnist_1_split <- label
I0408 12:10:11.355809 10414 net.cpp:380] label_mnist_1_split -> label_mnist_1_split_0
I0408 12:10:11.355815 10414 net.cpp:380] label_mnist_1_split -> label_mnist_1_split_1
I0408 12:10:11.355949 10414 net.cpp:122] Setting up label_mnist_1_split
I0408 12:10:11.355953 10414 net.cpp:129] Top shape: 100 (100)
I0408 12:10:11.355957 10414 net.cpp:129] Top shape: 100 (100)
I0408 12:10:11.355957 10414 net.cpp:137] Memory required for data: 314800
I0408 12:10:11.355979 10414 layer_factory.hpp:77] Creating layer conv0
I0408 12:10:11.355988 10414 net.cpp:84] Creating Layer conv0
I0408 12:10:11.355991 10414 net.cpp:406] conv0 <- data
I0408 12:10:11.356012 10414 net.cpp:380] conv0 -> conv0
I0408 12:10:11.356222 10414 net.cpp:122] Setting up conv0
I0408 12:10:11.356228 10414 net.cpp:129] Top shape: 100 50 24 24 (2880000)
I0408 12:10:11.356245 10414 net.cpp:137] Memory required for data: 11834800
I0408 12:10:11.356251 10414 layer_factory.hpp:77] Creating layer pool0
I0408 12:10:11.356256 10414 net.cpp:84] Creating Layer pool0
I0408 12:10:11.356258 10414 net.cpp:406] pool0 <- conv0
I0408 12:10:11.356262 10414 net.cpp:380] pool0 -> pool0
I0408 12:10:11.356287 10414 net.cpp:122] Setting up pool0
I0408 12:10:11.356292 10414 net.cpp:129] Top shape: 100 50 12 12 (720000)
I0408 12:10:11.356294 10414 net.cpp:137] Memory required for data: 14714800
I0408 12:10:11.356297 10414 layer_factory.hpp:77] Creating layer conv1
I0408 12:10:11.356317 10414 net.cpp:84] Creating Layer conv1
I0408 12:10:11.356319 10414 net.cpp:406] conv1 <- pool0
I0408 12:10:11.356323 10414 net.cpp:380] conv1 -> conv1
I0408 12:10:11.356878 10414 net.cpp:122] Setting up conv1
I0408 12:10:11.356886 10414 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0408 12:10:11.356889 10414 net.cpp:137] Memory required for data: 15994800
I0408 12:10:11.356899 10414 layer_factory.hpp:77] Creating layer pool1
I0408 12:10:11.356906 10414 net.cpp:84] Creating Layer pool1
I0408 12:10:11.356910 10414 net.cpp:406] pool1 <- conv1
I0408 12:10:11.356916 10414 net.cpp:380] pool1 -> pool1
I0408 12:10:11.356962 10414 net.cpp:122] Setting up pool1
I0408 12:10:11.356967 10414 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0408 12:10:11.356971 10414 net.cpp:137] Memory required for data: 16314800
I0408 12:10:11.356974 10414 layer_factory.hpp:77] Creating layer ip1
I0408 12:10:11.356982 10414 net.cpp:84] Creating Layer ip1
I0408 12:10:11.356986 10414 net.cpp:406] ip1 <- pool1
I0408 12:10:11.356992 10414 net.cpp:380] ip1 -> ip1
I0408 12:10:11.358544 10414 net.cpp:122] Setting up ip1
I0408 12:10:11.358553 10414 net.cpp:129] Top shape: 100 300 (30000)
I0408 12:10:11.358556 10414 net.cpp:137] Memory required for data: 16434800
I0408 12:10:11.358566 10414 layer_factory.hpp:77] Creating layer relu1
I0408 12:10:11.358575 10414 net.cpp:84] Creating Layer relu1
I0408 12:10:11.358579 10414 net.cpp:406] relu1 <- ip1
I0408 12:10:11.358585 10414 net.cpp:367] relu1 -> ip1 (in-place)
I0408 12:10:11.358592 10414 net.cpp:122] Setting up relu1
I0408 12:10:11.358599 10414 net.cpp:129] Top shape: 100 300 (30000)
I0408 12:10:11.358603 10414 net.cpp:137] Memory required for data: 16554800
I0408 12:10:11.358605 10414 layer_factory.hpp:77] Creating layer ip2
I0408 12:10:11.358614 10414 net.cpp:84] Creating Layer ip2
I0408 12:10:11.358618 10414 net.cpp:406] ip2 <- ip1
I0408 12:10:11.358624 10414 net.cpp:380] ip2 -> ip2
I0408 12:10:11.358752 10414 net.cpp:122] Setting up ip2
I0408 12:10:11.358757 10414 net.cpp:129] Top shape: 100 10 (1000)
I0408 12:10:11.358762 10414 net.cpp:137] Memory required for data: 16558800
I0408 12:10:11.358767 10414 layer_factory.hpp:77] Creating layer relu2
I0408 12:10:11.358773 10414 net.cpp:84] Creating Layer relu2
I0408 12:10:11.358777 10414 net.cpp:406] relu2 <- ip2
I0408 12:10:11.358784 10414 net.cpp:367] relu2 -> ip2 (in-place)
I0408 12:10:11.358790 10414 net.cpp:122] Setting up relu2
I0408 12:10:11.358795 10414 net.cpp:129] Top shape: 100 10 (1000)
I0408 12:10:11.358798 10414 net.cpp:137] Memory required for data: 16562800
I0408 12:10:11.358803 10414 layer_factory.hpp:77] Creating layer ip2_relu2_0_split
I0408 12:10:11.358808 10414 net.cpp:84] Creating Layer ip2_relu2_0_split
I0408 12:10:11.358811 10414 net.cpp:406] ip2_relu2_0_split <- ip2
I0408 12:10:11.358815 10414 net.cpp:380] ip2_relu2_0_split -> ip2_relu2_0_split_0
I0408 12:10:11.358835 10414 net.cpp:380] ip2_relu2_0_split -> ip2_relu2_0_split_1
I0408 12:10:11.358878 10414 net.cpp:122] Setting up ip2_relu2_0_split
I0408 12:10:11.358883 10414 net.cpp:129] Top shape: 100 10 (1000)
I0408 12:10:11.358887 10414 net.cpp:129] Top shape: 100 10 (1000)
I0408 12:10:11.358891 10414 net.cpp:137] Memory required for data: 16570800
I0408 12:10:11.358893 10414 layer_factory.hpp:77] Creating layer accuracy
I0408 12:10:11.358901 10414 net.cpp:84] Creating Layer accuracy
I0408 12:10:11.358906 10414 net.cpp:406] accuracy <- ip2_relu2_0_split_0
I0408 12:10:11.358911 10414 net.cpp:406] accuracy <- label_mnist_1_split_0
I0408 12:10:11.358930 10414 net.cpp:380] accuracy -> accuracy
I0408 12:10:11.358940 10414 net.cpp:122] Setting up accuracy
I0408 12:10:11.358945 10414 net.cpp:129] Top shape: (1)
I0408 12:10:11.358948 10414 net.cpp:137] Memory required for data: 16570804
I0408 12:10:11.358952 10414 layer_factory.hpp:77] Creating layer loss
I0408 12:10:11.358956 10414 net.cpp:84] Creating Layer loss
I0408 12:10:11.358960 10414 net.cpp:406] loss <- ip2_relu2_0_split_1
I0408 12:10:11.358965 10414 net.cpp:406] loss <- label_mnist_1_split_1
I0408 12:10:11.358971 10414 net.cpp:380] loss -> loss
I0408 12:10:11.358979 10414 layer_factory.hpp:77] Creating layer loss
I0408 12:10:11.359061 10414 net.cpp:122] Setting up loss
I0408 12:10:11.359066 10414 net.cpp:129] Top shape: (1)
I0408 12:10:11.359069 10414 net.cpp:132]     with loss weight 1
I0408 12:10:11.359079 10414 net.cpp:137] Memory required for data: 16570808
I0408 12:10:11.359082 10414 net.cpp:198] loss needs backward computation.
I0408 12:10:11.359087 10414 net.cpp:200] accuracy does not need backward computation.
I0408 12:10:11.359091 10414 net.cpp:198] ip2_relu2_0_split needs backward computation.
I0408 12:10:11.359096 10414 net.cpp:198] relu2 needs backward computation.
I0408 12:10:11.359098 10414 net.cpp:198] ip2 needs backward computation.
I0408 12:10:11.359102 10414 net.cpp:198] relu1 needs backward computation.
I0408 12:10:11.359105 10414 net.cpp:198] ip1 needs backward computation.
I0408 12:10:11.359122 10414 net.cpp:198] pool1 needs backward computation.
I0408 12:10:11.359124 10414 net.cpp:198] conv1 needs backward computation.
I0408 12:10:11.359128 10414 net.cpp:198] pool0 needs backward computation.
I0408 12:10:11.359132 10414 net.cpp:198] conv0 needs backward computation.
I0408 12:10:11.359135 10414 net.cpp:200] label_mnist_1_split does not need backward computation.
I0408 12:10:11.359139 10414 net.cpp:200] mnist does not need backward computation.
I0408 12:10:11.359143 10414 net.cpp:242] This network produces output accuracy
I0408 12:10:11.359148 10414 net.cpp:242] This network produces output loss
I0408 12:10:11.359161 10414 net.cpp:255] Network initialization done.
I0408 12:10:11.359208 10414 solver.cpp:56] Solver scaffolding done.
I0408 12:10:11.359413 10414 caffe.cpp:248] Starting Optimization
I0408 12:10:11.359417 10414 solver.cpp:273] Solving LeNet
I0408 12:10:11.359421 10414 solver.cpp:274] Learning Rate Policy: inv
I0408 12:10:11.360282 10414 solver.cpp:331] Iteration 0, Testing net (#0)
I0408 12:10:18.651468 10422 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:10:18.971379 10414 solver.cpp:398]     Test net output #0: accuracy = 0.1316
I0408 12:10:18.971401 10414 solver.cpp:398]     Test net output #1: loss = 2.31634 (* 1 = 2.31634 loss)
I0408 12:10:19.079176 10414 solver.cpp:219] Iteration 0 (0 iter/s, 7.71972s/100 iters), loss = 2.31791
I0408 12:10:19.079205 10414 solver.cpp:238]     Train net output #0: loss = 2.31791 (* 1 = 2.31791 loss)
I0408 12:10:19.079244 10414 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0408 12:10:28.382994 10414 solver.cpp:219] Iteration 100 (10.7483 iter/s, 9.30377s/100 iters), loss = 0.555721
I0408 12:10:28.383026 10414 solver.cpp:238]     Train net output #0: loss = 0.555721 (* 1 = 0.555721 loss)
I0408 12:10:28.383034 10414 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0408 12:10:37.715195 10414 solver.cpp:219] Iteration 200 (10.7156 iter/s, 9.33215s/100 iters), loss = 0.16903
I0408 12:10:37.715241 10414 solver.cpp:238]     Train net output #0: loss = 0.16903 (* 1 = 0.16903 loss)
I0408 12:10:37.715266 10414 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0408 12:10:47.037549 10414 solver.cpp:219] Iteration 300 (10.727 iter/s, 9.32229s/100 iters), loss = 0.159852
I0408 12:10:47.037605 10414 solver.cpp:238]     Train net output #0: loss = 0.159852 (* 1 = 0.159852 loss)
I0408 12:10:47.037631 10414 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0408 12:10:56.813984 10414 solver.cpp:219] Iteration 400 (10.2288 iter/s, 9.77637s/100 iters), loss = 0.0742307
I0408 12:10:56.814016 10414 solver.cpp:238]     Train net output #0: loss = 0.0742307 (* 1 = 0.0742307 loss)
I0408 12:10:56.814040 10414 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0408 12:11:08.058395 10414 solver.cpp:331] Iteration 500, Testing net (#0)
I0408 12:11:15.463871 10422 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:11:15.772621 10414 solver.cpp:398]     Test net output #0: accuracy = 0.975
I0408 12:11:15.772663 10414 solver.cpp:398]     Test net output #1: loss = 0.0780896 (* 1 = 0.0780896 loss)
I0408 12:11:15.875457 10414 solver.cpp:219] Iteration 500 (5.2462 iter/s, 19.0614s/100 iters), loss = 0.092256
I0408 12:11:15.875501 10414 solver.cpp:238]     Train net output #0: loss = 0.092256 (* 1 = 0.092256 loss)
I0408 12:11:15.875521 10414 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0408 12:11:25.929074 10414 solver.cpp:219] Iteration 600 (9.94673 iter/s, 10.0536s/100 iters), loss = 0.08789
I0408 12:11:25.929239 10414 solver.cpp:238]     Train net output #0: loss = 0.08789 (* 1 = 0.08789 loss)
I0408 12:11:25.929244 10414 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0408 12:11:35.376730 10414 solver.cpp:219] Iteration 700 (10.5849 iter/s, 9.44745s/100 iters), loss = 0.1184
I0408 12:11:35.376818 10414 solver.cpp:238]     Train net output #0: loss = 0.1184 (* 1 = 0.1184 loss)
I0408 12:11:35.376845 10414 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0408 12:11:44.864758 10414 solver.cpp:219] Iteration 800 (10.5418 iter/s, 9.48604s/100 iters), loss = 0.17705
I0408 12:11:44.864821 10414 solver.cpp:238]     Train net output #0: loss = 0.17705 (* 1 = 0.17705 loss)
I0408 12:11:44.864827 10414 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0408 12:11:54.233286 10414 solver.cpp:219] Iteration 900 (10.6741 iter/s, 9.36843s/100 iters), loss = 0.133289
I0408 12:11:54.233332 10414 solver.cpp:238]     Train net output #0: loss = 0.133289 (* 1 = 0.133289 loss)
I0408 12:11:54.233340 10414 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0408 12:11:57.311808 10421 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:12:03.446235 10414 solver.cpp:331] Iteration 1000, Testing net (#0)
I0408 12:12:10.765633 10422 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:12:11.075021 10414 solver.cpp:398]     Test net output #0: accuracy = 0.9828
I0408 12:12:11.075043 10414 solver.cpp:398]     Test net output #1: loss = 0.0542773 (* 1 = 0.0542773 loss)
I0408 12:12:11.179433 10414 solver.cpp:219] Iteration 1000 (5.90107 iter/s, 16.9461s/100 iters), loss = 0.0850165
I0408 12:12:11.179486 10414 solver.cpp:238]     Train net output #0: loss = 0.0850166 (* 1 = 0.0850166 loss)
I0408 12:12:11.179512 10414 sgd_solver.cpp:105] Iteration 1000, lr = 0.00931012
I0408 12:12:20.517109 10414 solver.cpp:219] Iteration 1100 (10.7094 iter/s, 9.33761s/100 iters), loss = 0.00431718
I0408 12:12:20.517139 10414 solver.cpp:238]     Train net output #0: loss = 0.00431724 (* 1 = 0.00431724 loss)
I0408 12:12:20.517145 10414 sgd_solver.cpp:105] Iteration 1100, lr = 0.00924715
I0408 12:12:30.023746 10414 solver.cpp:219] Iteration 1200 (10.519 iter/s, 9.50658s/100 iters), loss = 0.0396432
I0408 12:12:30.023890 10414 solver.cpp:238]     Train net output #0: loss = 0.0396432 (* 1 = 0.0396432 loss)
I0408 12:12:30.023895 10414 sgd_solver.cpp:105] Iteration 1200, lr = 0.00918515
I0408 12:12:39.465071 10414 solver.cpp:219] Iteration 1300 (10.5919 iter/s, 9.44118s/100 iters), loss = 0.011864
I0408 12:12:39.465118 10414 solver.cpp:238]     Train net output #0: loss = 0.011864 (* 1 = 0.011864 loss)
I0408 12:12:39.465124 10414 sgd_solver.cpp:105] Iteration 1300, lr = 0.00912412
I0408 12:12:48.884353 10414 solver.cpp:219] Iteration 1400 (10.6166 iter/s, 9.41922s/100 iters), loss = 0.00577492
I0408 12:12:48.884415 10414 solver.cpp:238]     Train net output #0: loss = 0.00577499 (* 1 = 0.00577499 loss)
I0408 12:12:48.884421 10414 sgd_solver.cpp:105] Iteration 1400, lr = 0.00906403
I0408 12:12:58.173430 10414 solver.cpp:331] Iteration 1500, Testing net (#0)
I0408 12:13:05.770846 10422 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:13:06.142508 10414 solver.cpp:398]     Test net output #0: accuracy = 0.9857
I0408 12:13:06.142534 10414 solver.cpp:398]     Test net output #1: loss = 0.0467504 (* 1 = 0.0467504 loss)
I0408 12:13:06.260260 10414 solver.cpp:219] Iteration 1500 (5.75512 iter/s, 17.3758s/100 iters), loss = 0.0745595
I0408 12:13:06.260314 10414 solver.cpp:238]     Train net output #0: loss = 0.0745596 (* 1 = 0.0745596 loss)
I0408 12:13:06.260324 10414 sgd_solver.cpp:105] Iteration 1500, lr = 0.00900485
I0408 12:13:16.103334 10414 solver.cpp:219] Iteration 1600 (10.1595 iter/s, 9.84301s/100 iters), loss = 0.112881
I0408 12:13:16.103365 10414 solver.cpp:238]     Train net output #0: loss = 0.112881 (* 1 = 0.112881 loss)
I0408 12:13:16.103374 10414 sgd_solver.cpp:105] Iteration 1600, lr = 0.00894657
I0408 12:13:25.678949 10414 solver.cpp:219] Iteration 1700 (10.4432 iter/s, 9.57557s/100 iters), loss = 0.0156975
I0408 12:13:25.678998 10414 solver.cpp:238]     Train net output #0: loss = 0.0156976 (* 1 = 0.0156976 loss)
I0408 12:13:25.679004 10414 sgd_solver.cpp:105] Iteration 1700, lr = 0.00888916
I0408 12:13:35.524127 10414 solver.cpp:219] Iteration 1800 (10.1573 iter/s, 9.84512s/100 iters), loss = 0.0213752
I0408 12:13:35.524173 10414 solver.cpp:238]     Train net output #0: loss = 0.0213753 (* 1 = 0.0213753 loss)
I0408 12:13:35.524193 10414 sgd_solver.cpp:105] Iteration 1800, lr = 0.0088326
I0408 12:13:42.211149 10421 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:13:45.034044 10414 solver.cpp:219] Iteration 1900 (10.5154 iter/s, 9.50986s/100 iters), loss = 0.109367
I0408 12:13:45.034073 10414 solver.cpp:238]     Train net output #0: loss = 0.109367 (* 1 = 0.109367 loss)
I0408 12:13:45.034080 10414 sgd_solver.cpp:105] Iteration 1900, lr = 0.00877687
I0408 12:13:54.301007 10414 solver.cpp:331] Iteration 2000, Testing net (#0)
I0408 12:14:01.737390 10422 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:14:02.054719 10414 solver.cpp:398]     Test net output #0: accuracy = 0.9868
I0408 12:14:02.054744 10414 solver.cpp:398]     Test net output #1: loss = 0.0403183 (* 1 = 0.0403183 loss)
I0408 12:14:02.159550 10414 solver.cpp:219] Iteration 2000 (5.83926 iter/s, 17.1255s/100 iters), loss = 0.00944732
I0408 12:14:02.159581 10414 solver.cpp:238]     Train net output #0: loss = 0.00944743 (* 1 = 0.00944743 loss)
I0408 12:14:02.159588 10414 sgd_solver.cpp:105] Iteration 2000, lr = 0.00872196
I0408 12:14:11.624964 10414 solver.cpp:219] Iteration 2100 (10.5648 iter/s, 9.46537s/100 iters), loss = 0.0196405
I0408 12:14:11.625010 10414 solver.cpp:238]     Train net output #0: loss = 0.0196406 (* 1 = 0.0196406 loss)
I0408 12:14:11.625016 10414 sgd_solver.cpp:105] Iteration 2100, lr = 0.00866784
I0408 12:14:21.070185 10414 solver.cpp:219] Iteration 2200 (10.5874 iter/s, 9.44516s/100 iters), loss = 0.0203965
I0408 12:14:21.070338 10414 solver.cpp:238]     Train net output #0: loss = 0.0203966 (* 1 = 0.0203966 loss)
I0408 12:14:21.070344 10414 sgd_solver.cpp:105] Iteration 2200, lr = 0.0086145
I0408 12:14:30.628849 10414 solver.cpp:219] Iteration 2300 (10.4619 iter/s, 9.55851s/100 iters), loss = 0.0839749
I0408 12:14:30.628907 10414 solver.cpp:238]     Train net output #0: loss = 0.083975 (* 1 = 0.083975 loss)
I0408 12:14:30.628913 10414 sgd_solver.cpp:105] Iteration 2300, lr = 0.00856192
I0408 12:14:40.143641 10414 solver.cpp:219] Iteration 2400 (10.51 iter/s, 9.51472s/100 iters), loss = 0.00919496
I0408 12:14:40.143672 10414 solver.cpp:238]     Train net output #0: loss = 0.00919505 (* 1 = 0.00919505 loss)
I0408 12:14:40.143679 10414 sgd_solver.cpp:105] Iteration 2400, lr = 0.00851008
I0408 12:14:49.538141 10414 solver.cpp:331] Iteration 2500, Testing net (#0)
I0408 12:14:57.048765 10422 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:14:57.372305 10414 solver.cpp:398]     Test net output #0: accuracy = 0.9848
I0408 12:14:57.372354 10414 solver.cpp:398]     Test net output #1: loss = 0.0468027 (* 1 = 0.0468027 loss)
I0408 12:14:57.476274 10414 solver.cpp:219] Iteration 2500 (5.76948 iter/s, 17.3326s/100 iters), loss = 0.0200167
I0408 12:14:57.476320 10414 solver.cpp:238]     Train net output #0: loss = 0.0200168 (* 1 = 0.0200168 loss)
I0408 12:14:57.476326 10414 sgd_solver.cpp:105] Iteration 2500, lr = 0.00845897
I0408 12:15:06.966939 10414 solver.cpp:219] Iteration 2600 (10.5367 iter/s, 9.49061s/100 iters), loss = 0.0561591
I0408 12:15:06.966976 10414 solver.cpp:238]     Train net output #0: loss = 0.0561592 (* 1 = 0.0561592 loss)
I0408 12:15:06.966984 10414 sgd_solver.cpp:105] Iteration 2600, lr = 0.00840857
I0408 12:15:16.467239 10414 solver.cpp:219] Iteration 2700 (10.5261 iter/s, 9.50021s/100 iters), loss = 0.04663
I0408 12:15:16.467286 10414 solver.cpp:238]     Train net output #0: loss = 0.0466301 (* 1 = 0.0466301 loss)
I0408 12:15:16.467293 10414 sgd_solver.cpp:105] Iteration 2700, lr = 0.00835886
I0408 12:15:26.037611 10414 solver.cpp:219] Iteration 2800 (10.4491 iter/s, 9.57021s/100 iters), loss = 0.00100952
I0408 12:15:26.037664 10414 solver.cpp:238]     Train net output #0: loss = 0.0010096 (* 1 = 0.0010096 loss)
I0408 12:15:26.037672 10414 sgd_solver.cpp:105] Iteration 2800, lr = 0.00830984
I0408 12:15:26.807049 10421 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:15:35.550019 10414 solver.cpp:219] Iteration 2900 (10.5128 iter/s, 9.51223s/100 iters), loss = 0.0130827
I0408 12:15:35.550205 10414 solver.cpp:238]     Train net output #0: loss = 0.0130828 (* 1 = 0.0130828 loss)
I0408 12:15:35.550211 10414 sgd_solver.cpp:105] Iteration 2900, lr = 0.00826148
I0408 12:15:44.831944 10414 solver.cpp:331] Iteration 3000, Testing net (#0)
I0408 12:15:52.237990 10422 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:15:52.555965 10414 solver.cpp:398]     Test net output #0: accuracy = 0.9877
I0408 12:15:52.556025 10414 solver.cpp:398]     Test net output #1: loss = 0.0355107 (* 1 = 0.0355107 loss)
I0408 12:15:52.662377 10414 solver.cpp:219] Iteration 3000 (5.84386 iter/s, 17.112s/100 iters), loss = 0.0204319
I0408 12:15:52.662425 10414 solver.cpp:238]     Train net output #0: loss = 0.020432 (* 1 = 0.020432 loss)
I0408 12:15:52.662431 10414 sgd_solver.cpp:105] Iteration 3000, lr = 0.00821377
I0408 12:16:02.100569 10414 solver.cpp:219] Iteration 3100 (10.5954 iter/s, 9.43803s/100 iters), loss = 0.00664851
I0408 12:16:02.100605 10414 solver.cpp:238]     Train net output #0: loss = 0.0066486 (* 1 = 0.0066486 loss)
I0408 12:16:02.100630 10414 sgd_solver.cpp:105] Iteration 3100, lr = 0.0081667
I0408 12:16:11.537204 10414 solver.cpp:219] Iteration 3200 (10.5972 iter/s, 9.4365s/100 iters), loss = 0.00584415
I0408 12:16:11.537443 10414 solver.cpp:238]     Train net output #0: loss = 0.00584425 (* 1 = 0.00584425 loss)
I0408 12:16:11.537451 10414 sgd_solver.cpp:105] Iteration 3200, lr = 0.00812025
I0408 12:16:20.976929 10414 solver.cpp:219] Iteration 3300 (10.5939 iter/s, 9.43941s/100 iters), loss = 0.0138433
I0408 12:16:20.976958 10414 solver.cpp:238]     Train net output #0: loss = 0.0138435 (* 1 = 0.0138435 loss)
I0408 12:16:20.976963 10414 sgd_solver.cpp:105] Iteration 3300, lr = 0.00807442
I0408 12:16:30.425410 10414 solver.cpp:219] Iteration 3400 (10.5839 iter/s, 9.44835s/100 iters), loss = 0.0097279
I0408 12:16:30.425459 10414 solver.cpp:238]     Train net output #0: loss = 0.00972801 (* 1 = 0.00972801 loss)
I0408 12:16:30.425467 10414 sgd_solver.cpp:105] Iteration 3400, lr = 0.00802918
I0408 12:16:39.876137 10414 solver.cpp:331] Iteration 3500, Testing net (#0)
I0408 12:16:47.347527 10422 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:16:47.664032 10414 solver.cpp:398]     Test net output #0: accuracy = 0.9881
I0408 12:16:47.664055 10414 solver.cpp:398]     Test net output #1: loss = 0.0366428 (* 1 = 0.0366428 loss)
I0408 12:16:47.767318 10414 solver.cpp:219] Iteration 3500 (5.76645 iter/s, 17.3417s/100 iters), loss = 0.00511308
I0408 12:16:47.767379 10414 solver.cpp:238]     Train net output #0: loss = 0.00511318 (* 1 = 0.00511318 loss)
I0408 12:16:47.767386 10414 sgd_solver.cpp:105] Iteration 3500, lr = 0.00798454
I0408 12:16:57.251792 10414 solver.cpp:219] Iteration 3600 (10.5437 iter/s, 9.48432s/100 iters), loss = 0.0278818
I0408 12:16:57.251842 10414 solver.cpp:238]     Train net output #0: loss = 0.0278819 (* 1 = 0.0278819 loss)
I0408 12:16:57.251849 10414 sgd_solver.cpp:105] Iteration 3600, lr = 0.00794046
I0408 12:17:06.651562 10414 solver.cpp:219] Iteration 3700 (10.6387 iter/s, 9.39962s/100 iters), loss = 0.0176337
I0408 12:17:06.651612 10414 solver.cpp:238]     Train net output #0: loss = 0.0176339 (* 1 = 0.0176339 loss)
I0408 12:17:06.651617 10414 sgd_solver.cpp:105] Iteration 3700, lr = 0.00789695
I0408 12:17:10.926491 10421 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:17:16.116343 10414 solver.cpp:219] Iteration 3800 (10.5656 iter/s, 9.46464s/100 iters), loss = 0.00621035
I0408 12:17:16.116394 10414 solver.cpp:238]     Train net output #0: loss = 0.00621046 (* 1 = 0.00621046 loss)
I0408 12:17:16.116400 10414 sgd_solver.cpp:105] Iteration 3800, lr = 0.007854
I0408 12:17:25.536655 10414 solver.cpp:219] Iteration 3900 (10.6155 iter/s, 9.42017s/100 iters), loss = 0.0176592
I0408 12:17:25.536810 10414 solver.cpp:238]     Train net output #0: loss = 0.0176593 (* 1 = 0.0176593 loss)
I0408 12:17:25.536834 10414 sgd_solver.cpp:105] Iteration 3900, lr = 0.00781158
I0408 12:17:34.817080 10414 solver.cpp:331] Iteration 4000, Testing net (#0)
I0408 12:17:42.188724 10422 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:17:42.502959 10414 solver.cpp:398]     Test net output #0: accuracy = 0.9903
I0408 12:17:42.503000 10414 solver.cpp:398]     Test net output #1: loss = 0.0309439 (* 1 = 0.0309439 loss)
I0408 12:17:42.608389 10414 solver.cpp:219] Iteration 4000 (5.85774 iter/s, 17.0714s/100 iters), loss = 0.0170025
I0408 12:17:42.608436 10414 solver.cpp:238]     Train net output #0: loss = 0.0170026 (* 1 = 0.0170026 loss)
I0408 12:17:42.608455 10414 sgd_solver.cpp:105] Iteration 4000, lr = 0.0077697
I0408 12:17:51.967229 10414 solver.cpp:219] Iteration 4100 (10.6852 iter/s, 9.35871s/100 iters), loss = 0.0213148
I0408 12:17:51.967277 10414 solver.cpp:238]     Train net output #0: loss = 0.0213149 (* 1 = 0.0213149 loss)
I0408 12:17:51.967283 10414 sgd_solver.cpp:105] Iteration 4100, lr = 0.00772833
I0408 12:18:01.385262 10414 solver.cpp:219] Iteration 4200 (10.6181 iter/s, 9.41789s/100 iters), loss = 0.00744528
I0408 12:18:01.385453 10414 solver.cpp:238]     Train net output #0: loss = 0.00744537 (* 1 = 0.00744537 loss)
I0408 12:18:01.385460 10414 sgd_solver.cpp:105] Iteration 4200, lr = 0.00768748
I0408 12:18:11.018028 10414 solver.cpp:219] Iteration 4300 (10.3815 iter/s, 9.63249s/100 iters), loss = 0.0353505
I0408 12:18:11.018075 10414 solver.cpp:238]     Train net output #0: loss = 0.0353506 (* 1 = 0.0353506 loss)
I0408 12:18:11.018080 10414 sgd_solver.cpp:105] Iteration 4300, lr = 0.00764712
I0408 12:18:20.406533 10414 solver.cpp:219] Iteration 4400 (10.6515 iter/s, 9.38837s/100 iters), loss = 0.0253839
I0408 12:18:20.406563 10414 solver.cpp:238]     Train net output #0: loss = 0.025384 (* 1 = 0.025384 loss)
I0408 12:18:20.406569 10414 sgd_solver.cpp:105] Iteration 4400, lr = 0.00760726
I0408 12:18:29.900730 10414 solver.cpp:331] Iteration 4500, Testing net (#0)
I0408 12:18:37.371384 10422 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:18:37.686499 10414 solver.cpp:398]     Test net output #0: accuracy = 0.989
I0408 12:18:37.686523 10414 solver.cpp:398]     Test net output #1: loss = 0.0344028 (* 1 = 0.0344028 loss)
I0408 12:18:37.791203 10414 solver.cpp:219] Iteration 4500 (5.75225 iter/s, 17.3845s/100 iters), loss = 0.00442716
I0408 12:18:37.791247 10414 solver.cpp:238]     Train net output #0: loss = 0.00442725 (* 1 = 0.00442725 loss)
I0408 12:18:37.791254 10414 sgd_solver.cpp:105] Iteration 4500, lr = 0.00756788
I0408 12:18:47.209327 10414 solver.cpp:219] Iteration 4600 (10.618 iter/s, 9.418s/100 iters), loss = 0.0222216
I0408 12:18:47.209380 10414 solver.cpp:238]     Train net output #0: loss = 0.0222217 (* 1 = 0.0222217 loss)
I0408 12:18:47.209388 10414 sgd_solver.cpp:105] Iteration 4600, lr = 0.00752897
I0408 12:18:55.159584 10421 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:18:56.767380 10414 solver.cpp:219] Iteration 4700 (10.4625 iter/s, 9.55792s/100 iters), loss = 0.00329509
I0408 12:18:56.767415 10414 solver.cpp:238]     Train net output #0: loss = 0.00329518 (* 1 = 0.00329518 loss)
I0408 12:18:56.767438 10414 sgd_solver.cpp:105] Iteration 4700, lr = 0.00749052
I0408 12:19:06.227051 10414 solver.cpp:219] Iteration 4800 (10.5713 iter/s, 9.45956s/100 iters), loss = 0.0209476
I0408 12:19:06.227098 10414 solver.cpp:238]     Train net output #0: loss = 0.0209476 (* 1 = 0.0209476 loss)
I0408 12:19:06.227118 10414 sgd_solver.cpp:105] Iteration 4800, lr = 0.00745253
I0408 12:19:15.591121 10414 solver.cpp:219] Iteration 4900 (10.6793 iter/s, 9.36395s/100 iters), loss = 0.00519801
I0408 12:19:15.591264 10414 solver.cpp:238]     Train net output #0: loss = 0.00519808 (* 1 = 0.00519808 loss)
I0408 12:19:15.591269 10414 sgd_solver.cpp:105] Iteration 4900, lr = 0.00741498
I0408 12:19:24.898061 10414 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_5000.caffemodel
I0408 12:19:24.956418 10414 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_5000.solverstate
I0408 12:19:24.958289 10414 solver.cpp:331] Iteration 5000, Testing net (#0)
I0408 12:19:32.311703 10422 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:19:32.621201 10414 solver.cpp:398]     Test net output #0: accuracy = 0.9897
I0408 12:19:32.621227 10414 solver.cpp:398]     Test net output #1: loss = 0.0296553 (* 1 = 0.0296553 loss)
I0408 12:19:32.725036 10414 solver.cpp:219] Iteration 5000 (5.83647 iter/s, 17.1336s/100 iters), loss = 0.021759
I0408 12:19:32.725067 10414 solver.cpp:238]     Train net output #0: loss = 0.0217591 (* 1 = 0.0217591 loss)
I0408 12:19:32.725093 10414 sgd_solver.cpp:105] Iteration 5000, lr = 0.00737788
I0408 12:19:42.172127 10414 solver.cpp:219] Iteration 5100 (10.5854 iter/s, 9.44699s/100 iters), loss = 0.022488
I0408 12:19:42.172160 10414 solver.cpp:238]     Train net output #0: loss = 0.0224881 (* 1 = 0.0224881 loss)
I0408 12:19:42.172168 10414 sgd_solver.cpp:105] Iteration 5100, lr = 0.0073412
I0408 12:19:51.627610 10414 solver.cpp:219] Iteration 5200 (10.576 iter/s, 9.45538s/100 iters), loss = 0.00963433
I0408 12:19:51.627751 10414 solver.cpp:238]     Train net output #0: loss = 0.00963442 (* 1 = 0.00963442 loss)
I0408 12:19:51.627774 10414 sgd_solver.cpp:105] Iteration 5200, lr = 0.00730495
I0408 12:20:01.074079 10414 solver.cpp:219] Iteration 5300 (10.5862 iter/s, 9.44626s/100 iters), loss = 0.00223378
I0408 12:20:01.074141 10414 solver.cpp:238]     Train net output #0: loss = 0.00223388 (* 1 = 0.00223388 loss)
I0408 12:20:01.074162 10414 sgd_solver.cpp:105] Iteration 5300, lr = 0.00726911
I0408 12:20:10.457783 10414 solver.cpp:219] Iteration 5400 (10.6569 iter/s, 9.38357s/100 iters), loss = 0.00741517
I0408 12:20:10.457814 10414 solver.cpp:238]     Train net output #0: loss = 0.00741526 (* 1 = 0.00741526 loss)
I0408 12:20:10.457819 10414 sgd_solver.cpp:105] Iteration 5400, lr = 0.00723368
I0408 12:20:19.754899 10414 solver.cpp:331] Iteration 5500, Testing net (#0)
I0408 12:20:27.130909 10422 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:20:27.442616 10414 solver.cpp:398]     Test net output #0: accuracy = 0.9907
I0408 12:20:27.442674 10414 solver.cpp:398]     Test net output #1: loss = 0.0304831 (* 1 = 0.0304831 loss)
I0408 12:20:27.545801 10414 solver.cpp:219] Iteration 5500 (5.85218 iter/s, 17.0877s/100 iters), loss = 0.00931997
I0408 12:20:27.545847 10414 solver.cpp:238]     Train net output #0: loss = 0.00932006 (* 1 = 0.00932006 loss)
I0408 12:20:27.545853 10414 sgd_solver.cpp:105] Iteration 5500, lr = 0.00719865
I0408 12:20:37.002272 10414 solver.cpp:219] Iteration 5600 (10.5749 iter/s, 9.45636s/100 iters), loss = 0.000632754
I0408 12:20:37.002320 10414 solver.cpp:238]     Train net output #0: loss = 0.000632842 (* 1 = 0.000632842 loss)
I0408 12:20:37.002326 10414 sgd_solver.cpp:105] Iteration 5600, lr = 0.00716402
I0408 12:20:38.885535 10421 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:20:46.430012 10414 solver.cpp:219] Iteration 5700 (10.6071 iter/s, 9.42761s/100 iters), loss = 0.00342396
I0408 12:20:46.430042 10414 solver.cpp:238]     Train net output #0: loss = 0.00342405 (* 1 = 0.00342405 loss)
I0408 12:20:46.430066 10414 sgd_solver.cpp:105] Iteration 5700, lr = 0.00712977
I0408 12:20:55.809705 10414 solver.cpp:219] Iteration 5800 (10.6614 iter/s, 9.3796s/100 iters), loss = 0.0234055
I0408 12:20:55.809751 10414 solver.cpp:238]     Train net output #0: loss = 0.0234056 (* 1 = 0.0234056 loss)
I0408 12:20:55.809756 10414 sgd_solver.cpp:105] Iteration 5800, lr = 0.0070959
I0408 12:21:05.138948 10414 solver.cpp:219] Iteration 5900 (10.7191 iter/s, 9.32913s/100 iters), loss = 0.00445072
I0408 12:21:05.139174 10414 solver.cpp:238]     Train net output #0: loss = 0.00445082 (* 1 = 0.00445082 loss)
I0408 12:21:05.139183 10414 sgd_solver.cpp:105] Iteration 5900, lr = 0.0070624
I0408 12:21:14.379256 10414 solver.cpp:331] Iteration 6000, Testing net (#0)
I0408 12:21:21.878037 10422 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:21:22.191620 10414 solver.cpp:398]     Test net output #0: accuracy = 0.9914
I0408 12:21:22.191643 10414 solver.cpp:398]     Test net output #1: loss = 0.0287485 (* 1 = 0.0287485 loss)
I0408 12:21:22.298583 10414 solver.cpp:219] Iteration 6000 (5.82774 iter/s, 17.1593s/100 iters), loss = 0.00490636
I0408 12:21:22.298629 10414 solver.cpp:238]     Train net output #0: loss = 0.00490646 (* 1 = 0.00490646 loss)
I0408 12:21:22.298635 10414 sgd_solver.cpp:105] Iteration 6000, lr = 0.00702927
I0408 12:21:31.820025 10414 solver.cpp:219] Iteration 6100 (10.5027 iter/s, 9.52133s/100 iters), loss = 0.00126168
I0408 12:21:31.820075 10414 solver.cpp:238]     Train net output #0: loss = 0.00126178 (* 1 = 0.00126178 loss)
I0408 12:21:31.820080 10414 sgd_solver.cpp:105] Iteration 6100, lr = 0.0069965
I0408 12:21:41.274276 10414 solver.cpp:219] Iteration 6200 (10.5774 iter/s, 9.45413s/100 iters), loss = 0.00937155
I0408 12:21:41.274437 10414 solver.cpp:238]     Train net output #0: loss = 0.00937166 (* 1 = 0.00937166 loss)
I0408 12:21:41.274458 10414 sgd_solver.cpp:105] Iteration 6200, lr = 0.00696408
I0408 12:21:50.826671 10414 solver.cpp:219] Iteration 6300 (10.4688 iter/s, 9.55219s/100 iters), loss = 0.00498444
I0408 12:21:50.826709 10414 solver.cpp:238]     Train net output #0: loss = 0.00498454 (* 1 = 0.00498454 loss)
I0408 12:21:50.826715 10414 sgd_solver.cpp:105] Iteration 6300, lr = 0.00693201
I0408 12:22:00.521204 10414 solver.cpp:219] Iteration 6400 (10.3152 iter/s, 9.69444s/100 iters), loss = 0.00597822
I0408 12:22:00.521251 10414 solver.cpp:238]     Train net output #0: loss = 0.00597832 (* 1 = 0.00597832 loss)
I0408 12:22:00.521257 10414 sgd_solver.cpp:105] Iteration 6400, lr = 0.00690029
I0408 12:22:10.006419 10414 solver.cpp:331] Iteration 6500, Testing net (#0)
I0408 12:22:17.403419 10422 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:22:17.722831 10414 solver.cpp:398]     Test net output #0: accuracy = 0.9912
I0408 12:22:17.722856 10414 solver.cpp:398]     Test net output #1: loss = 0.0303949 (* 1 = 0.0303949 loss)
I0408 12:22:17.831032 10414 solver.cpp:219] Iteration 6500 (5.77711 iter/s, 17.3097s/100 iters), loss = 0.0120179
I0408 12:22:17.831080 10414 solver.cpp:238]     Train net output #0: loss = 0.012018 (* 1 = 0.012018 loss)
I0408 12:22:17.831100 10414 sgd_solver.cpp:105] Iteration 6500, lr = 0.0068689
I0408 12:22:23.289348 10421 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:22:27.240761 10414 solver.cpp:219] Iteration 6600 (10.6274 iter/s, 9.40962s/100 iters), loss = 0.029766
I0408 12:22:27.240809 10414 solver.cpp:238]     Train net output #0: loss = 0.0297661 (* 1 = 0.0297661 loss)
I0408 12:22:27.240814 10414 sgd_solver.cpp:105] Iteration 6600, lr = 0.00683784
I0408 12:22:36.730685 10414 solver.cpp:219] Iteration 6700 (10.5376 iter/s, 9.48982s/100 iters), loss = 0.0109631
I0408 12:22:36.730742 10414 solver.cpp:238]     Train net output #0: loss = 0.0109632 (* 1 = 0.0109632 loss)
I0408 12:22:36.730764 10414 sgd_solver.cpp:105] Iteration 6700, lr = 0.00680711
I0408 12:22:46.348930 10414 solver.cpp:219] Iteration 6800 (10.397 iter/s, 9.61813s/100 iters), loss = 0.00290106
I0408 12:22:46.348963 10414 solver.cpp:238]     Train net output #0: loss = 0.00290117 (* 1 = 0.00290117 loss)
I0408 12:22:46.348968 10414 sgd_solver.cpp:105] Iteration 6800, lr = 0.0067767
I0408 12:22:55.669174 10414 solver.cpp:219] Iteration 6900 (10.7294 iter/s, 9.32016s/100 iters), loss = 0.00410908
I0408 12:22:55.669332 10414 solver.cpp:238]     Train net output #0: loss = 0.00410919 (* 1 = 0.00410919 loss)
I0408 12:22:55.669359 10414 sgd_solver.cpp:105] Iteration 6900, lr = 0.0067466
I0408 12:23:04.837407 10414 solver.cpp:331] Iteration 7000, Testing net (#0)
I0408 12:23:12.130244 10422 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:23:12.446460 10414 solver.cpp:398]     Test net output #0: accuracy = 0.9906
I0408 12:23:12.446502 10414 solver.cpp:398]     Test net output #1: loss = 0.03053 (* 1 = 0.03053 loss)
I0408 12:23:12.552937 10414 solver.cpp:219] Iteration 7000 (5.92294 iter/s, 16.8835s/100 iters), loss = 0.00373304
I0408 12:23:12.552987 10414 solver.cpp:238]     Train net output #0: loss = 0.00373316 (* 1 = 0.00373316 loss)
I0408 12:23:12.552994 10414 sgd_solver.cpp:105] Iteration 7000, lr = 0.00671681
I0408 12:23:21.936038 10414 solver.cpp:219] Iteration 7100 (10.6578 iter/s, 9.38278s/100 iters), loss = 0.0145773
I0408 12:23:21.936072 10414 solver.cpp:238]     Train net output #0: loss = 0.0145775 (* 1 = 0.0145775 loss)
I0408 12:23:21.936096 10414 sgd_solver.cpp:105] Iteration 7100, lr = 0.00668733
I0408 12:23:31.313941 10414 solver.cpp:219] Iteration 7200 (10.6635 iter/s, 9.37782s/100 iters), loss = 0.00335774
I0408 12:23:31.314081 10414 solver.cpp:238]     Train net output #0: loss = 0.00335786 (* 1 = 0.00335786 loss)
I0408 12:23:31.314087 10414 sgd_solver.cpp:105] Iteration 7200, lr = 0.00665815
I0408 12:23:40.766522 10414 solver.cpp:219] Iteration 7300 (10.5793 iter/s, 9.4524s/100 iters), loss = 0.0189409
I0408 12:23:40.766559 10414 solver.cpp:238]     Train net output #0: loss = 0.0189411 (* 1 = 0.0189411 loss)
I0408 12:23:40.766584 10414 sgd_solver.cpp:105] Iteration 7300, lr = 0.00662927
I0408 12:23:50.068944 10414 solver.cpp:219] Iteration 7400 (10.75 iter/s, 9.30234s/100 iters), loss = 0.00650779
I0408 12:23:50.068991 10414 solver.cpp:238]     Train net output #0: loss = 0.00650791 (* 1 = 0.00650791 loss)
I0408 12:23:50.068996 10414 sgd_solver.cpp:105] Iteration 7400, lr = 0.00660067
I0408 12:23:58.917141 10421 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:23:59.259423 10414 solver.cpp:331] Iteration 7500, Testing net (#0)
I0408 12:24:06.656244 10422 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:24:06.968749 10414 solver.cpp:398]     Test net output #0: accuracy = 0.9903
I0408 12:24:06.968794 10414 solver.cpp:398]     Test net output #1: loss = 0.031503 (* 1 = 0.031503 loss)
I0408 12:24:07.073199 10414 solver.cpp:219] Iteration 7500 (5.88092 iter/s, 17.0041s/100 iters), loss = 0.00173258
I0408 12:24:07.073246 10414 solver.cpp:238]     Train net output #0: loss = 0.0017327 (* 1 = 0.0017327 loss)
I0408 12:24:07.073251 10414 sgd_solver.cpp:105] Iteration 7500, lr = 0.00657236
I0408 12:24:16.430588 10414 solver.cpp:219] Iteration 7600 (10.6869 iter/s, 9.35729s/100 iters), loss = 0.00709498
I0408 12:24:16.430634 10414 solver.cpp:238]     Train net output #0: loss = 0.00709509 (* 1 = 0.00709509 loss)
I0408 12:24:16.430639 10414 sgd_solver.cpp:105] Iteration 7600, lr = 0.00654433
I0408 12:24:25.781738 10414 solver.cpp:219] Iteration 7700 (10.694 iter/s, 9.35106s/100 iters), loss = 0.0227147
I0408 12:24:25.781793 10414 solver.cpp:238]     Train net output #0: loss = 0.0227148 (* 1 = 0.0227148 loss)
I0408 12:24:25.781800 10414 sgd_solver.cpp:105] Iteration 7700, lr = 0.00651658
I0408 12:24:35.200841 10414 solver.cpp:219] Iteration 7800 (10.6168 iter/s, 9.419s/100 iters), loss = 0.00265229
I0408 12:24:35.200887 10414 solver.cpp:238]     Train net output #0: loss = 0.0026524 (* 1 = 0.0026524 loss)
I0408 12:24:35.200894 10414 sgd_solver.cpp:105] Iteration 7800, lr = 0.00648911
I0408 12:24:44.564118 10414 solver.cpp:219] Iteration 7900 (10.6801 iter/s, 9.36319s/100 iters), loss = 0.00294302
I0408 12:24:44.564216 10414 solver.cpp:238]     Train net output #0: loss = 0.00294313 (* 1 = 0.00294313 loss)
I0408 12:24:44.564225 10414 sgd_solver.cpp:105] Iteration 7900, lr = 0.0064619
I0408 12:24:53.908165 10414 solver.cpp:331] Iteration 8000, Testing net (#0)
I0408 12:25:01.361325 10422 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:25:01.682670 10414 solver.cpp:398]     Test net output #0: accuracy = 0.9901
I0408 12:25:01.682693 10414 solver.cpp:398]     Test net output #1: loss = 0.0303028 (* 1 = 0.0303028 loss)
I0408 12:25:01.788962 10414 solver.cpp:219] Iteration 8000 (5.80563 iter/s, 17.2247s/100 iters), loss = 0.00748745
I0408 12:25:01.789011 10414 solver.cpp:238]     Train net output #0: loss = 0.00748756 (* 1 = 0.00748756 loss)
I0408 12:25:01.789018 10414 sgd_solver.cpp:105] Iteration 8000, lr = 0.00643496
I0408 12:25:11.362265 10414 solver.cpp:219] Iteration 8100 (10.4458 iter/s, 9.5732s/100 iters), loss = 0.0118028
I0408 12:25:11.362334 10414 solver.cpp:238]     Train net output #0: loss = 0.0118029 (* 1 = 0.0118029 loss)
I0408 12:25:11.362341 10414 sgd_solver.cpp:105] Iteration 8100, lr = 0.00640827
I0408 12:25:20.883162 10414 solver.cpp:219] Iteration 8200 (10.5033 iter/s, 9.5208s/100 iters), loss = 0.00618082
I0408 12:25:20.883322 10414 solver.cpp:238]     Train net output #0: loss = 0.00618092 (* 1 = 0.00618092 loss)
I0408 12:25:20.883327 10414 sgd_solver.cpp:105] Iteration 8200, lr = 0.00638185
I0408 12:25:30.359058 10414 solver.cpp:219] Iteration 8300 (10.5533 iter/s, 9.4757s/100 iters), loss = 0.0236159
I0408 12:25:30.359104 10414 solver.cpp:238]     Train net output #0: loss = 0.023616 (* 1 = 0.023616 loss)
I0408 12:25:30.359109 10414 sgd_solver.cpp:105] Iteration 8300, lr = 0.00635567
I0408 12:25:39.716651 10414 solver.cpp:219] Iteration 8400 (10.6866 iter/s, 9.3575s/100 iters), loss = 0.00498211
I0408 12:25:39.716697 10414 solver.cpp:238]     Train net output #0: loss = 0.0049822 (* 1 = 0.0049822 loss)
I0408 12:25:39.716717 10414 sgd_solver.cpp:105] Iteration 8400, lr = 0.00632975
I0408 12:25:42.803441 10421 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:25:49.008852 10414 solver.cpp:331] Iteration 8500, Testing net (#0)
I0408 12:25:56.480382 10422 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:25:56.791748 10414 solver.cpp:398]     Test net output #0: accuracy = 0.9906
I0408 12:25:56.791790 10414 solver.cpp:398]     Test net output #1: loss = 0.0275856 (* 1 = 0.0275856 loss)
I0408 12:25:56.894338 10414 solver.cpp:219] Iteration 8500 (5.82154 iter/s, 17.1776s/100 iters), loss = 0.0055205
I0408 12:25:56.894366 10414 solver.cpp:238]     Train net output #0: loss = 0.00552057 (* 1 = 0.00552057 loss)
I0408 12:25:56.894372 10414 sgd_solver.cpp:105] Iteration 8500, lr = 0.00630407
I0408 12:26:06.378479 10414 solver.cpp:219] Iteration 8600 (10.544 iter/s, 9.48407s/100 iters), loss = 0.00132749
I0408 12:26:06.378525 10414 solver.cpp:238]     Train net output #0: loss = 0.00132756 (* 1 = 0.00132756 loss)
I0408 12:26:06.378531 10414 sgd_solver.cpp:105] Iteration 8600, lr = 0.00627864
I0408 12:26:15.812994 10414 solver.cpp:219] Iteration 8700 (10.5995 iter/s, 9.43443s/100 iters), loss = 0.00247794
I0408 12:26:15.813024 10414 solver.cpp:238]     Train net output #0: loss = 0.00247802 (* 1 = 0.00247802 loss)
I0408 12:26:15.813030 10414 sgd_solver.cpp:105] Iteration 8700, lr = 0.00625344
I0408 12:26:25.503803 10414 solver.cpp:219] Iteration 8800 (10.3191 iter/s, 9.69073s/100 iters), loss = 0.00198308
I0408 12:26:25.503839 10414 solver.cpp:238]     Train net output #0: loss = 0.00198315 (* 1 = 0.00198315 loss)
I0408 12:26:25.503862 10414 sgd_solver.cpp:105] Iteration 8800, lr = 0.00622847
I0408 12:26:35.149338 10414 solver.cpp:219] Iteration 8900 (10.368 iter/s, 9.64505s/100 iters), loss = 0.000766361
I0408 12:26:35.149523 10414 solver.cpp:238]     Train net output #0: loss = 0.00076643 (* 1 = 0.00076643 loss)
I0408 12:26:35.149533 10414 sgd_solver.cpp:105] Iteration 8900, lr = 0.00620374
I0408 12:26:44.484444 10414 solver.cpp:331] Iteration 9000, Testing net (#0)
I0408 12:26:51.844725 10422 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:26:52.164763 10414 solver.cpp:398]     Test net output #0: accuracy = 0.9904
I0408 12:26:52.164788 10414 solver.cpp:398]     Test net output #1: loss = 0.0286448 (* 1 = 0.0286448 loss)
I0408 12:26:52.269867 10414 solver.cpp:219] Iteration 9000 (5.84102 iter/s, 17.1203s/100 iters), loss = 0.0156178
I0408 12:26:52.269914 10414 solver.cpp:238]     Train net output #0: loss = 0.0156179 (* 1 = 0.0156179 loss)
I0408 12:26:52.269920 10414 sgd_solver.cpp:105] Iteration 9000, lr = 0.00617924
I0408 12:27:01.614497 10414 solver.cpp:219] Iteration 9100 (10.7014 iter/s, 9.34455s/100 iters), loss = 0.00842544
I0408 12:27:01.614539 10414 solver.cpp:238]     Train net output #0: loss = 0.0084255 (* 1 = 0.0084255 loss)
I0408 12:27:01.614544 10414 sgd_solver.cpp:105] Iteration 9100, lr = 0.00615496
I0408 12:27:10.959810 10414 solver.cpp:219] Iteration 9200 (10.7006 iter/s, 9.34524s/100 iters), loss = 0.00393845
I0408 12:27:10.960016 10414 solver.cpp:238]     Train net output #0: loss = 0.00393851 (* 1 = 0.00393851 loss)
I0408 12:27:10.960022 10414 sgd_solver.cpp:105] Iteration 9200, lr = 0.0061309
I0408 12:27:20.401803 10414 solver.cpp:219] Iteration 9300 (10.5912 iter/s, 9.44176s/100 iters), loss = 0.00572767
I0408 12:27:20.401849 10414 solver.cpp:238]     Train net output #0: loss = 0.00572773 (* 1 = 0.00572773 loss)
I0408 12:27:20.401854 10414 sgd_solver.cpp:105] Iteration 9300, lr = 0.00610706
I0408 12:27:27.018378 10421 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:27:29.831228 10414 solver.cpp:219] Iteration 9400 (10.6052 iter/s, 9.42934s/100 iters), loss = 0.0216505
I0408 12:27:29.831257 10414 solver.cpp:238]     Train net output #0: loss = 0.0216506 (* 1 = 0.0216506 loss)
I0408 12:27:29.831262 10414 sgd_solver.cpp:105] Iteration 9400, lr = 0.00608343
I0408 12:27:39.276376 10414 solver.cpp:331] Iteration 9500, Testing net (#0)
I0408 12:27:46.777518 10422 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:27:47.097345 10414 solver.cpp:398]     Test net output #0: accuracy = 0.9891
I0408 12:27:47.097369 10414 solver.cpp:398]     Test net output #1: loss = 0.0345117 (* 1 = 0.0345117 loss)
I0408 12:27:47.205366 10414 solver.cpp:219] Iteration 9500 (5.75572 iter/s, 17.374s/100 iters), loss = 0.00373729
I0408 12:27:47.205431 10414 solver.cpp:238]     Train net output #0: loss = 0.00373733 (* 1 = 0.00373733 loss)
I0408 12:27:47.205440 10414 sgd_solver.cpp:105] Iteration 9500, lr = 0.00606002
I0408 12:27:56.806171 10414 solver.cpp:219] Iteration 9600 (10.4159 iter/s, 9.60072s/100 iters), loss = 0.00384419
I0408 12:27:56.806217 10414 solver.cpp:238]     Train net output #0: loss = 0.00384423 (* 1 = 0.00384423 loss)
I0408 12:27:56.806237 10414 sgd_solver.cpp:105] Iteration 9600, lr = 0.00603682
I0408 12:28:06.409690 10414 solver.cpp:219] Iteration 9700 (10.4129 iter/s, 9.60344s/100 iters), loss = 0.00281893
I0408 12:28:06.409739 10414 solver.cpp:238]     Train net output #0: loss = 0.00281897 (* 1 = 0.00281897 loss)
I0408 12:28:06.409744 10414 sgd_solver.cpp:105] Iteration 9700, lr = 0.00601382
I0408 12:28:16.040139 10414 solver.cpp:219] Iteration 9800 (10.3838 iter/s, 9.63036s/100 iters), loss = 0.0123818
I0408 12:28:16.040194 10414 solver.cpp:238]     Train net output #0: loss = 0.0123818 (* 1 = 0.0123818 loss)
I0408 12:28:16.040200 10414 sgd_solver.cpp:105] Iteration 9800, lr = 0.00599102
I0408 12:28:25.620905 10414 solver.cpp:219] Iteration 9900 (10.4377 iter/s, 9.5807s/100 iters), loss = 0.00581584
I0408 12:28:25.621054 10414 solver.cpp:238]     Train net output #0: loss = 0.00581588 (* 1 = 0.00581588 loss)
I0408 12:28:25.621062 10414 sgd_solver.cpp:105] Iteration 9900, lr = 0.00596843
I0408 12:28:34.959192 10414 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_10000.caffemodel
I0408 12:28:35.016016 10414 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_10000.solverstate
I0408 12:28:35.079694 10414 solver.cpp:311] Iteration 10000, loss = 0.00260079
I0408 12:28:35.079713 10414 solver.cpp:331] Iteration 10000, Testing net (#0)
I0408 12:28:42.422219 10422 data_layer.cpp:73] Restarting data prefetching from start.
I0408 12:28:42.740672 10414 solver.cpp:398]     Test net output #0: accuracy = 0.9913
I0408 12:28:42.740713 10414 solver.cpp:398]     Test net output #1: loss = 0.0277457 (* 1 = 0.0277457 loss)
I0408 12:28:42.740717 10414 solver.cpp:316] Optimization Done.
I0408 12:28:42.740720 10414 caffe.cpp:259] Optimization Done.
