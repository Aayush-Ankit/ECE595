I0408 15:22:02.176074 13563 caffe.cpp:218] Using GPUs 0
I0408 15:22:02.190644 13563 caffe.cpp:223] GPU 0: GeForce GTX 950M
I0408 15:22:02.372777 13563 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 10000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_num_feature/lenet_train_test_featuresize150.prototxt"
train_state {
  level: 0
  stage: ""
}
I0408 15:22:02.372958 13563 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_num_feature/lenet_train_test_featuresize150.prototxt
I0408 15:22:02.373183 13563 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0408 15:22:02.373194 13563 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0408 15:22:02.373293 13563 net.cpp:51] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 150
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 150
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0408 15:22:02.373399 13563 layer_factory.hpp:77] Creating layer mnist
I0408 15:22:02.373497 13563 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0408 15:22:02.373551 13563 net.cpp:84] Creating Layer mnist
I0408 15:22:02.373574 13563 net.cpp:380] mnist -> data
I0408 15:22:02.373613 13563 net.cpp:380] mnist -> label
I0408 15:22:02.374176 13563 data_layer.cpp:45] output data size: 64,1,28,28
I0408 15:22:02.375696 13563 net.cpp:122] Setting up mnist
I0408 15:22:02.375728 13563 net.cpp:129] Top shape: 64 1 28 28 (50176)
I0408 15:22:02.375733 13563 net.cpp:129] Top shape: 64 (64)
I0408 15:22:02.375735 13563 net.cpp:137] Memory required for data: 200960
I0408 15:22:02.375746 13563 layer_factory.hpp:77] Creating layer conv0
I0408 15:22:02.375763 13563 net.cpp:84] Creating Layer conv0
I0408 15:22:02.375769 13563 net.cpp:406] conv0 <- data
I0408 15:22:02.375782 13563 net.cpp:380] conv0 -> conv0
I0408 15:22:02.376760 13563 net.cpp:122] Setting up conv0
I0408 15:22:02.376770 13563 net.cpp:129] Top shape: 64 150 24 24 (5529600)
I0408 15:22:02.376773 13563 net.cpp:137] Memory required for data: 22319360
I0408 15:22:02.376822 13563 layer_factory.hpp:77] Creating layer pool0
I0408 15:22:02.376833 13563 net.cpp:84] Creating Layer pool0
I0408 15:22:02.376838 13563 net.cpp:406] pool0 <- conv0
I0408 15:22:02.376845 13563 net.cpp:380] pool0 -> pool0
I0408 15:22:02.376900 13563 net.cpp:122] Setting up pool0
I0408 15:22:02.376906 13563 net.cpp:129] Top shape: 64 150 12 12 (1382400)
I0408 15:22:02.376910 13563 net.cpp:137] Memory required for data: 27848960
I0408 15:22:02.376914 13563 layer_factory.hpp:77] Creating layer conv1
I0408 15:22:02.376922 13563 net.cpp:84] Creating Layer conv1
I0408 15:22:02.376927 13563 net.cpp:406] conv1 <- pool0
I0408 15:22:02.376935 13563 net.cpp:380] conv1 -> conv1
I0408 15:22:02.380228 13563 net.cpp:122] Setting up conv1
I0408 15:22:02.380237 13563 net.cpp:129] Top shape: 64 150 8 8 (614400)
I0408 15:22:02.380241 13563 net.cpp:137] Memory required for data: 30306560
I0408 15:22:02.380264 13563 layer_factory.hpp:77] Creating layer pool1
I0408 15:22:02.380273 13563 net.cpp:84] Creating Layer pool1
I0408 15:22:02.380295 13563 net.cpp:406] pool1 <- conv1
I0408 15:22:02.380316 13563 net.cpp:380] pool1 -> pool1
I0408 15:22:02.380358 13563 net.cpp:122] Setting up pool1
I0408 15:22:02.380364 13563 net.cpp:129] Top shape: 64 150 4 4 (153600)
I0408 15:22:02.380367 13563 net.cpp:137] Memory required for data: 30920960
I0408 15:22:02.380370 13563 layer_factory.hpp:77] Creating layer ip1
I0408 15:22:02.380390 13563 net.cpp:84] Creating Layer ip1
I0408 15:22:02.380394 13563 net.cpp:406] ip1 <- pool1
I0408 15:22:02.380401 13563 net.cpp:380] ip1 -> ip1
I0408 15:22:02.381985 13563 net.cpp:122] Setting up ip1
I0408 15:22:02.381994 13563 net.cpp:129] Top shape: 64 100 (6400)
I0408 15:22:02.381996 13563 net.cpp:137] Memory required for data: 30946560
I0408 15:22:02.382005 13563 layer_factory.hpp:77] Creating layer relu1
I0408 15:22:02.382012 13563 net.cpp:84] Creating Layer relu1
I0408 15:22:02.382035 13563 net.cpp:406] relu1 <- ip1
I0408 15:22:02.382057 13563 net.cpp:367] relu1 -> ip1 (in-place)
I0408 15:22:02.382064 13563 net.cpp:122] Setting up relu1
I0408 15:22:02.382087 13563 net.cpp:129] Top shape: 64 100 (6400)
I0408 15:22:02.382091 13563 net.cpp:137] Memory required for data: 30972160
I0408 15:22:02.382107 13563 layer_factory.hpp:77] Creating layer ip2
I0408 15:22:02.382112 13563 net.cpp:84] Creating Layer ip2
I0408 15:22:02.382130 13563 net.cpp:406] ip2 <- ip1
I0408 15:22:02.382138 13563 net.cpp:380] ip2 -> ip2
I0408 15:22:02.382247 13563 net.cpp:122] Setting up ip2
I0408 15:22:02.382252 13563 net.cpp:129] Top shape: 64 10 (640)
I0408 15:22:02.382256 13563 net.cpp:137] Memory required for data: 30974720
I0408 15:22:02.382262 13563 layer_factory.hpp:77] Creating layer relu2
I0408 15:22:02.382269 13563 net.cpp:84] Creating Layer relu2
I0408 15:22:02.382273 13563 net.cpp:406] relu2 <- ip2
I0408 15:22:02.382279 13563 net.cpp:367] relu2 -> ip2 (in-place)
I0408 15:22:02.382287 13563 net.cpp:122] Setting up relu2
I0408 15:22:02.382292 13563 net.cpp:129] Top shape: 64 10 (640)
I0408 15:22:02.382294 13563 net.cpp:137] Memory required for data: 30977280
I0408 15:22:02.382298 13563 layer_factory.hpp:77] Creating layer loss
I0408 15:22:02.382318 13563 net.cpp:84] Creating Layer loss
I0408 15:22:02.382321 13563 net.cpp:406] loss <- ip2
I0408 15:22:02.382329 13563 net.cpp:406] loss <- label
I0408 15:22:02.382336 13563 net.cpp:380] loss -> loss
I0408 15:22:02.382350 13563 layer_factory.hpp:77] Creating layer loss
I0408 15:22:02.382417 13563 net.cpp:122] Setting up loss
I0408 15:22:02.382422 13563 net.cpp:129] Top shape: (1)
I0408 15:22:02.382426 13563 net.cpp:132]     with loss weight 1
I0408 15:22:02.382441 13563 net.cpp:137] Memory required for data: 30977284
I0408 15:22:02.382444 13563 net.cpp:198] loss needs backward computation.
I0408 15:22:02.382464 13563 net.cpp:198] relu2 needs backward computation.
I0408 15:22:02.382470 13563 net.cpp:198] ip2 needs backward computation.
I0408 15:22:02.382473 13563 net.cpp:198] relu1 needs backward computation.
I0408 15:22:02.382477 13563 net.cpp:198] ip1 needs backward computation.
I0408 15:22:02.382488 13563 net.cpp:198] pool1 needs backward computation.
I0408 15:22:02.382493 13563 net.cpp:198] conv1 needs backward computation.
I0408 15:22:02.382498 13563 net.cpp:198] pool0 needs backward computation.
I0408 15:22:02.382501 13563 net.cpp:198] conv0 needs backward computation.
I0408 15:22:02.382505 13563 net.cpp:200] mnist does not need backward computation.
I0408 15:22:02.382509 13563 net.cpp:242] This network produces output loss
I0408 15:22:02.382519 13563 net.cpp:255] Network initialization done.
I0408 15:22:02.382663 13563 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_num_feature/lenet_train_test_featuresize150.prototxt
I0408 15:22:02.382683 13563 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0408 15:22:02.382760 13563 net.cpp:51] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 150
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 150
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0408 15:22:02.382845 13563 layer_factory.hpp:77] Creating layer mnist
I0408 15:22:02.382894 13563 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0408 15:22:02.382907 13563 net.cpp:84] Creating Layer mnist
I0408 15:22:02.382913 13563 net.cpp:380] mnist -> data
I0408 15:22:02.382921 13563 net.cpp:380] mnist -> label
I0408 15:22:02.382999 13563 data_layer.cpp:45] output data size: 100,1,28,28
I0408 15:22:02.384644 13563 net.cpp:122] Setting up mnist
I0408 15:22:02.384672 13563 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0408 15:22:02.384696 13563 net.cpp:129] Top shape: 100 (100)
I0408 15:22:02.384697 13563 net.cpp:137] Memory required for data: 314000
I0408 15:22:02.384701 13563 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0408 15:22:02.384711 13563 net.cpp:84] Creating Layer label_mnist_1_split
I0408 15:22:02.384714 13563 net.cpp:406] label_mnist_1_split <- label
I0408 15:22:02.384721 13563 net.cpp:380] label_mnist_1_split -> label_mnist_1_split_0
I0408 15:22:02.384747 13563 net.cpp:380] label_mnist_1_split -> label_mnist_1_split_1
I0408 15:22:02.384804 13563 net.cpp:122] Setting up label_mnist_1_split
I0408 15:22:02.384810 13563 net.cpp:129] Top shape: 100 (100)
I0408 15:22:02.384814 13563 net.cpp:129] Top shape: 100 (100)
I0408 15:22:02.384816 13563 net.cpp:137] Memory required for data: 314800
I0408 15:22:02.384821 13563 layer_factory.hpp:77] Creating layer conv0
I0408 15:22:02.384852 13563 net.cpp:84] Creating Layer conv0
I0408 15:22:02.384856 13563 net.cpp:406] conv0 <- data
I0408 15:22:02.384879 13563 net.cpp:380] conv0 -> conv0
I0408 15:22:02.385159 13563 net.cpp:122] Setting up conv0
I0408 15:22:02.385165 13563 net.cpp:129] Top shape: 100 150 24 24 (8640000)
I0408 15:22:02.385169 13563 net.cpp:137] Memory required for data: 34874800
I0408 15:22:02.385196 13563 layer_factory.hpp:77] Creating layer pool0
I0408 15:22:02.385202 13563 net.cpp:84] Creating Layer pool0
I0408 15:22:02.385205 13563 net.cpp:406] pool0 <- conv0
I0408 15:22:02.385215 13563 net.cpp:380] pool0 -> pool0
I0408 15:22:02.385267 13563 net.cpp:122] Setting up pool0
I0408 15:22:02.385273 13563 net.cpp:129] Top shape: 100 150 12 12 (2160000)
I0408 15:22:02.385277 13563 net.cpp:137] Memory required for data: 43514800
I0408 15:22:02.385279 13563 layer_factory.hpp:77] Creating layer conv1
I0408 15:22:02.385308 13563 net.cpp:84] Creating Layer conv1
I0408 15:22:02.385313 13563 net.cpp:406] conv1 <- pool0
I0408 15:22:02.385318 13563 net.cpp:380] conv1 -> conv1
I0408 15:22:02.390987 13563 net.cpp:122] Setting up conv1
I0408 15:22:02.391000 13563 net.cpp:129] Top shape: 100 150 8 8 (960000)
I0408 15:22:02.391003 13563 net.cpp:137] Memory required for data: 47354800
I0408 15:22:02.391010 13563 layer_factory.hpp:77] Creating layer pool1
I0408 15:22:02.391016 13563 net.cpp:84] Creating Layer pool1
I0408 15:22:02.391018 13563 net.cpp:406] pool1 <- conv1
I0408 15:22:02.391024 13563 net.cpp:380] pool1 -> pool1
I0408 15:22:02.391052 13563 net.cpp:122] Setting up pool1
I0408 15:22:02.391055 13563 net.cpp:129] Top shape: 100 150 4 4 (240000)
I0408 15:22:02.391057 13563 net.cpp:137] Memory required for data: 48314800
I0408 15:22:02.391060 13563 layer_factory.hpp:77] Creating layer ip1
I0408 15:22:02.391065 13563 net.cpp:84] Creating Layer ip1
I0408 15:22:02.391067 13563 net.cpp:406] ip1 <- pool1
I0408 15:22:02.391072 13563 net.cpp:380] ip1 -> ip1
I0408 15:22:02.392609 13563 net.cpp:122] Setting up ip1
I0408 15:22:02.392617 13563 net.cpp:129] Top shape: 100 100 (10000)
I0408 15:22:02.392638 13563 net.cpp:137] Memory required for data: 48354800
I0408 15:22:02.392644 13563 layer_factory.hpp:77] Creating layer relu1
I0408 15:22:02.392650 13563 net.cpp:84] Creating Layer relu1
I0408 15:22:02.392652 13563 net.cpp:406] relu1 <- ip1
I0408 15:22:02.392657 13563 net.cpp:367] relu1 -> ip1 (in-place)
I0408 15:22:02.392676 13563 net.cpp:122] Setting up relu1
I0408 15:22:02.392680 13563 net.cpp:129] Top shape: 100 100 (10000)
I0408 15:22:02.392681 13563 net.cpp:137] Memory required for data: 48394800
I0408 15:22:02.392683 13563 layer_factory.hpp:77] Creating layer ip2
I0408 15:22:02.392688 13563 net.cpp:84] Creating Layer ip2
I0408 15:22:02.392691 13563 net.cpp:406] ip2 <- ip1
I0408 15:22:02.392696 13563 net.cpp:380] ip2 -> ip2
I0408 15:22:02.392786 13563 net.cpp:122] Setting up ip2
I0408 15:22:02.392791 13563 net.cpp:129] Top shape: 100 10 (1000)
I0408 15:22:02.392812 13563 net.cpp:137] Memory required for data: 48398800
I0408 15:22:02.392814 13563 layer_factory.hpp:77] Creating layer relu2
I0408 15:22:02.392818 13563 net.cpp:84] Creating Layer relu2
I0408 15:22:02.392819 13563 net.cpp:406] relu2 <- ip2
I0408 15:22:02.392824 13563 net.cpp:367] relu2 -> ip2 (in-place)
I0408 15:22:02.392827 13563 net.cpp:122] Setting up relu2
I0408 15:22:02.392829 13563 net.cpp:129] Top shape: 100 10 (1000)
I0408 15:22:02.392832 13563 net.cpp:137] Memory required for data: 48402800
I0408 15:22:02.392833 13563 layer_factory.hpp:77] Creating layer ip2_relu2_0_split
I0408 15:22:02.392838 13563 net.cpp:84] Creating Layer ip2_relu2_0_split
I0408 15:22:02.392838 13563 net.cpp:406] ip2_relu2_0_split <- ip2
I0408 15:22:02.392853 13563 net.cpp:380] ip2_relu2_0_split -> ip2_relu2_0_split_0
I0408 15:22:02.392858 13563 net.cpp:380] ip2_relu2_0_split -> ip2_relu2_0_split_1
I0408 15:22:02.392899 13563 net.cpp:122] Setting up ip2_relu2_0_split
I0408 15:22:02.392902 13563 net.cpp:129] Top shape: 100 10 (1000)
I0408 15:22:02.392923 13563 net.cpp:129] Top shape: 100 10 (1000)
I0408 15:22:02.392925 13563 net.cpp:137] Memory required for data: 48410800
I0408 15:22:02.392928 13563 layer_factory.hpp:77] Creating layer accuracy
I0408 15:22:02.392933 13563 net.cpp:84] Creating Layer accuracy
I0408 15:22:02.392936 13563 net.cpp:406] accuracy <- ip2_relu2_0_split_0
I0408 15:22:02.392938 13563 net.cpp:406] accuracy <- label_mnist_1_split_0
I0408 15:22:02.392942 13563 net.cpp:380] accuracy -> accuracy
I0408 15:22:02.392947 13563 net.cpp:122] Setting up accuracy
I0408 15:22:02.392951 13563 net.cpp:129] Top shape: (1)
I0408 15:22:02.392953 13563 net.cpp:137] Memory required for data: 48410804
I0408 15:22:02.392956 13563 layer_factory.hpp:77] Creating layer loss
I0408 15:22:02.392958 13563 net.cpp:84] Creating Layer loss
I0408 15:22:02.392961 13563 net.cpp:406] loss <- ip2_relu2_0_split_1
I0408 15:22:02.392963 13563 net.cpp:406] loss <- label_mnist_1_split_1
I0408 15:22:02.392967 13563 net.cpp:380] loss -> loss
I0408 15:22:02.392985 13563 layer_factory.hpp:77] Creating layer loss
I0408 15:22:02.393046 13563 net.cpp:122] Setting up loss
I0408 15:22:02.393050 13563 net.cpp:129] Top shape: (1)
I0408 15:22:02.393052 13563 net.cpp:132]     with loss weight 1
I0408 15:22:02.393072 13563 net.cpp:137] Memory required for data: 48410808
I0408 15:22:02.393074 13563 net.cpp:198] loss needs backward computation.
I0408 15:22:02.393077 13563 net.cpp:200] accuracy does not need backward computation.
I0408 15:22:02.393098 13563 net.cpp:198] ip2_relu2_0_split needs backward computation.
I0408 15:22:02.393100 13563 net.cpp:198] relu2 needs backward computation.
I0408 15:22:02.393102 13563 net.cpp:198] ip2 needs backward computation.
I0408 15:22:02.393105 13563 net.cpp:198] relu1 needs backward computation.
I0408 15:22:02.393106 13563 net.cpp:198] ip1 needs backward computation.
I0408 15:22:02.393108 13563 net.cpp:198] pool1 needs backward computation.
I0408 15:22:02.393110 13563 net.cpp:198] conv1 needs backward computation.
I0408 15:22:02.393112 13563 net.cpp:198] pool0 needs backward computation.
I0408 15:22:02.393115 13563 net.cpp:198] conv0 needs backward computation.
I0408 15:22:02.393117 13563 net.cpp:200] label_mnist_1_split does not need backward computation.
I0408 15:22:02.393121 13563 net.cpp:200] mnist does not need backward computation.
I0408 15:22:02.393139 13563 net.cpp:242] This network produces output accuracy
I0408 15:22:02.393142 13563 net.cpp:242] This network produces output loss
I0408 15:22:02.393165 13563 net.cpp:255] Network initialization done.
I0408 15:22:02.393198 13563 solver.cpp:56] Solver scaffolding done.
I0408 15:22:02.393426 13563 caffe.cpp:248] Starting Optimization
I0408 15:22:02.393430 13563 solver.cpp:273] Solving LeNet
I0408 15:22:02.393432 13563 solver.cpp:274] Learning Rate Policy: inv
I0408 15:22:02.394282 13563 solver.cpp:331] Iteration 0, Testing net (#0)
I0408 15:22:13.919206 13572 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:22:14.411043 13563 solver.cpp:398]     Test net output #0: accuracy = 0.1243
I0408 15:22:14.411108 13563 solver.cpp:398]     Test net output #1: loss = 2.26844 (* 1 = 2.26844 loss)
I0408 15:22:14.638973 13563 solver.cpp:219] Iteration 0 (0 iter/s, 12.2455s/100 iters), loss = 2.29042
I0408 15:22:14.639091 13563 solver.cpp:238]     Train net output #0: loss = 2.29042 (* 1 = 2.29042 loss)
I0408 15:22:14.639140 13563 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0408 15:22:35.626473 13563 solver.cpp:219] Iteration 100 (4.76476 iter/s, 20.9874s/100 iters), loss = 0.184636
I0408 15:22:35.626708 13563 solver.cpp:238]     Train net output #0: loss = 0.184636 (* 1 = 0.184636 loss)
I0408 15:22:35.626732 13563 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0408 15:22:56.614624 13563 solver.cpp:219] Iteration 200 (4.76465 iter/s, 20.9879s/100 iters), loss = 0.166202
I0408 15:22:56.614679 13563 solver.cpp:238]     Train net output #0: loss = 0.166202 (* 1 = 0.166202 loss)
I0408 15:22:56.614684 13563 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0408 15:23:17.602543 13563 solver.cpp:219] Iteration 300 (4.76465 iter/s, 20.9879s/100 iters), loss = 0.181188
I0408 15:23:17.602721 13563 solver.cpp:238]     Train net output #0: loss = 0.181188 (* 1 = 0.181188 loss)
I0408 15:23:17.602727 13563 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0408 15:23:38.588476 13563 solver.cpp:219] Iteration 400 (4.76514 iter/s, 20.9858s/100 iters), loss = 0.030372
I0408 15:23:38.588503 13563 solver.cpp:238]     Train net output #0: loss = 0.030372 (* 1 = 0.030372 loss)
I0408 15:23:38.588508 13563 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0408 15:23:59.236918 13563 solver.cpp:331] Iteration 500, Testing net (#0)
I0408 15:24:10.869781 13572 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:24:11.359871 13563 solver.cpp:398]     Test net output #0: accuracy = 0.9749
I0408 15:24:11.359895 13563 solver.cpp:398]     Test net output #1: loss = 0.0796745 (* 1 = 0.0796745 loss)
I0408 15:24:11.579740 13563 solver.cpp:219] Iteration 500 (3.03111 iter/s, 32.9912s/100 iters), loss = 0.0964195
I0408 15:24:11.579766 13563 solver.cpp:238]     Train net output #0: loss = 0.0964195 (* 1 = 0.0964195 loss)
I0408 15:24:11.579788 13563 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0408 15:24:32.565699 13563 solver.cpp:219] Iteration 600 (4.76509 iter/s, 20.9859s/100 iters), loss = 0.0739209
I0408 15:24:32.565873 13563 solver.cpp:238]     Train net output #0: loss = 0.0739208 (* 1 = 0.0739208 loss)
I0408 15:24:32.565881 13563 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0408 15:24:53.550201 13563 solver.cpp:219] Iteration 700 (4.76546 iter/s, 20.9843s/100 iters), loss = 0.127678
I0408 15:24:53.550231 13563 solver.cpp:238]     Train net output #0: loss = 0.127678 (* 1 = 0.127678 loss)
I0408 15:24:53.550253 13563 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0408 15:25:14.553875 13563 solver.cpp:219] Iteration 800 (4.76108 iter/s, 21.0036s/100 iters), loss = 0.203661
I0408 15:25:14.554077 13563 solver.cpp:238]     Train net output #0: loss = 0.203661 (* 1 = 0.203661 loss)
I0408 15:25:14.554087 13563 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0408 15:25:35.538112 13563 solver.cpp:219] Iteration 900 (4.76552 iter/s, 20.9841s/100 iters), loss = 0.0850377
I0408 15:25:35.538139 13563 solver.cpp:238]     Train net output #0: loss = 0.0850377 (* 1 = 0.0850377 loss)
I0408 15:25:35.538144 13563 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0408 15:25:42.471907 13571 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:25:56.194103 13563 solver.cpp:331] Iteration 1000, Testing net (#0)
I0408 15:26:07.832202 13572 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:26:08.321898 13563 solver.cpp:398]     Test net output #0: accuracy = 0.9827
I0408 15:26:08.321923 13563 solver.cpp:398]     Test net output #1: loss = 0.0569107 (* 1 = 0.0569107 loss)
I0408 15:26:08.542572 13563 solver.cpp:219] Iteration 1000 (3.02989 iter/s, 33.0044s/100 iters), loss = 0.0877966
I0408 15:26:08.542615 13563 solver.cpp:238]     Train net output #0: loss = 0.0877965 (* 1 = 0.0877965 loss)
I0408 15:26:08.542634 13563 sgd_solver.cpp:105] Iteration 1000, lr = 0.00931012
I0408 15:26:29.527117 13563 solver.cpp:219] Iteration 1100 (4.76542 iter/s, 20.9845s/100 iters), loss = 0.00312396
I0408 15:26:29.527290 13563 solver.cpp:238]     Train net output #0: loss = 0.00312396 (* 1 = 0.00312396 loss)
I0408 15:26:29.527297 13563 sgd_solver.cpp:105] Iteration 1100, lr = 0.00924715
I0408 15:26:50.512421 13563 solver.cpp:219] Iteration 1200 (4.76527 iter/s, 20.9852s/100 iters), loss = 0.0128812
I0408 15:26:50.512467 13563 solver.cpp:238]     Train net output #0: loss = 0.0128812 (* 1 = 0.0128812 loss)
I0408 15:26:50.512473 13563 sgd_solver.cpp:105] Iteration 1200, lr = 0.00918515
I0408 15:27:11.510314 13563 solver.cpp:219] Iteration 1300 (4.76239 iter/s, 20.9979s/100 iters), loss = 0.0174636
I0408 15:27:11.510506 13563 solver.cpp:238]     Train net output #0: loss = 0.0174637 (* 1 = 0.0174637 loss)
I0408 15:27:11.510514 13563 sgd_solver.cpp:105] Iteration 1300, lr = 0.00912412
I0408 15:27:32.507580 13563 solver.cpp:219] Iteration 1400 (4.76257 iter/s, 20.9971s/100 iters), loss = 0.00352032
I0408 15:27:32.507607 13563 solver.cpp:238]     Train net output #0: loss = 0.00352035 (* 1 = 0.00352035 loss)
I0408 15:27:32.507612 13563 sgd_solver.cpp:105] Iteration 1400, lr = 0.00906403
I0408 15:27:53.151260 13563 solver.cpp:331] Iteration 1500, Testing net (#0)
I0408 15:28:04.787609 13572 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:28:05.276357 13563 solver.cpp:398]     Test net output #0: accuracy = 0.9854
I0408 15:28:05.276398 13563 solver.cpp:398]     Test net output #1: loss = 0.0435955 (* 1 = 0.0435955 loss)
I0408 15:28:05.496222 13563 solver.cpp:219] Iteration 1500 (3.03135 iter/s, 32.9886s/100 iters), loss = 0.0659344
I0408 15:28:05.496269 13563 solver.cpp:238]     Train net output #0: loss = 0.0659344 (* 1 = 0.0659344 loss)
I0408 15:28:05.496275 13563 sgd_solver.cpp:105] Iteration 1500, lr = 0.00900485
I0408 15:28:26.485692 13563 solver.cpp:219] Iteration 1600 (4.7643 iter/s, 20.9895s/100 iters), loss = 0.104574
I0408 15:28:26.485853 13563 solver.cpp:238]     Train net output #0: loss = 0.104574 (* 1 = 0.104574 loss)
I0408 15:28:26.485862 13563 sgd_solver.cpp:105] Iteration 1600, lr = 0.00894657
I0408 15:28:47.471527 13563 solver.cpp:219] Iteration 1700 (4.76515 iter/s, 20.9857s/100 iters), loss = 0.0176557
I0408 15:28:47.471555 13563 solver.cpp:238]     Train net output #0: loss = 0.0176557 (* 1 = 0.0176557 loss)
I0408 15:28:47.471560 13563 sgd_solver.cpp:105] Iteration 1700, lr = 0.00888916
I0408 15:29:08.475574 13563 solver.cpp:219] Iteration 1800 (4.76099 iter/s, 21.004s/100 iters), loss = 0.0107822
I0408 15:29:08.475615 13563 solver.cpp:238]     Train net output #0: loss = 0.0107823 (* 1 = 0.0107823 loss)
I0408 15:29:08.475620 13563 sgd_solver.cpp:105] Iteration 1800, lr = 0.0088326
I0408 15:29:23.175283 13571 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:29:29.477763 13563 solver.cpp:219] Iteration 1900 (4.76142 iter/s, 21.0022s/100 iters), loss = 0.107246
I0408 15:29:29.477792 13563 solver.cpp:238]     Train net output #0: loss = 0.107246 (* 1 = 0.107246 loss)
I0408 15:29:29.477795 13563 sgd_solver.cpp:105] Iteration 1900, lr = 0.00877687
I0408 15:29:50.123090 13563 solver.cpp:331] Iteration 2000, Testing net (#0)
I0408 15:30:01.762464 13572 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:30:02.253442 13563 solver.cpp:398]     Test net output #0: accuracy = 0.9879
I0408 15:30:02.253465 13563 solver.cpp:398]     Test net output #1: loss = 0.0385754 (* 1 = 0.0385754 loss)
I0408 15:30:02.473382 13563 solver.cpp:219] Iteration 2000 (3.03071 iter/s, 32.9956s/100 iters), loss = 0.0214488
I0408 15:30:02.473413 13563 solver.cpp:238]     Train net output #0: loss = 0.0214489 (* 1 = 0.0214489 loss)
I0408 15:30:02.473419 13563 sgd_solver.cpp:105] Iteration 2000, lr = 0.00872196
I0408 15:30:23.510887 13563 solver.cpp:219] Iteration 2100 (4.75342 iter/s, 21.0375s/100 iters), loss = 0.0223724
I0408 15:30:23.511092 13563 solver.cpp:238]     Train net output #0: loss = 0.0223724 (* 1 = 0.0223724 loss)
I0408 15:30:23.511101 13563 sgd_solver.cpp:105] Iteration 2100, lr = 0.00866784
I0408 15:30:44.508941 13563 solver.cpp:219] Iteration 2200 (4.76239 iter/s, 20.9979s/100 iters), loss = 0.0155332
I0408 15:30:44.508970 13563 solver.cpp:238]     Train net output #0: loss = 0.0155333 (* 1 = 0.0155333 loss)
I0408 15:30:44.508975 13563 sgd_solver.cpp:105] Iteration 2200, lr = 0.0086145
I0408 15:31:05.497907 13563 solver.cpp:219] Iteration 2300 (4.76441 iter/s, 20.9889s/100 iters), loss = 0.0703333
I0408 15:31:05.498113 13563 solver.cpp:238]     Train net output #0: loss = 0.0703334 (* 1 = 0.0703334 loss)
I0408 15:31:05.498121 13563 sgd_solver.cpp:105] Iteration 2300, lr = 0.00856192
I0408 15:31:26.486475 13563 solver.cpp:219] Iteration 2400 (4.76454 iter/s, 20.9884s/100 iters), loss = 0.0123191
I0408 15:31:26.486505 13563 solver.cpp:238]     Train net output #0: loss = 0.0123192 (* 1 = 0.0123192 loss)
I0408 15:31:26.486510 13563 sgd_solver.cpp:105] Iteration 2400, lr = 0.00851008
I0408 15:31:47.126766 13563 solver.cpp:331] Iteration 2500, Testing net (#0)
I0408 15:31:58.752665 13572 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:31:59.242439 13563 solver.cpp:398]     Test net output #0: accuracy = 0.9871
I0408 15:31:59.242478 13563 solver.cpp:398]     Test net output #1: loss = 0.0412899 (* 1 = 0.0412899 loss)
I0408 15:31:59.462496 13563 solver.cpp:219] Iteration 2500 (3.03251 iter/s, 32.976s/100 iters), loss = 0.0412742
I0408 15:31:59.462543 13563 solver.cpp:238]     Train net output #0: loss = 0.0412743 (* 1 = 0.0412743 loss)
I0408 15:31:59.462561 13563 sgd_solver.cpp:105] Iteration 2500, lr = 0.00845897
I0408 15:32:20.453429 13563 solver.cpp:219] Iteration 2600 (4.76397 iter/s, 20.9909s/100 iters), loss = 0.0333381
I0408 15:32:20.453598 13563 solver.cpp:238]     Train net output #0: loss = 0.0333382 (* 1 = 0.0333382 loss)
I0408 15:32:20.453604 13563 sgd_solver.cpp:105] Iteration 2600, lr = 0.00840857
I0408 15:32:41.433156 13563 solver.cpp:219] Iteration 2700 (4.76654 iter/s, 20.9796s/100 iters), loss = 0.053053
I0408 15:32:41.433202 13563 solver.cpp:238]     Train net output #0: loss = 0.053053 (* 1 = 0.053053 loss)
I0408 15:32:41.433205 13563 sgd_solver.cpp:105] Iteration 2700, lr = 0.00835886
I0408 15:33:02.435250 13563 solver.cpp:219] Iteration 2800 (4.76144 iter/s, 21.0021s/100 iters), loss = 0.00789305
I0408 15:33:02.435377 13563 solver.cpp:238]     Train net output #0: loss = 0.00789306 (* 1 = 0.00789306 loss)
I0408 15:33:02.435384 13563 sgd_solver.cpp:105] Iteration 2800, lr = 0.00830984
I0408 15:33:04.126451 13571 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:33:23.434988 13563 solver.cpp:219] Iteration 2900 (4.76199 iter/s, 20.9996s/100 iters), loss = 0.0344417
I0408 15:33:23.435016 13563 solver.cpp:238]     Train net output #0: loss = 0.0344418 (* 1 = 0.0344418 loss)
I0408 15:33:23.435021 13563 sgd_solver.cpp:105] Iteration 2900, lr = 0.00826148
I0408 15:33:44.076092 13563 solver.cpp:331] Iteration 3000, Testing net (#0)
I0408 15:33:55.707217 13572 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:33:56.196861 13563 solver.cpp:398]     Test net output #0: accuracy = 0.9893
I0408 15:33:56.196887 13563 solver.cpp:398]     Test net output #1: loss = 0.0325877 (* 1 = 0.0325877 loss)
I0408 15:33:56.416440 13563 solver.cpp:219] Iteration 3000 (3.03201 iter/s, 32.9814s/100 iters), loss = 0.00996081
I0408 15:33:56.416476 13563 solver.cpp:238]     Train net output #0: loss = 0.00996083 (* 1 = 0.00996083 loss)
I0408 15:33:56.416481 13563 sgd_solver.cpp:105] Iteration 3000, lr = 0.00821377
I0408 15:34:17.407397 13563 solver.cpp:219] Iteration 3100 (4.76396 iter/s, 20.9909s/100 iters), loss = 0.00570555
I0408 15:34:17.407518 13563 solver.cpp:238]     Train net output #0: loss = 0.00570557 (* 1 = 0.00570557 loss)
I0408 15:34:17.407526 13563 sgd_solver.cpp:105] Iteration 3100, lr = 0.0081667
I0408 15:34:38.383270 13563 solver.cpp:219] Iteration 3200 (4.76741 iter/s, 20.9758s/100 iters), loss = 0.00700048
I0408 15:34:38.383297 13563 solver.cpp:238]     Train net output #0: loss = 0.00700051 (* 1 = 0.00700051 loss)
I0408 15:34:38.383321 13563 sgd_solver.cpp:105] Iteration 3200, lr = 0.00812025
I0408 15:34:59.372268 13563 solver.cpp:219] Iteration 3300 (4.76441 iter/s, 20.989s/100 iters), loss = 0.0297848
I0408 15:34:59.372439 13563 solver.cpp:238]     Train net output #0: loss = 0.0297848 (* 1 = 0.0297848 loss)
I0408 15:34:59.372447 13563 sgd_solver.cpp:105] Iteration 3300, lr = 0.00807442
I0408 15:35:20.368635 13563 solver.cpp:219] Iteration 3400 (4.76276 iter/s, 20.9962s/100 iters), loss = 0.00754194
I0408 15:35:20.368680 13563 solver.cpp:238]     Train net output #0: loss = 0.00754196 (* 1 = 0.00754196 loss)
I0408 15:35:20.368685 13563 sgd_solver.cpp:105] Iteration 3400, lr = 0.00802918
I0408 15:35:41.011287 13563 solver.cpp:331] Iteration 3500, Testing net (#0)
I0408 15:35:52.639897 13572 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:35:53.130892 13563 solver.cpp:398]     Test net output #0: accuracy = 0.9893
I0408 15:35:53.130918 13563 solver.cpp:398]     Test net output #1: loss = 0.0330953 (* 1 = 0.0330953 loss)
I0408 15:35:53.351048 13563 solver.cpp:219] Iteration 3500 (3.03192 iter/s, 32.9824s/100 iters), loss = 0.00384604
I0408 15:35:53.351076 13563 solver.cpp:238]     Train net output #0: loss = 0.00384607 (* 1 = 0.00384607 loss)
I0408 15:35:53.351083 13563 sgd_solver.cpp:105] Iteration 3500, lr = 0.00798454
I0408 15:36:14.343334 13563 solver.cpp:219] Iteration 3600 (4.76366 iter/s, 20.9923s/100 iters), loss = 0.035432
I0408 15:36:14.343521 13563 solver.cpp:238]     Train net output #0: loss = 0.035432 (* 1 = 0.035432 loss)
I0408 15:36:14.343528 13563 sgd_solver.cpp:105] Iteration 3600, lr = 0.00794046
I0408 15:36:35.327807 13563 solver.cpp:219] Iteration 3700 (4.76546 iter/s, 20.9843s/100 iters), loss = 0.0133608
I0408 15:36:35.327852 13563 solver.cpp:238]     Train net output #0: loss = 0.0133608 (* 1 = 0.0133608 loss)
I0408 15:36:35.327870 13563 sgd_solver.cpp:105] Iteration 3700, lr = 0.00789695
I0408 15:36:44.781246 13571 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:36:56.324030 13563 solver.cpp:219] Iteration 3800 (4.76277 iter/s, 20.9962s/100 iters), loss = 0.00536573
I0408 15:36:56.324057 13563 solver.cpp:238]     Train net output #0: loss = 0.00536572 (* 1 = 0.00536572 loss)
I0408 15:36:56.324081 13563 sgd_solver.cpp:105] Iteration 3800, lr = 0.007854
I0408 15:37:17.321719 13563 solver.cpp:219] Iteration 3900 (4.76243 iter/s, 20.9977s/100 iters), loss = 0.0136087
I0408 15:37:17.321885 13563 solver.cpp:238]     Train net output #0: loss = 0.0136087 (* 1 = 0.0136087 loss)
I0408 15:37:17.321892 13563 sgd_solver.cpp:105] Iteration 3900, lr = 0.00781158
I0408 15:37:37.966284 13563 solver.cpp:331] Iteration 4000, Testing net (#0)
I0408 15:37:49.599349 13572 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:37:50.090320 13563 solver.cpp:398]     Test net output #0: accuracy = 0.9907
I0408 15:37:50.090345 13563 solver.cpp:398]     Test net output #1: loss = 0.0288738 (* 1 = 0.0288738 loss)
I0408 15:37:50.310178 13563 solver.cpp:219] Iteration 4000 (3.03138 iter/s, 32.9883s/100 iters), loss = 0.00762224
I0408 15:37:50.310209 13563 solver.cpp:238]     Train net output #0: loss = 0.00762223 (* 1 = 0.00762223 loss)
I0408 15:37:50.310214 13563 sgd_solver.cpp:105] Iteration 4000, lr = 0.0077697
I0408 15:38:11.301175 13563 solver.cpp:219] Iteration 4100 (4.76395 iter/s, 20.991s/100 iters), loss = 0.029961
I0408 15:38:11.301221 13563 solver.cpp:238]     Train net output #0: loss = 0.029961 (* 1 = 0.029961 loss)
I0408 15:38:11.301225 13563 sgd_solver.cpp:105] Iteration 4100, lr = 0.00772833
I0408 15:38:32.286857 13563 solver.cpp:219] Iteration 4200 (4.76516 iter/s, 20.9857s/100 iters), loss = 0.0086659
I0408 15:38:32.286984 13563 solver.cpp:238]     Train net output #0: loss = 0.00866589 (* 1 = 0.00866589 loss)
I0408 15:38:32.286991 13563 sgd_solver.cpp:105] Iteration 4200, lr = 0.00768748
I0408 15:38:53.284323 13563 solver.cpp:219] Iteration 4300 (4.7633 iter/s, 20.9938s/100 iters), loss = 0.0379005
I0408 15:38:53.284354 13563 solver.cpp:238]     Train net output #0: loss = 0.0379005 (* 1 = 0.0379005 loss)
I0408 15:38:53.284373 13563 sgd_solver.cpp:105] Iteration 4300, lr = 0.00764712
I0408 15:39:14.301352 13563 solver.cpp:219] Iteration 4400 (4.75805 iter/s, 21.017s/100 iters), loss = 0.0104399
I0408 15:39:14.301534 13563 solver.cpp:238]     Train net output #0: loss = 0.0104399 (* 1 = 0.0104399 loss)
I0408 15:39:14.301540 13563 sgd_solver.cpp:105] Iteration 4400, lr = 0.00760726
I0408 15:39:34.955490 13563 solver.cpp:331] Iteration 4500, Testing net (#0)
I0408 15:39:46.586689 13572 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:39:47.077761 13563 solver.cpp:398]     Test net output #0: accuracy = 0.9902
I0408 15:39:47.077785 13563 solver.cpp:398]     Test net output #1: loss = 0.0315172 (* 1 = 0.0315172 loss)
I0408 15:39:47.298251 13563 solver.cpp:219] Iteration 4500 (3.0306 iter/s, 32.9968s/100 iters), loss = 0.00272186
I0408 15:39:47.298282 13563 solver.cpp:238]     Train net output #0: loss = 0.00272185 (* 1 = 0.00272185 loss)
I0408 15:39:47.298288 13563 sgd_solver.cpp:105] Iteration 4500, lr = 0.00756788
I0408 15:40:08.297773 13563 solver.cpp:219] Iteration 4600 (4.76202 iter/s, 20.9995s/100 iters), loss = 0.012603
I0408 15:40:08.297819 13563 solver.cpp:238]     Train net output #0: loss = 0.012603 (* 1 = 0.012603 loss)
I0408 15:40:08.297824 13563 sgd_solver.cpp:105] Iteration 4600, lr = 0.00752897
I0408 15:40:25.728101 13571 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:40:29.303321 13563 solver.cpp:219] Iteration 4700 (4.76066 iter/s, 21.0055s/100 iters), loss = 0.002483
I0408 15:40:29.303349 13563 solver.cpp:238]     Train net output #0: loss = 0.00248298 (* 1 = 0.00248298 loss)
I0408 15:40:29.303354 13563 sgd_solver.cpp:105] Iteration 4700, lr = 0.00749052
I0408 15:40:50.285965 13563 solver.cpp:219] Iteration 4800 (4.76585 iter/s, 20.9826s/100 iters), loss = 0.010602
I0408 15:40:50.285993 13563 solver.cpp:238]     Train net output #0: loss = 0.010602 (* 1 = 0.010602 loss)
I0408 15:40:50.286016 13563 sgd_solver.cpp:105] Iteration 4800, lr = 0.00745253
I0408 15:41:11.287803 13563 solver.cpp:219] Iteration 4900 (4.76149 iter/s, 21.0018s/100 iters), loss = 0.00605942
I0408 15:41:11.288007 13563 solver.cpp:238]     Train net output #0: loss = 0.00605939 (* 1 = 0.00605939 loss)
I0408 15:41:11.288012 13563 sgd_solver.cpp:105] Iteration 4900, lr = 0.00741498
I0408 15:41:31.943850 13563 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_5000.caffemodel
I0408 15:41:32.098645 13563 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_5000.solverstate
I0408 15:41:32.103574 13563 solver.cpp:331] Iteration 5000, Testing net (#0)
I0408 15:41:43.606158 13572 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:41:44.097867 13563 solver.cpp:398]     Test net output #0: accuracy = 0.9904
I0408 15:41:44.097909 13563 solver.cpp:398]     Test net output #1: loss = 0.0286181 (* 1 = 0.0286181 loss)
I0408 15:41:44.318548 13563 solver.cpp:219] Iteration 5000 (3.0275 iter/s, 33.0306s/100 iters), loss = 0.0162768
I0408 15:41:44.318596 13563 solver.cpp:238]     Train net output #0: loss = 0.0162768 (* 1 = 0.0162768 loss)
I0408 15:41:44.318603 13563 sgd_solver.cpp:105] Iteration 5000, lr = 0.00737788
I0408 15:42:05.311051 13563 solver.cpp:219] Iteration 5100 (4.76361 iter/s, 20.9925s/100 iters), loss = 0.0160326
I0408 15:42:05.311096 13563 solver.cpp:238]     Train net output #0: loss = 0.0160326 (* 1 = 0.0160326 loss)
I0408 15:42:05.311100 13563 sgd_solver.cpp:105] Iteration 5100, lr = 0.0073412
I0408 15:42:26.299347 13563 solver.cpp:219] Iteration 5200 (4.76457 iter/s, 20.9883s/100 iters), loss = 0.00574746
I0408 15:42:26.299518 13563 solver.cpp:238]     Train net output #0: loss = 0.00574742 (* 1 = 0.00574742 loss)
I0408 15:42:26.299525 13563 sgd_solver.cpp:105] Iteration 5200, lr = 0.00730495
I0408 15:42:47.291340 13563 solver.cpp:219] Iteration 5300 (4.76376 iter/s, 20.9918s/100 iters), loss = 0.00135658
I0408 15:42:47.291371 13563 solver.cpp:238]     Train net output #0: loss = 0.00135653 (* 1 = 0.00135653 loss)
I0408 15:42:47.291376 13563 sgd_solver.cpp:105] Iteration 5300, lr = 0.00726911
I0408 15:43:08.284056 13563 solver.cpp:219] Iteration 5400 (4.76356 iter/s, 20.9927s/100 iters), loss = 0.0111978
I0408 15:43:08.284243 13563 solver.cpp:238]     Train net output #0: loss = 0.0111977 (* 1 = 0.0111977 loss)
I0408 15:43:08.284250 13563 sgd_solver.cpp:105] Iteration 5400, lr = 0.00723368
I0408 15:43:28.933614 13563 solver.cpp:331] Iteration 5500, Testing net (#0)
I0408 15:43:40.561370 13572 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:43:41.050652 13563 solver.cpp:398]     Test net output #0: accuracy = 0.989
I0408 15:43:41.050693 13563 solver.cpp:398]     Test net output #1: loss = 0.0312728 (* 1 = 0.0312728 loss)
I0408 15:43:41.271337 13563 solver.cpp:219] Iteration 5500 (3.03149 iter/s, 32.9871s/100 iters), loss = 0.00570572
I0408 15:43:41.271384 13563 solver.cpp:238]     Train net output #0: loss = 0.00570566 (* 1 = 0.00570566 loss)
I0408 15:43:41.271390 13563 sgd_solver.cpp:105] Iteration 5500, lr = 0.00719865
I0408 15:44:02.259565 13563 solver.cpp:219] Iteration 5600 (4.76458 iter/s, 20.9882s/100 iters), loss = 0.000984708
I0408 15:44:02.259596 13563 solver.cpp:238]     Train net output #0: loss = 0.000984647 (* 1 = 0.000984647 loss)
I0408 15:44:02.259601 13563 sgd_solver.cpp:105] Iteration 5600, lr = 0.00716402
I0408 15:44:06.466760 13571 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:44:23.259521 13563 solver.cpp:219] Iteration 5700 (4.76192 iter/s, 20.9999s/100 iters), loss = 0.00312505
I0408 15:44:23.259686 13563 solver.cpp:238]     Train net output #0: loss = 0.003125 (* 1 = 0.003125 loss)
I0408 15:44:23.259693 13563 sgd_solver.cpp:105] Iteration 5700, lr = 0.00712977
I0408 15:44:44.244642 13563 solver.cpp:219] Iteration 5800 (4.76531 iter/s, 20.985s/100 iters), loss = 0.013495
I0408 15:44:44.244702 13563 solver.cpp:238]     Train net output #0: loss = 0.0134949 (* 1 = 0.0134949 loss)
I0408 15:44:44.244707 13563 sgd_solver.cpp:105] Iteration 5800, lr = 0.0070959
I0408 15:45:05.245265 13563 solver.cpp:219] Iteration 5900 (4.76178 iter/s, 21.0006s/100 iters), loss = 0.0068295
I0408 15:45:05.245468 13563 solver.cpp:238]     Train net output #0: loss = 0.00682945 (* 1 = 0.00682945 loss)
I0408 15:45:05.245474 13563 sgd_solver.cpp:105] Iteration 5900, lr = 0.0070624
I0408 15:45:25.894659 13563 solver.cpp:331] Iteration 6000, Testing net (#0)
I0408 15:45:37.526749 13572 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:45:38.017696 13563 solver.cpp:398]     Test net output #0: accuracy = 0.99
I0408 15:45:38.017720 13563 solver.cpp:398]     Test net output #1: loss = 0.0282654 (* 1 = 0.0282654 loss)
I0408 15:45:38.238204 13563 solver.cpp:219] Iteration 6000 (3.03097 iter/s, 32.9928s/100 iters), loss = 0.00204017
I0408 15:45:38.238250 13563 solver.cpp:238]     Train net output #0: loss = 0.00204011 (* 1 = 0.00204011 loss)
I0408 15:45:38.238270 13563 sgd_solver.cpp:105] Iteration 6000, lr = 0.00702927
I0408 15:45:59.223769 13563 solver.cpp:219] Iteration 6100 (4.76519 iter/s, 20.9855s/100 iters), loss = 0.00241471
I0408 15:45:59.223815 13563 solver.cpp:238]     Train net output #0: loss = 0.00241466 (* 1 = 0.00241466 loss)
I0408 15:45:59.223819 13563 sgd_solver.cpp:105] Iteration 6100, lr = 0.0069965
I0408 15:46:20.215116 13563 solver.cpp:219] Iteration 6200 (4.76388 iter/s, 20.9913s/100 iters), loss = 0.00769526
I0408 15:46:20.215297 13563 solver.cpp:238]     Train net output #0: loss = 0.00769522 (* 1 = 0.00769522 loss)
I0408 15:46:20.215303 13563 sgd_solver.cpp:105] Iteration 6200, lr = 0.00696408
I0408 15:46:41.206535 13563 solver.cpp:219] Iteration 6300 (4.76389 iter/s, 20.9913s/100 iters), loss = 0.00640869
I0408 15:46:41.206563 13563 solver.cpp:238]     Train net output #0: loss = 0.00640864 (* 1 = 0.00640864 loss)
I0408 15:46:41.206568 13563 sgd_solver.cpp:105] Iteration 6300, lr = 0.00693201
I0408 15:47:02.214004 13563 solver.cpp:219] Iteration 6400 (4.76022 iter/s, 21.0075s/100 iters), loss = 0.00681465
I0408 15:47:02.214051 13563 solver.cpp:238]     Train net output #0: loss = 0.00681461 (* 1 = 0.00681461 loss)
I0408 15:47:02.214056 13563 sgd_solver.cpp:105] Iteration 6400, lr = 0.00690029
I0408 15:47:22.869016 13563 solver.cpp:331] Iteration 6500, Testing net (#0)
I0408 15:47:34.506088 13572 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:47:34.995702 13563 solver.cpp:398]     Test net output #0: accuracy = 0.9915
I0408 15:47:34.995728 13563 solver.cpp:398]     Test net output #1: loss = 0.0282141 (* 1 = 0.0282141 loss)
I0408 15:47:35.216053 13563 solver.cpp:219] Iteration 6500 (3.03012 iter/s, 33.002s/100 iters), loss = 0.00731119
I0408 15:47:35.216084 13563 solver.cpp:238]     Train net output #0: loss = 0.00731114 (* 1 = 0.00731114 loss)
I0408 15:47:35.216089 13563 sgd_solver.cpp:105] Iteration 6500, lr = 0.0068689
I0408 15:47:47.399766 13571 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:47:56.220904 13563 solver.cpp:219] Iteration 6600 (4.76081 iter/s, 21.0048s/100 iters), loss = 0.0236033
I0408 15:47:56.220933 13563 solver.cpp:238]     Train net output #0: loss = 0.0236033 (* 1 = 0.0236033 loss)
I0408 15:47:56.220938 13563 sgd_solver.cpp:105] Iteration 6600, lr = 0.00683784
I0408 15:48:17.215389 13563 solver.cpp:219] Iteration 6700 (4.76316 iter/s, 20.9945s/100 iters), loss = 0.00669592
I0408 15:48:17.215590 13563 solver.cpp:238]     Train net output #0: loss = 0.00669589 (* 1 = 0.00669589 loss)
I0408 15:48:17.215608 13563 sgd_solver.cpp:105] Iteration 6700, lr = 0.00680711
I0408 15:48:38.196652 13563 solver.cpp:219] Iteration 6800 (4.7662 iter/s, 20.9811s/100 iters), loss = 0.00253815
I0408 15:48:38.196698 13563 solver.cpp:238]     Train net output #0: loss = 0.00253812 (* 1 = 0.00253812 loss)
I0408 15:48:38.196704 13563 sgd_solver.cpp:105] Iteration 6800, lr = 0.0067767
I0408 15:48:59.180986 13563 solver.cpp:219] Iteration 6900 (4.76547 iter/s, 20.9843s/100 iters), loss = 0.00390402
I0408 15:48:59.181115 13563 solver.cpp:238]     Train net output #0: loss = 0.00390399 (* 1 = 0.00390399 loss)
I0408 15:48:59.181123 13563 sgd_solver.cpp:105] Iteration 6900, lr = 0.0067466
I0408 15:49:19.828618 13563 solver.cpp:331] Iteration 7000, Testing net (#0)
I0408 15:49:31.457653 13572 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:49:31.948179 13563 solver.cpp:398]     Test net output #0: accuracy = 0.9902
I0408 15:49:31.948202 13563 solver.cpp:398]     Test net output #1: loss = 0.0281882 (* 1 = 0.0281882 loss)
I0408 15:49:32.168661 13563 solver.cpp:219] Iteration 7000 (3.03145 iter/s, 32.9876s/100 iters), loss = 0.00984077
I0408 15:49:32.168694 13563 solver.cpp:238]     Train net output #0: loss = 0.00984074 (* 1 = 0.00984074 loss)
I0408 15:49:32.168699 13563 sgd_solver.cpp:105] Iteration 7000, lr = 0.00671681
I0408 15:49:53.153800 13563 solver.cpp:219] Iteration 7100 (4.76528 iter/s, 20.9851s/100 iters), loss = 0.0108719
I0408 15:49:53.153844 13563 solver.cpp:238]     Train net output #0: loss = 0.0108719 (* 1 = 0.0108719 loss)
I0408 15:49:53.153851 13563 sgd_solver.cpp:105] Iteration 7100, lr = 0.00668733
I0408 15:50:14.152895 13563 solver.cpp:219] Iteration 7200 (4.76212 iter/s, 20.9991s/100 iters), loss = 0.00514396
I0408 15:50:14.153065 13563 solver.cpp:238]     Train net output #0: loss = 0.00514391 (* 1 = 0.00514391 loss)
I0408 15:50:14.153072 13563 sgd_solver.cpp:105] Iteration 7200, lr = 0.00665815
I0408 15:50:35.140779 13563 solver.cpp:219] Iteration 7300 (4.76469 iter/s, 20.9877s/100 iters), loss = 0.021124
I0408 15:50:35.140807 13563 solver.cpp:238]     Train net output #0: loss = 0.0211239 (* 1 = 0.0211239 loss)
I0408 15:50:35.140812 13563 sgd_solver.cpp:105] Iteration 7300, lr = 0.00662927
I0408 15:50:56.129957 13563 solver.cpp:219] Iteration 7400 (4.76436 iter/s, 20.9892s/100 iters), loss = 0.014614
I0408 15:50:56.130143 13563 solver.cpp:238]     Train net output #0: loss = 0.014614 (* 1 = 0.014614 loss)
I0408 15:50:56.130151 13563 sgd_solver.cpp:105] Iteration 7400, lr = 0.00660067
I0408 15:51:16.077733 13571 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:51:16.796334 13563 solver.cpp:331] Iteration 7500, Testing net (#0)
I0408 15:51:28.429852 13572 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:51:28.920958 13563 solver.cpp:398]     Test net output #0: accuracy = 0.991
I0408 15:51:28.920980 13563 solver.cpp:398]     Test net output #1: loss = 0.0302177 (* 1 = 0.0302177 loss)
I0408 15:51:29.142312 13563 solver.cpp:219] Iteration 7500 (3.02918 iter/s, 33.0122s/100 iters), loss = 0.00592314
I0408 15:51:29.142355 13563 solver.cpp:238]     Train net output #0: loss = 0.00592308 (* 1 = 0.00592308 loss)
I0408 15:51:29.142360 13563 sgd_solver.cpp:105] Iteration 7500, lr = 0.00657236
I0408 15:51:50.129784 13563 solver.cpp:219] Iteration 7600 (4.76476 iter/s, 20.9874s/100 iters), loss = 0.00637135
I0408 15:51:50.129814 13563 solver.cpp:238]     Train net output #0: loss = 0.00637129 (* 1 = 0.00637129 loss)
I0408 15:51:50.129820 13563 sgd_solver.cpp:105] Iteration 7600, lr = 0.00654433
I0408 15:52:11.121924 13563 solver.cpp:219] Iteration 7700 (4.76369 iter/s, 20.9921s/100 iters), loss = 0.010755
I0408 15:52:11.122122 13563 solver.cpp:238]     Train net output #0: loss = 0.0107549 (* 1 = 0.0107549 loss)
I0408 15:52:11.122128 13563 sgd_solver.cpp:105] Iteration 7700, lr = 0.00651658
I0408 15:52:32.105082 13563 solver.cpp:219] Iteration 7800 (4.76577 iter/s, 20.983s/100 iters), loss = 0.00321199
I0408 15:52:32.105128 13563 solver.cpp:238]     Train net output #0: loss = 0.00321193 (* 1 = 0.00321193 loss)
I0408 15:52:32.105131 13563 sgd_solver.cpp:105] Iteration 7800, lr = 0.00648911
I0408 15:52:53.092466 13563 solver.cpp:219] Iteration 7900 (4.76478 iter/s, 20.9873s/100 iters), loss = 0.00232349
I0408 15:52:53.092664 13563 solver.cpp:238]     Train net output #0: loss = 0.00232344 (* 1 = 0.00232344 loss)
I0408 15:52:53.092674 13563 sgd_solver.cpp:105] Iteration 7900, lr = 0.0064619
I0408 15:53:13.750210 13563 solver.cpp:331] Iteration 8000, Testing net (#0)
I0408 15:53:25.387107 13572 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:53:25.878423 13563 solver.cpp:398]     Test net output #0: accuracy = 0.9915
I0408 15:53:25.878448 13563 solver.cpp:398]     Test net output #1: loss = 0.0279197 (* 1 = 0.0279197 loss)
I0408 15:53:26.098336 13563 solver.cpp:219] Iteration 8000 (3.02978 iter/s, 33.0057s/100 iters), loss = 0.00809627
I0408 15:53:26.098368 13563 solver.cpp:238]     Train net output #0: loss = 0.00809622 (* 1 = 0.00809622 loss)
I0408 15:53:26.098374 13563 sgd_solver.cpp:105] Iteration 8000, lr = 0.00643496
I0408 15:53:47.086134 13563 solver.cpp:219] Iteration 8100 (4.76468 iter/s, 20.9878s/100 iters), loss = 0.00763609
I0408 15:53:47.086181 13563 solver.cpp:238]     Train net output #0: loss = 0.00763604 (* 1 = 0.00763604 loss)
I0408 15:53:47.086186 13563 sgd_solver.cpp:105] Iteration 8100, lr = 0.00640827
I0408 15:54:08.077601 13563 solver.cpp:219] Iteration 8200 (4.76385 iter/s, 20.9914s/100 iters), loss = 0.00831868
I0408 15:54:08.077814 13563 solver.cpp:238]     Train net output #0: loss = 0.00831863 (* 1 = 0.00831863 loss)
I0408 15:54:08.077834 13563 sgd_solver.cpp:105] Iteration 8200, lr = 0.00638185
I0408 15:54:29.063396 13563 solver.cpp:219] Iteration 8300 (4.76517 iter/s, 20.9856s/100 iters), loss = 0.0312411
I0408 15:54:29.063441 13563 solver.cpp:238]     Train net output #0: loss = 0.0312411 (* 1 = 0.0312411 loss)
I0408 15:54:29.063446 13563 sgd_solver.cpp:105] Iteration 8300, lr = 0.00635567
I0408 15:54:50.051988 13563 solver.cpp:219] Iteration 8400 (4.7645 iter/s, 20.9886s/100 iters), loss = 0.0088545
I0408 15:54:50.052111 13563 solver.cpp:238]     Train net output #0: loss = 0.00885446 (* 1 = 0.00885446 loss)
I0408 15:54:50.052119 13563 sgd_solver.cpp:105] Iteration 8400, lr = 0.00632975
I0408 15:54:56.987437 13571 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:55:10.719516 13563 solver.cpp:331] Iteration 8500, Testing net (#0)
I0408 15:55:22.356523 13572 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:55:22.846916 13563 solver.cpp:398]     Test net output #0: accuracy = 0.9912
I0408 15:55:22.846942 13563 solver.cpp:398]     Test net output #1: loss = 0.0276122 (* 1 = 0.0276122 loss)
I0408 15:55:23.067057 13563 solver.cpp:219] Iteration 8500 (3.02893 iter/s, 33.015s/100 iters), loss = 0.00615039
I0408 15:55:23.067088 13563 solver.cpp:238]     Train net output #0: loss = 0.00615036 (* 1 = 0.00615036 loss)
I0408 15:55:23.067095 13563 sgd_solver.cpp:105] Iteration 8500, lr = 0.00630407
I0408 15:55:44.052491 13563 solver.cpp:219] Iteration 8600 (4.76522 iter/s, 20.9854s/100 iters), loss = 0.000995525
I0408 15:55:44.052520 13563 solver.cpp:238]     Train net output #0: loss = 0.000995501 (* 1 = 0.000995501 loss)
I0408 15:55:44.052525 13563 sgd_solver.cpp:105] Iteration 8600, lr = 0.00627864
I0408 15:56:05.037605 13563 solver.cpp:219] Iteration 8700 (4.76529 iter/s, 20.9851s/100 iters), loss = 0.00244836
I0408 15:56:05.037828 13563 solver.cpp:238]     Train net output #0: loss = 0.00244833 (* 1 = 0.00244833 loss)
I0408 15:56:05.037835 13563 sgd_solver.cpp:105] Iteration 8700, lr = 0.00625344
I0408 15:56:26.024055 13563 solver.cpp:219] Iteration 8800 (4.76502 iter/s, 20.9863s/100 iters), loss = 0.00124566
I0408 15:56:26.024085 13563 solver.cpp:238]     Train net output #0: loss = 0.00124564 (* 1 = 0.00124564 loss)
I0408 15:56:26.024091 13563 sgd_solver.cpp:105] Iteration 8800, lr = 0.00622847
I0408 15:56:47.004321 13563 solver.cpp:219] Iteration 8900 (4.76639 iter/s, 20.9802s/100 iters), loss = 0.000598776
I0408 15:56:47.004508 13563 solver.cpp:238]     Train net output #0: loss = 0.000598754 (* 1 = 0.000598754 loss)
I0408 15:56:47.004515 13563 sgd_solver.cpp:105] Iteration 8900, lr = 0.00620374
I0408 15:57:07.654568 13563 solver.cpp:331] Iteration 9000, Testing net (#0)
I0408 15:57:19.286048 13572 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:57:19.776604 13563 solver.cpp:398]     Test net output #0: accuracy = 0.9902
I0408 15:57:19.776628 13563 solver.cpp:398]     Test net output #1: loss = 0.0274379 (* 1 = 0.0274379 loss)
I0408 15:57:19.996515 13563 solver.cpp:219] Iteration 9000 (3.03103 iter/s, 32.992s/100 iters), loss = 0.0151648
I0408 15:57:19.996562 13563 solver.cpp:238]     Train net output #0: loss = 0.0151648 (* 1 = 0.0151648 loss)
I0408 15:57:19.996568 13563 sgd_solver.cpp:105] Iteration 9000, lr = 0.00617924
I0408 15:57:40.979009 13563 solver.cpp:219] Iteration 9100 (4.76589 iter/s, 20.9825s/100 iters), loss = 0.00935039
I0408 15:57:40.979038 13563 solver.cpp:238]     Train net output #0: loss = 0.00935037 (* 1 = 0.00935037 loss)
I0408 15:57:40.979060 13563 sgd_solver.cpp:105] Iteration 9100, lr = 0.00615496
I0408 15:58:01.968070 13563 solver.cpp:219] Iteration 9200 (4.76439 iter/s, 20.989s/100 iters), loss = 0.00469015
I0408 15:58:01.968116 13563 solver.cpp:238]     Train net output #0: loss = 0.00469013 (* 1 = 0.00469013 loss)
I0408 15:58:01.968122 13563 sgd_solver.cpp:105] Iteration 9200, lr = 0.0061309
I0408 15:58:22.958057 13563 solver.cpp:219] Iteration 9300 (4.76419 iter/s, 20.9899s/100 iters), loss = 0.00361884
I0408 15:58:22.958086 13563 solver.cpp:238]     Train net output #0: loss = 0.00361882 (* 1 = 0.00361882 loss)
I0408 15:58:22.958089 13563 sgd_solver.cpp:105] Iteration 9300, lr = 0.00610706
I0408 15:58:37.655405 13571 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:58:43.957697 13563 solver.cpp:219] Iteration 9400 (4.76199 iter/s, 20.9996s/100 iters), loss = 0.0292662
I0408 15:58:43.957727 13563 solver.cpp:238]     Train net output #0: loss = 0.0292662 (* 1 = 0.0292662 loss)
I0408 15:58:43.957732 13563 sgd_solver.cpp:105] Iteration 9400, lr = 0.00608343
I0408 15:59:04.619472 13563 solver.cpp:331] Iteration 9500, Testing net (#0)
I0408 15:59:16.257725 13572 data_layer.cpp:73] Restarting data prefetching from start.
I0408 15:59:16.747581 13563 solver.cpp:398]     Test net output #0: accuracy = 0.9895
I0408 15:59:16.747606 13563 solver.cpp:398]     Test net output #1: loss = 0.0327353 (* 1 = 0.0327353 loss)
I0408 15:59:16.967536 13563 solver.cpp:219] Iteration 9500 (3.0294 iter/s, 33.0098s/100 iters), loss = 0.00451189
I0408 15:59:16.967567 13563 solver.cpp:238]     Train net output #0: loss = 0.00451188 (* 1 = 0.00451188 loss)
I0408 15:59:16.967573 13563 sgd_solver.cpp:105] Iteration 9500, lr = 0.00606002
I0408 15:59:37.942399 13563 solver.cpp:219] Iteration 9600 (4.76762 iter/s, 20.9748s/100 iters), loss = 0.00294179
I0408 15:59:37.942443 13563 solver.cpp:238]     Train net output #0: loss = 0.00294178 (* 1 = 0.00294178 loss)
I0408 15:59:37.942466 13563 sgd_solver.cpp:105] Iteration 9600, lr = 0.00603682
I0408 15:59:58.922346 13563 solver.cpp:219] Iteration 9700 (4.76646 iter/s, 20.9799s/100 iters), loss = 0.00250035
I0408 15:59:58.922483 13563 solver.cpp:238]     Train net output #0: loss = 0.00250035 (* 1 = 0.00250035 loss)
I0408 15:59:58.922492 13563 sgd_solver.cpp:105] Iteration 9700, lr = 0.00601382
I0408 16:00:19.917117 13563 solver.cpp:219] Iteration 9800 (4.76312 iter/s, 20.9946s/100 iters), loss = 0.00757805
I0408 16:00:19.917163 13563 solver.cpp:238]     Train net output #0: loss = 0.00757805 (* 1 = 0.00757805 loss)
I0408 16:00:19.917168 13563 sgd_solver.cpp:105] Iteration 9800, lr = 0.00599102
I0408 16:00:40.900707 13563 solver.cpp:219] Iteration 9900 (4.76564 iter/s, 20.9836s/100 iters), loss = 0.00403059
I0408 16:00:40.900890 13563 solver.cpp:238]     Train net output #0: loss = 0.00403059 (* 1 = 0.00403059 loss)
I0408 16:00:40.900897 13563 sgd_solver.cpp:105] Iteration 9900, lr = 0.00596843
I0408 16:01:01.549521 13563 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_10000.caffemodel
I0408 16:01:01.703716 13563 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_10000.solverstate
I0408 16:01:01.797250 13563 solver.cpp:311] Iteration 10000, loss = 0.00331988
I0408 16:01:01.797271 13563 solver.cpp:331] Iteration 10000, Testing net (#0)
I0408 16:01:13.298162 13572 data_layer.cpp:73] Restarting data prefetching from start.
I0408 16:01:13.789070 13563 solver.cpp:398]     Test net output #0: accuracy = 0.9913
I0408 16:01:13.789094 13563 solver.cpp:398]     Test net output #1: loss = 0.026032 (* 1 = 0.026032 loss)
I0408 16:01:13.789098 13563 solver.cpp:316] Optimization Done.
I0408 16:01:13.789100 13563 caffe.cpp:259] Optimization Done.
