I0426 20:54:59.964506   314 caffe.cpp:218] Using GPUs 0
I0426 20:54:59.995481   314 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0426 20:55:00.515503   314 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything/lenet_train_test171.prototxt"
train_state {
  level: 0
  stage: ""
}
I0426 20:55:00.515646   314 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything/lenet_train_test171.prototxt
I0426 20:55:00.516022   314 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0426 20:55:00.516041   314 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0426 20:55:00.516131   314 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0426 20:55:00.516209   314 layer_factory.hpp:77] Creating layer mnist
I0426 20:55:00.516310   314 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0426 20:55:00.516333   314 net.cpp:86] Creating Layer mnist
I0426 20:55:00.516342   314 net.cpp:382] mnist -> data
I0426 20:55:00.516365   314 net.cpp:382] mnist -> label
I0426 20:55:00.517458   314 data_layer.cpp:45] output data size: 64,1,28,28
I0426 20:55:00.520200   314 net.cpp:124] Setting up mnist
I0426 20:55:00.520220   314 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0426 20:55:00.520227   314 net.cpp:131] Top shape: 64 (64)
I0426 20:55:00.520231   314 net.cpp:139] Memory required for data: 200960
I0426 20:55:00.520237   314 layer_factory.hpp:77] Creating layer conv0
I0426 20:55:00.520253   314 net.cpp:86] Creating Layer conv0
I0426 20:55:00.520258   314 net.cpp:408] conv0 <- data
I0426 20:55:00.520272   314 net.cpp:382] conv0 -> conv0
I0426 20:55:00.765822   314 net.cpp:124] Setting up conv0
I0426 20:55:00.765847   314 net.cpp:131] Top shape: 64 50 24 24 (1843200)
I0426 20:55:00.765851   314 net.cpp:139] Memory required for data: 7573760
I0426 20:55:00.765888   314 layer_factory.hpp:77] Creating layer pool0
I0426 20:55:00.765900   314 net.cpp:86] Creating Layer pool0
I0426 20:55:00.765920   314 net.cpp:408] pool0 <- conv0
I0426 20:55:00.765925   314 net.cpp:382] pool0 -> pool0
I0426 20:55:00.765970   314 net.cpp:124] Setting up pool0
I0426 20:55:00.765975   314 net.cpp:131] Top shape: 64 50 12 12 (460800)
I0426 20:55:00.765979   314 net.cpp:139] Memory required for data: 9416960
I0426 20:55:00.765981   314 layer_factory.hpp:77] Creating layer conv1
I0426 20:55:00.765991   314 net.cpp:86] Creating Layer conv1
I0426 20:55:00.765995   314 net.cpp:408] conv1 <- pool0
I0426 20:55:00.766000   314 net.cpp:382] conv1 -> conv1
I0426 20:55:00.768182   314 net.cpp:124] Setting up conv1
I0426 20:55:00.768196   314 net.cpp:131] Top shape: 64 50 8 8 (204800)
I0426 20:55:00.768199   314 net.cpp:139] Memory required for data: 10236160
I0426 20:55:00.768208   314 layer_factory.hpp:77] Creating layer pool1
I0426 20:55:00.768214   314 net.cpp:86] Creating Layer pool1
I0426 20:55:00.768218   314 net.cpp:408] pool1 <- conv1
I0426 20:55:00.768221   314 net.cpp:382] pool1 -> pool1
I0426 20:55:00.768268   314 net.cpp:124] Setting up pool1
I0426 20:55:00.768273   314 net.cpp:131] Top shape: 64 50 4 4 (51200)
I0426 20:55:00.768276   314 net.cpp:139] Memory required for data: 10440960
I0426 20:55:00.768280   314 layer_factory.hpp:77] Creating layer ip1
I0426 20:55:00.768286   314 net.cpp:86] Creating Layer ip1
I0426 20:55:00.768288   314 net.cpp:408] ip1 <- pool1
I0426 20:55:00.768293   314 net.cpp:382] ip1 -> ip1
I0426 20:55:00.769675   314 net.cpp:124] Setting up ip1
I0426 20:55:00.769687   314 net.cpp:131] Top shape: 64 100 (6400)
I0426 20:55:00.769706   314 net.cpp:139] Memory required for data: 10466560
I0426 20:55:00.769714   314 layer_factory.hpp:77] Creating layer relu1
I0426 20:55:00.769738   314 net.cpp:86] Creating Layer relu1
I0426 20:55:00.769742   314 net.cpp:408] relu1 <- ip1
I0426 20:55:00.769747   314 net.cpp:369] relu1 -> ip1 (in-place)
I0426 20:55:00.769906   314 net.cpp:124] Setting up relu1
I0426 20:55:00.769915   314 net.cpp:131] Top shape: 64 100 (6400)
I0426 20:55:00.769918   314 net.cpp:139] Memory required for data: 10492160
I0426 20:55:00.769922   314 layer_factory.hpp:77] Creating layer ip2
I0426 20:55:00.769927   314 net.cpp:86] Creating Layer ip2
I0426 20:55:00.769930   314 net.cpp:408] ip2 <- ip1
I0426 20:55:00.769935   314 net.cpp:382] ip2 -> ip2
I0426 20:55:00.770042   314 net.cpp:124] Setting up ip2
I0426 20:55:00.770051   314 net.cpp:131] Top shape: 64 10 (640)
I0426 20:55:00.770052   314 net.cpp:139] Memory required for data: 10494720
I0426 20:55:00.770057   314 layer_factory.hpp:77] Creating layer relu2
I0426 20:55:00.770063   314 net.cpp:86] Creating Layer relu2
I0426 20:55:00.770066   314 net.cpp:408] relu2 <- ip2
I0426 20:55:00.770071   314 net.cpp:369] relu2 -> ip2 (in-place)
I0426 20:55:00.770822   314 net.cpp:124] Setting up relu2
I0426 20:55:00.770833   314 net.cpp:131] Top shape: 64 10 (640)
I0426 20:55:00.770838   314 net.cpp:139] Memory required for data: 10497280
I0426 20:55:00.770840   314 layer_factory.hpp:77] Creating layer loss
I0426 20:55:00.770846   314 net.cpp:86] Creating Layer loss
I0426 20:55:00.770849   314 net.cpp:408] loss <- ip2
I0426 20:55:00.770854   314 net.cpp:408] loss <- label
I0426 20:55:00.770859   314 net.cpp:382] loss -> loss
I0426 20:55:00.770874   314 layer_factory.hpp:77] Creating layer loss
I0426 20:55:00.771091   314 net.cpp:124] Setting up loss
I0426 20:55:00.771101   314 net.cpp:131] Top shape: (1)
I0426 20:55:00.771118   314 net.cpp:134]     with loss weight 1
I0426 20:55:00.771147   314 net.cpp:139] Memory required for data: 10497284
I0426 20:55:00.771152   314 net.cpp:200] loss needs backward computation.
I0426 20:55:00.771155   314 net.cpp:200] relu2 needs backward computation.
I0426 20:55:00.771157   314 net.cpp:200] ip2 needs backward computation.
I0426 20:55:00.771160   314 net.cpp:200] relu1 needs backward computation.
I0426 20:55:00.771163   314 net.cpp:200] ip1 needs backward computation.
I0426 20:55:00.771178   314 net.cpp:200] pool1 needs backward computation.
I0426 20:55:00.771181   314 net.cpp:200] conv1 needs backward computation.
I0426 20:55:00.771184   314 net.cpp:200] pool0 needs backward computation.
I0426 20:55:00.771188   314 net.cpp:200] conv0 needs backward computation.
I0426 20:55:00.771191   314 net.cpp:202] mnist does not need backward computation.
I0426 20:55:00.771193   314 net.cpp:244] This network produces output loss
I0426 20:55:00.771203   314 net.cpp:257] Network initialization done.
I0426 20:55:00.771488   314 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything/lenet_train_test171.prototxt
I0426 20:55:00.771512   314 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0426 20:55:00.771591   314 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0426 20:55:00.771651   314 layer_factory.hpp:77] Creating layer mnist
I0426 20:55:00.771694   314 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0426 20:55:00.771706   314 net.cpp:86] Creating Layer mnist
I0426 20:55:00.771711   314 net.cpp:382] mnist -> data
I0426 20:55:00.771718   314 net.cpp:382] mnist -> label
I0426 20:55:00.771793   314 data_layer.cpp:45] output data size: 100,1,28,28
I0426 20:55:00.773039   314 net.cpp:124] Setting up mnist
I0426 20:55:00.773052   314 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0426 20:55:00.773058   314 net.cpp:131] Top shape: 100 (100)
I0426 20:55:00.773061   314 net.cpp:139] Memory required for data: 314000
I0426 20:55:00.773066   314 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0426 20:55:00.773072   314 net.cpp:86] Creating Layer label_mnist_1_split
I0426 20:55:00.773074   314 net.cpp:408] label_mnist_1_split <- label
I0426 20:55:00.773079   314 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0426 20:55:00.773087   314 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0426 20:55:00.773226   314 net.cpp:124] Setting up label_mnist_1_split
I0426 20:55:00.773236   314 net.cpp:131] Top shape: 100 (100)
I0426 20:55:00.773238   314 net.cpp:131] Top shape: 100 (100)
I0426 20:55:00.773241   314 net.cpp:139] Memory required for data: 314800
I0426 20:55:00.773244   314 layer_factory.hpp:77] Creating layer conv0
I0426 20:55:00.773252   314 net.cpp:86] Creating Layer conv0
I0426 20:55:00.773255   314 net.cpp:408] conv0 <- data
I0426 20:55:00.773260   314 net.cpp:382] conv0 -> conv0
I0426 20:55:00.774945   314 net.cpp:124] Setting up conv0
I0426 20:55:00.774960   314 net.cpp:131] Top shape: 100 50 24 24 (2880000)
I0426 20:55:00.774973   314 net.cpp:139] Memory required for data: 11834800
I0426 20:55:00.774981   314 layer_factory.hpp:77] Creating layer pool0
I0426 20:55:00.774987   314 net.cpp:86] Creating Layer pool0
I0426 20:55:00.774991   314 net.cpp:408] pool0 <- conv0
I0426 20:55:00.774996   314 net.cpp:382] pool0 -> pool0
I0426 20:55:00.775027   314 net.cpp:124] Setting up pool0
I0426 20:55:00.775032   314 net.cpp:131] Top shape: 100 50 12 12 (720000)
I0426 20:55:00.775034   314 net.cpp:139] Memory required for data: 14714800
I0426 20:55:00.775038   314 layer_factory.hpp:77] Creating layer conv1
I0426 20:55:00.775045   314 net.cpp:86] Creating Layer conv1
I0426 20:55:00.775048   314 net.cpp:408] conv1 <- pool0
I0426 20:55:00.775053   314 net.cpp:382] conv1 -> conv1
I0426 20:55:00.776957   314 net.cpp:124] Setting up conv1
I0426 20:55:00.776980   314 net.cpp:131] Top shape: 100 50 8 8 (320000)
I0426 20:55:00.776985   314 net.cpp:139] Memory required for data: 15994800
I0426 20:55:00.776993   314 layer_factory.hpp:77] Creating layer pool1
I0426 20:55:00.776999   314 net.cpp:86] Creating Layer pool1
I0426 20:55:00.777003   314 net.cpp:408] pool1 <- conv1
I0426 20:55:00.777007   314 net.cpp:382] pool1 -> pool1
I0426 20:55:00.777046   314 net.cpp:124] Setting up pool1
I0426 20:55:00.777053   314 net.cpp:131] Top shape: 100 50 4 4 (80000)
I0426 20:55:00.777056   314 net.cpp:139] Memory required for data: 16314800
I0426 20:55:00.777058   314 layer_factory.hpp:77] Creating layer ip1
I0426 20:55:00.777065   314 net.cpp:86] Creating Layer ip1
I0426 20:55:00.777067   314 net.cpp:408] ip1 <- pool1
I0426 20:55:00.777072   314 net.cpp:382] ip1 -> ip1
I0426 20:55:00.777662   314 net.cpp:124] Setting up ip1
I0426 20:55:00.777686   314 net.cpp:131] Top shape: 100 100 (10000)
I0426 20:55:00.777689   314 net.cpp:139] Memory required for data: 16354800
I0426 20:55:00.777696   314 layer_factory.hpp:77] Creating layer relu1
I0426 20:55:00.777703   314 net.cpp:86] Creating Layer relu1
I0426 20:55:00.777705   314 net.cpp:408] relu1 <- ip1
I0426 20:55:00.777709   314 net.cpp:369] relu1 -> ip1 (in-place)
I0426 20:55:00.777858   314 net.cpp:124] Setting up relu1
I0426 20:55:00.777866   314 net.cpp:131] Top shape: 100 100 (10000)
I0426 20:55:00.777869   314 net.cpp:139] Memory required for data: 16394800
I0426 20:55:00.777873   314 layer_factory.hpp:77] Creating layer ip2
I0426 20:55:00.777879   314 net.cpp:86] Creating Layer ip2
I0426 20:55:00.777882   314 net.cpp:408] ip2 <- ip1
I0426 20:55:00.777896   314 net.cpp:382] ip2 -> ip2
I0426 20:55:00.777993   314 net.cpp:124] Setting up ip2
I0426 20:55:00.778000   314 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:55:00.778002   314 net.cpp:139] Memory required for data: 16398800
I0426 20:55:00.778008   314 layer_factory.hpp:77] Creating layer relu2
I0426 20:55:00.778013   314 net.cpp:86] Creating Layer relu2
I0426 20:55:00.778017   314 net.cpp:408] relu2 <- ip2
I0426 20:55:00.778026   314 net.cpp:369] relu2 -> ip2 (in-place)
I0426 20:55:00.778200   314 net.cpp:124] Setting up relu2
I0426 20:55:00.778209   314 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:55:00.778213   314 net.cpp:139] Memory required for data: 16402800
I0426 20:55:00.778214   314 layer_factory.hpp:77] Creating layer ip2_relu2_0_split
I0426 20:55:00.778220   314 net.cpp:86] Creating Layer ip2_relu2_0_split
I0426 20:55:00.778223   314 net.cpp:408] ip2_relu2_0_split <- ip2
I0426 20:55:00.778226   314 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_0
I0426 20:55:00.778244   314 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_1
I0426 20:55:00.778287   314 net.cpp:124] Setting up ip2_relu2_0_split
I0426 20:55:00.778293   314 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:55:00.778297   314 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:55:00.778301   314 net.cpp:139] Memory required for data: 16410800
I0426 20:55:00.778308   314 layer_factory.hpp:77] Creating layer accuracy
I0426 20:55:00.778314   314 net.cpp:86] Creating Layer accuracy
I0426 20:55:00.778317   314 net.cpp:408] accuracy <- ip2_relu2_0_split_0
I0426 20:55:00.778321   314 net.cpp:408] accuracy <- label_mnist_1_split_0
I0426 20:55:00.778324   314 net.cpp:382] accuracy -> accuracy
I0426 20:55:00.778331   314 net.cpp:124] Setting up accuracy
I0426 20:55:00.778334   314 net.cpp:131] Top shape: (1)
I0426 20:55:00.778337   314 net.cpp:139] Memory required for data: 16410804
I0426 20:55:00.778339   314 layer_factory.hpp:77] Creating layer loss
I0426 20:55:00.778343   314 net.cpp:86] Creating Layer loss
I0426 20:55:00.778347   314 net.cpp:408] loss <- ip2_relu2_0_split_1
I0426 20:55:00.778349   314 net.cpp:408] loss <- label_mnist_1_split_1
I0426 20:55:00.778353   314 net.cpp:382] loss -> loss
I0426 20:55:00.778359   314 layer_factory.hpp:77] Creating layer loss
I0426 20:55:00.778611   314 net.cpp:124] Setting up loss
I0426 20:55:00.778621   314 net.cpp:131] Top shape: (1)
I0426 20:55:00.778625   314 net.cpp:134]     with loss weight 1
I0426 20:55:00.778630   314 net.cpp:139] Memory required for data: 16410808
I0426 20:55:00.778633   314 net.cpp:200] loss needs backward computation.
I0426 20:55:00.778637   314 net.cpp:202] accuracy does not need backward computation.
I0426 20:55:00.778641   314 net.cpp:200] ip2_relu2_0_split needs backward computation.
I0426 20:55:00.778643   314 net.cpp:200] relu2 needs backward computation.
I0426 20:55:00.778646   314 net.cpp:200] ip2 needs backward computation.
I0426 20:55:00.778650   314 net.cpp:200] relu1 needs backward computation.
I0426 20:55:00.778652   314 net.cpp:200] ip1 needs backward computation.
I0426 20:55:00.778656   314 net.cpp:200] pool1 needs backward computation.
I0426 20:55:00.778663   314 net.cpp:200] conv1 needs backward computation.
I0426 20:55:00.778666   314 net.cpp:200] pool0 needs backward computation.
I0426 20:55:00.778669   314 net.cpp:200] conv0 needs backward computation.
I0426 20:55:00.778673   314 net.cpp:202] label_mnist_1_split does not need backward computation.
I0426 20:55:00.778676   314 net.cpp:202] mnist does not need backward computation.
I0426 20:55:00.778684   314 net.cpp:244] This network produces output accuracy
I0426 20:55:00.778687   314 net.cpp:244] This network produces output loss
I0426 20:55:00.778697   314 net.cpp:257] Network initialization done.
I0426 20:55:00.778733   314 solver.cpp:56] Solver scaffolding done.
I0426 20:55:00.778982   314 caffe.cpp:248] Starting Optimization
I0426 20:55:00.778988   314 solver.cpp:273] Solving LeNet
I0426 20:55:00.778991   314 solver.cpp:274] Learning Rate Policy: inv
I0426 20:55:00.779784   314 solver.cpp:331] Iteration 0, Testing net (#0)
I0426 20:55:00.786775   314 blocking_queue.cpp:49] Waiting for data
I0426 20:55:00.862607   329 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:55:00.863600   314 solver.cpp:398]     Test net output #0: accuracy = 0.128
I0426 20:55:00.863634   314 solver.cpp:398]     Test net output #1: loss = 2.33346 (* 1 = 2.33346 loss)
I0426 20:55:00.868065   314 solver.cpp:219] Iteration 0 (0 iter/s, 0.0890369s/100 iters), loss = 2.36938
I0426 20:55:00.868119   314 solver.cpp:238]     Train net output #0: loss = 2.36938 (* 1 = 2.36938 loss)
I0426 20:55:00.868130   314 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0426 20:55:01.026517   314 solver.cpp:219] Iteration 100 (631.327 iter/s, 0.158397s/100 iters), loss = 0.63974
I0426 20:55:01.026561   314 solver.cpp:238]     Train net output #0: loss = 0.63974 (* 1 = 0.63974 loss)
I0426 20:55:01.026576   314 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0426 20:55:01.183327   314 solver.cpp:219] Iteration 200 (637.876 iter/s, 0.15677s/100 iters), loss = 0.335153
I0426 20:55:01.183369   314 solver.cpp:238]     Train net output #0: loss = 0.335153 (* 1 = 0.335153 loss)
I0426 20:55:01.183377   314 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0426 20:55:01.340087   314 solver.cpp:219] Iteration 300 (638.08 iter/s, 0.15672s/100 iters), loss = 0.330361
I0426 20:55:01.340129   314 solver.cpp:238]     Train net output #0: loss = 0.330361 (* 1 = 0.330361 loss)
I0426 20:55:01.340137   314 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0426 20:55:01.495769   314 solver.cpp:219] Iteration 400 (642.505 iter/s, 0.155641s/100 iters), loss = 0.529208
I0426 20:55:01.495810   314 solver.cpp:238]     Train net output #0: loss = 0.529208 (* 1 = 0.529208 loss)
I0426 20:55:01.495817   314 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0426 20:55:01.649212   314 solver.cpp:331] Iteration 500, Testing net (#0)
I0426 20:55:01.722872   329 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:55:01.725527   314 solver.cpp:398]     Test net output #0: accuracy = 0.8771
I0426 20:55:01.725548   314 solver.cpp:398]     Test net output #1: loss = 0.320136 (* 1 = 0.320136 loss)
I0426 20:55:01.727015   314 solver.cpp:219] Iteration 500 (432.549 iter/s, 0.231188s/100 iters), loss = 0.485855
I0426 20:55:01.727037   314 solver.cpp:238]     Train net output #0: loss = 0.485855 (* 1 = 0.485855 loss)
I0426 20:55:01.727061   314 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0426 20:55:01.883805   314 solver.cpp:219] Iteration 600 (637.943 iter/s, 0.156754s/100 iters), loss = 0.163781
I0426 20:55:01.883848   314 solver.cpp:238]     Train net output #0: loss = 0.163781 (* 1 = 0.163781 loss)
I0426 20:55:01.883855   314 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0426 20:55:02.041960   314 solver.cpp:219] Iteration 700 (632.521 iter/s, 0.158097s/100 iters), loss = 0.29199
I0426 20:55:02.042002   314 solver.cpp:238]     Train net output #0: loss = 0.29199 (* 1 = 0.29199 loss)
I0426 20:55:02.042009   314 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0426 20:55:02.202595   314 solver.cpp:219] Iteration 800 (622.748 iter/s, 0.160579s/100 iters), loss = 0.337129
I0426 20:55:02.202625   314 solver.cpp:238]     Train net output #0: loss = 0.337129 (* 1 = 0.337129 loss)
I0426 20:55:02.202633   314 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0426 20:55:02.381856   314 solver.cpp:219] Iteration 900 (558 iter/s, 0.179211s/100 iters), loss = 0.236442
I0426 20:55:02.381906   314 solver.cpp:238]     Train net output #0: loss = 0.236442 (* 1 = 0.236442 loss)
I0426 20:55:02.381920   314 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0426 20:55:02.443577   328 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:55:02.563941   314 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0426 20:55:02.568794   314 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0426 20:55:02.571588   314 solver.cpp:311] Iteration 1000, loss = 0.210224
I0426 20:55:02.571615   314 solver.cpp:331] Iteration 1000, Testing net (#0)
I0426 20:55:02.657891   329 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:55:02.660852   314 solver.cpp:398]     Test net output #0: accuracy = 0.884
I0426 20:55:02.660894   314 solver.cpp:398]     Test net output #1: loss = 0.29062 (* 1 = 0.29062 loss)
I0426 20:55:02.660902   314 solver.cpp:316] Optimization Done.
I0426 20:55:02.660910   314 caffe.cpp:259] Optimization Done.
