I0426 20:48:58.514536 31419 caffe.cpp:218] Using GPUs 0
I0426 20:48:58.546983 31419 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0426 20:48:59.041649 31419 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything/lenet_train_test23.prototxt"
train_state {
  level: 0
  stage: ""
}
I0426 20:48:59.041779 31419 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything/lenet_train_test23.prototxt
I0426 20:48:59.042081 31419 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0426 20:48:59.042095 31419 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0426 20:48:59.042163 31419 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool0"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0426 20:48:59.042217 31419 layer_factory.hpp:77] Creating layer mnist
I0426 20:48:59.042297 31419 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0426 20:48:59.042316 31419 net.cpp:86] Creating Layer mnist
I0426 20:48:59.042322 31419 net.cpp:382] mnist -> data
I0426 20:48:59.042358 31419 net.cpp:382] mnist -> label
I0426 20:48:59.043301 31419 data_layer.cpp:45] output data size: 64,1,28,28
I0426 20:48:59.045406 31419 net.cpp:124] Setting up mnist
I0426 20:48:59.045419 31419 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0426 20:48:59.045424 31419 net.cpp:131] Top shape: 64 (64)
I0426 20:48:59.045428 31419 net.cpp:139] Memory required for data: 200960
I0426 20:48:59.045433 31419 layer_factory.hpp:77] Creating layer conv0
I0426 20:48:59.045445 31419 net.cpp:86] Creating Layer conv0
I0426 20:48:59.045449 31419 net.cpp:408] conv0 <- data
I0426 20:48:59.045459 31419 net.cpp:382] conv0 -> conv0
I0426 20:48:59.270596 31419 net.cpp:124] Setting up conv0
I0426 20:48:59.270635 31419 net.cpp:131] Top shape: 64 10 24 24 (368640)
I0426 20:48:59.270638 31419 net.cpp:139] Memory required for data: 1675520
I0426 20:48:59.270653 31419 layer_factory.hpp:77] Creating layer pool0
I0426 20:48:59.270665 31419 net.cpp:86] Creating Layer pool0
I0426 20:48:59.270701 31419 net.cpp:408] pool0 <- conv0
I0426 20:48:59.270707 31419 net.cpp:382] pool0 -> pool0
I0426 20:48:59.270753 31419 net.cpp:124] Setting up pool0
I0426 20:48:59.270761 31419 net.cpp:131] Top shape: 64 10 12 12 (92160)
I0426 20:48:59.270764 31419 net.cpp:139] Memory required for data: 2044160
I0426 20:48:59.270767 31419 layer_factory.hpp:77] Creating layer ip1
I0426 20:48:59.270776 31419 net.cpp:86] Creating Layer ip1
I0426 20:48:59.270778 31419 net.cpp:408] ip1 <- pool0
I0426 20:48:59.270782 31419 net.cpp:382] ip1 -> ip1
I0426 20:48:59.272513 31419 net.cpp:124] Setting up ip1
I0426 20:48:59.272524 31419 net.cpp:131] Top shape: 64 100 (6400)
I0426 20:48:59.272543 31419 net.cpp:139] Memory required for data: 2069760
I0426 20:48:59.272552 31419 layer_factory.hpp:77] Creating layer relu1
I0426 20:48:59.272557 31419 net.cpp:86] Creating Layer relu1
I0426 20:48:59.272560 31419 net.cpp:408] relu1 <- ip1
I0426 20:48:59.272564 31419 net.cpp:369] relu1 -> ip1 (in-place)
I0426 20:48:59.272768 31419 net.cpp:124] Setting up relu1
I0426 20:48:59.272778 31419 net.cpp:131] Top shape: 64 100 (6400)
I0426 20:48:59.272781 31419 net.cpp:139] Memory required for data: 2095360
I0426 20:48:59.272784 31419 layer_factory.hpp:77] Creating layer ip2
I0426 20:48:59.272791 31419 net.cpp:86] Creating Layer ip2
I0426 20:48:59.272794 31419 net.cpp:408] ip2 <- ip1
I0426 20:48:59.272799 31419 net.cpp:382] ip2 -> ip2
I0426 20:48:59.273089 31419 net.cpp:124] Setting up ip2
I0426 20:48:59.273097 31419 net.cpp:131] Top shape: 64 300 (19200)
I0426 20:48:59.273100 31419 net.cpp:139] Memory required for data: 2172160
I0426 20:48:59.273124 31419 layer_factory.hpp:77] Creating layer relu2
I0426 20:48:59.273129 31419 net.cpp:86] Creating Layer relu2
I0426 20:48:59.273133 31419 net.cpp:408] relu2 <- ip2
I0426 20:48:59.273136 31419 net.cpp:369] relu2 -> ip2 (in-place)
I0426 20:48:59.273973 31419 net.cpp:124] Setting up relu2
I0426 20:48:59.273985 31419 net.cpp:131] Top shape: 64 300 (19200)
I0426 20:48:59.274004 31419 net.cpp:139] Memory required for data: 2248960
I0426 20:48:59.274008 31419 layer_factory.hpp:77] Creating layer ip3
I0426 20:48:59.274014 31419 net.cpp:86] Creating Layer ip3
I0426 20:48:59.274019 31419 net.cpp:408] ip3 <- ip2
I0426 20:48:59.274024 31419 net.cpp:382] ip3 -> ip3
I0426 20:48:59.275022 31419 net.cpp:124] Setting up ip3
I0426 20:48:59.275033 31419 net.cpp:131] Top shape: 64 10 (640)
I0426 20:48:59.275051 31419 net.cpp:139] Memory required for data: 2251520
I0426 20:48:59.275058 31419 layer_factory.hpp:77] Creating layer relu3
I0426 20:48:59.275079 31419 net.cpp:86] Creating Layer relu3
I0426 20:48:59.275081 31419 net.cpp:408] relu3 <- ip3
I0426 20:48:59.275085 31419 net.cpp:369] relu3 -> ip3 (in-place)
I0426 20:48:59.275270 31419 net.cpp:124] Setting up relu3
I0426 20:48:59.275279 31419 net.cpp:131] Top shape: 64 10 (640)
I0426 20:48:59.275282 31419 net.cpp:139] Memory required for data: 2254080
I0426 20:48:59.275285 31419 layer_factory.hpp:77] Creating layer loss
I0426 20:48:59.275291 31419 net.cpp:86] Creating Layer loss
I0426 20:48:59.275295 31419 net.cpp:408] loss <- ip3
I0426 20:48:59.275298 31419 net.cpp:408] loss <- label
I0426 20:48:59.275303 31419 net.cpp:382] loss -> loss
I0426 20:48:59.275318 31419 layer_factory.hpp:77] Creating layer loss
I0426 20:48:59.275537 31419 net.cpp:124] Setting up loss
I0426 20:48:59.275545 31419 net.cpp:131] Top shape: (1)
I0426 20:48:59.275549 31419 net.cpp:134]     with loss weight 1
I0426 20:48:59.275563 31419 net.cpp:139] Memory required for data: 2254084
I0426 20:48:59.275565 31419 net.cpp:200] loss needs backward computation.
I0426 20:48:59.275569 31419 net.cpp:200] relu3 needs backward computation.
I0426 20:48:59.275571 31419 net.cpp:200] ip3 needs backward computation.
I0426 20:48:59.275574 31419 net.cpp:200] relu2 needs backward computation.
I0426 20:48:59.275578 31419 net.cpp:200] ip2 needs backward computation.
I0426 20:48:59.275580 31419 net.cpp:200] relu1 needs backward computation.
I0426 20:48:59.275583 31419 net.cpp:200] ip1 needs backward computation.
I0426 20:48:59.275595 31419 net.cpp:200] pool0 needs backward computation.
I0426 20:48:59.275599 31419 net.cpp:200] conv0 needs backward computation.
I0426 20:48:59.275602 31419 net.cpp:202] mnist does not need backward computation.
I0426 20:48:59.275605 31419 net.cpp:244] This network produces output loss
I0426 20:48:59.275614 31419 net.cpp:257] Network initialization done.
I0426 20:48:59.275921 31419 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything/lenet_train_test23.prototxt
I0426 20:48:59.275950 31419 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0426 20:48:59.276031 31419 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool0"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0426 20:48:59.276093 31419 layer_factory.hpp:77] Creating layer mnist
I0426 20:48:59.276140 31419 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0426 20:48:59.276154 31419 net.cpp:86] Creating Layer mnist
I0426 20:48:59.276157 31419 net.cpp:382] mnist -> data
I0426 20:48:59.276165 31419 net.cpp:382] mnist -> label
I0426 20:48:59.276249 31419 data_layer.cpp:45] output data size: 100,1,28,28
I0426 20:48:59.277431 31419 net.cpp:124] Setting up mnist
I0426 20:48:59.277458 31419 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0426 20:48:59.277464 31419 net.cpp:131] Top shape: 100 (100)
I0426 20:48:59.277467 31419 net.cpp:139] Memory required for data: 314000
I0426 20:48:59.277477 31419 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0426 20:48:59.277483 31419 net.cpp:86] Creating Layer label_mnist_1_split
I0426 20:48:59.277487 31419 net.cpp:408] label_mnist_1_split <- label
I0426 20:48:59.277492 31419 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0426 20:48:59.277498 31419 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0426 20:48:59.277561 31419 net.cpp:124] Setting up label_mnist_1_split
I0426 20:48:59.277567 31419 net.cpp:131] Top shape: 100 (100)
I0426 20:48:59.277570 31419 net.cpp:131] Top shape: 100 (100)
I0426 20:48:59.277573 31419 net.cpp:139] Memory required for data: 314800
I0426 20:48:59.277586 31419 layer_factory.hpp:77] Creating layer conv0
I0426 20:48:59.277595 31419 net.cpp:86] Creating Layer conv0
I0426 20:48:59.277598 31419 net.cpp:408] conv0 <- data
I0426 20:48:59.277603 31419 net.cpp:382] conv0 -> conv0
I0426 20:48:59.279263 31419 net.cpp:124] Setting up conv0
I0426 20:48:59.279276 31419 net.cpp:131] Top shape: 100 10 24 24 (576000)
I0426 20:48:59.279280 31419 net.cpp:139] Memory required for data: 2618800
I0426 20:48:59.279289 31419 layer_factory.hpp:77] Creating layer pool0
I0426 20:48:59.279309 31419 net.cpp:86] Creating Layer pool0
I0426 20:48:59.279314 31419 net.cpp:408] pool0 <- conv0
I0426 20:48:59.279317 31419 net.cpp:382] pool0 -> pool0
I0426 20:48:59.279352 31419 net.cpp:124] Setting up pool0
I0426 20:48:59.279357 31419 net.cpp:131] Top shape: 100 10 12 12 (144000)
I0426 20:48:59.279359 31419 net.cpp:139] Memory required for data: 3194800
I0426 20:48:59.279362 31419 layer_factory.hpp:77] Creating layer ip1
I0426 20:48:59.279368 31419 net.cpp:86] Creating Layer ip1
I0426 20:48:59.279371 31419 net.cpp:408] ip1 <- pool0
I0426 20:48:59.279376 31419 net.cpp:382] ip1 -> ip1
I0426 20:48:59.281163 31419 net.cpp:124] Setting up ip1
I0426 20:48:59.281191 31419 net.cpp:131] Top shape: 100 100 (10000)
I0426 20:48:59.281195 31419 net.cpp:139] Memory required for data: 3234800
I0426 20:48:59.281203 31419 layer_factory.hpp:77] Creating layer relu1
I0426 20:48:59.281208 31419 net.cpp:86] Creating Layer relu1
I0426 20:48:59.281226 31419 net.cpp:408] relu1 <- ip1
I0426 20:48:59.281230 31419 net.cpp:369] relu1 -> ip1 (in-place)
I0426 20:48:59.281467 31419 net.cpp:124] Setting up relu1
I0426 20:48:59.281476 31419 net.cpp:131] Top shape: 100 100 (10000)
I0426 20:48:59.281481 31419 net.cpp:139] Memory required for data: 3274800
I0426 20:48:59.281483 31419 layer_factory.hpp:77] Creating layer ip2
I0426 20:48:59.281489 31419 net.cpp:86] Creating Layer ip2
I0426 20:48:59.281492 31419 net.cpp:408] ip2 <- ip1
I0426 20:48:59.281497 31419 net.cpp:382] ip2 -> ip2
I0426 20:48:59.281759 31419 net.cpp:124] Setting up ip2
I0426 20:48:59.281765 31419 net.cpp:131] Top shape: 100 300 (30000)
I0426 20:48:59.281769 31419 net.cpp:139] Memory required for data: 3394800
I0426 20:48:59.281787 31419 layer_factory.hpp:77] Creating layer relu2
I0426 20:48:59.281798 31419 net.cpp:86] Creating Layer relu2
I0426 20:48:59.281801 31419 net.cpp:408] relu2 <- ip2
I0426 20:48:59.281805 31419 net.cpp:369] relu2 -> ip2 (in-place)
I0426 20:48:59.282655 31419 net.cpp:124] Setting up relu2
I0426 20:48:59.282665 31419 net.cpp:131] Top shape: 100 300 (30000)
I0426 20:48:59.282670 31419 net.cpp:139] Memory required for data: 3514800
I0426 20:48:59.282672 31419 layer_factory.hpp:77] Creating layer ip3
I0426 20:48:59.282680 31419 net.cpp:86] Creating Layer ip3
I0426 20:48:59.282699 31419 net.cpp:408] ip3 <- ip2
I0426 20:48:59.282704 31419 net.cpp:382] ip3 -> ip3
I0426 20:48:59.282872 31419 net.cpp:124] Setting up ip3
I0426 20:48:59.282881 31419 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:48:59.282883 31419 net.cpp:139] Memory required for data: 3518800
I0426 20:48:59.282888 31419 layer_factory.hpp:77] Creating layer relu3
I0426 20:48:59.282894 31419 net.cpp:86] Creating Layer relu3
I0426 20:48:59.282897 31419 net.cpp:408] relu3 <- ip3
I0426 20:48:59.282902 31419 net.cpp:369] relu3 -> ip3 (in-place)
I0426 20:48:59.283072 31419 net.cpp:124] Setting up relu3
I0426 20:48:59.283082 31419 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:48:59.283083 31419 net.cpp:139] Memory required for data: 3522800
I0426 20:48:59.283087 31419 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0426 20:48:59.283093 31419 net.cpp:86] Creating Layer ip3_relu3_0_split
I0426 20:48:59.283095 31419 net.cpp:408] ip3_relu3_0_split <- ip3
I0426 20:48:59.283099 31419 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0426 20:48:59.283107 31419 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0426 20:48:59.283138 31419 net.cpp:124] Setting up ip3_relu3_0_split
I0426 20:48:59.283143 31419 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:48:59.283155 31419 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:48:59.283159 31419 net.cpp:139] Memory required for data: 3530800
I0426 20:48:59.283161 31419 layer_factory.hpp:77] Creating layer accuracy
I0426 20:48:59.283166 31419 net.cpp:86] Creating Layer accuracy
I0426 20:48:59.283176 31419 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0426 20:48:59.283179 31419 net.cpp:408] accuracy <- label_mnist_1_split_0
I0426 20:48:59.283184 31419 net.cpp:382] accuracy -> accuracy
I0426 20:48:59.283190 31419 net.cpp:124] Setting up accuracy
I0426 20:48:59.283195 31419 net.cpp:131] Top shape: (1)
I0426 20:48:59.283200 31419 net.cpp:139] Memory required for data: 3530804
I0426 20:48:59.283205 31419 layer_factory.hpp:77] Creating layer loss
I0426 20:48:59.283210 31419 net.cpp:86] Creating Layer loss
I0426 20:48:59.283213 31419 net.cpp:408] loss <- ip3_relu3_0_split_1
I0426 20:48:59.283217 31419 net.cpp:408] loss <- label_mnist_1_split_1
I0426 20:48:59.283222 31419 net.cpp:382] loss -> loss
I0426 20:48:59.283228 31419 layer_factory.hpp:77] Creating layer loss
I0426 20:48:59.283490 31419 net.cpp:124] Setting up loss
I0426 20:48:59.283500 31419 net.cpp:131] Top shape: (1)
I0426 20:48:59.283504 31419 net.cpp:134]     with loss weight 1
I0426 20:48:59.283510 31419 net.cpp:139] Memory required for data: 3530808
I0426 20:48:59.283514 31419 net.cpp:200] loss needs backward computation.
I0426 20:48:59.283519 31419 net.cpp:202] accuracy does not need backward computation.
I0426 20:48:59.283521 31419 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0426 20:48:59.283524 31419 net.cpp:200] relu3 needs backward computation.
I0426 20:48:59.283527 31419 net.cpp:200] ip3 needs backward computation.
I0426 20:48:59.283530 31419 net.cpp:200] relu2 needs backward computation.
I0426 20:48:59.283534 31419 net.cpp:200] ip2 needs backward computation.
I0426 20:48:59.283535 31419 net.cpp:200] relu1 needs backward computation.
I0426 20:48:59.283538 31419 net.cpp:200] ip1 needs backward computation.
I0426 20:48:59.283541 31419 net.cpp:200] pool0 needs backward computation.
I0426 20:48:59.283556 31419 net.cpp:200] conv0 needs backward computation.
I0426 20:48:59.283560 31419 net.cpp:202] label_mnist_1_split does not need backward computation.
I0426 20:48:59.283565 31419 net.cpp:202] mnist does not need backward computation.
I0426 20:48:59.283566 31419 net.cpp:244] This network produces output accuracy
I0426 20:48:59.283571 31419 net.cpp:244] This network produces output loss
I0426 20:48:59.283582 31419 net.cpp:257] Network initialization done.
I0426 20:48:59.283618 31419 solver.cpp:56] Solver scaffolding done.
I0426 20:48:59.283918 31419 caffe.cpp:248] Starting Optimization
I0426 20:48:59.283926 31419 solver.cpp:273] Solving LeNet
I0426 20:48:59.283928 31419 solver.cpp:274] Learning Rate Policy: inv
I0426 20:48:59.285436 31419 solver.cpp:331] Iteration 0, Testing net (#0)
I0426 20:48:59.289259 31419 blocking_queue.cpp:49] Waiting for data
I0426 20:48:59.360707 31426 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:48:59.361335 31419 solver.cpp:398]     Test net output #0: accuracy = 0.1
I0426 20:48:59.361371 31419 solver.cpp:398]     Test net output #1: loss = 2.31343 (* 1 = 2.31343 loss)
I0426 20:48:59.365810 31419 solver.cpp:219] Iteration 0 (0 iter/s, 0.0818523s/100 iters), loss = 2.3241
I0426 20:48:59.365847 31419 solver.cpp:238]     Train net output #0: loss = 2.3241 (* 1 = 2.3241 loss)
I0426 20:48:59.365888 31419 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0426 20:48:59.446266 31419 solver.cpp:219] Iteration 100 (1243.66 iter/s, 0.0804076s/100 iters), loss = 0.751947
I0426 20:48:59.446296 31419 solver.cpp:238]     Train net output #0: loss = 0.751947 (* 1 = 0.751947 loss)
I0426 20:48:59.446305 31419 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0426 20:48:59.523797 31419 solver.cpp:219] Iteration 200 (1290.5 iter/s, 0.0774892s/100 iters), loss = 0.264693
I0426 20:48:59.523828 31419 solver.cpp:238]     Train net output #0: loss = 0.264693 (* 1 = 0.264693 loss)
I0426 20:48:59.523834 31419 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0426 20:48:59.601312 31419 solver.cpp:219] Iteration 300 (1290.72 iter/s, 0.0774758s/100 iters), loss = 0.244474
I0426 20:48:59.601341 31419 solver.cpp:238]     Train net output #0: loss = 0.244474 (* 1 = 0.244474 loss)
I0426 20:48:59.601349 31419 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0426 20:48:59.682184 31419 solver.cpp:219] Iteration 400 (1237.2 iter/s, 0.0808274s/100 iters), loss = 0.103597
I0426 20:48:59.682240 31419 solver.cpp:238]     Train net output #0: loss = 0.103597 (* 1 = 0.103597 loss)
I0426 20:48:59.682255 31419 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0426 20:48:59.774583 31419 solver.cpp:331] Iteration 500, Testing net (#0)
I0426 20:48:59.834964 31426 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:48:59.835551 31419 solver.cpp:398]     Test net output #0: accuracy = 0.9539
I0426 20:48:59.835580 31419 solver.cpp:398]     Test net output #1: loss = 0.150762 (* 1 = 0.150762 loss)
I0426 20:48:59.836447 31419 solver.cpp:219] Iteration 500 (648.517 iter/s, 0.154198s/100 iters), loss = 0.196486
I0426 20:48:59.836479 31419 solver.cpp:238]     Train net output #0: loss = 0.196486 (* 1 = 0.196486 loss)
I0426 20:48:59.836494 31419 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0426 20:48:59.917362 31419 solver.cpp:219] Iteration 600 (1236.5 iter/s, 0.0808735s/100 iters), loss = 0.125491
I0426 20:48:59.917394 31419 solver.cpp:238]     Train net output #0: loss = 0.125491 (* 1 = 0.125491 loss)
I0426 20:48:59.917402 31419 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0426 20:48:59.995414 31419 solver.cpp:219] Iteration 700 (1281.86 iter/s, 0.0780114s/100 iters), loss = 0.21757
I0426 20:48:59.995445 31419 solver.cpp:238]     Train net output #0: loss = 0.217569 (* 1 = 0.217569 loss)
I0426 20:48:59.995452 31419 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0426 20:49:00.075693 31419 solver.cpp:219] Iteration 800 (1246.28 iter/s, 0.0802386s/100 iters), loss = 0.215623
I0426 20:49:00.075726 31419 solver.cpp:238]     Train net output #0: loss = 0.215623 (* 1 = 0.215623 loss)
I0426 20:49:00.075733 31419 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0426 20:49:00.156667 31419 solver.cpp:219] Iteration 900 (1235.59 iter/s, 0.0809329s/100 iters), loss = 0.193579
I0426 20:49:00.156697 31419 solver.cpp:238]     Train net output #0: loss = 0.193579 (* 1 = 0.193579 loss)
I0426 20:49:00.156705 31419 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0426 20:49:00.182656 31425 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:49:00.234069 31419 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0426 20:49:00.238684 31419 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0426 20:49:00.240557 31419 solver.cpp:311] Iteration 1000, loss = 0.0916359
I0426 20:49:00.240578 31419 solver.cpp:331] Iteration 1000, Testing net (#0)
I0426 20:49:00.315366 31426 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:49:00.315927 31419 solver.cpp:398]     Test net output #0: accuracy = 0.9711
I0426 20:49:00.315954 31419 solver.cpp:398]     Test net output #1: loss = 0.0912589 (* 1 = 0.0912589 loss)
I0426 20:49:00.315960 31419 solver.cpp:316] Optimization Done.
I0426 20:49:00.315965 31419 caffe.cpp:259] Optimization Done.
