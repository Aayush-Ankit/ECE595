I0426 20:51:00.678838 31889 caffe.cpp:218] Using GPUs 0
I0426 20:51:00.717022 31889 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0426 20:51:01.241991 31889 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything/lenet_train_test76.prototxt"
train_state {
  level: 0
  stage: ""
}
I0426 20:51:01.242130 31889 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything/lenet_train_test76.prototxt
I0426 20:51:01.242544 31889 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0426 20:51:01.242563 31889 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0426 20:51:01.242664 31889 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0426 20:51:01.242745 31889 layer_factory.hpp:77] Creating layer mnist
I0426 20:51:01.242843 31889 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0426 20:51:01.242869 31889 net.cpp:86] Creating Layer mnist
I0426 20:51:01.242880 31889 net.cpp:382] mnist -> data
I0426 20:51:01.242907 31889 net.cpp:382] mnist -> label
I0426 20:51:01.244004 31889 data_layer.cpp:45] output data size: 64,1,28,28
I0426 20:51:01.246544 31889 net.cpp:124] Setting up mnist
I0426 20:51:01.246562 31889 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0426 20:51:01.246568 31889 net.cpp:131] Top shape: 64 (64)
I0426 20:51:01.246572 31889 net.cpp:139] Memory required for data: 200960
I0426 20:51:01.246578 31889 layer_factory.hpp:77] Creating layer conv0
I0426 20:51:01.246595 31889 net.cpp:86] Creating Layer conv0
I0426 20:51:01.246615 31889 net.cpp:408] conv0 <- data
I0426 20:51:01.246628 31889 net.cpp:382] conv0 -> conv0
I0426 20:51:01.537765 31889 net.cpp:124] Setting up conv0
I0426 20:51:01.537792 31889 net.cpp:131] Top shape: 64 10 24 24 (368640)
I0426 20:51:01.537796 31889 net.cpp:139] Memory required for data: 1675520
I0426 20:51:01.537811 31889 layer_factory.hpp:77] Creating layer pool0
I0426 20:51:01.537823 31889 net.cpp:86] Creating Layer pool0
I0426 20:51:01.537828 31889 net.cpp:408] pool0 <- conv0
I0426 20:51:01.537834 31889 net.cpp:382] pool0 -> pool0
I0426 20:51:01.537880 31889 net.cpp:124] Setting up pool0
I0426 20:51:01.537889 31889 net.cpp:131] Top shape: 64 10 12 12 (92160)
I0426 20:51:01.537891 31889 net.cpp:139] Memory required for data: 2044160
I0426 20:51:01.537895 31889 layer_factory.hpp:77] Creating layer conv1
I0426 20:51:01.537905 31889 net.cpp:86] Creating Layer conv1
I0426 20:51:01.537911 31889 net.cpp:408] conv1 <- pool0
I0426 20:51:01.537917 31889 net.cpp:382] conv1 -> conv1
I0426 20:51:01.540892 31889 net.cpp:124] Setting up conv1
I0426 20:51:01.540907 31889 net.cpp:131] Top shape: 64 25 8 8 (102400)
I0426 20:51:01.540911 31889 net.cpp:139] Memory required for data: 2453760
I0426 20:51:01.540920 31889 layer_factory.hpp:77] Creating layer pool1
I0426 20:51:01.540928 31889 net.cpp:86] Creating Layer pool1
I0426 20:51:01.540931 31889 net.cpp:408] pool1 <- conv1
I0426 20:51:01.540936 31889 net.cpp:382] pool1 -> pool1
I0426 20:51:01.540977 31889 net.cpp:124] Setting up pool1
I0426 20:51:01.540982 31889 net.cpp:131] Top shape: 64 25 4 4 (25600)
I0426 20:51:01.540984 31889 net.cpp:139] Memory required for data: 2556160
I0426 20:51:01.540987 31889 layer_factory.hpp:77] Creating layer ip1
I0426 20:51:01.540994 31889 net.cpp:86] Creating Layer ip1
I0426 20:51:01.540997 31889 net.cpp:408] ip1 <- pool1
I0426 20:51:01.541002 31889 net.cpp:382] ip1 -> ip1
I0426 20:51:01.541340 31889 net.cpp:124] Setting up ip1
I0426 20:51:01.541347 31889 net.cpp:131] Top shape: 64 100 (6400)
I0426 20:51:01.541350 31889 net.cpp:139] Memory required for data: 2581760
I0426 20:51:01.541358 31889 layer_factory.hpp:77] Creating layer relu1
I0426 20:51:01.541363 31889 net.cpp:86] Creating Layer relu1
I0426 20:51:01.541366 31889 net.cpp:408] relu1 <- ip1
I0426 20:51:01.541371 31889 net.cpp:369] relu1 -> ip1 (in-place)
I0426 20:51:01.541537 31889 net.cpp:124] Setting up relu1
I0426 20:51:01.541545 31889 net.cpp:131] Top shape: 64 100 (6400)
I0426 20:51:01.541549 31889 net.cpp:139] Memory required for data: 2607360
I0426 20:51:01.541553 31889 layer_factory.hpp:77] Creating layer ip2
I0426 20:51:01.541558 31889 net.cpp:86] Creating Layer ip2
I0426 20:51:01.541561 31889 net.cpp:408] ip2 <- ip1
I0426 20:51:01.541566 31889 net.cpp:382] ip2 -> ip2
I0426 20:51:01.541720 31889 net.cpp:124] Setting up ip2
I0426 20:51:01.541728 31889 net.cpp:131] Top shape: 64 100 (6400)
I0426 20:51:01.541730 31889 net.cpp:139] Memory required for data: 2632960
I0426 20:51:01.541736 31889 layer_factory.hpp:77] Creating layer relu2
I0426 20:51:01.541743 31889 net.cpp:86] Creating Layer relu2
I0426 20:51:01.541746 31889 net.cpp:408] relu2 <- ip2
I0426 20:51:01.541750 31889 net.cpp:369] relu2 -> ip2 (in-place)
I0426 20:51:01.542554 31889 net.cpp:124] Setting up relu2
I0426 20:51:01.542567 31889 net.cpp:131] Top shape: 64 100 (6400)
I0426 20:51:01.542570 31889 net.cpp:139] Memory required for data: 2658560
I0426 20:51:01.542574 31889 layer_factory.hpp:77] Creating layer ip3
I0426 20:51:01.542580 31889 net.cpp:86] Creating Layer ip3
I0426 20:51:01.542584 31889 net.cpp:408] ip3 <- ip2
I0426 20:51:01.542589 31889 net.cpp:382] ip3 -> ip3
I0426 20:51:01.542697 31889 net.cpp:124] Setting up ip3
I0426 20:51:01.542706 31889 net.cpp:131] Top shape: 64 10 (640)
I0426 20:51:01.542708 31889 net.cpp:139] Memory required for data: 2661120
I0426 20:51:01.542716 31889 layer_factory.hpp:77] Creating layer relu3
I0426 20:51:01.542721 31889 net.cpp:86] Creating Layer relu3
I0426 20:51:01.542726 31889 net.cpp:408] relu3 <- ip3
I0426 20:51:01.542729 31889 net.cpp:369] relu3 -> ip3 (in-place)
I0426 20:51:01.542897 31889 net.cpp:124] Setting up relu3
I0426 20:51:01.542906 31889 net.cpp:131] Top shape: 64 10 (640)
I0426 20:51:01.542909 31889 net.cpp:139] Memory required for data: 2663680
I0426 20:51:01.542913 31889 layer_factory.hpp:77] Creating layer loss
I0426 20:51:01.542918 31889 net.cpp:86] Creating Layer loss
I0426 20:51:01.542922 31889 net.cpp:408] loss <- ip3
I0426 20:51:01.542927 31889 net.cpp:408] loss <- label
I0426 20:51:01.542932 31889 net.cpp:382] loss -> loss
I0426 20:51:01.542948 31889 layer_factory.hpp:77] Creating layer loss
I0426 20:51:01.543177 31889 net.cpp:124] Setting up loss
I0426 20:51:01.543186 31889 net.cpp:131] Top shape: (1)
I0426 20:51:01.543190 31889 net.cpp:134]     with loss weight 1
I0426 20:51:01.543205 31889 net.cpp:139] Memory required for data: 2663684
I0426 20:51:01.543208 31889 net.cpp:200] loss needs backward computation.
I0426 20:51:01.543212 31889 net.cpp:200] relu3 needs backward computation.
I0426 20:51:01.543215 31889 net.cpp:200] ip3 needs backward computation.
I0426 20:51:01.543218 31889 net.cpp:200] relu2 needs backward computation.
I0426 20:51:01.543221 31889 net.cpp:200] ip2 needs backward computation.
I0426 20:51:01.543225 31889 net.cpp:200] relu1 needs backward computation.
I0426 20:51:01.543227 31889 net.cpp:200] ip1 needs backward computation.
I0426 20:51:01.543231 31889 net.cpp:200] pool1 needs backward computation.
I0426 20:51:01.543233 31889 net.cpp:200] conv1 needs backward computation.
I0426 20:51:01.543237 31889 net.cpp:200] pool0 needs backward computation.
I0426 20:51:01.543241 31889 net.cpp:200] conv0 needs backward computation.
I0426 20:51:01.543244 31889 net.cpp:202] mnist does not need backward computation.
I0426 20:51:01.543247 31889 net.cpp:244] This network produces output loss
I0426 20:51:01.543256 31889 net.cpp:257] Network initialization done.
I0426 20:51:01.543582 31889 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything/lenet_train_test76.prototxt
I0426 20:51:01.543611 31889 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0426 20:51:01.543701 31889 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0426 20:51:01.543779 31889 layer_factory.hpp:77] Creating layer mnist
I0426 20:51:01.543825 31889 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0426 20:51:01.543839 31889 net.cpp:86] Creating Layer mnist
I0426 20:51:01.543844 31889 net.cpp:382] mnist -> data
I0426 20:51:01.543851 31889 net.cpp:382] mnist -> label
I0426 20:51:01.543938 31889 data_layer.cpp:45] output data size: 100,1,28,28
I0426 20:51:01.545917 31889 net.cpp:124] Setting up mnist
I0426 20:51:01.545930 31889 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0426 20:51:01.545936 31889 net.cpp:131] Top shape: 100 (100)
I0426 20:51:01.545939 31889 net.cpp:139] Memory required for data: 314000
I0426 20:51:01.545943 31889 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0426 20:51:01.545953 31889 net.cpp:86] Creating Layer label_mnist_1_split
I0426 20:51:01.545956 31889 net.cpp:408] label_mnist_1_split <- label
I0426 20:51:01.545961 31889 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0426 20:51:01.545969 31889 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0426 20:51:01.546017 31889 net.cpp:124] Setting up label_mnist_1_split
I0426 20:51:01.546022 31889 net.cpp:131] Top shape: 100 (100)
I0426 20:51:01.546026 31889 net.cpp:131] Top shape: 100 (100)
I0426 20:51:01.546030 31889 net.cpp:139] Memory required for data: 314800
I0426 20:51:01.546032 31889 layer_factory.hpp:77] Creating layer conv0
I0426 20:51:01.546041 31889 net.cpp:86] Creating Layer conv0
I0426 20:51:01.546043 31889 net.cpp:408] conv0 <- data
I0426 20:51:01.546048 31889 net.cpp:382] conv0 -> conv0
I0426 20:51:01.547626 31889 net.cpp:124] Setting up conv0
I0426 20:51:01.547641 31889 net.cpp:131] Top shape: 100 10 24 24 (576000)
I0426 20:51:01.547643 31889 net.cpp:139] Memory required for data: 2618800
I0426 20:51:01.547653 31889 layer_factory.hpp:77] Creating layer pool0
I0426 20:51:01.547659 31889 net.cpp:86] Creating Layer pool0
I0426 20:51:01.547663 31889 net.cpp:408] pool0 <- conv0
I0426 20:51:01.547668 31889 net.cpp:382] pool0 -> pool0
I0426 20:51:01.547704 31889 net.cpp:124] Setting up pool0
I0426 20:51:01.547710 31889 net.cpp:131] Top shape: 100 10 12 12 (144000)
I0426 20:51:01.547713 31889 net.cpp:139] Memory required for data: 3194800
I0426 20:51:01.547716 31889 layer_factory.hpp:77] Creating layer conv1
I0426 20:51:01.547724 31889 net.cpp:86] Creating Layer conv1
I0426 20:51:01.547727 31889 net.cpp:408] conv1 <- pool0
I0426 20:51:01.547732 31889 net.cpp:382] conv1 -> conv1
I0426 20:51:01.549978 31889 net.cpp:124] Setting up conv1
I0426 20:51:01.549993 31889 net.cpp:131] Top shape: 100 25 8 8 (160000)
I0426 20:51:01.549996 31889 net.cpp:139] Memory required for data: 3834800
I0426 20:51:01.550006 31889 layer_factory.hpp:77] Creating layer pool1
I0426 20:51:01.550014 31889 net.cpp:86] Creating Layer pool1
I0426 20:51:01.550017 31889 net.cpp:408] pool1 <- conv1
I0426 20:51:01.550022 31889 net.cpp:382] pool1 -> pool1
I0426 20:51:01.550079 31889 net.cpp:124] Setting up pool1
I0426 20:51:01.550086 31889 net.cpp:131] Top shape: 100 25 4 4 (40000)
I0426 20:51:01.550089 31889 net.cpp:139] Memory required for data: 3994800
I0426 20:51:01.550092 31889 layer_factory.hpp:77] Creating layer ip1
I0426 20:51:01.550098 31889 net.cpp:86] Creating Layer ip1
I0426 20:51:01.550102 31889 net.cpp:408] ip1 <- pool1
I0426 20:51:01.550107 31889 net.cpp:382] ip1 -> ip1
I0426 20:51:01.550482 31889 net.cpp:124] Setting up ip1
I0426 20:51:01.550490 31889 net.cpp:131] Top shape: 100 100 (10000)
I0426 20:51:01.550504 31889 net.cpp:139] Memory required for data: 4034800
I0426 20:51:01.550513 31889 layer_factory.hpp:77] Creating layer relu1
I0426 20:51:01.550518 31889 net.cpp:86] Creating Layer relu1
I0426 20:51:01.550521 31889 net.cpp:408] relu1 <- ip1
I0426 20:51:01.550525 31889 net.cpp:369] relu1 -> ip1 (in-place)
I0426 20:51:01.550694 31889 net.cpp:124] Setting up relu1
I0426 20:51:01.550704 31889 net.cpp:131] Top shape: 100 100 (10000)
I0426 20:51:01.550706 31889 net.cpp:139] Memory required for data: 4074800
I0426 20:51:01.550710 31889 layer_factory.hpp:77] Creating layer ip2
I0426 20:51:01.550717 31889 net.cpp:86] Creating Layer ip2
I0426 20:51:01.550721 31889 net.cpp:408] ip2 <- ip1
I0426 20:51:01.550726 31889 net.cpp:382] ip2 -> ip2
I0426 20:51:01.550882 31889 net.cpp:124] Setting up ip2
I0426 20:51:01.550889 31889 net.cpp:131] Top shape: 100 100 (10000)
I0426 20:51:01.550892 31889 net.cpp:139] Memory required for data: 4114800
I0426 20:51:01.550899 31889 layer_factory.hpp:77] Creating layer relu2
I0426 20:51:01.550902 31889 net.cpp:86] Creating Layer relu2
I0426 20:51:01.550906 31889 net.cpp:408] relu2 <- ip2
I0426 20:51:01.550910 31889 net.cpp:369] relu2 -> ip2 (in-place)
I0426 20:51:01.551123 31889 net.cpp:124] Setting up relu2
I0426 20:51:01.551132 31889 net.cpp:131] Top shape: 100 100 (10000)
I0426 20:51:01.551136 31889 net.cpp:139] Memory required for data: 4154800
I0426 20:51:01.551139 31889 layer_factory.hpp:77] Creating layer ip3
I0426 20:51:01.551146 31889 net.cpp:86] Creating Layer ip3
I0426 20:51:01.551149 31889 net.cpp:408] ip3 <- ip2
I0426 20:51:01.551154 31889 net.cpp:382] ip3 -> ip3
I0426 20:51:01.551303 31889 net.cpp:124] Setting up ip3
I0426 20:51:01.551311 31889 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:51:01.551316 31889 net.cpp:139] Memory required for data: 4158800
I0426 20:51:01.551324 31889 layer_factory.hpp:77] Creating layer relu3
I0426 20:51:01.551329 31889 net.cpp:86] Creating Layer relu3
I0426 20:51:01.551332 31889 net.cpp:408] relu3 <- ip3
I0426 20:51:01.551337 31889 net.cpp:369] relu3 -> ip3 (in-place)
I0426 20:51:01.552182 31889 net.cpp:124] Setting up relu3
I0426 20:51:01.552194 31889 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:51:01.552198 31889 net.cpp:139] Memory required for data: 4162800
I0426 20:51:01.552202 31889 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0426 20:51:01.552208 31889 net.cpp:86] Creating Layer ip3_relu3_0_split
I0426 20:51:01.552211 31889 net.cpp:408] ip3_relu3_0_split <- ip3
I0426 20:51:01.552217 31889 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0426 20:51:01.552223 31889 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0426 20:51:01.552263 31889 net.cpp:124] Setting up ip3_relu3_0_split
I0426 20:51:01.552268 31889 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:51:01.552273 31889 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:51:01.552275 31889 net.cpp:139] Memory required for data: 4170800
I0426 20:51:01.552278 31889 layer_factory.hpp:77] Creating layer accuracy
I0426 20:51:01.552283 31889 net.cpp:86] Creating Layer accuracy
I0426 20:51:01.552286 31889 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0426 20:51:01.552290 31889 net.cpp:408] accuracy <- label_mnist_1_split_0
I0426 20:51:01.552295 31889 net.cpp:382] accuracy -> accuracy
I0426 20:51:01.552302 31889 net.cpp:124] Setting up accuracy
I0426 20:51:01.552306 31889 net.cpp:131] Top shape: (1)
I0426 20:51:01.552309 31889 net.cpp:139] Memory required for data: 4170804
I0426 20:51:01.552311 31889 layer_factory.hpp:77] Creating layer loss
I0426 20:51:01.552316 31889 net.cpp:86] Creating Layer loss
I0426 20:51:01.552320 31889 net.cpp:408] loss <- ip3_relu3_0_split_1
I0426 20:51:01.552323 31889 net.cpp:408] loss <- label_mnist_1_split_1
I0426 20:51:01.552328 31889 net.cpp:382] loss -> loss
I0426 20:51:01.552335 31889 layer_factory.hpp:77] Creating layer loss
I0426 20:51:01.552563 31889 net.cpp:124] Setting up loss
I0426 20:51:01.552572 31889 net.cpp:131] Top shape: (1)
I0426 20:51:01.552577 31889 net.cpp:134]     with loss weight 1
I0426 20:51:01.552592 31889 net.cpp:139] Memory required for data: 4170808
I0426 20:51:01.552597 31889 net.cpp:200] loss needs backward computation.
I0426 20:51:01.552600 31889 net.cpp:202] accuracy does not need backward computation.
I0426 20:51:01.552605 31889 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0426 20:51:01.552608 31889 net.cpp:200] relu3 needs backward computation.
I0426 20:51:01.552613 31889 net.cpp:200] ip3 needs backward computation.
I0426 20:51:01.552615 31889 net.cpp:200] relu2 needs backward computation.
I0426 20:51:01.552618 31889 net.cpp:200] ip2 needs backward computation.
I0426 20:51:01.552623 31889 net.cpp:200] relu1 needs backward computation.
I0426 20:51:01.552625 31889 net.cpp:200] ip1 needs backward computation.
I0426 20:51:01.552629 31889 net.cpp:200] pool1 needs backward computation.
I0426 20:51:01.552633 31889 net.cpp:200] conv1 needs backward computation.
I0426 20:51:01.552635 31889 net.cpp:200] pool0 needs backward computation.
I0426 20:51:01.552639 31889 net.cpp:200] conv0 needs backward computation.
I0426 20:51:01.552644 31889 net.cpp:202] label_mnist_1_split does not need backward computation.
I0426 20:51:01.552647 31889 net.cpp:202] mnist does not need backward computation.
I0426 20:51:01.552650 31889 net.cpp:244] This network produces output accuracy
I0426 20:51:01.552654 31889 net.cpp:244] This network produces output loss
I0426 20:51:01.552664 31889 net.cpp:257] Network initialization done.
I0426 20:51:01.552707 31889 solver.cpp:56] Solver scaffolding done.
I0426 20:51:01.553063 31889 caffe.cpp:248] Starting Optimization
I0426 20:51:01.553071 31889 solver.cpp:273] Solving LeNet
I0426 20:51:01.553073 31889 solver.cpp:274] Learning Rate Policy: inv
I0426 20:51:01.554034 31889 solver.cpp:331] Iteration 0, Testing net (#0)
I0426 20:51:01.558410 31889 blocking_queue.cpp:49] Waiting for data
I0426 20:51:01.631214 31898 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:51:01.631803 31889 solver.cpp:398]     Test net output #0: accuracy = 0.1131
I0426 20:51:01.631839 31889 solver.cpp:398]     Test net output #1: loss = 2.31015 (* 1 = 2.31015 loss)
I0426 20:51:01.634613 31889 solver.cpp:219] Iteration 0 (-1.45381e-31 iter/s, 0.0815145s/100 iters), loss = 2.30006
I0426 20:51:01.634652 31889 solver.cpp:238]     Train net output #0: loss = 2.30006 (* 1 = 2.30006 loss)
I0426 20:51:01.634663 31889 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0426 20:51:01.739101 31889 solver.cpp:219] Iteration 100 (957.358 iter/s, 0.104454s/100 iters), loss = 0.320686
I0426 20:51:01.739142 31889 solver.cpp:238]     Train net output #0: loss = 0.320686 (* 1 = 0.320686 loss)
I0426 20:51:01.739150 31889 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0426 20:51:01.839808 31889 solver.cpp:219] Iteration 200 (993.49 iter/s, 0.100655s/100 iters), loss = 0.15973
I0426 20:51:01.839850 31889 solver.cpp:238]     Train net output #0: loss = 0.15973 (* 1 = 0.15973 loss)
I0426 20:51:01.839857 31889 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0426 20:51:01.941062 31889 solver.cpp:219] Iteration 300 (988.114 iter/s, 0.101203s/100 iters), loss = 0.161481
I0426 20:51:01.941102 31889 solver.cpp:238]     Train net output #0: loss = 0.161481 (* 1 = 0.161481 loss)
I0426 20:51:01.941108 31889 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0426 20:51:02.041357 31889 solver.cpp:219] Iteration 400 (997.562 iter/s, 0.100244s/100 iters), loss = 0.0784936
I0426 20:51:02.041395 31889 solver.cpp:238]     Train net output #0: loss = 0.0784936 (* 1 = 0.0784936 loss)
I0426 20:51:02.041402 31889 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0426 20:51:02.139612 31889 solver.cpp:331] Iteration 500, Testing net (#0)
I0426 20:51:02.193994 31898 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:51:02.194608 31889 solver.cpp:398]     Test net output #0: accuracy = 0.9681
I0426 20:51:02.194641 31889 solver.cpp:398]     Test net output #1: loss = 0.0986738 (* 1 = 0.0986738 loss)
I0426 20:51:02.195626 31889 solver.cpp:219] Iteration 500 (648.435 iter/s, 0.154217s/100 iters), loss = 0.122333
I0426 20:51:02.195678 31889 solver.cpp:238]     Train net output #0: loss = 0.122334 (* 1 = 0.122334 loss)
I0426 20:51:02.195686 31889 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0426 20:51:02.301278 31889 solver.cpp:219] Iteration 600 (946.9 iter/s, 0.105608s/100 iters), loss = 0.110572
I0426 20:51:02.301321 31889 solver.cpp:238]     Train net output #0: loss = 0.110572 (* 1 = 0.110572 loss)
I0426 20:51:02.301328 31889 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0426 20:51:02.405258 31889 solver.cpp:219] Iteration 700 (962.206 iter/s, 0.103928s/100 iters), loss = 0.154033
I0426 20:51:02.405299 31889 solver.cpp:238]     Train net output #0: loss = 0.154033 (* 1 = 0.154033 loss)
I0426 20:51:02.405306 31889 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0426 20:51:02.505192 31889 solver.cpp:219] Iteration 800 (1001.17 iter/s, 0.099883s/100 iters), loss = 0.252425
I0426 20:51:02.505234 31889 solver.cpp:238]     Train net output #0: loss = 0.252425 (* 1 = 0.252425 loss)
I0426 20:51:02.505240 31889 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0426 20:51:02.602452 31889 solver.cpp:219] Iteration 900 (1028.72 iter/s, 0.0972083s/100 iters), loss = 0.14778
I0426 20:51:02.602493 31889 solver.cpp:238]     Train net output #0: loss = 0.14778 (* 1 = 0.14778 loss)
I0426 20:51:02.602499 31889 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0426 20:51:02.634820 31897 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:51:02.698948 31889 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0426 20:51:02.700601 31889 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0426 20:51:02.701649 31889 solver.cpp:311] Iteration 1000, loss = 0.0912261
I0426 20:51:02.701680 31889 solver.cpp:331] Iteration 1000, Testing net (#0)
I0426 20:51:02.776180 31898 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:51:02.776849 31889 solver.cpp:398]     Test net output #0: accuracy = 0.9783
I0426 20:51:02.776885 31889 solver.cpp:398]     Test net output #1: loss = 0.0679615 (* 1 = 0.0679615 loss)
I0426 20:51:02.776890 31889 solver.cpp:316] Optimization Done.
I0426 20:51:02.776893 31889 caffe.cpp:259] Optimization Done.
