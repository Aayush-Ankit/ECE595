I0426 20:52:52.019299 32328 caffe.cpp:218] Using GPUs 0
I0426 20:52:52.058207 32328 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0426 20:52:52.582710 32328 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything/lenet_train_test123.prototxt"
train_state {
  level: 0
  stage: ""
}
I0426 20:52:52.582854 32328 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything/lenet_train_test123.prototxt
I0426 20:52:52.583266 32328 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0426 20:52:52.583284 32328 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0426 20:52:52.583389 32328 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0426 20:52:52.583472 32328 layer_factory.hpp:77] Creating layer mnist
I0426 20:52:52.583572 32328 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0426 20:52:52.583596 32328 net.cpp:86] Creating Layer mnist
I0426 20:52:52.583606 32328 net.cpp:382] mnist -> data
I0426 20:52:52.583628 32328 net.cpp:382] mnist -> label
I0426 20:52:52.584728 32328 data_layer.cpp:45] output data size: 64,1,28,28
I0426 20:52:52.587471 32328 net.cpp:124] Setting up mnist
I0426 20:52:52.587489 32328 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0426 20:52:52.587496 32328 net.cpp:131] Top shape: 64 (64)
I0426 20:52:52.587498 32328 net.cpp:139] Memory required for data: 200960
I0426 20:52:52.587505 32328 layer_factory.hpp:77] Creating layer conv0
I0426 20:52:52.587561 32328 net.cpp:86] Creating Layer conv0
I0426 20:52:52.587582 32328 net.cpp:408] conv0 <- data
I0426 20:52:52.587599 32328 net.cpp:382] conv0 -> conv0
I0426 20:52:52.881137 32328 net.cpp:124] Setting up conv0
I0426 20:52:52.881167 32328 net.cpp:131] Top shape: 64 25 24 24 (921600)
I0426 20:52:52.881172 32328 net.cpp:139] Memory required for data: 3887360
I0426 20:52:52.881189 32328 layer_factory.hpp:77] Creating layer pool0
I0426 20:52:52.881203 32328 net.cpp:86] Creating Layer pool0
I0426 20:52:52.881208 32328 net.cpp:408] pool0 <- conv0
I0426 20:52:52.881214 32328 net.cpp:382] pool0 -> pool0
I0426 20:52:52.881266 32328 net.cpp:124] Setting up pool0
I0426 20:52:52.881273 32328 net.cpp:131] Top shape: 64 25 12 12 (230400)
I0426 20:52:52.881278 32328 net.cpp:139] Memory required for data: 4808960
I0426 20:52:52.881280 32328 layer_factory.hpp:77] Creating layer conv1
I0426 20:52:52.881292 32328 net.cpp:86] Creating Layer conv1
I0426 20:52:52.881296 32328 net.cpp:408] conv1 <- pool0
I0426 20:52:52.881301 32328 net.cpp:382] conv1 -> conv1
I0426 20:52:52.884670 32328 net.cpp:124] Setting up conv1
I0426 20:52:52.884687 32328 net.cpp:131] Top shape: 64 25 8 8 (102400)
I0426 20:52:52.884692 32328 net.cpp:139] Memory required for data: 5218560
I0426 20:52:52.884703 32328 layer_factory.hpp:77] Creating layer pool1
I0426 20:52:52.884712 32328 net.cpp:86] Creating Layer pool1
I0426 20:52:52.884716 32328 net.cpp:408] pool1 <- conv1
I0426 20:52:52.884722 32328 net.cpp:382] pool1 -> pool1
I0426 20:52:52.884768 32328 net.cpp:124] Setting up pool1
I0426 20:52:52.884779 32328 net.cpp:131] Top shape: 64 25 4 4 (25600)
I0426 20:52:52.884783 32328 net.cpp:139] Memory required for data: 5320960
I0426 20:52:52.884786 32328 layer_factory.hpp:77] Creating layer ip1
I0426 20:52:52.884794 32328 net.cpp:86] Creating Layer ip1
I0426 20:52:52.884798 32328 net.cpp:408] ip1 <- pool1
I0426 20:52:52.884804 32328 net.cpp:382] ip1 -> ip1
I0426 20:52:52.886656 32328 net.cpp:124] Setting up ip1
I0426 20:52:52.886672 32328 net.cpp:131] Top shape: 64 300 (19200)
I0426 20:52:52.886677 32328 net.cpp:139] Memory required for data: 5397760
I0426 20:52:52.886687 32328 layer_factory.hpp:77] Creating layer relu1
I0426 20:52:52.886693 32328 net.cpp:86] Creating Layer relu1
I0426 20:52:52.886698 32328 net.cpp:408] relu1 <- ip1
I0426 20:52:52.886704 32328 net.cpp:369] relu1 -> ip1 (in-place)
I0426 20:52:52.886904 32328 net.cpp:124] Setting up relu1
I0426 20:52:52.886915 32328 net.cpp:131] Top shape: 64 300 (19200)
I0426 20:52:52.886919 32328 net.cpp:139] Memory required for data: 5474560
I0426 20:52:52.886924 32328 layer_factory.hpp:77] Creating layer ip2
I0426 20:52:52.886931 32328 net.cpp:86] Creating Layer ip2
I0426 20:52:52.886935 32328 net.cpp:408] ip2 <- ip1
I0426 20:52:52.886941 32328 net.cpp:382] ip2 -> ip2
I0426 20:52:52.887261 32328 net.cpp:124] Setting up ip2
I0426 20:52:52.887269 32328 net.cpp:131] Top shape: 64 100 (6400)
I0426 20:52:52.887274 32328 net.cpp:139] Memory required for data: 5500160
I0426 20:52:52.887280 32328 layer_factory.hpp:77] Creating layer relu2
I0426 20:52:52.887289 32328 net.cpp:86] Creating Layer relu2
I0426 20:52:52.887292 32328 net.cpp:408] relu2 <- ip2
I0426 20:52:52.887297 32328 net.cpp:369] relu2 -> ip2 (in-place)
I0426 20:52:52.888226 32328 net.cpp:124] Setting up relu2
I0426 20:52:52.888242 32328 net.cpp:131] Top shape: 64 100 (6400)
I0426 20:52:52.888245 32328 net.cpp:139] Memory required for data: 5525760
I0426 20:52:52.888249 32328 layer_factory.hpp:77] Creating layer ip3
I0426 20:52:52.888257 32328 net.cpp:86] Creating Layer ip3
I0426 20:52:52.888260 32328 net.cpp:408] ip3 <- ip2
I0426 20:52:52.888267 32328 net.cpp:382] ip3 -> ip3
I0426 20:52:52.888391 32328 net.cpp:124] Setting up ip3
I0426 20:52:52.888401 32328 net.cpp:131] Top shape: 64 10 (640)
I0426 20:52:52.888404 32328 net.cpp:139] Memory required for data: 5528320
I0426 20:52:52.888413 32328 layer_factory.hpp:77] Creating layer relu3
I0426 20:52:52.888420 32328 net.cpp:86] Creating Layer relu3
I0426 20:52:52.888424 32328 net.cpp:408] relu3 <- ip3
I0426 20:52:52.888428 32328 net.cpp:369] relu3 -> ip3 (in-place)
I0426 20:52:52.888629 32328 net.cpp:124] Setting up relu3
I0426 20:52:52.888639 32328 net.cpp:131] Top shape: 64 10 (640)
I0426 20:52:52.888643 32328 net.cpp:139] Memory required for data: 5530880
I0426 20:52:52.888646 32328 layer_factory.hpp:77] Creating layer loss
I0426 20:52:52.888653 32328 net.cpp:86] Creating Layer loss
I0426 20:52:52.888658 32328 net.cpp:408] loss <- ip3
I0426 20:52:52.888662 32328 net.cpp:408] loss <- label
I0426 20:52:52.888667 32328 net.cpp:382] loss -> loss
I0426 20:52:52.888689 32328 layer_factory.hpp:77] Creating layer loss
I0426 20:52:52.888980 32328 net.cpp:124] Setting up loss
I0426 20:52:52.888993 32328 net.cpp:131] Top shape: (1)
I0426 20:52:52.888995 32328 net.cpp:134]     with loss weight 1
I0426 20:52:52.889011 32328 net.cpp:139] Memory required for data: 5530884
I0426 20:52:52.889015 32328 net.cpp:200] loss needs backward computation.
I0426 20:52:52.889020 32328 net.cpp:200] relu3 needs backward computation.
I0426 20:52:52.889024 32328 net.cpp:200] ip3 needs backward computation.
I0426 20:52:52.889026 32328 net.cpp:200] relu2 needs backward computation.
I0426 20:52:52.889029 32328 net.cpp:200] ip2 needs backward computation.
I0426 20:52:52.889032 32328 net.cpp:200] relu1 needs backward computation.
I0426 20:52:52.889036 32328 net.cpp:200] ip1 needs backward computation.
I0426 20:52:52.889039 32328 net.cpp:200] pool1 needs backward computation.
I0426 20:52:52.889044 32328 net.cpp:200] conv1 needs backward computation.
I0426 20:52:52.889046 32328 net.cpp:200] pool0 needs backward computation.
I0426 20:52:52.889050 32328 net.cpp:200] conv0 needs backward computation.
I0426 20:52:52.889053 32328 net.cpp:202] mnist does not need backward computation.
I0426 20:52:52.889056 32328 net.cpp:244] This network produces output loss
I0426 20:52:52.889068 32328 net.cpp:257] Network initialization done.
I0426 20:52:52.889461 32328 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything/lenet_train_test123.prototxt
I0426 20:52:52.889494 32328 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0426 20:52:52.889605 32328 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0426 20:52:52.889705 32328 layer_factory.hpp:77] Creating layer mnist
I0426 20:52:52.889758 32328 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0426 20:52:52.889775 32328 net.cpp:86] Creating Layer mnist
I0426 20:52:52.889780 32328 net.cpp:382] mnist -> data
I0426 20:52:52.889788 32328 net.cpp:382] mnist -> label
I0426 20:52:52.889890 32328 data_layer.cpp:45] output data size: 100,1,28,28
I0426 20:52:52.891213 32328 net.cpp:124] Setting up mnist
I0426 20:52:52.891229 32328 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0426 20:52:52.891235 32328 net.cpp:131] Top shape: 100 (100)
I0426 20:52:52.891238 32328 net.cpp:139] Memory required for data: 314000
I0426 20:52:52.891242 32328 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0426 20:52:52.891252 32328 net.cpp:86] Creating Layer label_mnist_1_split
I0426 20:52:52.891257 32328 net.cpp:408] label_mnist_1_split <- label
I0426 20:52:52.891263 32328 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0426 20:52:52.891271 32328 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0426 20:52:52.891362 32328 net.cpp:124] Setting up label_mnist_1_split
I0426 20:52:52.891378 32328 net.cpp:131] Top shape: 100 (100)
I0426 20:52:52.891383 32328 net.cpp:131] Top shape: 100 (100)
I0426 20:52:52.891386 32328 net.cpp:139] Memory required for data: 314800
I0426 20:52:52.891389 32328 layer_factory.hpp:77] Creating layer conv0
I0426 20:52:52.891402 32328 net.cpp:86] Creating Layer conv0
I0426 20:52:52.891405 32328 net.cpp:408] conv0 <- data
I0426 20:52:52.891412 32328 net.cpp:382] conv0 -> conv0
I0426 20:52:52.893332 32328 net.cpp:124] Setting up conv0
I0426 20:52:52.893347 32328 net.cpp:131] Top shape: 100 25 24 24 (1440000)
I0426 20:52:52.893353 32328 net.cpp:139] Memory required for data: 6074800
I0426 20:52:52.893364 32328 layer_factory.hpp:77] Creating layer pool0
I0426 20:52:52.893373 32328 net.cpp:86] Creating Layer pool0
I0426 20:52:52.893376 32328 net.cpp:408] pool0 <- conv0
I0426 20:52:52.893381 32328 net.cpp:382] pool0 -> pool0
I0426 20:52:52.893427 32328 net.cpp:124] Setting up pool0
I0426 20:52:52.893435 32328 net.cpp:131] Top shape: 100 25 12 12 (360000)
I0426 20:52:52.893440 32328 net.cpp:139] Memory required for data: 7514800
I0426 20:52:52.893442 32328 layer_factory.hpp:77] Creating layer conv1
I0426 20:52:52.893453 32328 net.cpp:86] Creating Layer conv1
I0426 20:52:52.893457 32328 net.cpp:408] conv1 <- pool0
I0426 20:52:52.893465 32328 net.cpp:382] conv1 -> conv1
I0426 20:52:52.896173 32328 net.cpp:124] Setting up conv1
I0426 20:52:52.896188 32328 net.cpp:131] Top shape: 100 25 8 8 (160000)
I0426 20:52:52.896191 32328 net.cpp:139] Memory required for data: 8154800
I0426 20:52:52.896201 32328 layer_factory.hpp:77] Creating layer pool1
I0426 20:52:52.896209 32328 net.cpp:86] Creating Layer pool1
I0426 20:52:52.896212 32328 net.cpp:408] pool1 <- conv1
I0426 20:52:52.896220 32328 net.cpp:382] pool1 -> pool1
I0426 20:52:52.896270 32328 net.cpp:124] Setting up pool1
I0426 20:52:52.896278 32328 net.cpp:131] Top shape: 100 25 4 4 (40000)
I0426 20:52:52.896281 32328 net.cpp:139] Memory required for data: 8314800
I0426 20:52:52.896284 32328 layer_factory.hpp:77] Creating layer ip1
I0426 20:52:52.896292 32328 net.cpp:86] Creating Layer ip1
I0426 20:52:52.896296 32328 net.cpp:408] ip1 <- pool1
I0426 20:52:52.896301 32328 net.cpp:382] ip1 -> ip1
I0426 20:52:52.898116 32328 net.cpp:124] Setting up ip1
I0426 20:52:52.898130 32328 net.cpp:131] Top shape: 100 300 (30000)
I0426 20:52:52.898145 32328 net.cpp:139] Memory required for data: 8434800
I0426 20:52:52.898155 32328 layer_factory.hpp:77] Creating layer relu1
I0426 20:52:52.898161 32328 net.cpp:86] Creating Layer relu1
I0426 20:52:52.898165 32328 net.cpp:408] relu1 <- ip1
I0426 20:52:52.898174 32328 net.cpp:369] relu1 -> ip1 (in-place)
I0426 20:52:52.898438 32328 net.cpp:124] Setting up relu1
I0426 20:52:52.898448 32328 net.cpp:131] Top shape: 100 300 (30000)
I0426 20:52:52.898453 32328 net.cpp:139] Memory required for data: 8554800
I0426 20:52:52.898458 32328 layer_factory.hpp:77] Creating layer ip2
I0426 20:52:52.898468 32328 net.cpp:86] Creating Layer ip2
I0426 20:52:52.898473 32328 net.cpp:408] ip2 <- ip1
I0426 20:52:52.898478 32328 net.cpp:382] ip2 -> ip2
I0426 20:52:52.898797 32328 net.cpp:124] Setting up ip2
I0426 20:52:52.898807 32328 net.cpp:131] Top shape: 100 100 (10000)
I0426 20:52:52.898810 32328 net.cpp:139] Memory required for data: 8594800
I0426 20:52:52.898816 32328 layer_factory.hpp:77] Creating layer relu2
I0426 20:52:52.898823 32328 net.cpp:86] Creating Layer relu2
I0426 20:52:52.898826 32328 net.cpp:408] relu2 <- ip2
I0426 20:52:52.898833 32328 net.cpp:369] relu2 -> ip2 (in-place)
I0426 20:52:52.899019 32328 net.cpp:124] Setting up relu2
I0426 20:52:52.899027 32328 net.cpp:131] Top shape: 100 100 (10000)
I0426 20:52:52.899031 32328 net.cpp:139] Memory required for data: 8634800
I0426 20:52:52.899035 32328 layer_factory.hpp:77] Creating layer ip3
I0426 20:52:52.899042 32328 net.cpp:86] Creating Layer ip3
I0426 20:52:52.899045 32328 net.cpp:408] ip3 <- ip2
I0426 20:52:52.899052 32328 net.cpp:382] ip3 -> ip3
I0426 20:52:52.899176 32328 net.cpp:124] Setting up ip3
I0426 20:52:52.899183 32328 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:52:52.899188 32328 net.cpp:139] Memory required for data: 8638800
I0426 20:52:52.899196 32328 layer_factory.hpp:77] Creating layer relu3
I0426 20:52:52.899201 32328 net.cpp:86] Creating Layer relu3
I0426 20:52:52.899205 32328 net.cpp:408] relu3 <- ip3
I0426 20:52:52.899209 32328 net.cpp:369] relu3 -> ip3 (in-place)
I0426 20:52:52.900135 32328 net.cpp:124] Setting up relu3
I0426 20:52:52.900148 32328 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:52:52.900152 32328 net.cpp:139] Memory required for data: 8642800
I0426 20:52:52.900156 32328 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0426 20:52:52.900163 32328 net.cpp:86] Creating Layer ip3_relu3_0_split
I0426 20:52:52.900167 32328 net.cpp:408] ip3_relu3_0_split <- ip3
I0426 20:52:52.900172 32328 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0426 20:52:52.900180 32328 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0426 20:52:52.900223 32328 net.cpp:124] Setting up ip3_relu3_0_split
I0426 20:52:52.900233 32328 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:52:52.900238 32328 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:52:52.900240 32328 net.cpp:139] Memory required for data: 8650800
I0426 20:52:52.900243 32328 layer_factory.hpp:77] Creating layer accuracy
I0426 20:52:52.900250 32328 net.cpp:86] Creating Layer accuracy
I0426 20:52:52.900254 32328 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0426 20:52:52.900259 32328 net.cpp:408] accuracy <- label_mnist_1_split_0
I0426 20:52:52.900264 32328 net.cpp:382] accuracy -> accuracy
I0426 20:52:52.900270 32328 net.cpp:124] Setting up accuracy
I0426 20:52:52.900274 32328 net.cpp:131] Top shape: (1)
I0426 20:52:52.900279 32328 net.cpp:139] Memory required for data: 8650804
I0426 20:52:52.900281 32328 layer_factory.hpp:77] Creating layer loss
I0426 20:52:52.900286 32328 net.cpp:86] Creating Layer loss
I0426 20:52:52.900290 32328 net.cpp:408] loss <- ip3_relu3_0_split_1
I0426 20:52:52.900293 32328 net.cpp:408] loss <- label_mnist_1_split_1
I0426 20:52:52.900300 32328 net.cpp:382] loss -> loss
I0426 20:52:52.900305 32328 layer_factory.hpp:77] Creating layer loss
I0426 20:52:52.900575 32328 net.cpp:124] Setting up loss
I0426 20:52:52.900585 32328 net.cpp:131] Top shape: (1)
I0426 20:52:52.900588 32328 net.cpp:134]     with loss weight 1
I0426 20:52:52.900607 32328 net.cpp:139] Memory required for data: 8650808
I0426 20:52:52.900611 32328 net.cpp:200] loss needs backward computation.
I0426 20:52:52.900615 32328 net.cpp:202] accuracy does not need backward computation.
I0426 20:52:52.900619 32328 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0426 20:52:52.900624 32328 net.cpp:200] relu3 needs backward computation.
I0426 20:52:52.900626 32328 net.cpp:200] ip3 needs backward computation.
I0426 20:52:52.900629 32328 net.cpp:200] relu2 needs backward computation.
I0426 20:52:52.900632 32328 net.cpp:200] ip2 needs backward computation.
I0426 20:52:52.900635 32328 net.cpp:200] relu1 needs backward computation.
I0426 20:52:52.900638 32328 net.cpp:200] ip1 needs backward computation.
I0426 20:52:52.900641 32328 net.cpp:200] pool1 needs backward computation.
I0426 20:52:52.900645 32328 net.cpp:200] conv1 needs backward computation.
I0426 20:52:52.900648 32328 net.cpp:200] pool0 needs backward computation.
I0426 20:52:52.900651 32328 net.cpp:200] conv0 needs backward computation.
I0426 20:52:52.900655 32328 net.cpp:202] label_mnist_1_split does not need backward computation.
I0426 20:52:52.900660 32328 net.cpp:202] mnist does not need backward computation.
I0426 20:52:52.900662 32328 net.cpp:244] This network produces output accuracy
I0426 20:52:52.900666 32328 net.cpp:244] This network produces output loss
I0426 20:52:52.900679 32328 net.cpp:257] Network initialization done.
I0426 20:52:52.900725 32328 solver.cpp:56] Solver scaffolding done.
I0426 20:52:52.901110 32328 caffe.cpp:248] Starting Optimization
I0426 20:52:52.901118 32328 solver.cpp:273] Solving LeNet
I0426 20:52:52.901121 32328 solver.cpp:274] Learning Rate Policy: inv
I0426 20:52:52.902045 32328 solver.cpp:331] Iteration 0, Testing net (#0)
I0426 20:52:52.909170 32328 blocking_queue.cpp:49] Waiting for data
I0426 20:52:52.980521 32335 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:52:52.981185 32328 solver.cpp:398]     Test net output #0: accuracy = 0.1274
I0426 20:52:52.981204 32328 solver.cpp:398]     Test net output #1: loss = 2.29807 (* 1 = 2.29807 loss)
I0426 20:52:52.984972 32328 solver.cpp:219] Iteration 0 (-6.80546e-39 iter/s, 0.0838235s/100 iters), loss = 2.29415
I0426 20:52:52.985010 32328 solver.cpp:238]     Train net output #0: loss = 2.29415 (* 1 = 2.29415 loss)
I0426 20:52:52.985021 32328 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0426 20:52:53.112723 32328 solver.cpp:219] Iteration 100 (782.992 iter/s, 0.127715s/100 iters), loss = 0.53208
I0426 20:52:53.112763 32328 solver.cpp:238]     Train net output #0: loss = 0.53208 (* 1 = 0.53208 loss)
I0426 20:52:53.112769 32328 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0426 20:52:53.231446 32328 solver.cpp:219] Iteration 200 (842.656 iter/s, 0.118672s/100 iters), loss = 0.350631
I0426 20:52:53.231485 32328 solver.cpp:238]     Train net output #0: loss = 0.350631 (* 1 = 0.350631 loss)
I0426 20:52:53.231492 32328 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0426 20:52:53.348294 32328 solver.cpp:219] Iteration 300 (856.071 iter/s, 0.116813s/100 iters), loss = 0.36545
I0426 20:52:53.348315 32328 solver.cpp:238]     Train net output #0: loss = 0.36545 (* 1 = 0.36545 loss)
I0426 20:52:53.348338 32328 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0426 20:52:53.462651 32328 solver.cpp:219] Iteration 400 (874.703 iter/s, 0.114324s/100 iters), loss = 0.295636
I0426 20:52:53.462674 32328 solver.cpp:238]     Train net output #0: loss = 0.295636 (* 1 = 0.295636 loss)
I0426 20:52:53.462680 32328 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0426 20:52:53.576936 32328 solver.cpp:331] Iteration 500, Testing net (#0)
I0426 20:52:53.653187 32335 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:52:53.653867 32328 solver.cpp:398]     Test net output #0: accuracy = 0.8796
I0426 20:52:53.653884 32328 solver.cpp:398]     Test net output #1: loss = 0.310283 (* 1 = 0.310283 loss)
I0426 20:52:53.655026 32328 solver.cpp:219] Iteration 500 (519.921 iter/s, 0.192337s/100 iters), loss = 0.306865
I0426 20:52:53.655066 32328 solver.cpp:238]     Train net output #0: loss = 0.306865 (* 1 = 0.306865 loss)
I0426 20:52:53.655091 32328 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0426 20:52:53.775599 32328 solver.cpp:219] Iteration 600 (829.725 iter/s, 0.120522s/100 iters), loss = 0.371651
I0426 20:52:53.775625 32328 solver.cpp:238]     Train net output #0: loss = 0.371651 (* 1 = 0.371651 loss)
I0426 20:52:53.775631 32328 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0426 20:52:53.894253 32328 solver.cpp:219] Iteration 700 (843.078 iter/s, 0.118613s/100 iters), loss = 0.343174
I0426 20:52:53.894299 32328 solver.cpp:238]     Train net output #0: loss = 0.343174 (* 1 = 0.343174 loss)
I0426 20:52:53.894309 32328 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0426 20:52:54.010495 32328 solver.cpp:219] Iteration 800 (860.673 iter/s, 0.116188s/100 iters), loss = 0.455485
I0426 20:52:54.010524 32328 solver.cpp:238]     Train net output #0: loss = 0.455485 (* 1 = 0.455485 loss)
I0426 20:52:54.010532 32328 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0426 20:52:54.131451 32328 solver.cpp:219] Iteration 900 (827.049 iter/s, 0.120912s/100 iters), loss = 0.152528
I0426 20:52:54.131505 32328 solver.cpp:238]     Train net output #0: loss = 0.152528 (* 1 = 0.152528 loss)
I0426 20:52:54.131515 32328 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0426 20:52:54.172029 32334 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:52:54.247601 32328 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0426 20:52:54.252049 32328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0426 20:52:54.253998 32328 solver.cpp:311] Iteration 1000, loss = 0.119352
I0426 20:52:54.254015 32328 solver.cpp:331] Iteration 1000, Testing net (#0)
I0426 20:52:54.327320 32335 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:52:54.328402 32328 solver.cpp:398]     Test net output #0: accuracy = 0.9784
I0426 20:52:54.328426 32328 solver.cpp:398]     Test net output #1: loss = 0.070048 (* 1 = 0.070048 loss)
I0426 20:52:54.328433 32328 solver.cpp:316] Optimization Done.
I0426 20:52:54.328436 32328 caffe.cpp:259] Optimization Done.
