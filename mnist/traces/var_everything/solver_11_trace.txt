I0426 20:48:35.720024 31293 caffe.cpp:218] Using GPUs 0
I0426 20:48:35.755054 31293 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0426 20:48:36.219424 31293 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything/lenet_train_test11.prototxt"
train_state {
  level: 0
  stage: ""
}
I0426 20:48:36.219573 31293 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything/lenet_train_test11.prototxt
I0426 20:48:36.219820 31293 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0426 20:48:36.219832 31293 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0426 20:48:36.219907 31293 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0426 20:48:36.219961 31293 layer_factory.hpp:77] Creating layer mnist
I0426 20:48:36.220043 31293 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0426 20:48:36.220062 31293 net.cpp:86] Creating Layer mnist
I0426 20:48:36.220069 31293 net.cpp:382] mnist -> data
I0426 20:48:36.220103 31293 net.cpp:382] mnist -> label
I0426 20:48:36.221074 31293 data_layer.cpp:45] output data size: 64,1,28,28
I0426 20:48:36.223451 31293 net.cpp:124] Setting up mnist
I0426 20:48:36.223465 31293 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0426 20:48:36.223470 31293 net.cpp:131] Top shape: 64 (64)
I0426 20:48:36.223474 31293 net.cpp:139] Memory required for data: 200960
I0426 20:48:36.223479 31293 layer_factory.hpp:77] Creating layer ip1
I0426 20:48:36.223487 31293 net.cpp:86] Creating Layer ip1
I0426 20:48:36.223491 31293 net.cpp:408] ip1 <- data
I0426 20:48:36.223500 31293 net.cpp:382] ip1 -> ip1
I0426 20:48:36.226660 31293 net.cpp:124] Setting up ip1
I0426 20:48:36.226671 31293 net.cpp:131] Top shape: 64 300 (19200)
I0426 20:48:36.226682 31293 net.cpp:139] Memory required for data: 277760
I0426 20:48:36.226693 31293 layer_factory.hpp:77] Creating layer relu1
I0426 20:48:36.226718 31293 net.cpp:86] Creating Layer relu1
I0426 20:48:36.226722 31293 net.cpp:408] relu1 <- ip1
I0426 20:48:36.226727 31293 net.cpp:369] relu1 -> ip1 (in-place)
I0426 20:48:36.497264 31293 net.cpp:124] Setting up relu1
I0426 20:48:36.497303 31293 net.cpp:131] Top shape: 64 300 (19200)
I0426 20:48:36.497308 31293 net.cpp:139] Memory required for data: 354560
I0426 20:48:36.497314 31293 layer_factory.hpp:77] Creating layer ip2
I0426 20:48:36.497329 31293 net.cpp:86] Creating Layer ip2
I0426 20:48:36.497355 31293 net.cpp:408] ip2 <- ip1
I0426 20:48:36.497364 31293 net.cpp:382] ip2 -> ip2
I0426 20:48:36.497706 31293 net.cpp:124] Setting up ip2
I0426 20:48:36.497722 31293 net.cpp:131] Top shape: 64 100 (6400)
I0426 20:48:36.497726 31293 net.cpp:139] Memory required for data: 380160
I0426 20:48:36.497737 31293 layer_factory.hpp:77] Creating layer relu2
I0426 20:48:36.497745 31293 net.cpp:86] Creating Layer relu2
I0426 20:48:36.497750 31293 net.cpp:408] relu2 <- ip2
I0426 20:48:36.497755 31293 net.cpp:369] relu2 -> ip2 (in-place)
I0426 20:48:36.498785 31293 net.cpp:124] Setting up relu2
I0426 20:48:36.498800 31293 net.cpp:131] Top shape: 64 100 (6400)
I0426 20:48:36.498805 31293 net.cpp:139] Memory required for data: 405760
I0426 20:48:36.498808 31293 layer_factory.hpp:77] Creating layer ip3
I0426 20:48:36.498817 31293 net.cpp:86] Creating Layer ip3
I0426 20:48:36.498821 31293 net.cpp:408] ip3 <- ip2
I0426 20:48:36.498827 31293 net.cpp:382] ip3 -> ip3
I0426 20:48:36.498953 31293 net.cpp:124] Setting up ip3
I0426 20:48:36.498963 31293 net.cpp:131] Top shape: 64 10 (640)
I0426 20:48:36.498966 31293 net.cpp:139] Memory required for data: 408320
I0426 20:48:36.498976 31293 layer_factory.hpp:77] Creating layer relu3
I0426 20:48:36.498982 31293 net.cpp:86] Creating Layer relu3
I0426 20:48:36.498986 31293 net.cpp:408] relu3 <- ip3
I0426 20:48:36.498991 31293 net.cpp:369] relu3 -> ip3 (in-place)
I0426 20:48:36.499166 31293 net.cpp:124] Setting up relu3
I0426 20:48:36.499176 31293 net.cpp:131] Top shape: 64 10 (640)
I0426 20:48:36.499179 31293 net.cpp:139] Memory required for data: 410880
I0426 20:48:36.499183 31293 layer_factory.hpp:77] Creating layer loss
I0426 20:48:36.499189 31293 net.cpp:86] Creating Layer loss
I0426 20:48:36.499193 31293 net.cpp:408] loss <- ip3
I0426 20:48:36.499198 31293 net.cpp:408] loss <- label
I0426 20:48:36.499204 31293 net.cpp:382] loss -> loss
I0426 20:48:36.499227 31293 layer_factory.hpp:77] Creating layer loss
I0426 20:48:36.499505 31293 net.cpp:124] Setting up loss
I0426 20:48:36.499516 31293 net.cpp:131] Top shape: (1)
I0426 20:48:36.499521 31293 net.cpp:134]     with loss weight 1
I0426 20:48:36.499537 31293 net.cpp:139] Memory required for data: 410884
I0426 20:48:36.499541 31293 net.cpp:200] loss needs backward computation.
I0426 20:48:36.499546 31293 net.cpp:200] relu3 needs backward computation.
I0426 20:48:36.499549 31293 net.cpp:200] ip3 needs backward computation.
I0426 20:48:36.499552 31293 net.cpp:200] relu2 needs backward computation.
I0426 20:48:36.499555 31293 net.cpp:200] ip2 needs backward computation.
I0426 20:48:36.499559 31293 net.cpp:200] relu1 needs backward computation.
I0426 20:48:36.499562 31293 net.cpp:200] ip1 needs backward computation.
I0426 20:48:36.499567 31293 net.cpp:202] mnist does not need backward computation.
I0426 20:48:36.499570 31293 net.cpp:244] This network produces output loss
I0426 20:48:36.499578 31293 net.cpp:257] Network initialization done.
I0426 20:48:36.499858 31293 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything/lenet_train_test11.prototxt
I0426 20:48:36.499893 31293 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0426 20:48:36.499974 31293 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0426 20:48:36.500051 31293 layer_factory.hpp:77] Creating layer mnist
I0426 20:48:36.500104 31293 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0426 20:48:36.500118 31293 net.cpp:86] Creating Layer mnist
I0426 20:48:36.500138 31293 net.cpp:382] mnist -> data
I0426 20:48:36.500149 31293 net.cpp:382] mnist -> label
I0426 20:48:36.500247 31293 data_layer.cpp:45] output data size: 100,1,28,28
I0426 20:48:36.502600 31293 net.cpp:124] Setting up mnist
I0426 20:48:36.502616 31293 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0426 20:48:36.502622 31293 net.cpp:131] Top shape: 100 (100)
I0426 20:48:36.502626 31293 net.cpp:139] Memory required for data: 314000
I0426 20:48:36.502640 31293 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0426 20:48:36.502652 31293 net.cpp:86] Creating Layer label_mnist_1_split
I0426 20:48:36.502656 31293 net.cpp:408] label_mnist_1_split <- label
I0426 20:48:36.502667 31293 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0426 20:48:36.502676 31293 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0426 20:48:36.502728 31293 net.cpp:124] Setting up label_mnist_1_split
I0426 20:48:36.502735 31293 net.cpp:131] Top shape: 100 (100)
I0426 20:48:36.502739 31293 net.cpp:131] Top shape: 100 (100)
I0426 20:48:36.502743 31293 net.cpp:139] Memory required for data: 314800
I0426 20:48:36.502746 31293 layer_factory.hpp:77] Creating layer ip1
I0426 20:48:36.502753 31293 net.cpp:86] Creating Layer ip1
I0426 20:48:36.502758 31293 net.cpp:408] ip1 <- data
I0426 20:48:36.502763 31293 net.cpp:382] ip1 -> ip1
I0426 20:48:36.504547 31293 net.cpp:124] Setting up ip1
I0426 20:48:36.504559 31293 net.cpp:131] Top shape: 100 300 (30000)
I0426 20:48:36.504562 31293 net.cpp:139] Memory required for data: 434800
I0426 20:48:36.504573 31293 layer_factory.hpp:77] Creating layer relu1
I0426 20:48:36.504580 31293 net.cpp:86] Creating Layer relu1
I0426 20:48:36.504585 31293 net.cpp:408] relu1 <- ip1
I0426 20:48:36.504590 31293 net.cpp:369] relu1 -> ip1 (in-place)
I0426 20:48:36.505645 31293 net.cpp:124] Setting up relu1
I0426 20:48:36.505658 31293 net.cpp:131] Top shape: 100 300 (30000)
I0426 20:48:36.505663 31293 net.cpp:139] Memory required for data: 554800
I0426 20:48:36.505667 31293 layer_factory.hpp:77] Creating layer ip2
I0426 20:48:36.505676 31293 net.cpp:86] Creating Layer ip2
I0426 20:48:36.505681 31293 net.cpp:408] ip2 <- ip1
I0426 20:48:36.505687 31293 net.cpp:382] ip2 -> ip2
I0426 20:48:36.506022 31293 net.cpp:124] Setting up ip2
I0426 20:48:36.506032 31293 net.cpp:131] Top shape: 100 100 (10000)
I0426 20:48:36.506037 31293 net.cpp:139] Memory required for data: 594800
I0426 20:48:36.506047 31293 layer_factory.hpp:77] Creating layer relu2
I0426 20:48:36.506052 31293 net.cpp:86] Creating Layer relu2
I0426 20:48:36.506057 31293 net.cpp:408] relu2 <- ip2
I0426 20:48:36.506062 31293 net.cpp:369] relu2 -> ip2 (in-place)
I0426 20:48:36.506258 31293 net.cpp:124] Setting up relu2
I0426 20:48:36.506269 31293 net.cpp:131] Top shape: 100 100 (10000)
I0426 20:48:36.506274 31293 net.cpp:139] Memory required for data: 634800
I0426 20:48:36.506278 31293 layer_factory.hpp:77] Creating layer ip3
I0426 20:48:36.506297 31293 net.cpp:86] Creating Layer ip3
I0426 20:48:36.506301 31293 net.cpp:408] ip3 <- ip2
I0426 20:48:36.506319 31293 net.cpp:382] ip3 -> ip3
I0426 20:48:36.506453 31293 net.cpp:124] Setting up ip3
I0426 20:48:36.506464 31293 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:48:36.506467 31293 net.cpp:139] Memory required for data: 638800
I0426 20:48:36.506476 31293 layer_factory.hpp:77] Creating layer relu3
I0426 20:48:36.506482 31293 net.cpp:86] Creating Layer relu3
I0426 20:48:36.506485 31293 net.cpp:408] relu3 <- ip3
I0426 20:48:36.506490 31293 net.cpp:369] relu3 -> ip3 (in-place)
I0426 20:48:36.506678 31293 net.cpp:124] Setting up relu3
I0426 20:48:36.506688 31293 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:48:36.506692 31293 net.cpp:139] Memory required for data: 642800
I0426 20:48:36.506696 31293 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0426 20:48:36.506703 31293 net.cpp:86] Creating Layer ip3_relu3_0_split
I0426 20:48:36.506706 31293 net.cpp:408] ip3_relu3_0_split <- ip3
I0426 20:48:36.506712 31293 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0426 20:48:36.506719 31293 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0426 20:48:36.506764 31293 net.cpp:124] Setting up ip3_relu3_0_split
I0426 20:48:36.506772 31293 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:48:36.506777 31293 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:48:36.506780 31293 net.cpp:139] Memory required for data: 650800
I0426 20:48:36.506783 31293 layer_factory.hpp:77] Creating layer accuracy
I0426 20:48:36.506789 31293 net.cpp:86] Creating Layer accuracy
I0426 20:48:36.506793 31293 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0426 20:48:36.506798 31293 net.cpp:408] accuracy <- label_mnist_1_split_0
I0426 20:48:36.506803 31293 net.cpp:382] accuracy -> accuracy
I0426 20:48:36.506815 31293 net.cpp:124] Setting up accuracy
I0426 20:48:36.506820 31293 net.cpp:131] Top shape: (1)
I0426 20:48:36.506824 31293 net.cpp:139] Memory required for data: 650804
I0426 20:48:36.506826 31293 layer_factory.hpp:77] Creating layer loss
I0426 20:48:36.506831 31293 net.cpp:86] Creating Layer loss
I0426 20:48:36.506835 31293 net.cpp:408] loss <- ip3_relu3_0_split_1
I0426 20:48:36.506839 31293 net.cpp:408] loss <- label_mnist_1_split_1
I0426 20:48:36.506844 31293 net.cpp:382] loss -> loss
I0426 20:48:36.506850 31293 layer_factory.hpp:77] Creating layer loss
I0426 20:48:36.507129 31293 net.cpp:124] Setting up loss
I0426 20:48:36.507141 31293 net.cpp:131] Top shape: (1)
I0426 20:48:36.507153 31293 net.cpp:134]     with loss weight 1
I0426 20:48:36.507160 31293 net.cpp:139] Memory required for data: 650808
I0426 20:48:36.507164 31293 net.cpp:200] loss needs backward computation.
I0426 20:48:36.507169 31293 net.cpp:202] accuracy does not need backward computation.
I0426 20:48:36.507174 31293 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0426 20:48:36.507177 31293 net.cpp:200] relu3 needs backward computation.
I0426 20:48:36.507180 31293 net.cpp:200] ip3 needs backward computation.
I0426 20:48:36.507184 31293 net.cpp:200] relu2 needs backward computation.
I0426 20:48:36.507187 31293 net.cpp:200] ip2 needs backward computation.
I0426 20:48:36.507191 31293 net.cpp:200] relu1 needs backward computation.
I0426 20:48:36.507203 31293 net.cpp:200] ip1 needs backward computation.
I0426 20:48:36.507207 31293 net.cpp:202] label_mnist_1_split does not need backward computation.
I0426 20:48:36.507217 31293 net.cpp:202] mnist does not need backward computation.
I0426 20:48:36.507220 31293 net.cpp:244] This network produces output accuracy
I0426 20:48:36.507225 31293 net.cpp:244] This network produces output loss
I0426 20:48:36.507236 31293 net.cpp:257] Network initialization done.
I0426 20:48:36.507272 31293 solver.cpp:56] Solver scaffolding done.
I0426 20:48:36.507508 31293 caffe.cpp:248] Starting Optimization
I0426 20:48:36.507514 31293 solver.cpp:273] Solving LeNet
I0426 20:48:36.507519 31293 solver.cpp:274] Learning Rate Policy: inv
I0426 20:48:36.509244 31293 solver.cpp:331] Iteration 0, Testing net (#0)
I0426 20:48:36.511353 31293 blocking_queue.cpp:49] Waiting for data
I0426 20:48:36.585677 31300 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:48:36.586138 31293 solver.cpp:398]     Test net output #0: accuracy = 0.0967
I0426 20:48:36.586160 31293 solver.cpp:398]     Test net output #1: loss = 2.30499 (* 1 = 2.30499 loss)
I0426 20:48:36.586902 31293 solver.cpp:219] Iteration 0 (-5.89281e-31 iter/s, 0.0793591s/100 iters), loss = 2.30569
I0426 20:48:36.586930 31293 solver.cpp:238]     Train net output #0: loss = 2.30569 (* 1 = 2.30569 loss)
I0426 20:48:36.586942 31293 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0426 20:48:36.646319 31293 solver.cpp:219] Iteration 100 (1684.04 iter/s, 0.0593809s/100 iters), loss = 0.995726
I0426 20:48:36.646351 31293 solver.cpp:238]     Train net output #0: loss = 0.995726 (* 1 = 0.995726 loss)
I0426 20:48:36.646358 31293 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0426 20:48:36.707672 31293 solver.cpp:219] Iteration 200 (1631.05 iter/s, 0.06131s/100 iters), loss = 0.589853
I0426 20:48:36.707711 31293 solver.cpp:238]     Train net output #0: loss = 0.589853 (* 1 = 0.589853 loss)
I0426 20:48:36.707720 31293 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0426 20:48:36.772650 31293 solver.cpp:219] Iteration 300 (1540.13 iter/s, 0.0649298s/100 iters), loss = 0.327488
I0426 20:48:36.772691 31293 solver.cpp:238]     Train net output #0: loss = 0.327488 (* 1 = 0.327488 loss)
I0426 20:48:36.772711 31293 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0426 20:48:36.832669 31293 solver.cpp:219] Iteration 400 (1667.43 iter/s, 0.0599726s/100 iters), loss = 0.208187
I0426 20:48:36.832701 31293 solver.cpp:238]     Train net output #0: loss = 0.208187 (* 1 = 0.208187 loss)
I0426 20:48:36.832710 31293 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0426 20:48:36.887220 31293 solver.cpp:331] Iteration 500, Testing net (#0)
I0426 20:48:36.945637 31300 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:48:36.946086 31293 solver.cpp:398]     Test net output #0: accuracy = 0.9232
I0426 20:48:36.946107 31293 solver.cpp:398]     Test net output #1: loss = 0.267108 (* 1 = 0.267108 loss)
I0426 20:48:36.946662 31293 solver.cpp:219] Iteration 500 (877.557 iter/s, 0.113953s/100 iters), loss = 0.326527
I0426 20:48:36.946691 31293 solver.cpp:238]     Train net output #0: loss = 0.326527 (* 1 = 0.326527 loss)
I0426 20:48:36.946712 31293 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0426 20:48:37.000334 31293 solver.cpp:219] Iteration 600 (1864.45 iter/s, 0.0536351s/100 iters), loss = 0.204235
I0426 20:48:37.000365 31293 solver.cpp:238]     Train net output #0: loss = 0.204235 (* 1 = 0.204235 loss)
I0426 20:48:37.000371 31293 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0426 20:48:37.053786 31293 solver.cpp:219] Iteration 700 (1872.06 iter/s, 0.053417s/100 iters), loss = 0.343909
I0426 20:48:37.053824 31293 solver.cpp:238]     Train net output #0: loss = 0.343909 (* 1 = 0.343909 loss)
I0426 20:48:37.053831 31293 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0426 20:48:37.107039 31293 solver.cpp:219] Iteration 800 (1879.52 iter/s, 0.053205s/100 iters), loss = 0.353383
I0426 20:48:37.107069 31293 solver.cpp:238]     Train net output #0: loss = 0.353383 (* 1 = 0.353383 loss)
I0426 20:48:37.107074 31293 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0426 20:48:37.160531 31293 solver.cpp:219] Iteration 900 (1870.67 iter/s, 0.0534569s/100 iters), loss = 0.276734
I0426 20:48:37.160560 31293 solver.cpp:238]     Train net output #0: loss = 0.276734 (* 1 = 0.276734 loss)
I0426 20:48:37.160567 31293 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0426 20:48:37.178544 31299 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:48:37.213570 31293 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0426 20:48:37.217859 31293 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0426 20:48:37.220428 31293 solver.cpp:311] Iteration 1000, loss = 0.247092
I0426 20:48:37.220445 31293 solver.cpp:331] Iteration 1000, Testing net (#0)
I0426 20:48:37.276394 31300 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:48:37.276856 31293 solver.cpp:398]     Test net output #0: accuracy = 0.9444
I0426 20:48:37.276877 31293 solver.cpp:398]     Test net output #1: loss = 0.186831 (* 1 = 0.186831 loss)
I0426 20:48:37.276883 31293 solver.cpp:316] Optimization Done.
I0426 20:48:37.276887 31293 caffe.cpp:259] Optimization Done.
