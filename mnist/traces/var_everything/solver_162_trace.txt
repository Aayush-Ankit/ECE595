I0426 20:54:34.751698 32697 caffe.cpp:218] Using GPUs 0
I0426 20:54:34.784490 32697 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0426 20:54:35.247419 32697 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything/lenet_train_test162.prototxt"
train_state {
  level: 0
  stage: ""
}
I0426 20:54:35.247541 32697 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything/lenet_train_test162.prototxt
I0426 20:54:35.247912 32697 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0426 20:54:35.247930 32697 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0426 20:54:35.248018 32697 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0426 20:54:35.248111 32697 layer_factory.hpp:77] Creating layer mnist
I0426 20:54:35.248236 32697 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0426 20:54:35.248263 32697 net.cpp:86] Creating Layer mnist
I0426 20:54:35.248273 32697 net.cpp:382] mnist -> data
I0426 20:54:35.248298 32697 net.cpp:382] mnist -> label
I0426 20:54:35.249486 32697 data_layer.cpp:45] output data size: 64,1,28,28
I0426 20:54:35.251549 32697 net.cpp:124] Setting up mnist
I0426 20:54:35.251562 32697 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0426 20:54:35.251567 32697 net.cpp:131] Top shape: 64 (64)
I0426 20:54:35.251569 32697 net.cpp:139] Memory required for data: 200960
I0426 20:54:35.251574 32697 layer_factory.hpp:77] Creating layer conv0
I0426 20:54:35.251616 32697 net.cpp:86] Creating Layer conv0
I0426 20:54:35.251641 32697 net.cpp:408] conv0 <- data
I0426 20:54:35.251655 32697 net.cpp:382] conv0 -> conv0
I0426 20:54:35.475723 32697 net.cpp:124] Setting up conv0
I0426 20:54:35.475765 32697 net.cpp:131] Top shape: 64 50 24 24 (1843200)
I0426 20:54:35.475769 32697 net.cpp:139] Memory required for data: 7573760
I0426 20:54:35.475785 32697 layer_factory.hpp:77] Creating layer pool0
I0426 20:54:35.475797 32697 net.cpp:86] Creating Layer pool0
I0426 20:54:35.475801 32697 net.cpp:408] pool0 <- conv0
I0426 20:54:35.475807 32697 net.cpp:382] pool0 -> pool0
I0426 20:54:35.475878 32697 net.cpp:124] Setting up pool0
I0426 20:54:35.475890 32697 net.cpp:131] Top shape: 64 50 12 12 (460800)
I0426 20:54:35.475894 32697 net.cpp:139] Memory required for data: 9416960
I0426 20:54:35.475899 32697 layer_factory.hpp:77] Creating layer conv1
I0426 20:54:35.475914 32697 net.cpp:86] Creating Layer conv1
I0426 20:54:35.475919 32697 net.cpp:408] conv1 <- pool0
I0426 20:54:35.475926 32697 net.cpp:382] conv1 -> conv1
I0426 20:54:35.477941 32697 net.cpp:124] Setting up conv1
I0426 20:54:35.477969 32697 net.cpp:131] Top shape: 64 25 8 8 (102400)
I0426 20:54:35.477973 32697 net.cpp:139] Memory required for data: 9826560
I0426 20:54:35.477980 32697 layer_factory.hpp:77] Creating layer pool1
I0426 20:54:35.477988 32697 net.cpp:86] Creating Layer pool1
I0426 20:54:35.477993 32697 net.cpp:408] pool1 <- conv1
I0426 20:54:35.477996 32697 net.cpp:382] pool1 -> pool1
I0426 20:54:35.478061 32697 net.cpp:124] Setting up pool1
I0426 20:54:35.478070 32697 net.cpp:131] Top shape: 64 25 4 4 (25600)
I0426 20:54:35.478073 32697 net.cpp:139] Memory required for data: 9928960
I0426 20:54:35.478077 32697 layer_factory.hpp:77] Creating layer ip1
I0426 20:54:35.478087 32697 net.cpp:86] Creating Layer ip1
I0426 20:54:35.478091 32697 net.cpp:408] ip1 <- pool1
I0426 20:54:35.478101 32697 net.cpp:382] ip1 -> ip1
I0426 20:54:35.478410 32697 net.cpp:124] Setting up ip1
I0426 20:54:35.478418 32697 net.cpp:131] Top shape: 64 100 (6400)
I0426 20:54:35.478421 32697 net.cpp:139] Memory required for data: 9954560
I0426 20:54:35.478428 32697 layer_factory.hpp:77] Creating layer relu1
I0426 20:54:35.478437 32697 net.cpp:86] Creating Layer relu1
I0426 20:54:35.478442 32697 net.cpp:408] relu1 <- ip1
I0426 20:54:35.478451 32697 net.cpp:369] relu1 -> ip1 (in-place)
I0426 20:54:35.478617 32697 net.cpp:124] Setting up relu1
I0426 20:54:35.478627 32697 net.cpp:131] Top shape: 64 100 (6400)
I0426 20:54:35.478631 32697 net.cpp:139] Memory required for data: 9980160
I0426 20:54:35.478633 32697 layer_factory.hpp:77] Creating layer ip2
I0426 20:54:35.478643 32697 net.cpp:86] Creating Layer ip2
I0426 20:54:35.478648 32697 net.cpp:408] ip2 <- ip1
I0426 20:54:35.478655 32697 net.cpp:382] ip2 -> ip2
I0426 20:54:35.478935 32697 net.cpp:124] Setting up ip2
I0426 20:54:35.478943 32697 net.cpp:131] Top shape: 64 300 (19200)
I0426 20:54:35.478945 32697 net.cpp:139] Memory required for data: 10056960
I0426 20:54:35.478952 32697 layer_factory.hpp:77] Creating layer relu2
I0426 20:54:35.478963 32697 net.cpp:86] Creating Layer relu2
I0426 20:54:35.478968 32697 net.cpp:408] relu2 <- ip2
I0426 20:54:35.478976 32697 net.cpp:369] relu2 -> ip2 (in-place)
I0426 20:54:35.479773 32697 net.cpp:124] Setting up relu2
I0426 20:54:35.479784 32697 net.cpp:131] Top shape: 64 300 (19200)
I0426 20:54:35.479787 32697 net.cpp:139] Memory required for data: 10133760
I0426 20:54:35.479790 32697 layer_factory.hpp:77] Creating layer ip3
I0426 20:54:35.479800 32697 net.cpp:86] Creating Layer ip3
I0426 20:54:35.479805 32697 net.cpp:408] ip3 <- ip2
I0426 20:54:35.479815 32697 net.cpp:382] ip3 -> ip3
I0426 20:54:35.479959 32697 net.cpp:124] Setting up ip3
I0426 20:54:35.479969 32697 net.cpp:131] Top shape: 64 10 (640)
I0426 20:54:35.479971 32697 net.cpp:139] Memory required for data: 10136320
I0426 20:54:35.479982 32697 layer_factory.hpp:77] Creating layer relu3
I0426 20:54:35.479990 32697 net.cpp:86] Creating Layer relu3
I0426 20:54:35.479995 32697 net.cpp:408] relu3 <- ip3
I0426 20:54:35.480001 32697 net.cpp:369] relu3 -> ip3 (in-place)
I0426 20:54:35.480183 32697 net.cpp:124] Setting up relu3
I0426 20:54:35.480192 32697 net.cpp:131] Top shape: 64 10 (640)
I0426 20:54:35.480195 32697 net.cpp:139] Memory required for data: 10138880
I0426 20:54:35.480199 32697 layer_factory.hpp:77] Creating layer loss
I0426 20:54:35.480207 32697 net.cpp:86] Creating Layer loss
I0426 20:54:35.480212 32697 net.cpp:408] loss <- ip3
I0426 20:54:35.480217 32697 net.cpp:408] loss <- label
I0426 20:54:35.480226 32697 net.cpp:382] loss -> loss
I0426 20:54:35.480264 32697 layer_factory.hpp:77] Creating layer loss
I0426 20:54:35.480486 32697 net.cpp:124] Setting up loss
I0426 20:54:35.480494 32697 net.cpp:131] Top shape: (1)
I0426 20:54:35.480497 32697 net.cpp:134]     with loss weight 1
I0426 20:54:35.480515 32697 net.cpp:139] Memory required for data: 10138884
I0426 20:54:35.480520 32697 net.cpp:200] loss needs backward computation.
I0426 20:54:35.480523 32697 net.cpp:200] relu3 needs backward computation.
I0426 20:54:35.480528 32697 net.cpp:200] ip3 needs backward computation.
I0426 20:54:35.480532 32697 net.cpp:200] relu2 needs backward computation.
I0426 20:54:35.480535 32697 net.cpp:200] ip2 needs backward computation.
I0426 20:54:35.480540 32697 net.cpp:200] relu1 needs backward computation.
I0426 20:54:35.480543 32697 net.cpp:200] ip1 needs backward computation.
I0426 20:54:35.480548 32697 net.cpp:200] pool1 needs backward computation.
I0426 20:54:35.480552 32697 net.cpp:200] conv1 needs backward computation.
I0426 20:54:35.480556 32697 net.cpp:200] pool0 needs backward computation.
I0426 20:54:35.480561 32697 net.cpp:200] conv0 needs backward computation.
I0426 20:54:35.480566 32697 net.cpp:202] mnist does not need backward computation.
I0426 20:54:35.480569 32697 net.cpp:244] This network produces output loss
I0426 20:54:35.480581 32697 net.cpp:257] Network initialization done.
I0426 20:54:35.480955 32697 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything/lenet_train_test162.prototxt
I0426 20:54:35.480993 32697 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0426 20:54:35.481117 32697 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0426 20:54:35.481277 32697 layer_factory.hpp:77] Creating layer mnist
I0426 20:54:35.481329 32697 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0426 20:54:35.481364 32697 net.cpp:86] Creating Layer mnist
I0426 20:54:35.481374 32697 net.cpp:382] mnist -> data
I0426 20:54:35.481384 32697 net.cpp:382] mnist -> label
I0426 20:54:35.481495 32697 data_layer.cpp:45] output data size: 100,1,28,28
I0426 20:54:35.483536 32697 net.cpp:124] Setting up mnist
I0426 20:54:35.483547 32697 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0426 20:54:35.483553 32697 net.cpp:131] Top shape: 100 (100)
I0426 20:54:35.483556 32697 net.cpp:139] Memory required for data: 314000
I0426 20:54:35.483558 32697 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0426 20:54:35.483564 32697 net.cpp:86] Creating Layer label_mnist_1_split
I0426 20:54:35.483567 32697 net.cpp:408] label_mnist_1_split <- label
I0426 20:54:35.483572 32697 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0426 20:54:35.483577 32697 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0426 20:54:35.483638 32697 net.cpp:124] Setting up label_mnist_1_split
I0426 20:54:35.483645 32697 net.cpp:131] Top shape: 100 (100)
I0426 20:54:35.483650 32697 net.cpp:131] Top shape: 100 (100)
I0426 20:54:35.483654 32697 net.cpp:139] Memory required for data: 314800
I0426 20:54:35.483659 32697 layer_factory.hpp:77] Creating layer conv0
I0426 20:54:35.483671 32697 net.cpp:86] Creating Layer conv0
I0426 20:54:35.483676 32697 net.cpp:408] conv0 <- data
I0426 20:54:35.483685 32697 net.cpp:382] conv0 -> conv0
I0426 20:54:35.485265 32697 net.cpp:124] Setting up conv0
I0426 20:54:35.485277 32697 net.cpp:131] Top shape: 100 50 24 24 (2880000)
I0426 20:54:35.485280 32697 net.cpp:139] Memory required for data: 11834800
I0426 20:54:35.485288 32697 layer_factory.hpp:77] Creating layer pool0
I0426 20:54:35.485297 32697 net.cpp:86] Creating Layer pool0
I0426 20:54:35.485302 32697 net.cpp:408] pool0 <- conv0
I0426 20:54:35.485308 32697 net.cpp:382] pool0 -> pool0
I0426 20:54:35.485357 32697 net.cpp:124] Setting up pool0
I0426 20:54:35.485365 32697 net.cpp:131] Top shape: 100 50 12 12 (720000)
I0426 20:54:35.485369 32697 net.cpp:139] Memory required for data: 14714800
I0426 20:54:35.485374 32697 layer_factory.hpp:77] Creating layer conv1
I0426 20:54:35.485386 32697 net.cpp:86] Creating Layer conv1
I0426 20:54:35.485391 32697 net.cpp:408] conv1 <- pool0
I0426 20:54:35.485400 32697 net.cpp:382] conv1 -> conv1
I0426 20:54:35.487200 32697 net.cpp:124] Setting up conv1
I0426 20:54:35.487216 32697 net.cpp:131] Top shape: 100 25 8 8 (160000)
I0426 20:54:35.487221 32697 net.cpp:139] Memory required for data: 15354800
I0426 20:54:35.487236 32697 layer_factory.hpp:77] Creating layer pool1
I0426 20:54:35.487244 32697 net.cpp:86] Creating Layer pool1
I0426 20:54:35.487249 32697 net.cpp:408] pool1 <- conv1
I0426 20:54:35.487257 32697 net.cpp:382] pool1 -> pool1
I0426 20:54:35.487321 32697 net.cpp:124] Setting up pool1
I0426 20:54:35.487335 32697 net.cpp:131] Top shape: 100 25 4 4 (40000)
I0426 20:54:35.487344 32697 net.cpp:139] Memory required for data: 15514800
I0426 20:54:35.487347 32697 layer_factory.hpp:77] Creating layer ip1
I0426 20:54:35.487357 32697 net.cpp:86] Creating Layer ip1
I0426 20:54:35.487362 32697 net.cpp:408] ip1 <- pool1
I0426 20:54:35.487368 32697 net.cpp:382] ip1 -> ip1
I0426 20:54:35.487695 32697 net.cpp:124] Setting up ip1
I0426 20:54:35.487704 32697 net.cpp:131] Top shape: 100 100 (10000)
I0426 20:54:35.487717 32697 net.cpp:139] Memory required for data: 15554800
I0426 20:54:35.487728 32697 layer_factory.hpp:77] Creating layer relu1
I0426 20:54:35.487737 32697 net.cpp:86] Creating Layer relu1
I0426 20:54:35.487751 32697 net.cpp:408] relu1 <- ip1
I0426 20:54:35.487758 32697 net.cpp:369] relu1 -> ip1 (in-place)
I0426 20:54:35.487946 32697 net.cpp:124] Setting up relu1
I0426 20:54:35.487954 32697 net.cpp:131] Top shape: 100 100 (10000)
I0426 20:54:35.487958 32697 net.cpp:139] Memory required for data: 15594800
I0426 20:54:35.487962 32697 layer_factory.hpp:77] Creating layer ip2
I0426 20:54:35.487969 32697 net.cpp:86] Creating Layer ip2
I0426 20:54:35.487975 32697 net.cpp:408] ip2 <- ip1
I0426 20:54:35.487984 32697 net.cpp:382] ip2 -> ip2
I0426 20:54:35.488260 32697 net.cpp:124] Setting up ip2
I0426 20:54:35.488267 32697 net.cpp:131] Top shape: 100 300 (30000)
I0426 20:54:35.488271 32697 net.cpp:139] Memory required for data: 15714800
I0426 20:54:35.488276 32697 layer_factory.hpp:77] Creating layer relu2
I0426 20:54:35.488282 32697 net.cpp:86] Creating Layer relu2
I0426 20:54:35.488293 32697 net.cpp:408] relu2 <- ip2
I0426 20:54:35.488301 32697 net.cpp:369] relu2 -> ip2 (in-place)
I0426 20:54:35.488467 32697 net.cpp:124] Setting up relu2
I0426 20:54:35.488476 32697 net.cpp:131] Top shape: 100 300 (30000)
I0426 20:54:35.488479 32697 net.cpp:139] Memory required for data: 15834800
I0426 20:54:35.488482 32697 layer_factory.hpp:77] Creating layer ip3
I0426 20:54:35.488492 32697 net.cpp:86] Creating Layer ip3
I0426 20:54:35.488497 32697 net.cpp:408] ip3 <- ip2
I0426 20:54:35.488504 32697 net.cpp:382] ip3 -> ip3
I0426 20:54:35.488649 32697 net.cpp:124] Setting up ip3
I0426 20:54:35.488657 32697 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:54:35.488662 32697 net.cpp:139] Memory required for data: 15838800
I0426 20:54:35.488672 32697 layer_factory.hpp:77] Creating layer relu3
I0426 20:54:35.488679 32697 net.cpp:86] Creating Layer relu3
I0426 20:54:35.488683 32697 net.cpp:408] relu3 <- ip3
I0426 20:54:35.488692 32697 net.cpp:369] relu3 -> ip3 (in-place)
I0426 20:54:35.489689 32697 net.cpp:124] Setting up relu3
I0426 20:54:35.489701 32697 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:54:35.489704 32697 net.cpp:139] Memory required for data: 15842800
I0426 20:54:35.489707 32697 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0426 20:54:35.489712 32697 net.cpp:86] Creating Layer ip3_relu3_0_split
I0426 20:54:35.489715 32697 net.cpp:408] ip3_relu3_0_split <- ip3
I0426 20:54:35.489723 32697 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0426 20:54:35.489732 32697 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0426 20:54:35.489830 32697 net.cpp:124] Setting up ip3_relu3_0_split
I0426 20:54:35.489840 32697 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:54:35.489843 32697 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:54:35.489845 32697 net.cpp:139] Memory required for data: 15850800
I0426 20:54:35.489856 32697 layer_factory.hpp:77] Creating layer accuracy
I0426 20:54:35.489864 32697 net.cpp:86] Creating Layer accuracy
I0426 20:54:35.489868 32697 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0426 20:54:35.489876 32697 net.cpp:408] accuracy <- label_mnist_1_split_0
I0426 20:54:35.489882 32697 net.cpp:382] accuracy -> accuracy
I0426 20:54:35.489892 32697 net.cpp:124] Setting up accuracy
I0426 20:54:35.489899 32697 net.cpp:131] Top shape: (1)
I0426 20:54:35.489904 32697 net.cpp:139] Memory required for data: 15850804
I0426 20:54:35.489909 32697 layer_factory.hpp:77] Creating layer loss
I0426 20:54:35.489917 32697 net.cpp:86] Creating Layer loss
I0426 20:54:35.489923 32697 net.cpp:408] loss <- ip3_relu3_0_split_1
I0426 20:54:35.489928 32697 net.cpp:408] loss <- label_mnist_1_split_1
I0426 20:54:35.489934 32697 net.cpp:382] loss -> loss
I0426 20:54:35.489944 32697 layer_factory.hpp:77] Creating layer loss
I0426 20:54:35.490216 32697 net.cpp:124] Setting up loss
I0426 20:54:35.490226 32697 net.cpp:131] Top shape: (1)
I0426 20:54:35.490228 32697 net.cpp:134]     with loss weight 1
I0426 20:54:35.490245 32697 net.cpp:139] Memory required for data: 15850808
I0426 20:54:35.490252 32697 net.cpp:200] loss needs backward computation.
I0426 20:54:35.490257 32697 net.cpp:202] accuracy does not need backward computation.
I0426 20:54:35.490262 32697 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0426 20:54:35.490265 32697 net.cpp:200] relu3 needs backward computation.
I0426 20:54:35.490278 32697 net.cpp:200] ip3 needs backward computation.
I0426 20:54:35.490283 32697 net.cpp:200] relu2 needs backward computation.
I0426 20:54:35.490284 32697 net.cpp:200] ip2 needs backward computation.
I0426 20:54:35.490288 32697 net.cpp:200] relu1 needs backward computation.
I0426 20:54:35.490291 32697 net.cpp:200] ip1 needs backward computation.
I0426 20:54:35.490296 32697 net.cpp:200] pool1 needs backward computation.
I0426 20:54:35.490300 32697 net.cpp:200] conv1 needs backward computation.
I0426 20:54:35.490304 32697 net.cpp:200] pool0 needs backward computation.
I0426 20:54:35.490309 32697 net.cpp:200] conv0 needs backward computation.
I0426 20:54:35.490315 32697 net.cpp:202] label_mnist_1_split does not need backward computation.
I0426 20:54:35.490321 32697 net.cpp:202] mnist does not need backward computation.
I0426 20:54:35.490325 32697 net.cpp:244] This network produces output accuracy
I0426 20:54:35.490330 32697 net.cpp:244] This network produces output loss
I0426 20:54:35.490345 32697 net.cpp:257] Network initialization done.
I0426 20:54:35.490398 32697 solver.cpp:56] Solver scaffolding done.
I0426 20:54:35.490725 32697 caffe.cpp:248] Starting Optimization
I0426 20:54:35.490732 32697 solver.cpp:273] Solving LeNet
I0426 20:54:35.490741 32697 solver.cpp:274] Learning Rate Policy: inv
I0426 20:54:35.491580 32697 solver.cpp:331] Iteration 0, Testing net (#0)
I0426 20:54:35.498512 32697 blocking_queue.cpp:49] Waiting for data
I0426 20:54:35.572021 32704 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:54:35.573004 32697 solver.cpp:398]     Test net output #0: accuracy = 0.097
I0426 20:54:35.573026 32697 solver.cpp:398]     Test net output #1: loss = 2.31056 (* 1 = 2.31056 loss)
I0426 20:54:35.578301 32697 solver.cpp:219] Iteration 0 (-9.29061e-43 iter/s, 0.0875336s/100 iters), loss = 2.30854
I0426 20:54:35.578327 32697 solver.cpp:238]     Train net output #0: loss = 2.30854 (* 1 = 2.30854 loss)
I0426 20:54:35.578343 32697 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0426 20:54:35.732362 32697 solver.cpp:219] Iteration 100 (649.267 iter/s, 0.15402s/100 iters), loss = 0.817062
I0426 20:54:35.732386 32697 solver.cpp:238]     Train net output #0: loss = 0.817062 (* 1 = 0.817062 loss)
I0426 20:54:35.732393 32697 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0426 20:54:35.881168 32697 solver.cpp:219] Iteration 200 (672.179 iter/s, 0.14877s/100 iters), loss = 0.399781
I0426 20:54:35.881207 32697 solver.cpp:238]     Train net output #0: loss = 0.399781 (* 1 = 0.399781 loss)
I0426 20:54:35.881213 32697 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0426 20:54:36.030097 32697 solver.cpp:219] Iteration 300 (671.629 iter/s, 0.148892s/100 iters), loss = 0.592484
I0426 20:54:36.030134 32697 solver.cpp:238]     Train net output #0: loss = 0.592485 (* 1 = 0.592485 loss)
I0426 20:54:36.030156 32697 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0426 20:54:36.181097 32697 solver.cpp:219] Iteration 400 (662.426 iter/s, 0.15096s/100 iters), loss = 0.29351
I0426 20:54:36.181141 32697 solver.cpp:238]     Train net output #0: loss = 0.29351 (* 1 = 0.29351 loss)
I0426 20:54:36.181149 32697 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0426 20:54:36.325135 32697 solver.cpp:331] Iteration 500, Testing net (#0)
I0426 20:54:36.404680 32704 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:54:36.405719 32697 solver.cpp:398]     Test net output #0: accuracy = 0.9706
I0426 20:54:36.405740 32697 solver.cpp:398]     Test net output #1: loss = 0.0938667 (* 1 = 0.0938667 loss)
I0426 20:54:36.407169 32697 solver.cpp:219] Iteration 500 (442.454 iter/s, 0.226012s/100 iters), loss = 0.129755
I0426 20:54:36.407208 32697 solver.cpp:238]     Train net output #0: loss = 0.129755 (* 1 = 0.129755 loss)
I0426 20:54:36.407217 32697 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0426 20:54:36.559990 32697 solver.cpp:219] Iteration 600 (654.577 iter/s, 0.15277s/100 iters), loss = 0.0993347
I0426 20:54:36.560015 32697 solver.cpp:238]     Train net output #0: loss = 0.0993348 (* 1 = 0.0993348 loss)
I0426 20:54:36.560021 32697 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0426 20:54:36.709082 32697 solver.cpp:219] Iteration 700 (670.896 iter/s, 0.149054s/100 iters), loss = 0.145499
I0426 20:54:36.709121 32697 solver.cpp:238]     Train net output #0: loss = 0.145499 (* 1 = 0.145499 loss)
I0426 20:54:36.709127 32697 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0426 20:54:36.856834 32697 solver.cpp:219] Iteration 800 (677.054 iter/s, 0.147699s/100 iters), loss = 0.242623
I0426 20:54:36.856873 32697 solver.cpp:238]     Train net output #0: loss = 0.242624 (* 1 = 0.242624 loss)
I0426 20:54:36.856879 32697 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0426 20:54:37.008414 32697 solver.cpp:219] Iteration 900 (659.875 iter/s, 0.151544s/100 iters), loss = 0.10005
I0426 20:54:37.008455 32697 solver.cpp:238]     Train net output #0: loss = 0.10005 (* 1 = 0.10005 loss)
I0426 20:54:37.008460 32697 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0426 20:54:37.058171 32703 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:54:37.153367 32697 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0426 20:54:37.155653 32697 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0426 20:54:37.157059 32697 solver.cpp:311] Iteration 1000, loss = 0.136576
I0426 20:54:37.157075 32697 solver.cpp:331] Iteration 1000, Testing net (#0)
I0426 20:54:37.235935 32704 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:54:37.236913 32697 solver.cpp:398]     Test net output #0: accuracy = 0.979
I0426 20:54:37.236932 32697 solver.cpp:398]     Test net output #1: loss = 0.0638041 (* 1 = 0.0638041 loss)
I0426 20:54:37.236937 32697 solver.cpp:316] Optimization Done.
I0426 20:54:37.236939 32697 caffe.cpp:259] Optimization Done.
