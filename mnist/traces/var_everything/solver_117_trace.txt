I0426 20:52:37.416491 32236 caffe.cpp:218] Using GPUs 0
I0426 20:52:37.447001 32236 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0426 20:52:37.905436 32236 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything/lenet_train_test117.prototxt"
train_state {
  level: 0
  stage: ""
}
I0426 20:52:37.905565 32236 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything/lenet_train_test117.prototxt
I0426 20:52:37.905879 32236 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0426 20:52:37.905894 32236 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0426 20:52:37.905966 32236 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0426 20:52:37.906026 32236 layer_factory.hpp:77] Creating layer mnist
I0426 20:52:37.906106 32236 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0426 20:52:37.906126 32236 net.cpp:86] Creating Layer mnist
I0426 20:52:37.906132 32236 net.cpp:382] mnist -> data
I0426 20:52:37.906149 32236 net.cpp:382] mnist -> label
I0426 20:52:37.907059 32236 data_layer.cpp:45] output data size: 64,1,28,28
I0426 20:52:37.909137 32236 net.cpp:124] Setting up mnist
I0426 20:52:37.909149 32236 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0426 20:52:37.909154 32236 net.cpp:131] Top shape: 64 (64)
I0426 20:52:37.909157 32236 net.cpp:139] Memory required for data: 200960
I0426 20:52:37.909162 32236 layer_factory.hpp:77] Creating layer conv0
I0426 20:52:37.909175 32236 net.cpp:86] Creating Layer conv0
I0426 20:52:37.909180 32236 net.cpp:408] conv0 <- data
I0426 20:52:37.909214 32236 net.cpp:382] conv0 -> conv0
I0426 20:52:38.139904 32236 net.cpp:124] Setting up conv0
I0426 20:52:38.139943 32236 net.cpp:131] Top shape: 64 25 24 24 (921600)
I0426 20:52:38.139947 32236 net.cpp:139] Memory required for data: 3887360
I0426 20:52:38.139981 32236 layer_factory.hpp:77] Creating layer pool0
I0426 20:52:38.139992 32236 net.cpp:86] Creating Layer pool0
I0426 20:52:38.139997 32236 net.cpp:408] pool0 <- conv0
I0426 20:52:38.140017 32236 net.cpp:382] pool0 -> pool0
I0426 20:52:38.140063 32236 net.cpp:124] Setting up pool0
I0426 20:52:38.140067 32236 net.cpp:131] Top shape: 64 25 12 12 (230400)
I0426 20:52:38.140070 32236 net.cpp:139] Memory required for data: 4808960
I0426 20:52:38.140074 32236 layer_factory.hpp:77] Creating layer conv1
I0426 20:52:38.140100 32236 net.cpp:86] Creating Layer conv1
I0426 20:52:38.140102 32236 net.cpp:408] conv1 <- pool0
I0426 20:52:38.140107 32236 net.cpp:382] conv1 -> conv1
I0426 20:52:38.142874 32236 net.cpp:124] Setting up conv1
I0426 20:52:38.142886 32236 net.cpp:131] Top shape: 64 25 8 8 (102400)
I0426 20:52:38.142889 32236 net.cpp:139] Memory required for data: 5218560
I0426 20:52:38.142897 32236 layer_factory.hpp:77] Creating layer pool1
I0426 20:52:38.142904 32236 net.cpp:86] Creating Layer pool1
I0426 20:52:38.142907 32236 net.cpp:408] pool1 <- conv1
I0426 20:52:38.142911 32236 net.cpp:382] pool1 -> pool1
I0426 20:52:38.142961 32236 net.cpp:124] Setting up pool1
I0426 20:52:38.142966 32236 net.cpp:131] Top shape: 64 25 4 4 (25600)
I0426 20:52:38.142968 32236 net.cpp:139] Memory required for data: 5320960
I0426 20:52:38.142971 32236 layer_factory.hpp:77] Creating layer ip1
I0426 20:52:38.142978 32236 net.cpp:86] Creating Layer ip1
I0426 20:52:38.142980 32236 net.cpp:408] ip1 <- pool1
I0426 20:52:38.142985 32236 net.cpp:382] ip1 -> ip1
I0426 20:52:38.144574 32236 net.cpp:124] Setting up ip1
I0426 20:52:38.144585 32236 net.cpp:131] Top shape: 64 300 (19200)
I0426 20:52:38.144603 32236 net.cpp:139] Memory required for data: 5397760
I0426 20:52:38.144611 32236 layer_factory.hpp:77] Creating layer relu1
I0426 20:52:38.144618 32236 net.cpp:86] Creating Layer relu1
I0426 20:52:38.144620 32236 net.cpp:408] relu1 <- ip1
I0426 20:52:38.144624 32236 net.cpp:369] relu1 -> ip1 (in-place)
I0426 20:52:38.144855 32236 net.cpp:124] Setting up relu1
I0426 20:52:38.144863 32236 net.cpp:131] Top shape: 64 300 (19200)
I0426 20:52:38.144866 32236 net.cpp:139] Memory required for data: 5474560
I0426 20:52:38.144870 32236 layer_factory.hpp:77] Creating layer ip2
I0426 20:52:38.144876 32236 net.cpp:86] Creating Layer ip2
I0426 20:52:38.144879 32236 net.cpp:408] ip2 <- ip1
I0426 20:52:38.144884 32236 net.cpp:382] ip2 -> ip2
I0426 20:52:38.145890 32236 net.cpp:124] Setting up ip2
I0426 20:52:38.145901 32236 net.cpp:131] Top shape: 64 10 (640)
I0426 20:52:38.145920 32236 net.cpp:139] Memory required for data: 5477120
I0426 20:52:38.145926 32236 layer_factory.hpp:77] Creating layer relu2
I0426 20:52:38.145932 32236 net.cpp:86] Creating Layer relu2
I0426 20:52:38.145936 32236 net.cpp:408] relu2 <- ip2
I0426 20:52:38.145941 32236 net.cpp:369] relu2 -> ip2 (in-place)
I0426 20:52:38.146773 32236 net.cpp:124] Setting up relu2
I0426 20:52:38.146785 32236 net.cpp:131] Top shape: 64 10 (640)
I0426 20:52:38.146803 32236 net.cpp:139] Memory required for data: 5479680
I0426 20:52:38.146806 32236 layer_factory.hpp:77] Creating layer loss
I0426 20:52:38.146813 32236 net.cpp:86] Creating Layer loss
I0426 20:52:38.146816 32236 net.cpp:408] loss <- ip2
I0426 20:52:38.146832 32236 net.cpp:408] loss <- label
I0426 20:52:38.146837 32236 net.cpp:382] loss -> loss
I0426 20:52:38.146867 32236 layer_factory.hpp:77] Creating layer loss
I0426 20:52:38.147140 32236 net.cpp:124] Setting up loss
I0426 20:52:38.147150 32236 net.cpp:131] Top shape: (1)
I0426 20:52:38.147152 32236 net.cpp:134]     with loss weight 1
I0426 20:52:38.147182 32236 net.cpp:139] Memory required for data: 5479684
I0426 20:52:38.147186 32236 net.cpp:200] loss needs backward computation.
I0426 20:52:38.147188 32236 net.cpp:200] relu2 needs backward computation.
I0426 20:52:38.147192 32236 net.cpp:200] ip2 needs backward computation.
I0426 20:52:38.147194 32236 net.cpp:200] relu1 needs backward computation.
I0426 20:52:38.147197 32236 net.cpp:200] ip1 needs backward computation.
I0426 20:52:38.147223 32236 net.cpp:200] pool1 needs backward computation.
I0426 20:52:38.147227 32236 net.cpp:200] conv1 needs backward computation.
I0426 20:52:38.147229 32236 net.cpp:200] pool0 needs backward computation.
I0426 20:52:38.147231 32236 net.cpp:200] conv0 needs backward computation.
I0426 20:52:38.147235 32236 net.cpp:202] mnist does not need backward computation.
I0426 20:52:38.147238 32236 net.cpp:244] This network produces output loss
I0426 20:52:38.147248 32236 net.cpp:257] Network initialization done.
I0426 20:52:38.147550 32236 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything/lenet_train_test117.prototxt
I0426 20:52:38.147575 32236 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0426 20:52:38.147660 32236 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0426 20:52:38.147744 32236 layer_factory.hpp:77] Creating layer mnist
I0426 20:52:38.147790 32236 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0426 20:52:38.147802 32236 net.cpp:86] Creating Layer mnist
I0426 20:52:38.147807 32236 net.cpp:382] mnist -> data
I0426 20:52:38.147815 32236 net.cpp:382] mnist -> label
I0426 20:52:38.147904 32236 data_layer.cpp:45] output data size: 100,1,28,28
I0426 20:52:38.149114 32236 net.cpp:124] Setting up mnist
I0426 20:52:38.149142 32236 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0426 20:52:38.149147 32236 net.cpp:131] Top shape: 100 (100)
I0426 20:52:38.149148 32236 net.cpp:139] Memory required for data: 314000
I0426 20:52:38.149152 32236 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0426 20:52:38.149194 32236 net.cpp:86] Creating Layer label_mnist_1_split
I0426 20:52:38.149197 32236 net.cpp:408] label_mnist_1_split <- label
I0426 20:52:38.149225 32236 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0426 20:52:38.149232 32236 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0426 20:52:38.149292 32236 net.cpp:124] Setting up label_mnist_1_split
I0426 20:52:38.149298 32236 net.cpp:131] Top shape: 100 (100)
I0426 20:52:38.149302 32236 net.cpp:131] Top shape: 100 (100)
I0426 20:52:38.149304 32236 net.cpp:139] Memory required for data: 314800
I0426 20:52:38.149307 32236 layer_factory.hpp:77] Creating layer conv0
I0426 20:52:38.149317 32236 net.cpp:86] Creating Layer conv0
I0426 20:52:38.149320 32236 net.cpp:408] conv0 <- data
I0426 20:52:38.149325 32236 net.cpp:382] conv0 -> conv0
I0426 20:52:38.151190 32236 net.cpp:124] Setting up conv0
I0426 20:52:38.151204 32236 net.cpp:131] Top shape: 100 25 24 24 (1440000)
I0426 20:52:38.151208 32236 net.cpp:139] Memory required for data: 6074800
I0426 20:52:38.151216 32236 layer_factory.hpp:77] Creating layer pool0
I0426 20:52:38.151221 32236 net.cpp:86] Creating Layer pool0
I0426 20:52:38.151240 32236 net.cpp:408] pool0 <- conv0
I0426 20:52:38.151247 32236 net.cpp:382] pool0 -> pool0
I0426 20:52:38.151280 32236 net.cpp:124] Setting up pool0
I0426 20:52:38.151301 32236 net.cpp:131] Top shape: 100 25 12 12 (360000)
I0426 20:52:38.151304 32236 net.cpp:139] Memory required for data: 7514800
I0426 20:52:38.151306 32236 layer_factory.hpp:77] Creating layer conv1
I0426 20:52:38.151315 32236 net.cpp:86] Creating Layer conv1
I0426 20:52:38.151319 32236 net.cpp:408] conv1 <- pool0
I0426 20:52:38.151334 32236 net.cpp:382] conv1 -> conv1
I0426 20:52:38.153699 32236 net.cpp:124] Setting up conv1
I0426 20:52:38.153712 32236 net.cpp:131] Top shape: 100 25 8 8 (160000)
I0426 20:52:38.153715 32236 net.cpp:139] Memory required for data: 8154800
I0426 20:52:38.153723 32236 layer_factory.hpp:77] Creating layer pool1
I0426 20:52:38.153730 32236 net.cpp:86] Creating Layer pool1
I0426 20:52:38.153749 32236 net.cpp:408] pool1 <- conv1
I0426 20:52:38.153753 32236 net.cpp:382] pool1 -> pool1
I0426 20:52:38.153795 32236 net.cpp:124] Setting up pool1
I0426 20:52:38.153800 32236 net.cpp:131] Top shape: 100 25 4 4 (40000)
I0426 20:52:38.153802 32236 net.cpp:139] Memory required for data: 8314800
I0426 20:52:38.153805 32236 layer_factory.hpp:77] Creating layer ip1
I0426 20:52:38.153810 32236 net.cpp:86] Creating Layer ip1
I0426 20:52:38.153813 32236 net.cpp:408] ip1 <- pool1
I0426 20:52:38.153818 32236 net.cpp:382] ip1 -> ip1
I0426 20:52:38.155454 32236 net.cpp:124] Setting up ip1
I0426 20:52:38.155467 32236 net.cpp:131] Top shape: 100 300 (30000)
I0426 20:52:38.155470 32236 net.cpp:139] Memory required for data: 8434800
I0426 20:52:38.155478 32236 layer_factory.hpp:77] Creating layer relu1
I0426 20:52:38.155483 32236 net.cpp:86] Creating Layer relu1
I0426 20:52:38.155501 32236 net.cpp:408] relu1 <- ip1
I0426 20:52:38.155508 32236 net.cpp:369] relu1 -> ip1 (in-place)
I0426 20:52:38.155735 32236 net.cpp:124] Setting up relu1
I0426 20:52:38.155742 32236 net.cpp:131] Top shape: 100 300 (30000)
I0426 20:52:38.155745 32236 net.cpp:139] Memory required for data: 8554800
I0426 20:52:38.155748 32236 layer_factory.hpp:77] Creating layer ip2
I0426 20:52:38.155756 32236 net.cpp:86] Creating Layer ip2
I0426 20:52:38.155760 32236 net.cpp:408] ip2 <- ip1
I0426 20:52:38.155764 32236 net.cpp:382] ip2 -> ip2
I0426 20:52:38.155889 32236 net.cpp:124] Setting up ip2
I0426 20:52:38.155895 32236 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:52:38.155899 32236 net.cpp:139] Memory required for data: 8558800
I0426 20:52:38.155903 32236 layer_factory.hpp:77] Creating layer relu2
I0426 20:52:38.155910 32236 net.cpp:86] Creating Layer relu2
I0426 20:52:38.155911 32236 net.cpp:408] relu2 <- ip2
I0426 20:52:38.155915 32236 net.cpp:369] relu2 -> ip2 (in-place)
I0426 20:52:38.156124 32236 net.cpp:124] Setting up relu2
I0426 20:52:38.156133 32236 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:52:38.156137 32236 net.cpp:139] Memory required for data: 8562800
I0426 20:52:38.156141 32236 layer_factory.hpp:77] Creating layer ip2_relu2_0_split
I0426 20:52:38.156146 32236 net.cpp:86] Creating Layer ip2_relu2_0_split
I0426 20:52:38.156149 32236 net.cpp:408] ip2_relu2_0_split <- ip2
I0426 20:52:38.156160 32236 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_0
I0426 20:52:38.156177 32236 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_1
I0426 20:52:38.156213 32236 net.cpp:124] Setting up ip2_relu2_0_split
I0426 20:52:38.156219 32236 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:52:38.156222 32236 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:52:38.156225 32236 net.cpp:139] Memory required for data: 8570800
I0426 20:52:38.156229 32236 layer_factory.hpp:77] Creating layer accuracy
I0426 20:52:38.156234 32236 net.cpp:86] Creating Layer accuracy
I0426 20:52:38.156236 32236 net.cpp:408] accuracy <- ip2_relu2_0_split_0
I0426 20:52:38.156240 32236 net.cpp:408] accuracy <- label_mnist_1_split_0
I0426 20:52:38.156245 32236 net.cpp:382] accuracy -> accuracy
I0426 20:52:38.156252 32236 net.cpp:124] Setting up accuracy
I0426 20:52:38.156257 32236 net.cpp:131] Top shape: (1)
I0426 20:52:38.156260 32236 net.cpp:139] Memory required for data: 8570804
I0426 20:52:38.156263 32236 layer_factory.hpp:77] Creating layer loss
I0426 20:52:38.156267 32236 net.cpp:86] Creating Layer loss
I0426 20:52:38.156276 32236 net.cpp:408] loss <- ip2_relu2_0_split_1
I0426 20:52:38.156280 32236 net.cpp:408] loss <- label_mnist_1_split_1
I0426 20:52:38.156286 32236 net.cpp:382] loss -> loss
I0426 20:52:38.156296 32236 layer_factory.hpp:77] Creating layer loss
I0426 20:52:38.156558 32236 net.cpp:124] Setting up loss
I0426 20:52:38.156568 32236 net.cpp:131] Top shape: (1)
I0426 20:52:38.156570 32236 net.cpp:134]     with loss weight 1
I0426 20:52:38.156577 32236 net.cpp:139] Memory required for data: 8570808
I0426 20:52:38.156580 32236 net.cpp:200] loss needs backward computation.
I0426 20:52:38.156584 32236 net.cpp:202] accuracy does not need backward computation.
I0426 20:52:38.156587 32236 net.cpp:200] ip2_relu2_0_split needs backward computation.
I0426 20:52:38.156606 32236 net.cpp:200] relu2 needs backward computation.
I0426 20:52:38.156608 32236 net.cpp:200] ip2 needs backward computation.
I0426 20:52:38.156611 32236 net.cpp:200] relu1 needs backward computation.
I0426 20:52:38.156613 32236 net.cpp:200] ip1 needs backward computation.
I0426 20:52:38.156617 32236 net.cpp:200] pool1 needs backward computation.
I0426 20:52:38.156627 32236 net.cpp:200] conv1 needs backward computation.
I0426 20:52:38.156630 32236 net.cpp:200] pool0 needs backward computation.
I0426 20:52:38.156633 32236 net.cpp:200] conv0 needs backward computation.
I0426 20:52:38.156636 32236 net.cpp:202] label_mnist_1_split does not need backward computation.
I0426 20:52:38.156641 32236 net.cpp:202] mnist does not need backward computation.
I0426 20:52:38.156642 32236 net.cpp:244] This network produces output accuracy
I0426 20:52:38.156646 32236 net.cpp:244] This network produces output loss
I0426 20:52:38.156672 32236 net.cpp:257] Network initialization done.
I0426 20:52:38.156709 32236 solver.cpp:56] Solver scaffolding done.
I0426 20:52:38.157011 32236 caffe.cpp:248] Starting Optimization
I0426 20:52:38.157017 32236 solver.cpp:273] Solving LeNet
I0426 20:52:38.157021 32236 solver.cpp:274] Learning Rate Policy: inv
I0426 20:52:38.157944 32236 solver.cpp:331] Iteration 0, Testing net (#0)
I0426 20:52:38.164264 32236 blocking_queue.cpp:49] Waiting for data
I0426 20:52:38.233680 32243 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:52:38.234333 32236 solver.cpp:398]     Test net output #0: accuracy = 0.071
I0426 20:52:38.234352 32236 solver.cpp:398]     Test net output #1: loss = 2.32656 (* 1 = 2.32656 loss)
I0426 20:52:38.238261 32236 solver.cpp:219] Iteration 0 (0 iter/s, 0.0812157s/100 iters), loss = 2.31276
I0426 20:52:38.238284 32236 solver.cpp:238]     Train net output #0: loss = 2.31276 (* 1 = 2.31276 loss)
I0426 20:52:38.238294 32236 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0426 20:52:38.350901 32236 solver.cpp:219] Iteration 100 (888.081 iter/s, 0.112602s/100 iters), loss = 0.303593
I0426 20:52:38.350944 32236 solver.cpp:238]     Train net output #0: loss = 0.303593 (* 1 = 0.303593 loss)
I0426 20:52:38.350950 32236 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0426 20:52:38.454241 32236 solver.cpp:219] Iteration 200 (968.036 iter/s, 0.103302s/100 iters), loss = 0.154429
I0426 20:52:38.454280 32236 solver.cpp:238]     Train net output #0: loss = 0.154429 (* 1 = 0.154429 loss)
I0426 20:52:38.454286 32236 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0426 20:52:38.557626 32236 solver.cpp:219] Iteration 300 (967.583 iter/s, 0.10335s/100 iters), loss = 0.160633
I0426 20:52:38.557667 32236 solver.cpp:238]     Train net output #0: loss = 0.160633 (* 1 = 0.160633 loss)
I0426 20:52:38.557687 32236 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0426 20:52:38.666374 32236 solver.cpp:219] Iteration 400 (919.854 iter/s, 0.108713s/100 iters), loss = 0.10529
I0426 20:52:38.666414 32236 solver.cpp:238]     Train net output #0: loss = 0.10529 (* 1 = 0.10529 loss)
I0426 20:52:38.666419 32236 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0426 20:52:38.769590 32236 solver.cpp:331] Iteration 500, Testing net (#0)
I0426 20:52:38.826033 32243 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:52:38.826838 32236 solver.cpp:398]     Test net output #0: accuracy = 0.9674
I0426 20:52:38.826856 32236 solver.cpp:398]     Test net output #1: loss = 0.0994073 (* 1 = 0.0994073 loss)
I0426 20:52:38.827950 32236 solver.cpp:219] Iteration 500 (619.049 iter/s, 0.161538s/100 iters), loss = 0.0934106
I0426 20:52:38.827970 32236 solver.cpp:238]     Train net output #0: loss = 0.0934106 (* 1 = 0.0934106 loss)
I0426 20:52:38.827992 32236 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0426 20:52:38.935989 32236 solver.cpp:219] Iteration 600 (925.855 iter/s, 0.108008s/100 iters), loss = 0.131649
I0426 20:52:38.936015 32236 solver.cpp:238]     Train net output #0: loss = 0.131649 (* 1 = 0.131649 loss)
I0426 20:52:38.936040 32236 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0426 20:52:39.041436 32236 solver.cpp:219] Iteration 700 (948.708 iter/s, 0.105406s/100 iters), loss = 0.153029
I0426 20:52:39.041479 32236 solver.cpp:238]     Train net output #0: loss = 0.153029 (* 1 = 0.153029 loss)
I0426 20:52:39.041492 32236 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0426 20:52:39.149353 32236 solver.cpp:219] Iteration 800 (927.102 iter/s, 0.107863s/100 iters), loss = 0.220539
I0426 20:52:39.149384 32236 solver.cpp:238]     Train net output #0: loss = 0.220539 (* 1 = 0.220539 loss)
I0426 20:52:39.149396 32236 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0426 20:52:39.250941 32236 solver.cpp:219] Iteration 900 (984.776 iter/s, 0.101546s/100 iters), loss = 0.160814
I0426 20:52:39.250967 32236 solver.cpp:238]     Train net output #0: loss = 0.160814 (* 1 = 0.160814 loss)
I0426 20:52:39.250972 32236 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0426 20:52:39.289340 32242 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:52:39.359913 32236 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0426 20:52:39.363907 32236 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0426 20:52:39.365572 32236 solver.cpp:311] Iteration 1000, loss = 0.0644436
I0426 20:52:39.365591 32236 solver.cpp:331] Iteration 1000, Testing net (#0)
I0426 20:52:39.439843 32243 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:52:39.440490 32236 solver.cpp:398]     Test net output #0: accuracy = 0.9794
I0426 20:52:39.440507 32236 solver.cpp:398]     Test net output #1: loss = 0.0634339 (* 1 = 0.0634339 loss)
I0426 20:52:39.440512 32236 solver.cpp:316] Optimization Done.
I0426 20:52:39.440515 32236 caffe.cpp:259] Optimization Done.
