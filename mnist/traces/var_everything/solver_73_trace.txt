I0426 20:50:54.234961 31862 caffe.cpp:218] Using GPUs 0
I0426 20:50:54.264168 31862 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0426 20:50:54.725512 31862 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything/lenet_train_test73.prototxt"
train_state {
  level: 0
  stage: ""
}
I0426 20:50:54.725675 31862 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything/lenet_train_test73.prototxt
I0426 20:50:54.726006 31862 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0426 20:50:54.726032 31862 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0426 20:50:54.726135 31862 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0426 20:50:54.726191 31862 layer_factory.hpp:77] Creating layer mnist
I0426 20:50:54.726291 31862 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0426 20:50:54.726311 31862 net.cpp:86] Creating Layer mnist
I0426 20:50:54.726318 31862 net.cpp:382] mnist -> data
I0426 20:50:54.726337 31862 net.cpp:382] mnist -> label
I0426 20:50:54.727339 31862 data_layer.cpp:45] output data size: 64,1,28,28
I0426 20:50:54.729620 31862 net.cpp:124] Setting up mnist
I0426 20:50:54.729650 31862 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0426 20:50:54.729655 31862 net.cpp:131] Top shape: 64 (64)
I0426 20:50:54.729658 31862 net.cpp:139] Memory required for data: 200960
I0426 20:50:54.729663 31862 layer_factory.hpp:77] Creating layer conv0
I0426 20:50:54.729691 31862 net.cpp:86] Creating Layer conv0
I0426 20:50:54.729696 31862 net.cpp:408] conv0 <- data
I0426 20:50:54.729707 31862 net.cpp:382] conv0 -> conv0
I0426 20:50:54.960940 31862 net.cpp:124] Setting up conv0
I0426 20:50:54.960983 31862 net.cpp:131] Top shape: 64 10 24 24 (368640)
I0426 20:50:54.960986 31862 net.cpp:139] Memory required for data: 1675520
I0426 20:50:54.961035 31862 layer_factory.hpp:77] Creating layer pool0
I0426 20:50:54.961047 31862 net.cpp:86] Creating Layer pool0
I0426 20:50:54.961052 31862 net.cpp:408] pool0 <- conv0
I0426 20:50:54.961057 31862 net.cpp:382] pool0 -> pool0
I0426 20:50:54.961117 31862 net.cpp:124] Setting up pool0
I0426 20:50:54.961139 31862 net.cpp:131] Top shape: 64 10 12 12 (92160)
I0426 20:50:54.961143 31862 net.cpp:139] Memory required for data: 2044160
I0426 20:50:54.961145 31862 layer_factory.hpp:77] Creating layer conv1
I0426 20:50:54.961156 31862 net.cpp:86] Creating Layer conv1
I0426 20:50:54.961175 31862 net.cpp:408] conv1 <- pool0
I0426 20:50:54.961180 31862 net.cpp:382] conv1 -> conv1
I0426 20:50:54.963835 31862 net.cpp:124] Setting up conv1
I0426 20:50:54.963848 31862 net.cpp:131] Top shape: 64 25 8 8 (102400)
I0426 20:50:54.963851 31862 net.cpp:139] Memory required for data: 2453760
I0426 20:50:54.963860 31862 layer_factory.hpp:77] Creating layer pool1
I0426 20:50:54.963866 31862 net.cpp:86] Creating Layer pool1
I0426 20:50:54.963870 31862 net.cpp:408] pool1 <- conv1
I0426 20:50:54.963873 31862 net.cpp:382] pool1 -> pool1
I0426 20:50:54.963922 31862 net.cpp:124] Setting up pool1
I0426 20:50:54.963927 31862 net.cpp:131] Top shape: 64 25 4 4 (25600)
I0426 20:50:54.963930 31862 net.cpp:139] Memory required for data: 2556160
I0426 20:50:54.963933 31862 layer_factory.hpp:77] Creating layer ip1
I0426 20:50:54.963939 31862 net.cpp:86] Creating Layer ip1
I0426 20:50:54.963942 31862 net.cpp:408] ip1 <- pool1
I0426 20:50:54.963946 31862 net.cpp:382] ip1 -> ip1
I0426 20:50:54.964329 31862 net.cpp:124] Setting up ip1
I0426 20:50:54.964351 31862 net.cpp:131] Top shape: 64 100 (6400)
I0426 20:50:54.964354 31862 net.cpp:139] Memory required for data: 2581760
I0426 20:50:54.964377 31862 layer_factory.hpp:77] Creating layer relu1
I0426 20:50:54.964382 31862 net.cpp:86] Creating Layer relu1
I0426 20:50:54.964385 31862 net.cpp:408] relu1 <- ip1
I0426 20:50:54.964390 31862 net.cpp:369] relu1 -> ip1 (in-place)
I0426 20:50:54.964571 31862 net.cpp:124] Setting up relu1
I0426 20:50:54.964581 31862 net.cpp:131] Top shape: 64 100 (6400)
I0426 20:50:54.964584 31862 net.cpp:139] Memory required for data: 2607360
I0426 20:50:54.964601 31862 layer_factory.hpp:77] Creating layer ip2
I0426 20:50:54.964608 31862 net.cpp:86] Creating Layer ip2
I0426 20:50:54.964627 31862 net.cpp:408] ip2 <- ip1
I0426 20:50:54.964630 31862 net.cpp:382] ip2 -> ip2
I0426 20:50:54.964726 31862 net.cpp:124] Setting up ip2
I0426 20:50:54.964732 31862 net.cpp:131] Top shape: 64 10 (640)
I0426 20:50:54.964735 31862 net.cpp:139] Memory required for data: 2609920
I0426 20:50:54.964740 31862 layer_factory.hpp:77] Creating layer relu2
I0426 20:50:54.964746 31862 net.cpp:86] Creating Layer relu2
I0426 20:50:54.964750 31862 net.cpp:408] relu2 <- ip2
I0426 20:50:54.964753 31862 net.cpp:369] relu2 -> ip2 (in-place)
I0426 20:50:54.965575 31862 net.cpp:124] Setting up relu2
I0426 20:50:54.965587 31862 net.cpp:131] Top shape: 64 10 (640)
I0426 20:50:54.965606 31862 net.cpp:139] Memory required for data: 2612480
I0426 20:50:54.965610 31862 layer_factory.hpp:77] Creating layer loss
I0426 20:50:54.965620 31862 net.cpp:86] Creating Layer loss
I0426 20:50:54.965625 31862 net.cpp:408] loss <- ip2
I0426 20:50:54.965628 31862 net.cpp:408] loss <- label
I0426 20:50:54.965649 31862 net.cpp:382] loss -> loss
I0426 20:50:54.965664 31862 layer_factory.hpp:77] Creating layer loss
I0426 20:50:54.965927 31862 net.cpp:124] Setting up loss
I0426 20:50:54.965937 31862 net.cpp:131] Top shape: (1)
I0426 20:50:54.965940 31862 net.cpp:134]     with loss weight 1
I0426 20:50:54.965953 31862 net.cpp:139] Memory required for data: 2612484
I0426 20:50:54.965956 31862 net.cpp:200] loss needs backward computation.
I0426 20:50:54.965960 31862 net.cpp:200] relu2 needs backward computation.
I0426 20:50:54.965963 31862 net.cpp:200] ip2 needs backward computation.
I0426 20:50:54.965966 31862 net.cpp:200] relu1 needs backward computation.
I0426 20:50:54.965968 31862 net.cpp:200] ip1 needs backward computation.
I0426 20:50:54.965981 31862 net.cpp:200] pool1 needs backward computation.
I0426 20:50:54.965986 31862 net.cpp:200] conv1 needs backward computation.
I0426 20:50:54.965988 31862 net.cpp:200] pool0 needs backward computation.
I0426 20:50:54.965991 31862 net.cpp:200] conv0 needs backward computation.
I0426 20:50:54.965994 31862 net.cpp:202] mnist does not need backward computation.
I0426 20:50:54.965997 31862 net.cpp:244] This network produces output loss
I0426 20:50:54.966006 31862 net.cpp:257] Network initialization done.
I0426 20:50:54.966301 31862 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything/lenet_train_test73.prototxt
I0426 20:50:54.966341 31862 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0426 20:50:54.966440 31862 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0426 20:50:54.966502 31862 layer_factory.hpp:77] Creating layer mnist
I0426 20:50:54.966547 31862 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0426 20:50:54.966558 31862 net.cpp:86] Creating Layer mnist
I0426 20:50:54.966562 31862 net.cpp:382] mnist -> data
I0426 20:50:54.966569 31862 net.cpp:382] mnist -> label
I0426 20:50:54.966650 31862 data_layer.cpp:45] output data size: 100,1,28,28
I0426 20:50:54.968580 31862 net.cpp:124] Setting up mnist
I0426 20:50:54.968591 31862 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0426 20:50:54.968595 31862 net.cpp:131] Top shape: 100 (100)
I0426 20:50:54.968598 31862 net.cpp:139] Memory required for data: 314000
I0426 20:50:54.968601 31862 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0426 20:50:54.968607 31862 net.cpp:86] Creating Layer label_mnist_1_split
I0426 20:50:54.968621 31862 net.cpp:408] label_mnist_1_split <- label
I0426 20:50:54.968626 31862 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0426 20:50:54.968647 31862 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0426 20:50:54.968688 31862 net.cpp:124] Setting up label_mnist_1_split
I0426 20:50:54.968703 31862 net.cpp:131] Top shape: 100 (100)
I0426 20:50:54.968706 31862 net.cpp:131] Top shape: 100 (100)
I0426 20:50:54.968709 31862 net.cpp:139] Memory required for data: 314800
I0426 20:50:54.968713 31862 layer_factory.hpp:77] Creating layer conv0
I0426 20:50:54.968720 31862 net.cpp:86] Creating Layer conv0
I0426 20:50:54.968724 31862 net.cpp:408] conv0 <- data
I0426 20:50:54.968739 31862 net.cpp:382] conv0 -> conv0
I0426 20:50:54.970398 31862 net.cpp:124] Setting up conv0
I0426 20:50:54.970412 31862 net.cpp:131] Top shape: 100 10 24 24 (576000)
I0426 20:50:54.970414 31862 net.cpp:139] Memory required for data: 2618800
I0426 20:50:54.970423 31862 layer_factory.hpp:77] Creating layer pool0
I0426 20:50:54.970444 31862 net.cpp:86] Creating Layer pool0
I0426 20:50:54.970448 31862 net.cpp:408] pool0 <- conv0
I0426 20:50:54.970451 31862 net.cpp:382] pool0 -> pool0
I0426 20:50:54.970484 31862 net.cpp:124] Setting up pool0
I0426 20:50:54.970489 31862 net.cpp:131] Top shape: 100 10 12 12 (144000)
I0426 20:50:54.970491 31862 net.cpp:139] Memory required for data: 3194800
I0426 20:50:54.970494 31862 layer_factory.hpp:77] Creating layer conv1
I0426 20:50:54.970501 31862 net.cpp:86] Creating Layer conv1
I0426 20:50:54.970504 31862 net.cpp:408] conv1 <- pool0
I0426 20:50:54.970510 31862 net.cpp:382] conv1 -> conv1
I0426 20:50:54.972689 31862 net.cpp:124] Setting up conv1
I0426 20:50:54.972702 31862 net.cpp:131] Top shape: 100 25 8 8 (160000)
I0426 20:50:54.972704 31862 net.cpp:139] Memory required for data: 3834800
I0426 20:50:54.972712 31862 layer_factory.hpp:77] Creating layer pool1
I0426 20:50:54.972733 31862 net.cpp:86] Creating Layer pool1
I0426 20:50:54.972738 31862 net.cpp:408] pool1 <- conv1
I0426 20:50:54.972757 31862 net.cpp:382] pool1 -> pool1
I0426 20:50:54.972797 31862 net.cpp:124] Setting up pool1
I0426 20:50:54.972803 31862 net.cpp:131] Top shape: 100 25 4 4 (40000)
I0426 20:50:54.972806 31862 net.cpp:139] Memory required for data: 3994800
I0426 20:50:54.972831 31862 layer_factory.hpp:77] Creating layer ip1
I0426 20:50:54.972838 31862 net.cpp:86] Creating Layer ip1
I0426 20:50:54.972841 31862 net.cpp:408] ip1 <- pool1
I0426 20:50:54.972846 31862 net.cpp:382] ip1 -> ip1
I0426 20:50:54.973289 31862 net.cpp:124] Setting up ip1
I0426 20:50:54.973297 31862 net.cpp:131] Top shape: 100 100 (10000)
I0426 20:50:54.973301 31862 net.cpp:139] Memory required for data: 4034800
I0426 20:50:54.973309 31862 layer_factory.hpp:77] Creating layer relu1
I0426 20:50:54.973336 31862 net.cpp:86] Creating Layer relu1
I0426 20:50:54.973340 31862 net.cpp:408] relu1 <- ip1
I0426 20:50:54.973345 31862 net.cpp:369] relu1 -> ip1 (in-place)
I0426 20:50:54.973498 31862 net.cpp:124] Setting up relu1
I0426 20:50:54.973507 31862 net.cpp:131] Top shape: 100 100 (10000)
I0426 20:50:54.973510 31862 net.cpp:139] Memory required for data: 4074800
I0426 20:50:54.973513 31862 layer_factory.hpp:77] Creating layer ip2
I0426 20:50:54.973520 31862 net.cpp:86] Creating Layer ip2
I0426 20:50:54.973523 31862 net.cpp:408] ip2 <- ip1
I0426 20:50:54.973528 31862 net.cpp:382] ip2 -> ip2
I0426 20:50:54.973649 31862 net.cpp:124] Setting up ip2
I0426 20:50:54.973657 31862 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:50:54.973660 31862 net.cpp:139] Memory required for data: 4078800
I0426 20:50:54.973673 31862 layer_factory.hpp:77] Creating layer relu2
I0426 20:50:54.973677 31862 net.cpp:86] Creating Layer relu2
I0426 20:50:54.973680 31862 net.cpp:408] relu2 <- ip2
I0426 20:50:54.973685 31862 net.cpp:369] relu2 -> ip2 (in-place)
I0426 20:50:54.973835 31862 net.cpp:124] Setting up relu2
I0426 20:50:54.973844 31862 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:50:54.973846 31862 net.cpp:139] Memory required for data: 4082800
I0426 20:50:54.973850 31862 layer_factory.hpp:77] Creating layer ip2_relu2_0_split
I0426 20:50:54.973855 31862 net.cpp:86] Creating Layer ip2_relu2_0_split
I0426 20:50:54.973857 31862 net.cpp:408] ip2_relu2_0_split <- ip2
I0426 20:50:54.973861 31862 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_0
I0426 20:50:54.973877 31862 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_1
I0426 20:50:54.973915 31862 net.cpp:124] Setting up ip2_relu2_0_split
I0426 20:50:54.973920 31862 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:50:54.973924 31862 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:50:54.973927 31862 net.cpp:139] Memory required for data: 4090800
I0426 20:50:54.973929 31862 layer_factory.hpp:77] Creating layer accuracy
I0426 20:50:54.973934 31862 net.cpp:86] Creating Layer accuracy
I0426 20:50:54.973937 31862 net.cpp:408] accuracy <- ip2_relu2_0_split_0
I0426 20:50:54.973942 31862 net.cpp:408] accuracy <- label_mnist_1_split_0
I0426 20:50:54.973947 31862 net.cpp:382] accuracy -> accuracy
I0426 20:50:54.973953 31862 net.cpp:124] Setting up accuracy
I0426 20:50:54.973956 31862 net.cpp:131] Top shape: (1)
I0426 20:50:54.973959 31862 net.cpp:139] Memory required for data: 4090804
I0426 20:50:54.973961 31862 layer_factory.hpp:77] Creating layer loss
I0426 20:50:54.973965 31862 net.cpp:86] Creating Layer loss
I0426 20:50:54.973968 31862 net.cpp:408] loss <- ip2_relu2_0_split_1
I0426 20:50:54.973973 31862 net.cpp:408] loss <- label_mnist_1_split_1
I0426 20:50:54.973976 31862 net.cpp:382] loss -> loss
I0426 20:50:54.973981 31862 layer_factory.hpp:77] Creating layer loss
I0426 20:50:54.974277 31862 net.cpp:124] Setting up loss
I0426 20:50:54.974287 31862 net.cpp:131] Top shape: (1)
I0426 20:50:54.974290 31862 net.cpp:134]     with loss weight 1
I0426 20:50:54.974297 31862 net.cpp:139] Memory required for data: 4090808
I0426 20:50:54.974300 31862 net.cpp:200] loss needs backward computation.
I0426 20:50:54.974304 31862 net.cpp:202] accuracy does not need backward computation.
I0426 20:50:54.974308 31862 net.cpp:200] ip2_relu2_0_split needs backward computation.
I0426 20:50:54.974318 31862 net.cpp:200] relu2 needs backward computation.
I0426 20:50:54.974321 31862 net.cpp:200] ip2 needs backward computation.
I0426 20:50:54.974324 31862 net.cpp:200] relu1 needs backward computation.
I0426 20:50:54.974328 31862 net.cpp:200] ip1 needs backward computation.
I0426 20:50:54.974335 31862 net.cpp:200] pool1 needs backward computation.
I0426 20:50:54.974339 31862 net.cpp:200] conv1 needs backward computation.
I0426 20:50:54.974342 31862 net.cpp:200] pool0 needs backward computation.
I0426 20:50:54.974345 31862 net.cpp:200] conv0 needs backward computation.
I0426 20:50:54.974349 31862 net.cpp:202] label_mnist_1_split does not need backward computation.
I0426 20:50:54.974359 31862 net.cpp:202] mnist does not need backward computation.
I0426 20:50:54.974361 31862 net.cpp:244] This network produces output accuracy
I0426 20:50:54.974364 31862 net.cpp:244] This network produces output loss
I0426 20:50:54.974380 31862 net.cpp:257] Network initialization done.
I0426 20:50:54.974416 31862 solver.cpp:56] Solver scaffolding done.
I0426 20:50:54.974668 31862 caffe.cpp:248] Starting Optimization
I0426 20:50:54.974675 31862 solver.cpp:273] Solving LeNet
I0426 20:50:54.974678 31862 solver.cpp:274] Learning Rate Policy: inv
I0426 20:50:54.975484 31862 solver.cpp:331] Iteration 0, Testing net (#0)
I0426 20:50:54.979161 31862 blocking_queue.cpp:49] Waiting for data
I0426 20:50:55.043879 31870 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:50:55.044431 31862 solver.cpp:398]     Test net output #0: accuracy = 0.1218
I0426 20:50:55.044448 31862 solver.cpp:398]     Test net output #1: loss = 2.31603 (* 1 = 2.31603 loss)
I0426 20:50:55.046980 31862 solver.cpp:219] Iteration 0 (-1.5354e-31 iter/s, 0.0722801s/100 iters), loss = 2.29689
I0426 20:50:55.047005 31862 solver.cpp:238]     Train net output #0: loss = 2.29689 (* 1 = 2.29689 loss)
I0426 20:50:55.047032 31862 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0426 20:50:55.150965 31862 solver.cpp:219] Iteration 100 (962.015 iter/s, 0.103948s/100 iters), loss = 0.718038
I0426 20:50:55.151003 31862 solver.cpp:238]     Train net output #0: loss = 0.718038 (* 1 = 0.718038 loss)
I0426 20:50:55.151010 31862 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0426 20:50:55.250856 31862 solver.cpp:219] Iteration 200 (1001.43 iter/s, 0.0998574s/100 iters), loss = 0.56881
I0426 20:50:55.250897 31862 solver.cpp:238]     Train net output #0: loss = 0.56881 (* 1 = 0.56881 loss)
I0426 20:50:55.250903 31862 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0426 20:50:55.339493 31862 solver.cpp:219] Iteration 300 (1128.63 iter/s, 0.0886026s/100 iters), loss = 0.600039
I0426 20:50:55.339519 31862 solver.cpp:238]     Train net output #0: loss = 0.600039 (* 1 = 0.600039 loss)
I0426 20:50:55.339524 31862 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0426 20:50:55.426141 31862 solver.cpp:219] Iteration 400 (1154.56 iter/s, 0.086613s/100 iters), loss = 0.295635
I0426 20:50:55.426163 31862 solver.cpp:238]     Train net output #0: loss = 0.295635 (* 1 = 0.295635 loss)
I0426 20:50:55.426184 31862 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0426 20:50:55.511353 31862 solver.cpp:331] Iteration 500, Testing net (#0)
I0426 20:50:55.586817 31870 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:50:55.587399 31862 solver.cpp:398]     Test net output #0: accuracy = 0.8516
I0426 20:50:55.587419 31862 solver.cpp:398]     Test net output #1: loss = 0.391969 (* 1 = 0.391969 loss)
I0426 20:50:55.588351 31862 solver.cpp:219] Iteration 500 (616.633 iter/s, 0.162171s/100 iters), loss = 0.368546
I0426 20:50:55.588415 31862 solver.cpp:238]     Train net output #0: loss = 0.368546 (* 1 = 0.368546 loss)
I0426 20:50:55.588424 31862 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0426 20:50:55.687418 31862 solver.cpp:219] Iteration 600 (1010.16 iter/s, 0.0989943s/100 iters), loss = 0.565328
I0426 20:50:55.687441 31862 solver.cpp:238]     Train net output #0: loss = 0.565328 (* 1 = 0.565328 loss)
I0426 20:50:55.687448 31862 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0426 20:50:55.775846 31862 solver.cpp:219] Iteration 700 (1131.29 iter/s, 0.0883948s/100 iters), loss = 0.297397
I0426 20:50:55.775887 31862 solver.cpp:238]     Train net output #0: loss = 0.297397 (* 1 = 0.297397 loss)
I0426 20:50:55.775892 31862 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0426 20:50:55.862638 31862 solver.cpp:219] Iteration 800 (1152.63 iter/s, 0.0867584s/100 iters), loss = 0.426305
I0426 20:50:55.862664 31862 solver.cpp:238]     Train net output #0: loss = 0.426305 (* 1 = 0.426305 loss)
I0426 20:50:55.862670 31862 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0426 20:50:55.950811 31862 solver.cpp:219] Iteration 900 (1134.59 iter/s, 0.0881377s/100 iters), loss = 0.462878
I0426 20:50:55.950848 31862 solver.cpp:238]     Train net output #0: loss = 0.462878 (* 1 = 0.462878 loss)
I0426 20:50:55.950855 31862 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0426 20:50:55.980026 31869 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:50:56.037209 31862 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0426 20:50:56.038511 31862 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0426 20:50:56.039383 31862 solver.cpp:311] Iteration 1000, loss = 0.130703
I0426 20:50:56.039413 31862 solver.cpp:331] Iteration 1000, Testing net (#0)
I0426 20:50:56.114648 31870 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:50:56.115198 31862 solver.cpp:398]     Test net output #0: accuracy = 0.9738
I0426 20:50:56.115217 31862 solver.cpp:398]     Test net output #1: loss = 0.0828797 (* 1 = 0.0828797 loss)
I0426 20:50:56.115223 31862 solver.cpp:316] Optimization Done.
I0426 20:50:56.115226 31862 caffe.cpp:259] Optimization Done.
