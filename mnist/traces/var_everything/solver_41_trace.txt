I0426 20:49:38.361901 31571 caffe.cpp:218] Using GPUs 0
I0426 20:49:38.400418 31571 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0426 20:49:38.867668 31571 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything/lenet_train_test41.prototxt"
train_state {
  level: 0
  stage: ""
}
I0426 20:49:38.867815 31571 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything/lenet_train_test41.prototxt
I0426 20:49:38.868113 31571 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0426 20:49:38.868126 31571 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0426 20:49:38.868192 31571 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool0"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0426 20:49:38.868247 31571 layer_factory.hpp:77] Creating layer mnist
I0426 20:49:38.868327 31571 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0426 20:49:38.868346 31571 net.cpp:86] Creating Layer mnist
I0426 20:49:38.868351 31571 net.cpp:382] mnist -> data
I0426 20:49:38.868371 31571 net.cpp:382] mnist -> label
I0426 20:49:38.869437 31571 data_layer.cpp:45] output data size: 64,1,28,28
I0426 20:49:38.871518 31571 net.cpp:124] Setting up mnist
I0426 20:49:38.871531 31571 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0426 20:49:38.871536 31571 net.cpp:131] Top shape: 64 (64)
I0426 20:49:38.871539 31571 net.cpp:139] Memory required for data: 200960
I0426 20:49:38.871544 31571 layer_factory.hpp:77] Creating layer conv0
I0426 20:49:38.871556 31571 net.cpp:86] Creating Layer conv0
I0426 20:49:38.871562 31571 net.cpp:408] conv0 <- data
I0426 20:49:38.871569 31571 net.cpp:382] conv0 -> conv0
I0426 20:49:39.107699 31571 net.cpp:124] Setting up conv0
I0426 20:49:39.107727 31571 net.cpp:131] Top shape: 64 25 24 24 (921600)
I0426 20:49:39.107730 31571 net.cpp:139] Memory required for data: 3887360
I0426 20:49:39.107744 31571 layer_factory.hpp:77] Creating layer pool0
I0426 20:49:39.107756 31571 net.cpp:86] Creating Layer pool0
I0426 20:49:39.107779 31571 net.cpp:408] pool0 <- conv0
I0426 20:49:39.107800 31571 net.cpp:382] pool0 -> pool0
I0426 20:49:39.107842 31571 net.cpp:124] Setting up pool0
I0426 20:49:39.107847 31571 net.cpp:131] Top shape: 64 25 12 12 (230400)
I0426 20:49:39.107851 31571 net.cpp:139] Memory required for data: 4808960
I0426 20:49:39.107852 31571 layer_factory.hpp:77] Creating layer ip1
I0426 20:49:39.107861 31571 net.cpp:86] Creating Layer ip1
I0426 20:49:39.107863 31571 net.cpp:408] ip1 <- pool0
I0426 20:49:39.107867 31571 net.cpp:382] ip1 -> ip1
I0426 20:49:39.114749 31571 net.cpp:124] Setting up ip1
I0426 20:49:39.114761 31571 net.cpp:131] Top shape: 64 300 (19200)
I0426 20:49:39.114780 31571 net.cpp:139] Memory required for data: 4885760
I0426 20:49:39.114789 31571 layer_factory.hpp:77] Creating layer relu1
I0426 20:49:39.114794 31571 net.cpp:86] Creating Layer relu1
I0426 20:49:39.114797 31571 net.cpp:408] relu1 <- ip1
I0426 20:49:39.114802 31571 net.cpp:369] relu1 -> ip1 (in-place)
I0426 20:49:39.114959 31571 net.cpp:124] Setting up relu1
I0426 20:49:39.114967 31571 net.cpp:131] Top shape: 64 300 (19200)
I0426 20:49:39.114971 31571 net.cpp:139] Memory required for data: 4962560
I0426 20:49:39.114974 31571 layer_factory.hpp:77] Creating layer ip2
I0426 20:49:39.114980 31571 net.cpp:86] Creating Layer ip2
I0426 20:49:39.114984 31571 net.cpp:408] ip2 <- ip1
I0426 20:49:39.114987 31571 net.cpp:382] ip2 -> ip2
I0426 20:49:39.116343 31571 net.cpp:124] Setting up ip2
I0426 20:49:39.116355 31571 net.cpp:131] Top shape: 64 300 (19200)
I0426 20:49:39.116358 31571 net.cpp:139] Memory required for data: 5039360
I0426 20:49:39.116365 31571 layer_factory.hpp:77] Creating layer relu2
I0426 20:49:39.116370 31571 net.cpp:86] Creating Layer relu2
I0426 20:49:39.116374 31571 net.cpp:408] relu2 <- ip2
I0426 20:49:39.116377 31571 net.cpp:369] relu2 -> ip2 (in-place)
I0426 20:49:39.117247 31571 net.cpp:124] Setting up relu2
I0426 20:49:39.117259 31571 net.cpp:131] Top shape: 64 300 (19200)
I0426 20:49:39.117262 31571 net.cpp:139] Memory required for data: 5116160
I0426 20:49:39.117265 31571 layer_factory.hpp:77] Creating layer ip3
I0426 20:49:39.117271 31571 net.cpp:86] Creating Layer ip3
I0426 20:49:39.117274 31571 net.cpp:408] ip3 <- ip2
I0426 20:49:39.117280 31571 net.cpp:382] ip3 -> ip3
I0426 20:49:39.118197 31571 net.cpp:124] Setting up ip3
I0426 20:49:39.118208 31571 net.cpp:131] Top shape: 64 10 (640)
I0426 20:49:39.118227 31571 net.cpp:139] Memory required for data: 5118720
I0426 20:49:39.118232 31571 layer_factory.hpp:77] Creating layer relu3
I0426 20:49:39.118253 31571 net.cpp:86] Creating Layer relu3
I0426 20:49:39.118257 31571 net.cpp:408] relu3 <- ip3
I0426 20:49:39.118260 31571 net.cpp:369] relu3 -> ip3 (in-place)
I0426 20:49:39.118418 31571 net.cpp:124] Setting up relu3
I0426 20:49:39.118427 31571 net.cpp:131] Top shape: 64 10 (640)
I0426 20:49:39.118430 31571 net.cpp:139] Memory required for data: 5121280
I0426 20:49:39.118432 31571 layer_factory.hpp:77] Creating layer loss
I0426 20:49:39.118438 31571 net.cpp:86] Creating Layer loss
I0426 20:49:39.118440 31571 net.cpp:408] loss <- ip3
I0426 20:49:39.118444 31571 net.cpp:408] loss <- label
I0426 20:49:39.118449 31571 net.cpp:382] loss -> loss
I0426 20:49:39.118463 31571 layer_factory.hpp:77] Creating layer loss
I0426 20:49:39.118669 31571 net.cpp:124] Setting up loss
I0426 20:49:39.118677 31571 net.cpp:131] Top shape: (1)
I0426 20:49:39.118680 31571 net.cpp:134]     with loss weight 1
I0426 20:49:39.118693 31571 net.cpp:139] Memory required for data: 5121284
I0426 20:49:39.118696 31571 net.cpp:200] loss needs backward computation.
I0426 20:49:39.118700 31571 net.cpp:200] relu3 needs backward computation.
I0426 20:49:39.118703 31571 net.cpp:200] ip3 needs backward computation.
I0426 20:49:39.118705 31571 net.cpp:200] relu2 needs backward computation.
I0426 20:49:39.118708 31571 net.cpp:200] ip2 needs backward computation.
I0426 20:49:39.118711 31571 net.cpp:200] relu1 needs backward computation.
I0426 20:49:39.118713 31571 net.cpp:200] ip1 needs backward computation.
I0426 20:49:39.118726 31571 net.cpp:200] pool0 needs backward computation.
I0426 20:49:39.118729 31571 net.cpp:200] conv0 needs backward computation.
I0426 20:49:39.118732 31571 net.cpp:202] mnist does not need backward computation.
I0426 20:49:39.118736 31571 net.cpp:244] This network produces output loss
I0426 20:49:39.118742 31571 net.cpp:257] Network initialization done.
I0426 20:49:39.119025 31571 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything/lenet_train_test41.prototxt
I0426 20:49:39.119050 31571 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0426 20:49:39.119128 31571 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool0"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0426 20:49:39.119189 31571 layer_factory.hpp:77] Creating layer mnist
I0426 20:49:39.119230 31571 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0426 20:49:39.119242 31571 net.cpp:86] Creating Layer mnist
I0426 20:49:39.119246 31571 net.cpp:382] mnist -> data
I0426 20:49:39.119253 31571 net.cpp:382] mnist -> label
I0426 20:49:39.119330 31571 data_layer.cpp:45] output data size: 100,1,28,28
I0426 20:49:39.120436 31571 net.cpp:124] Setting up mnist
I0426 20:49:39.120463 31571 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0426 20:49:39.120468 31571 net.cpp:131] Top shape: 100 (100)
I0426 20:49:39.120472 31571 net.cpp:139] Memory required for data: 314000
I0426 20:49:39.120476 31571 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0426 20:49:39.120481 31571 net.cpp:86] Creating Layer label_mnist_1_split
I0426 20:49:39.120484 31571 net.cpp:408] label_mnist_1_split <- label
I0426 20:49:39.120489 31571 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0426 20:49:39.120496 31571 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0426 20:49:39.120532 31571 net.cpp:124] Setting up label_mnist_1_split
I0426 20:49:39.120537 31571 net.cpp:131] Top shape: 100 (100)
I0426 20:49:39.120540 31571 net.cpp:131] Top shape: 100 (100)
I0426 20:49:39.120543 31571 net.cpp:139] Memory required for data: 314800
I0426 20:49:39.120554 31571 layer_factory.hpp:77] Creating layer conv0
I0426 20:49:39.120563 31571 net.cpp:86] Creating Layer conv0
I0426 20:49:39.120566 31571 net.cpp:408] conv0 <- data
I0426 20:49:39.120570 31571 net.cpp:382] conv0 -> conv0
I0426 20:49:39.122170 31571 net.cpp:124] Setting up conv0
I0426 20:49:39.122195 31571 net.cpp:131] Top shape: 100 25 24 24 (1440000)
I0426 20:49:39.122198 31571 net.cpp:139] Memory required for data: 6074800
I0426 20:49:39.122206 31571 layer_factory.hpp:77] Creating layer pool0
I0426 20:49:39.122212 31571 net.cpp:86] Creating Layer pool0
I0426 20:49:39.122215 31571 net.cpp:408] pool0 <- conv0
I0426 20:49:39.122220 31571 net.cpp:382] pool0 -> pool0
I0426 20:49:39.122253 31571 net.cpp:124] Setting up pool0
I0426 20:49:39.122258 31571 net.cpp:131] Top shape: 100 25 12 12 (360000)
I0426 20:49:39.122262 31571 net.cpp:139] Memory required for data: 7514800
I0426 20:49:39.122263 31571 layer_factory.hpp:77] Creating layer ip1
I0426 20:49:39.122269 31571 net.cpp:86] Creating Layer ip1
I0426 20:49:39.122272 31571 net.cpp:408] ip1 <- pool0
I0426 20:49:39.122277 31571 net.cpp:382] ip1 -> ip1
I0426 20:49:39.129400 31571 net.cpp:124] Setting up ip1
I0426 20:49:39.129412 31571 net.cpp:131] Top shape: 100 300 (30000)
I0426 20:49:39.129416 31571 net.cpp:139] Memory required for data: 7634800
I0426 20:49:39.129425 31571 layer_factory.hpp:77] Creating layer relu1
I0426 20:49:39.129429 31571 net.cpp:86] Creating Layer relu1
I0426 20:49:39.129432 31571 net.cpp:408] relu1 <- ip1
I0426 20:49:39.129447 31571 net.cpp:369] relu1 -> ip1 (in-place)
I0426 20:49:39.129600 31571 net.cpp:124] Setting up relu1
I0426 20:49:39.129608 31571 net.cpp:131] Top shape: 100 300 (30000)
I0426 20:49:39.129611 31571 net.cpp:139] Memory required for data: 7754800
I0426 20:49:39.129614 31571 layer_factory.hpp:77] Creating layer ip2
I0426 20:49:39.129619 31571 net.cpp:86] Creating Layer ip2
I0426 20:49:39.129622 31571 net.cpp:408] ip2 <- ip1
I0426 20:49:39.129626 31571 net.cpp:382] ip2 -> ip2
I0426 20:49:39.130175 31571 net.cpp:124] Setting up ip2
I0426 20:49:39.130183 31571 net.cpp:131] Top shape: 100 300 (30000)
I0426 20:49:39.130187 31571 net.cpp:139] Memory required for data: 7874800
I0426 20:49:39.130192 31571 layer_factory.hpp:77] Creating layer relu2
I0426 20:49:39.130197 31571 net.cpp:86] Creating Layer relu2
I0426 20:49:39.130200 31571 net.cpp:408] relu2 <- ip2
I0426 20:49:39.130203 31571 net.cpp:369] relu2 -> ip2 (in-place)
I0426 20:49:39.130972 31571 net.cpp:124] Setting up relu2
I0426 20:49:39.130985 31571 net.cpp:131] Top shape: 100 300 (30000)
I0426 20:49:39.130988 31571 net.cpp:139] Memory required for data: 7994800
I0426 20:49:39.130991 31571 layer_factory.hpp:77] Creating layer ip3
I0426 20:49:39.130998 31571 net.cpp:86] Creating Layer ip3
I0426 20:49:39.131002 31571 net.cpp:408] ip3 <- ip2
I0426 20:49:39.131007 31571 net.cpp:382] ip3 -> ip3
I0426 20:49:39.131117 31571 net.cpp:124] Setting up ip3
I0426 20:49:39.131125 31571 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:49:39.131129 31571 net.cpp:139] Memory required for data: 7998800
I0426 20:49:39.131134 31571 layer_factory.hpp:77] Creating layer relu3
I0426 20:49:39.131137 31571 net.cpp:86] Creating Layer relu3
I0426 20:49:39.131140 31571 net.cpp:408] relu3 <- ip3
I0426 20:49:39.131145 31571 net.cpp:369] relu3 -> ip3 (in-place)
I0426 20:49:39.131283 31571 net.cpp:124] Setting up relu3
I0426 20:49:39.131291 31571 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:49:39.131294 31571 net.cpp:139] Memory required for data: 8002800
I0426 20:49:39.131296 31571 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0426 20:49:39.131300 31571 net.cpp:86] Creating Layer ip3_relu3_0_split
I0426 20:49:39.131304 31571 net.cpp:408] ip3_relu3_0_split <- ip3
I0426 20:49:39.131309 31571 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0426 20:49:39.131314 31571 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0426 20:49:39.131345 31571 net.cpp:124] Setting up ip3_relu3_0_split
I0426 20:49:39.131350 31571 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:49:39.131362 31571 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:49:39.131366 31571 net.cpp:139] Memory required for data: 8010800
I0426 20:49:39.131368 31571 layer_factory.hpp:77] Creating layer accuracy
I0426 20:49:39.131373 31571 net.cpp:86] Creating Layer accuracy
I0426 20:49:39.131376 31571 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0426 20:49:39.131381 31571 net.cpp:408] accuracy <- label_mnist_1_split_0
I0426 20:49:39.131399 31571 net.cpp:382] accuracy -> accuracy
I0426 20:49:39.131405 31571 net.cpp:124] Setting up accuracy
I0426 20:49:39.131409 31571 net.cpp:131] Top shape: (1)
I0426 20:49:39.131412 31571 net.cpp:139] Memory required for data: 8010804
I0426 20:49:39.131414 31571 layer_factory.hpp:77] Creating layer loss
I0426 20:49:39.131418 31571 net.cpp:86] Creating Layer loss
I0426 20:49:39.131420 31571 net.cpp:408] loss <- ip3_relu3_0_split_1
I0426 20:49:39.131424 31571 net.cpp:408] loss <- label_mnist_1_split_1
I0426 20:49:39.131428 31571 net.cpp:382] loss -> loss
I0426 20:49:39.131433 31571 layer_factory.hpp:77] Creating layer loss
I0426 20:49:39.131659 31571 net.cpp:124] Setting up loss
I0426 20:49:39.131667 31571 net.cpp:131] Top shape: (1)
I0426 20:49:39.131670 31571 net.cpp:134]     with loss weight 1
I0426 20:49:39.131676 31571 net.cpp:139] Memory required for data: 8010808
I0426 20:49:39.131680 31571 net.cpp:200] loss needs backward computation.
I0426 20:49:39.131682 31571 net.cpp:202] accuracy does not need backward computation.
I0426 20:49:39.131686 31571 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0426 20:49:39.131690 31571 net.cpp:200] relu3 needs backward computation.
I0426 20:49:39.131691 31571 net.cpp:200] ip3 needs backward computation.
I0426 20:49:39.131695 31571 net.cpp:200] relu2 needs backward computation.
I0426 20:49:39.131697 31571 net.cpp:200] ip2 needs backward computation.
I0426 20:49:39.131700 31571 net.cpp:200] relu1 needs backward computation.
I0426 20:49:39.131703 31571 net.cpp:200] ip1 needs backward computation.
I0426 20:49:39.131706 31571 net.cpp:200] pool0 needs backward computation.
I0426 20:49:39.131709 31571 net.cpp:200] conv0 needs backward computation.
I0426 20:49:39.131712 31571 net.cpp:202] label_mnist_1_split does not need backward computation.
I0426 20:49:39.131716 31571 net.cpp:202] mnist does not need backward computation.
I0426 20:49:39.131718 31571 net.cpp:244] This network produces output accuracy
I0426 20:49:39.131721 31571 net.cpp:244] This network produces output loss
I0426 20:49:39.131731 31571 net.cpp:257] Network initialization done.
I0426 20:49:39.131764 31571 solver.cpp:56] Solver scaffolding done.
I0426 20:49:39.132025 31571 caffe.cpp:248] Starting Optimization
I0426 20:49:39.132047 31571 solver.cpp:273] Solving LeNet
I0426 20:49:39.132050 31571 solver.cpp:274] Learning Rate Policy: inv
I0426 20:49:39.134124 31571 solver.cpp:331] Iteration 0, Testing net (#0)
I0426 20:49:39.140305 31571 blocking_queue.cpp:49] Waiting for data
I0426 20:49:39.211819 31578 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:49:39.212492 31571 solver.cpp:398]     Test net output #0: accuracy = 0.1099
I0426 20:49:39.212513 31571 solver.cpp:398]     Test net output #1: loss = 2.29594 (* 1 = 2.29594 loss)
I0426 20:49:39.215576 31571 solver.cpp:219] Iteration 0 (-4.64152e-31 iter/s, 0.0835021s/100 iters), loss = 2.31866
I0426 20:49:39.215611 31571 solver.cpp:238]     Train net output #0: loss = 2.31866 (* 1 = 2.31866 loss)
I0426 20:49:39.215637 31571 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0426 20:49:39.332799 31571 solver.cpp:219] Iteration 100 (853.407 iter/s, 0.117177s/100 iters), loss = 0.881435
I0426 20:49:39.332844 31571 solver.cpp:238]     Train net output #0: loss = 0.881435 (* 1 = 0.881435 loss)
I0426 20:49:39.332850 31571 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0426 20:49:39.444149 31571 solver.cpp:219] Iteration 200 (898.393 iter/s, 0.11131s/100 iters), loss = 0.86144
I0426 20:49:39.444187 31571 solver.cpp:238]     Train net output #0: loss = 0.86144 (* 1 = 0.86144 loss)
I0426 20:49:39.444195 31571 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0426 20:49:39.559195 31571 solver.cpp:219] Iteration 300 (869.485 iter/s, 0.115011s/100 iters), loss = 1.11916
I0426 20:49:39.559231 31571 solver.cpp:238]     Train net output #0: loss = 1.11916 (* 1 = 1.11916 loss)
I0426 20:49:39.559237 31571 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0426 20:49:39.671335 31571 solver.cpp:219] Iteration 400 (891.993 iter/s, 0.112109s/100 iters), loss = 0.868063
I0426 20:49:39.671360 31571 solver.cpp:238]     Train net output #0: loss = 0.868063 (* 1 = 0.868063 loss)
I0426 20:49:39.671365 31571 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0426 20:49:39.780172 31571 solver.cpp:331] Iteration 500, Testing net (#0)
I0426 20:49:39.857197 31578 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:49:39.857877 31571 solver.cpp:398]     Test net output #0: accuracy = 0.7554
I0426 20:49:39.857899 31571 solver.cpp:398]     Test net output #1: loss = 0.806649 (* 1 = 0.806649 loss)
I0426 20:49:39.858909 31571 solver.cpp:219] Iteration 500 (533.23 iter/s, 0.187536s/100 iters), loss = 0.679294
I0426 20:49:39.858965 31571 solver.cpp:238]     Train net output #0: loss = 0.679294 (* 1 = 0.679294 loss)
I0426 20:49:39.858973 31571 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0426 20:49:39.976374 31571 solver.cpp:219] Iteration 600 (851.784 iter/s, 0.117401s/100 iters), loss = 0.570425
I0426 20:49:39.976413 31571 solver.cpp:238]     Train net output #0: loss = 0.570425 (* 1 = 0.570425 loss)
I0426 20:49:39.976419 31571 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0426 20:49:40.088618 31571 solver.cpp:219] Iteration 700 (891.202 iter/s, 0.112208s/100 iters), loss = 0.863539
I0426 20:49:40.088660 31571 solver.cpp:238]     Train net output #0: loss = 0.863539 (* 1 = 0.863539 loss)
I0426 20:49:40.088665 31571 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0426 20:49:40.198603 31571 solver.cpp:219] Iteration 800 (909.632 iter/s, 0.109935s/100 iters), loss = 0.957491
I0426 20:49:40.198642 31571 solver.cpp:238]     Train net output #0: loss = 0.957491 (* 1 = 0.957491 loss)
I0426 20:49:40.198647 31571 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0426 20:49:40.308377 31571 solver.cpp:219] Iteration 900 (911.238 iter/s, 0.109741s/100 iters), loss = 0.8721
I0426 20:49:40.308401 31571 solver.cpp:238]     Train net output #0: loss = 0.8721 (* 1 = 0.8721 loss)
I0426 20:49:40.308406 31571 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0426 20:49:40.344823 31577 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:49:40.416505 31571 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0426 20:49:40.436388 31571 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0426 20:49:40.444447 31571 solver.cpp:311] Iteration 1000, loss = 0.612025
I0426 20:49:40.444468 31571 solver.cpp:331] Iteration 1000, Testing net (#0)
I0426 20:49:40.520462 31578 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:49:40.521368 31571 solver.cpp:398]     Test net output #0: accuracy = 0.7751
I0426 20:49:40.521400 31571 solver.cpp:398]     Test net output #1: loss = 0.578923 (* 1 = 0.578923 loss)
I0426 20:49:40.521407 31571 solver.cpp:316] Optimization Done.
I0426 20:49:40.521412 31571 caffe.cpp:259] Optimization Done.
