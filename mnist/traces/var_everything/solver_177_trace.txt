I0426 20:55:18.128234   371 caffe.cpp:218] Using GPUs 0
I0426 20:55:18.163831   371 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0426 20:55:18.696105   371 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything/lenet_train_test177.prototxt"
train_state {
  level: 0
  stage: ""
}
I0426 20:55:18.696244   371 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything/lenet_train_test177.prototxt
I0426 20:55:18.696658   371 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0426 20:55:18.696677   371 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0426 20:55:18.696781   371 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0426 20:55:18.696869   371 layer_factory.hpp:77] Creating layer mnist
I0426 20:55:18.696970   371 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0426 20:55:18.696995   371 net.cpp:86] Creating Layer mnist
I0426 20:55:18.697002   371 net.cpp:382] mnist -> data
I0426 20:55:18.697026   371 net.cpp:382] mnist -> label
I0426 20:55:18.698129   371 data_layer.cpp:45] output data size: 64,1,28,28
I0426 20:55:18.700680   371 net.cpp:124] Setting up mnist
I0426 20:55:18.700697   371 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0426 20:55:18.700703   371 net.cpp:131] Top shape: 64 (64)
I0426 20:55:18.700706   371 net.cpp:139] Memory required for data: 200960
I0426 20:55:18.700713   371 layer_factory.hpp:77] Creating layer conv0
I0426 20:55:18.700732   371 net.cpp:86] Creating Layer conv0
I0426 20:55:18.700754   371 net.cpp:408] conv0 <- data
I0426 20:55:18.700767   371 net.cpp:382] conv0 -> conv0
I0426 20:55:18.995164   371 net.cpp:124] Setting up conv0
I0426 20:55:18.995196   371 net.cpp:131] Top shape: 64 50 24 24 (1843200)
I0426 20:55:18.995200   371 net.cpp:139] Memory required for data: 7573760
I0426 20:55:18.995218   371 layer_factory.hpp:77] Creating layer pool0
I0426 20:55:18.995234   371 net.cpp:86] Creating Layer pool0
I0426 20:55:18.995239   371 net.cpp:408] pool0 <- conv0
I0426 20:55:18.995245   371 net.cpp:382] pool0 -> pool0
I0426 20:55:18.995301   371 net.cpp:124] Setting up pool0
I0426 20:55:18.995308   371 net.cpp:131] Top shape: 64 50 12 12 (460800)
I0426 20:55:18.995311   371 net.cpp:139] Memory required for data: 9416960
I0426 20:55:18.995316   371 layer_factory.hpp:77] Creating layer conv1
I0426 20:55:18.995327   371 net.cpp:86] Creating Layer conv1
I0426 20:55:18.995332   371 net.cpp:408] conv1 <- pool0
I0426 20:55:18.995337   371 net.cpp:382] conv1 -> conv1
I0426 20:55:18.998153   371 net.cpp:124] Setting up conv1
I0426 20:55:18.998172   371 net.cpp:131] Top shape: 64 50 8 8 (204800)
I0426 20:55:18.998175   371 net.cpp:139] Memory required for data: 10236160
I0426 20:55:18.998186   371 layer_factory.hpp:77] Creating layer pool1
I0426 20:55:18.998195   371 net.cpp:86] Creating Layer pool1
I0426 20:55:18.998199   371 net.cpp:408] pool1 <- conv1
I0426 20:55:18.998205   371 net.cpp:382] pool1 -> pool1
I0426 20:55:18.998250   371 net.cpp:124] Setting up pool1
I0426 20:55:18.998256   371 net.cpp:131] Top shape: 64 50 4 4 (51200)
I0426 20:55:18.998260   371 net.cpp:139] Memory required for data: 10440960
I0426 20:55:18.998262   371 layer_factory.hpp:77] Creating layer ip1
I0426 20:55:18.998270   371 net.cpp:86] Creating Layer ip1
I0426 20:55:18.998275   371 net.cpp:408] ip1 <- pool1
I0426 20:55:18.998281   371 net.cpp:382] ip1 -> ip1
I0426 20:55:18.999902   371 net.cpp:124] Setting up ip1
I0426 20:55:18.999917   371 net.cpp:131] Top shape: 64 100 (6400)
I0426 20:55:18.999920   371 net.cpp:139] Memory required for data: 10466560
I0426 20:55:18.999930   371 layer_factory.hpp:77] Creating layer relu1
I0426 20:55:18.999938   371 net.cpp:86] Creating Layer relu1
I0426 20:55:18.999941   371 net.cpp:408] relu1 <- ip1
I0426 20:55:18.999948   371 net.cpp:369] relu1 -> ip1 (in-place)
I0426 20:55:19.000151   371 net.cpp:124] Setting up relu1
I0426 20:55:19.000162   371 net.cpp:131] Top shape: 64 100 (6400)
I0426 20:55:19.000166   371 net.cpp:139] Memory required for data: 10492160
I0426 20:55:19.000170   371 layer_factory.hpp:77] Creating layer ip2
I0426 20:55:19.000177   371 net.cpp:86] Creating Layer ip2
I0426 20:55:19.000181   371 net.cpp:408] ip2 <- ip1
I0426 20:55:19.000187   371 net.cpp:382] ip2 -> ip2
I0426 20:55:19.000516   371 net.cpp:124] Setting up ip2
I0426 20:55:19.000524   371 net.cpp:131] Top shape: 64 300 (19200)
I0426 20:55:19.000529   371 net.cpp:139] Memory required for data: 10568960
I0426 20:55:19.000535   371 layer_factory.hpp:77] Creating layer relu2
I0426 20:55:19.000541   371 net.cpp:86] Creating Layer relu2
I0426 20:55:19.000545   371 net.cpp:408] relu2 <- ip2
I0426 20:55:19.000550   371 net.cpp:369] relu2 -> ip2 (in-place)
I0426 20:55:19.001505   371 net.cpp:124] Setting up relu2
I0426 20:55:19.001521   371 net.cpp:131] Top shape: 64 300 (19200)
I0426 20:55:19.001525   371 net.cpp:139] Memory required for data: 10645760
I0426 20:55:19.001530   371 layer_factory.hpp:77] Creating layer ip3
I0426 20:55:19.001538   371 net.cpp:86] Creating Layer ip3
I0426 20:55:19.001543   371 net.cpp:408] ip3 <- ip2
I0426 20:55:19.001549   371 net.cpp:382] ip3 -> ip3
I0426 20:55:19.001696   371 net.cpp:124] Setting up ip3
I0426 20:55:19.001705   371 net.cpp:131] Top shape: 64 10 (640)
I0426 20:55:19.001709   371 net.cpp:139] Memory required for data: 10648320
I0426 20:55:19.001718   371 layer_factory.hpp:77] Creating layer relu3
I0426 20:55:19.001724   371 net.cpp:86] Creating Layer relu3
I0426 20:55:19.001729   371 net.cpp:408] relu3 <- ip3
I0426 20:55:19.001732   371 net.cpp:369] relu3 -> ip3 (in-place)
I0426 20:55:19.001935   371 net.cpp:124] Setting up relu3
I0426 20:55:19.001946   371 net.cpp:131] Top shape: 64 10 (640)
I0426 20:55:19.001950   371 net.cpp:139] Memory required for data: 10650880
I0426 20:55:19.001953   371 layer_factory.hpp:77] Creating layer loss
I0426 20:55:19.001960   371 net.cpp:86] Creating Layer loss
I0426 20:55:19.001963   371 net.cpp:408] loss <- ip3
I0426 20:55:19.001968   371 net.cpp:408] loss <- label
I0426 20:55:19.001974   371 net.cpp:382] loss -> loss
I0426 20:55:19.001994   371 layer_factory.hpp:77] Creating layer loss
I0426 20:55:19.002266   371 net.cpp:124] Setting up loss
I0426 20:55:19.002276   371 net.cpp:131] Top shape: (1)
I0426 20:55:19.002280   371 net.cpp:134]     with loss weight 1
I0426 20:55:19.002297   371 net.cpp:139] Memory required for data: 10650884
I0426 20:55:19.002301   371 net.cpp:200] loss needs backward computation.
I0426 20:55:19.002305   371 net.cpp:200] relu3 needs backward computation.
I0426 20:55:19.002310   371 net.cpp:200] ip3 needs backward computation.
I0426 20:55:19.002313   371 net.cpp:200] relu2 needs backward computation.
I0426 20:55:19.002316   371 net.cpp:200] ip2 needs backward computation.
I0426 20:55:19.002320   371 net.cpp:200] relu1 needs backward computation.
I0426 20:55:19.002323   371 net.cpp:200] ip1 needs backward computation.
I0426 20:55:19.002326   371 net.cpp:200] pool1 needs backward computation.
I0426 20:55:19.002331   371 net.cpp:200] conv1 needs backward computation.
I0426 20:55:19.002334   371 net.cpp:200] pool0 needs backward computation.
I0426 20:55:19.002337   371 net.cpp:200] conv0 needs backward computation.
I0426 20:55:19.002342   371 net.cpp:202] mnist does not need backward computation.
I0426 20:55:19.002346   371 net.cpp:244] This network produces output loss
I0426 20:55:19.002357   371 net.cpp:257] Network initialization done.
I0426 20:55:19.002743   371 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything/lenet_train_test177.prototxt
I0426 20:55:19.002775   371 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0426 20:55:19.002887   371 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0426 20:55:19.002982   371 layer_factory.hpp:77] Creating layer mnist
I0426 20:55:19.003033   371 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0426 20:55:19.003051   371 net.cpp:86] Creating Layer mnist
I0426 20:55:19.003057   371 net.cpp:382] mnist -> data
I0426 20:55:19.003067   371 net.cpp:382] mnist -> label
I0426 20:55:19.003168   371 data_layer.cpp:45] output data size: 100,1,28,28
I0426 20:55:19.004488   371 net.cpp:124] Setting up mnist
I0426 20:55:19.004503   371 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0426 20:55:19.004510   371 net.cpp:131] Top shape: 100 (100)
I0426 20:55:19.004513   371 net.cpp:139] Memory required for data: 314000
I0426 20:55:19.004518   371 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0426 20:55:19.004525   371 net.cpp:86] Creating Layer label_mnist_1_split
I0426 20:55:19.004529   371 net.cpp:408] label_mnist_1_split <- label
I0426 20:55:19.004535   371 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0426 20:55:19.004544   371 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0426 20:55:19.004607   371 net.cpp:124] Setting up label_mnist_1_split
I0426 20:55:19.004616   371 net.cpp:131] Top shape: 100 (100)
I0426 20:55:19.004619   371 net.cpp:131] Top shape: 100 (100)
I0426 20:55:19.004623   371 net.cpp:139] Memory required for data: 314800
I0426 20:55:19.004626   371 layer_factory.hpp:77] Creating layer conv0
I0426 20:55:19.004637   371 net.cpp:86] Creating Layer conv0
I0426 20:55:19.004640   371 net.cpp:408] conv0 <- data
I0426 20:55:19.004645   371 net.cpp:382] conv0 -> conv0
I0426 20:55:19.006681   371 net.cpp:124] Setting up conv0
I0426 20:55:19.006698   371 net.cpp:131] Top shape: 100 50 24 24 (2880000)
I0426 20:55:19.006702   371 net.cpp:139] Memory required for data: 11834800
I0426 20:55:19.006713   371 layer_factory.hpp:77] Creating layer pool0
I0426 20:55:19.006721   371 net.cpp:86] Creating Layer pool0
I0426 20:55:19.006726   371 net.cpp:408] pool0 <- conv0
I0426 20:55:19.006731   371 net.cpp:382] pool0 -> pool0
I0426 20:55:19.006774   371 net.cpp:124] Setting up pool0
I0426 20:55:19.006780   371 net.cpp:131] Top shape: 100 50 12 12 (720000)
I0426 20:55:19.006783   371 net.cpp:139] Memory required for data: 14714800
I0426 20:55:19.006788   371 layer_factory.hpp:77] Creating layer conv1
I0426 20:55:19.006798   371 net.cpp:86] Creating Layer conv1
I0426 20:55:19.006801   371 net.cpp:408] conv1 <- pool0
I0426 20:55:19.006808   371 net.cpp:382] conv1 -> conv1
I0426 20:55:19.009013   371 net.cpp:124] Setting up conv1
I0426 20:55:19.009029   371 net.cpp:131] Top shape: 100 50 8 8 (320000)
I0426 20:55:19.009033   371 net.cpp:139] Memory required for data: 15994800
I0426 20:55:19.009044   371 layer_factory.hpp:77] Creating layer pool1
I0426 20:55:19.009053   371 net.cpp:86] Creating Layer pool1
I0426 20:55:19.009058   371 net.cpp:408] pool1 <- conv1
I0426 20:55:19.009064   371 net.cpp:382] pool1 -> pool1
I0426 20:55:19.009106   371 net.cpp:124] Setting up pool1
I0426 20:55:19.009114   371 net.cpp:131] Top shape: 100 50 4 4 (80000)
I0426 20:55:19.009116   371 net.cpp:139] Memory required for data: 16314800
I0426 20:55:19.009120   371 layer_factory.hpp:77] Creating layer ip1
I0426 20:55:19.009127   371 net.cpp:86] Creating Layer ip1
I0426 20:55:19.009130   371 net.cpp:408] ip1 <- pool1
I0426 20:55:19.009136   371 net.cpp:382] ip1 -> ip1
I0426 20:55:19.009786   371 net.cpp:124] Setting up ip1
I0426 20:55:19.009809   371 net.cpp:131] Top shape: 100 100 (10000)
I0426 20:55:19.009811   371 net.cpp:139] Memory required for data: 16354800
I0426 20:55:19.009821   371 layer_factory.hpp:77] Creating layer relu1
I0426 20:55:19.009827   371 net.cpp:86] Creating Layer relu1
I0426 20:55:19.009831   371 net.cpp:408] relu1 <- ip1
I0426 20:55:19.009836   371 net.cpp:369] relu1 -> ip1 (in-place)
I0426 20:55:19.010102   371 net.cpp:124] Setting up relu1
I0426 20:55:19.010112   371 net.cpp:131] Top shape: 100 100 (10000)
I0426 20:55:19.010121   371 net.cpp:139] Memory required for data: 16394800
I0426 20:55:19.010125   371 layer_factory.hpp:77] Creating layer ip2
I0426 20:55:19.010133   371 net.cpp:86] Creating Layer ip2
I0426 20:55:19.010138   371 net.cpp:408] ip2 <- ip1
I0426 20:55:19.010148   371 net.cpp:382] ip2 -> ip2
I0426 20:55:19.010479   371 net.cpp:124] Setting up ip2
I0426 20:55:19.010488   371 net.cpp:131] Top shape: 100 300 (30000)
I0426 20:55:19.010491   371 net.cpp:139] Memory required for data: 16514800
I0426 20:55:19.010499   371 layer_factory.hpp:77] Creating layer relu2
I0426 20:55:19.010504   371 net.cpp:86] Creating Layer relu2
I0426 20:55:19.010509   371 net.cpp:408] relu2 <- ip2
I0426 20:55:19.010514   371 net.cpp:369] relu2 -> ip2 (in-place)
I0426 20:55:19.010704   371 net.cpp:124] Setting up relu2
I0426 20:55:19.010715   371 net.cpp:131] Top shape: 100 300 (30000)
I0426 20:55:19.010718   371 net.cpp:139] Memory required for data: 16634800
I0426 20:55:19.010723   371 layer_factory.hpp:77] Creating layer ip3
I0426 20:55:19.010730   371 net.cpp:86] Creating Layer ip3
I0426 20:55:19.010733   371 net.cpp:408] ip3 <- ip2
I0426 20:55:19.010740   371 net.cpp:382] ip3 -> ip3
I0426 20:55:19.010881   371 net.cpp:124] Setting up ip3
I0426 20:55:19.010890   371 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:55:19.010893   371 net.cpp:139] Memory required for data: 16638800
I0426 20:55:19.010906   371 layer_factory.hpp:77] Creating layer relu3
I0426 20:55:19.010913   371 net.cpp:86] Creating Layer relu3
I0426 20:55:19.010916   371 net.cpp:408] relu3 <- ip3
I0426 20:55:19.010922   371 net.cpp:369] relu3 -> ip3 (in-place)
I0426 20:55:19.011956   371 net.cpp:124] Setting up relu3
I0426 20:55:19.011971   371 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:55:19.011976   371 net.cpp:139] Memory required for data: 16642800
I0426 20:55:19.011978   371 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0426 20:55:19.011986   371 net.cpp:86] Creating Layer ip3_relu3_0_split
I0426 20:55:19.011989   371 net.cpp:408] ip3_relu3_0_split <- ip3
I0426 20:55:19.011996   371 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0426 20:55:19.012003   371 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0426 20:55:19.012048   371 net.cpp:124] Setting up ip3_relu3_0_split
I0426 20:55:19.012060   371 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:55:19.012065   371 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:55:19.012068   371 net.cpp:139] Memory required for data: 16650800
I0426 20:55:19.012073   371 layer_factory.hpp:77] Creating layer accuracy
I0426 20:55:19.012079   371 net.cpp:86] Creating Layer accuracy
I0426 20:55:19.012084   371 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0426 20:55:19.012089   371 net.cpp:408] accuracy <- label_mnist_1_split_0
I0426 20:55:19.012094   371 net.cpp:382] accuracy -> accuracy
I0426 20:55:19.012102   371 net.cpp:124] Setting up accuracy
I0426 20:55:19.012106   371 net.cpp:131] Top shape: (1)
I0426 20:55:19.012109   371 net.cpp:139] Memory required for data: 16650804
I0426 20:55:19.012115   371 layer_factory.hpp:77] Creating layer loss
I0426 20:55:19.012120   371 net.cpp:86] Creating Layer loss
I0426 20:55:19.012123   371 net.cpp:408] loss <- ip3_relu3_0_split_1
I0426 20:55:19.012128   371 net.cpp:408] loss <- label_mnist_1_split_1
I0426 20:55:19.012133   371 net.cpp:382] loss -> loss
I0426 20:55:19.012140   371 layer_factory.hpp:77] Creating layer loss
I0426 20:55:19.012425   371 net.cpp:124] Setting up loss
I0426 20:55:19.012436   371 net.cpp:131] Top shape: (1)
I0426 20:55:19.012439   371 net.cpp:134]     with loss weight 1
I0426 20:55:19.012459   371 net.cpp:139] Memory required for data: 16650808
I0426 20:55:19.012464   371 net.cpp:200] loss needs backward computation.
I0426 20:55:19.012467   371 net.cpp:202] accuracy does not need backward computation.
I0426 20:55:19.012472   371 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0426 20:55:19.012476   371 net.cpp:200] relu3 needs backward computation.
I0426 20:55:19.012480   371 net.cpp:200] ip3 needs backward computation.
I0426 20:55:19.012483   371 net.cpp:200] relu2 needs backward computation.
I0426 20:55:19.012486   371 net.cpp:200] ip2 needs backward computation.
I0426 20:55:19.012490   371 net.cpp:200] relu1 needs backward computation.
I0426 20:55:19.012493   371 net.cpp:200] ip1 needs backward computation.
I0426 20:55:19.012498   371 net.cpp:200] pool1 needs backward computation.
I0426 20:55:19.012514   371 net.cpp:200] conv1 needs backward computation.
I0426 20:55:19.012519   371 net.cpp:200] pool0 needs backward computation.
I0426 20:55:19.012523   371 net.cpp:200] conv0 needs backward computation.
I0426 20:55:19.012527   371 net.cpp:202] label_mnist_1_split does not need backward computation.
I0426 20:55:19.012532   371 net.cpp:202] mnist does not need backward computation.
I0426 20:55:19.012536   371 net.cpp:244] This network produces output accuracy
I0426 20:55:19.012539   371 net.cpp:244] This network produces output loss
I0426 20:55:19.012552   371 net.cpp:257] Network initialization done.
I0426 20:55:19.012600   371 solver.cpp:56] Solver scaffolding done.
I0426 20:55:19.013033   371 caffe.cpp:248] Starting Optimization
I0426 20:55:19.013046   371 solver.cpp:273] Solving LeNet
I0426 20:55:19.013049   371 solver.cpp:274] Learning Rate Policy: inv
I0426 20:55:19.014045   371 solver.cpp:331] Iteration 0, Testing net (#0)
I0426 20:55:19.024528   371 blocking_queue.cpp:49] Waiting for data
I0426 20:55:19.099987   381 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:55:19.101335   371 solver.cpp:398]     Test net output #0: accuracy = 0.0808
I0426 20:55:19.101358   371 solver.cpp:398]     Test net output #1: loss = 2.31812 (* 1 = 2.31812 loss)
I0426 20:55:19.107484   371 solver.cpp:219] Iteration 0 (0 iter/s, 0.0944015s/100 iters), loss = 2.3213
I0426 20:55:19.107513   371 solver.cpp:238]     Train net output #0: loss = 2.3213 (* 1 = 2.3213 loss)
I0426 20:55:19.107527   371 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0426 20:55:19.292342   371 solver.cpp:219] Iteration 100 (541.122 iter/s, 0.184801s/100 iters), loss = 0.215408
I0426 20:55:19.292398   371 solver.cpp:238]     Train net output #0: loss = 0.215408 (* 1 = 0.215408 loss)
I0426 20:55:19.292412   371 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0426 20:55:19.481562   371 solver.cpp:219] Iteration 200 (528.68 iter/s, 0.18915s/100 iters), loss = 0.194231
I0426 20:55:19.481606   371 solver.cpp:238]     Train net output #0: loss = 0.194231 (* 1 = 0.194231 loss)
I0426 20:55:19.481617   371 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0426 20:55:19.653784   371 solver.cpp:219] Iteration 300 (580.838 iter/s, 0.172165s/100 iters), loss = 0.222861
I0426 20:55:19.653820   371 solver.cpp:238]     Train net output #0: loss = 0.222861 (* 1 = 0.222861 loss)
I0426 20:55:19.653827   371 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0426 20:55:19.825189   371 solver.cpp:219] Iteration 400 (583.58 iter/s, 0.171356s/100 iters), loss = 0.0836873
I0426 20:55:19.825222   371 solver.cpp:238]     Train net output #0: loss = 0.0836873 (* 1 = 0.0836873 loss)
I0426 20:55:19.825232   371 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0426 20:55:19.993571   371 solver.cpp:331] Iteration 500, Testing net (#0)
I0426 20:55:20.077548   381 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:55:20.079854   371 solver.cpp:398]     Test net output #0: accuracy = 0.9695
I0426 20:55:20.079880   371 solver.cpp:398]     Test net output #1: loss = 0.0860787 (* 1 = 0.0860787 loss)
I0426 20:55:20.081480   371 solver.cpp:219] Iteration 500 (390.26 iter/s, 0.25624s/100 iters), loss = 0.0807115
I0426 20:55:20.081527   371 solver.cpp:238]     Train net output #0: loss = 0.0807115 (* 1 = 0.0807115 loss)
I0426 20:55:20.081535   371 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0426 20:55:20.255431   371 solver.cpp:219] Iteration 600 (575.074 iter/s, 0.173891s/100 iters), loss = 0.0629276
I0426 20:55:20.255462   371 solver.cpp:238]     Train net output #0: loss = 0.0629276 (* 1 = 0.0629276 loss)
I0426 20:55:20.255470   371 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0426 20:55:20.424883   371 solver.cpp:219] Iteration 700 (590.293 iter/s, 0.169407s/100 iters), loss = 0.117283
I0426 20:55:20.424912   371 solver.cpp:238]     Train net output #0: loss = 0.117282 (* 1 = 0.117282 loss)
I0426 20:55:20.424919   371 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0426 20:55:20.594594   371 solver.cpp:219] Iteration 800 (589.381 iter/s, 0.16967s/100 iters), loss = 0.220532
I0426 20:55:20.594621   371 solver.cpp:238]     Train net output #0: loss = 0.220532 (* 1 = 0.220532 loss)
I0426 20:55:20.594627   371 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0426 20:55:20.764689   371 solver.cpp:219] Iteration 900 (588.05 iter/s, 0.170054s/100 iters), loss = 0.142978
I0426 20:55:20.764724   371 solver.cpp:238]     Train net output #0: loss = 0.142978 (* 1 = 0.142978 loss)
I0426 20:55:20.764732   371 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0426 20:55:20.821790   380 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:55:20.934155   371 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0426 20:55:20.938632   371 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0426 20:55:20.941589   371 solver.cpp:311] Iteration 1000, loss = 0.102437
I0426 20:55:20.941606   371 solver.cpp:331] Iteration 1000, Testing net (#0)
I0426 20:55:21.029508   381 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:55:21.030977   371 solver.cpp:398]     Test net output #0: accuracy = 0.9798
I0426 20:55:21.031014   371 solver.cpp:398]     Test net output #1: loss = 0.0626133 (* 1 = 0.0626133 loss)
I0426 20:55:21.031021   371 solver.cpp:316] Optimization Done.
I0426 20:55:21.031025   371 caffe.cpp:259] Optimization Done.
