I0426 20:48:32.086534 31277 caffe.cpp:218] Using GPUs 0
I0426 20:48:32.124757 31277 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0426 20:48:32.649003 31277 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything/lenet_train_test9.prototxt"
train_state {
  level: 0
  stage: ""
}
I0426 20:48:32.649138 31277 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything/lenet_train_test9.prototxt
I0426 20:48:32.649437 31277 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0426 20:48:32.649451 31277 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0426 20:48:32.649523 31277 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0426 20:48:32.649588 31277 layer_factory.hpp:77] Creating layer mnist
I0426 20:48:32.649686 31277 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0426 20:48:32.649710 31277 net.cpp:86] Creating Layer mnist
I0426 20:48:32.649718 31277 net.cpp:382] mnist -> data
I0426 20:48:32.649740 31277 net.cpp:382] mnist -> label
I0426 20:48:32.650831 31277 data_layer.cpp:45] output data size: 64,1,28,28
I0426 20:48:32.653365 31277 net.cpp:124] Setting up mnist
I0426 20:48:32.653383 31277 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0426 20:48:32.653389 31277 net.cpp:131] Top shape: 64 (64)
I0426 20:48:32.653393 31277 net.cpp:139] Memory required for data: 200960
I0426 20:48:32.653400 31277 layer_factory.hpp:77] Creating layer ip1
I0426 20:48:32.653410 31277 net.cpp:86] Creating Layer ip1
I0426 20:48:32.653415 31277 net.cpp:408] ip1 <- data
I0426 20:48:32.653426 31277 net.cpp:382] ip1 -> ip1
I0426 20:48:32.655109 31277 net.cpp:124] Setting up ip1
I0426 20:48:32.655125 31277 net.cpp:131] Top shape: 64 100 (6400)
I0426 20:48:32.655129 31277 net.cpp:139] Memory required for data: 226560
I0426 20:48:32.655144 31277 layer_factory.hpp:77] Creating layer relu1
I0426 20:48:32.655153 31277 net.cpp:86] Creating Layer relu1
I0426 20:48:32.655158 31277 net.cpp:408] relu1 <- ip1
I0426 20:48:32.655163 31277 net.cpp:369] relu1 -> ip1 (in-place)
I0426 20:48:32.937659 31277 net.cpp:124] Setting up relu1
I0426 20:48:32.937688 31277 net.cpp:131] Top shape: 64 100 (6400)
I0426 20:48:32.937692 31277 net.cpp:139] Memory required for data: 252160
I0426 20:48:32.937697 31277 layer_factory.hpp:77] Creating layer ip2
I0426 20:48:32.937710 31277 net.cpp:86] Creating Layer ip2
I0426 20:48:32.937732 31277 net.cpp:408] ip2 <- ip1
I0426 20:48:32.937741 31277 net.cpp:382] ip2 -> ip2
I0426 20:48:32.938920 31277 net.cpp:124] Setting up ip2
I0426 20:48:32.938933 31277 net.cpp:131] Top shape: 64 300 (19200)
I0426 20:48:32.938936 31277 net.cpp:139] Memory required for data: 328960
I0426 20:48:32.938946 31277 layer_factory.hpp:77] Creating layer relu2
I0426 20:48:32.938953 31277 net.cpp:86] Creating Layer relu2
I0426 20:48:32.938956 31277 net.cpp:408] relu2 <- ip2
I0426 20:48:32.938961 31277 net.cpp:369] relu2 -> ip2 (in-place)
I0426 20:48:32.939757 31277 net.cpp:124] Setting up relu2
I0426 20:48:32.939769 31277 net.cpp:131] Top shape: 64 300 (19200)
I0426 20:48:32.939787 31277 net.cpp:139] Memory required for data: 405760
I0426 20:48:32.939790 31277 layer_factory.hpp:77] Creating layer ip3
I0426 20:48:32.939797 31277 net.cpp:86] Creating Layer ip3
I0426 20:48:32.939801 31277 net.cpp:408] ip3 <- ip2
I0426 20:48:32.939807 31277 net.cpp:382] ip3 -> ip3
I0426 20:48:32.940764 31277 net.cpp:124] Setting up ip3
I0426 20:48:32.940775 31277 net.cpp:131] Top shape: 64 10 (640)
I0426 20:48:32.940778 31277 net.cpp:139] Memory required for data: 408320
I0426 20:48:32.940786 31277 layer_factory.hpp:77] Creating layer relu3
I0426 20:48:32.940791 31277 net.cpp:86] Creating Layer relu3
I0426 20:48:32.940794 31277 net.cpp:408] relu3 <- ip3
I0426 20:48:32.940798 31277 net.cpp:369] relu3 -> ip3 (in-place)
I0426 20:48:32.941006 31277 net.cpp:124] Setting up relu3
I0426 20:48:32.941016 31277 net.cpp:131] Top shape: 64 10 (640)
I0426 20:48:32.941020 31277 net.cpp:139] Memory required for data: 410880
I0426 20:48:32.941022 31277 layer_factory.hpp:77] Creating layer loss
I0426 20:48:32.941028 31277 net.cpp:86] Creating Layer loss
I0426 20:48:32.941031 31277 net.cpp:408] loss <- ip3
I0426 20:48:32.941035 31277 net.cpp:408] loss <- label
I0426 20:48:32.941041 31277 net.cpp:382] loss -> loss
I0426 20:48:32.941057 31277 layer_factory.hpp:77] Creating layer loss
I0426 20:48:32.941316 31277 net.cpp:124] Setting up loss
I0426 20:48:32.941324 31277 net.cpp:131] Top shape: (1)
I0426 20:48:32.941328 31277 net.cpp:134]     with loss weight 1
I0426 20:48:32.941341 31277 net.cpp:139] Memory required for data: 410884
I0426 20:48:32.941344 31277 net.cpp:200] loss needs backward computation.
I0426 20:48:32.941347 31277 net.cpp:200] relu3 needs backward computation.
I0426 20:48:32.941350 31277 net.cpp:200] ip3 needs backward computation.
I0426 20:48:32.941352 31277 net.cpp:200] relu2 needs backward computation.
I0426 20:48:32.941355 31277 net.cpp:200] ip2 needs backward computation.
I0426 20:48:32.941359 31277 net.cpp:200] relu1 needs backward computation.
I0426 20:48:32.941360 31277 net.cpp:200] ip1 needs backward computation.
I0426 20:48:32.941364 31277 net.cpp:202] mnist does not need backward computation.
I0426 20:48:32.941366 31277 net.cpp:244] This network produces output loss
I0426 20:48:32.941390 31277 net.cpp:257] Network initialization done.
I0426 20:48:32.941613 31277 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything/lenet_train_test9.prototxt
I0426 20:48:32.941634 31277 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0426 20:48:32.941701 31277 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 300
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0426 20:48:32.941766 31277 layer_factory.hpp:77] Creating layer mnist
I0426 20:48:32.941809 31277 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0426 20:48:32.941820 31277 net.cpp:86] Creating Layer mnist
I0426 20:48:32.941825 31277 net.cpp:382] mnist -> data
I0426 20:48:32.941833 31277 net.cpp:382] mnist -> label
I0426 20:48:32.941911 31277 data_layer.cpp:45] output data size: 100,1,28,28
I0426 20:48:32.943212 31277 net.cpp:124] Setting up mnist
I0426 20:48:32.943241 31277 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0426 20:48:32.943245 31277 net.cpp:131] Top shape: 100 (100)
I0426 20:48:32.943249 31277 net.cpp:139] Memory required for data: 314000
I0426 20:48:32.943253 31277 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0426 20:48:32.943289 31277 net.cpp:86] Creating Layer label_mnist_1_split
I0426 20:48:32.943292 31277 net.cpp:408] label_mnist_1_split <- label
I0426 20:48:32.943297 31277 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0426 20:48:32.943303 31277 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0426 20:48:32.943362 31277 net.cpp:124] Setting up label_mnist_1_split
I0426 20:48:32.943367 31277 net.cpp:131] Top shape: 100 (100)
I0426 20:48:32.943370 31277 net.cpp:131] Top shape: 100 (100)
I0426 20:48:32.943373 31277 net.cpp:139] Memory required for data: 314800
I0426 20:48:32.943377 31277 layer_factory.hpp:77] Creating layer ip1
I0426 20:48:32.943382 31277 net.cpp:86] Creating Layer ip1
I0426 20:48:32.943385 31277 net.cpp:408] ip1 <- data
I0426 20:48:32.943389 31277 net.cpp:382] ip1 -> ip1
I0426 20:48:32.943984 31277 net.cpp:124] Setting up ip1
I0426 20:48:32.943992 31277 net.cpp:131] Top shape: 100 100 (10000)
I0426 20:48:32.943995 31277 net.cpp:139] Memory required for data: 354800
I0426 20:48:32.944003 31277 layer_factory.hpp:77] Creating layer relu1
I0426 20:48:32.944008 31277 net.cpp:86] Creating Layer relu1
I0426 20:48:32.944011 31277 net.cpp:408] relu1 <- ip1
I0426 20:48:32.944026 31277 net.cpp:369] relu1 -> ip1 (in-place)
I0426 20:48:32.944979 31277 net.cpp:124] Setting up relu1
I0426 20:48:32.944994 31277 net.cpp:131] Top shape: 100 100 (10000)
I0426 20:48:32.944998 31277 net.cpp:139] Memory required for data: 394800
I0426 20:48:32.945003 31277 layer_factory.hpp:77] Creating layer ip2
I0426 20:48:32.945009 31277 net.cpp:86] Creating Layer ip2
I0426 20:48:32.945013 31277 net.cpp:408] ip2 <- ip1
I0426 20:48:32.945019 31277 net.cpp:382] ip2 -> ip2
I0426 20:48:32.945360 31277 net.cpp:124] Setting up ip2
I0426 20:48:32.945369 31277 net.cpp:131] Top shape: 100 300 (30000)
I0426 20:48:32.945371 31277 net.cpp:139] Memory required for data: 514800
I0426 20:48:32.945379 31277 layer_factory.hpp:77] Creating layer relu2
I0426 20:48:32.945382 31277 net.cpp:86] Creating Layer relu2
I0426 20:48:32.945386 31277 net.cpp:408] relu2 <- ip2
I0426 20:48:32.945389 31277 net.cpp:369] relu2 -> ip2 (in-place)
I0426 20:48:32.945565 31277 net.cpp:124] Setting up relu2
I0426 20:48:32.945574 31277 net.cpp:131] Top shape: 100 300 (30000)
I0426 20:48:32.945576 31277 net.cpp:139] Memory required for data: 634800
I0426 20:48:32.945580 31277 layer_factory.hpp:77] Creating layer ip3
I0426 20:48:32.945585 31277 net.cpp:86] Creating Layer ip3
I0426 20:48:32.945588 31277 net.cpp:408] ip3 <- ip2
I0426 20:48:32.945602 31277 net.cpp:382] ip3 -> ip3
I0426 20:48:32.945705 31277 net.cpp:124] Setting up ip3
I0426 20:48:32.945713 31277 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:48:32.945715 31277 net.cpp:139] Memory required for data: 638800
I0426 20:48:32.945722 31277 layer_factory.hpp:77] Creating layer relu3
I0426 20:48:32.945726 31277 net.cpp:86] Creating Layer relu3
I0426 20:48:32.945729 31277 net.cpp:408] relu3 <- ip3
I0426 20:48:32.945734 31277 net.cpp:369] relu3 -> ip3 (in-place)
I0426 20:48:32.945883 31277 net.cpp:124] Setting up relu3
I0426 20:48:32.945891 31277 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:48:32.945894 31277 net.cpp:139] Memory required for data: 642800
I0426 20:48:32.945896 31277 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0426 20:48:32.945902 31277 net.cpp:86] Creating Layer ip3_relu3_0_split
I0426 20:48:32.945904 31277 net.cpp:408] ip3_relu3_0_split <- ip3
I0426 20:48:32.945909 31277 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0426 20:48:32.945914 31277 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0426 20:48:32.945945 31277 net.cpp:124] Setting up ip3_relu3_0_split
I0426 20:48:32.945950 31277 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:48:32.945953 31277 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:48:32.945956 31277 net.cpp:139] Memory required for data: 650800
I0426 20:48:32.945960 31277 layer_factory.hpp:77] Creating layer accuracy
I0426 20:48:32.945963 31277 net.cpp:86] Creating Layer accuracy
I0426 20:48:32.945966 31277 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0426 20:48:32.945977 31277 net.cpp:408] accuracy <- label_mnist_1_split_0
I0426 20:48:32.945981 31277 net.cpp:382] accuracy -> accuracy
I0426 20:48:32.945991 31277 net.cpp:124] Setting up accuracy
I0426 20:48:32.945996 31277 net.cpp:131] Top shape: (1)
I0426 20:48:32.945997 31277 net.cpp:139] Memory required for data: 650804
I0426 20:48:32.946000 31277 layer_factory.hpp:77] Creating layer loss
I0426 20:48:32.946004 31277 net.cpp:86] Creating Layer loss
I0426 20:48:32.946012 31277 net.cpp:408] loss <- ip3_relu3_0_split_1
I0426 20:48:32.946015 31277 net.cpp:408] loss <- label_mnist_1_split_1
I0426 20:48:32.946019 31277 net.cpp:382] loss -> loss
I0426 20:48:32.946025 31277 layer_factory.hpp:77] Creating layer loss
I0426 20:48:32.946250 31277 net.cpp:124] Setting up loss
I0426 20:48:32.946259 31277 net.cpp:131] Top shape: (1)
I0426 20:48:32.946262 31277 net.cpp:134]     with loss weight 1
I0426 20:48:32.946269 31277 net.cpp:139] Memory required for data: 650808
I0426 20:48:32.946271 31277 net.cpp:200] loss needs backward computation.
I0426 20:48:32.946275 31277 net.cpp:202] accuracy does not need backward computation.
I0426 20:48:32.946279 31277 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0426 20:48:32.946281 31277 net.cpp:200] relu3 needs backward computation.
I0426 20:48:32.946285 31277 net.cpp:200] ip3 needs backward computation.
I0426 20:48:32.946287 31277 net.cpp:200] relu2 needs backward computation.
I0426 20:48:32.946301 31277 net.cpp:200] ip2 needs backward computation.
I0426 20:48:32.946305 31277 net.cpp:200] relu1 needs backward computation.
I0426 20:48:32.946307 31277 net.cpp:200] ip1 needs backward computation.
I0426 20:48:32.946326 31277 net.cpp:202] label_mnist_1_split does not need backward computation.
I0426 20:48:32.946331 31277 net.cpp:202] mnist does not need backward computation.
I0426 20:48:32.946332 31277 net.cpp:244] This network produces output accuracy
I0426 20:48:32.946336 31277 net.cpp:244] This network produces output loss
I0426 20:48:32.946351 31277 net.cpp:257] Network initialization done.
I0426 20:48:32.946380 31277 solver.cpp:56] Solver scaffolding done.
I0426 20:48:32.946599 31277 caffe.cpp:248] Starting Optimization
I0426 20:48:32.946605 31277 solver.cpp:273] Solving LeNet
I0426 20:48:32.946607 31277 solver.cpp:274] Learning Rate Policy: inv
I0426 20:48:32.948076 31277 solver.cpp:331] Iteration 0, Testing net (#0)
I0426 20:48:32.949002 31277 blocking_queue.cpp:49] Waiting for data
I0426 20:48:33.026589 31284 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:48:33.027045 31277 solver.cpp:398]     Test net output #0: accuracy = 0.1007
I0426 20:48:33.027065 31277 solver.cpp:398]     Test net output #1: loss = 2.30345 (* 1 = 2.30345 loss)
I0426 20:48:33.027732 31277 solver.cpp:219] Iteration 0 (-1.12922e-30 iter/s, 0.0810578s/100 iters), loss = 2.30092
I0426 20:48:33.027752 31277 solver.cpp:238]     Train net output #0: loss = 2.30092 (* 1 = 2.30092 loss)
I0426 20:48:33.027779 31277 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0426 20:48:33.084051 31277 solver.cpp:219] Iteration 100 (1776.54 iter/s, 0.0562893s/100 iters), loss = 0.802641
I0426 20:48:33.084079 31277 solver.cpp:238]     Train net output #0: loss = 0.802641 (* 1 = 0.802641 loss)
I0426 20:48:33.084085 31277 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0426 20:48:33.130489 31277 solver.cpp:219] Iteration 200 (2155.01 iter/s, 0.0464036s/100 iters), loss = 1.00522
I0426 20:48:33.130527 31277 solver.cpp:238]     Train net output #0: loss = 1.00522 (* 1 = 1.00522 loss)
I0426 20:48:33.130532 31277 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0426 20:48:33.175886 31277 solver.cpp:219] Iteration 300 (2204.18 iter/s, 0.0453683s/100 iters), loss = 0.708871
I0426 20:48:33.175925 31277 solver.cpp:238]     Train net output #0: loss = 0.708871 (* 1 = 0.708871 loss)
I0426 20:48:33.175931 31277 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0426 20:48:33.221868 31277 solver.cpp:219] Iteration 400 (2176.23 iter/s, 0.045951s/100 iters), loss = 0.860503
I0426 20:48:33.221906 31277 solver.cpp:238]     Train net output #0: loss = 0.860503 (* 1 = 0.860503 loss)
I0426 20:48:33.221915 31277 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0426 20:48:33.267896 31277 solver.cpp:331] Iteration 500, Testing net (#0)
I0426 20:48:33.345264 31284 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:48:33.345672 31277 solver.cpp:398]     Test net output #0: accuracy = 0.7915
I0426 20:48:33.345693 31277 solver.cpp:398]     Test net output #1: loss = 0.720541 (* 1 = 0.720541 loss)
I0426 20:48:33.346242 31277 solver.cpp:219] Iteration 500 (804.252 iter/s, 0.124339s/100 iters), loss = 0.802754
I0426 20:48:33.346263 31277 solver.cpp:238]     Train net output #0: loss = 0.802754 (* 1 = 0.802754 loss)
I0426 20:48:33.346285 31277 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0426 20:48:33.403551 31277 solver.cpp:219] Iteration 600 (1745.82 iter/s, 0.0572796s/100 iters), loss = 0.730728
I0426 20:48:33.403574 31277 solver.cpp:238]     Train net output #0: loss = 0.730728 (* 1 = 0.730728 loss)
I0426 20:48:33.403580 31277 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0426 20:48:33.452405 31277 solver.cpp:219] Iteration 700 (2048.2 iter/s, 0.0488233s/100 iters), loss = 0.731137
I0426 20:48:33.452450 31277 solver.cpp:238]     Train net output #0: loss = 0.731137 (* 1 = 0.731137 loss)
I0426 20:48:33.452457 31277 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0426 20:48:33.499951 31277 solver.cpp:219] Iteration 800 (2105.66 iter/s, 0.0474911s/100 iters), loss = 0.850478
I0426 20:48:33.499991 31277 solver.cpp:238]     Train net output #0: loss = 0.850478 (* 1 = 0.850478 loss)
I0426 20:48:33.500000 31277 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0426 20:48:33.546697 31277 solver.cpp:219] Iteration 900 (2140.65 iter/s, 0.0467149s/100 iters), loss = 0.992039
I0426 20:48:33.546736 31277 solver.cpp:238]     Train net output #0: loss = 0.992039 (* 1 = 0.992039 loss)
I0426 20:48:33.546741 31277 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0426 20:48:33.563174 31283 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:48:33.593107 31277 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0426 20:48:33.594789 31277 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0426 20:48:33.596050 31277 solver.cpp:311] Iteration 1000, loss = 0.633885
I0426 20:48:33.596062 31277 solver.cpp:331] Iteration 1000, Testing net (#0)
I0426 20:48:33.671978 31284 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:48:33.672442 31277 solver.cpp:398]     Test net output #0: accuracy = 0.8189
I0426 20:48:33.672463 31277 solver.cpp:398]     Test net output #1: loss = 0.654009 (* 1 = 0.654009 loss)
I0426 20:48:33.672468 31277 solver.cpp:316] Optimization Done.
I0426 20:48:33.672492 31277 caffe.cpp:259] Optimization Done.
