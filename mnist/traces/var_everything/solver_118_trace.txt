I0426 20:52:39.711704 32244 caffe.cpp:218] Using GPUs 0
I0426 20:52:39.741757 32244 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0426 20:52:40.194471 32244 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything/lenet_train_test118.prototxt"
train_state {
  level: 0
  stage: ""
}
I0426 20:52:40.194633 32244 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything/lenet_train_test118.prototxt
I0426 20:52:40.194978 32244 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0426 20:52:40.194993 32244 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0426 20:52:40.195077 32244 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0426 20:52:40.195142 32244 layer_factory.hpp:77] Creating layer mnist
I0426 20:52:40.195225 32244 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0426 20:52:40.195245 32244 net.cpp:86] Creating Layer mnist
I0426 20:52:40.195252 32244 net.cpp:382] mnist -> data
I0426 20:52:40.195286 32244 net.cpp:382] mnist -> label
I0426 20:52:40.196211 32244 data_layer.cpp:45] output data size: 64,1,28,28
I0426 20:52:40.198356 32244 net.cpp:124] Setting up mnist
I0426 20:52:40.198370 32244 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0426 20:52:40.198390 32244 net.cpp:131] Top shape: 64 (64)
I0426 20:52:40.198393 32244 net.cpp:139] Memory required for data: 200960
I0426 20:52:40.198398 32244 layer_factory.hpp:77] Creating layer conv0
I0426 20:52:40.198456 32244 net.cpp:86] Creating Layer conv0
I0426 20:52:40.198473 32244 net.cpp:408] conv0 <- data
I0426 20:52:40.198483 32244 net.cpp:382] conv0 -> conv0
I0426 20:52:40.429461 32244 net.cpp:124] Setting up conv0
I0426 20:52:40.429503 32244 net.cpp:131] Top shape: 64 25 24 24 (921600)
I0426 20:52:40.429507 32244 net.cpp:139] Memory required for data: 3887360
I0426 20:52:40.429522 32244 layer_factory.hpp:77] Creating layer pool0
I0426 20:52:40.429534 32244 net.cpp:86] Creating Layer pool0
I0426 20:52:40.429538 32244 net.cpp:408] pool0 <- conv0
I0426 20:52:40.429544 32244 net.cpp:382] pool0 -> pool0
I0426 20:52:40.429603 32244 net.cpp:124] Setting up pool0
I0426 20:52:40.429608 32244 net.cpp:131] Top shape: 64 25 12 12 (230400)
I0426 20:52:40.429611 32244 net.cpp:139] Memory required for data: 4808960
I0426 20:52:40.429615 32244 layer_factory.hpp:77] Creating layer conv1
I0426 20:52:40.429625 32244 net.cpp:86] Creating Layer conv1
I0426 20:52:40.429628 32244 net.cpp:408] conv1 <- pool0
I0426 20:52:40.429633 32244 net.cpp:382] conv1 -> conv1
I0426 20:52:40.432294 32244 net.cpp:124] Setting up conv1
I0426 20:52:40.432307 32244 net.cpp:131] Top shape: 64 25 8 8 (102400)
I0426 20:52:40.432310 32244 net.cpp:139] Memory required for data: 5218560
I0426 20:52:40.432318 32244 layer_factory.hpp:77] Creating layer pool1
I0426 20:52:40.432325 32244 net.cpp:86] Creating Layer pool1
I0426 20:52:40.432328 32244 net.cpp:408] pool1 <- conv1
I0426 20:52:40.432332 32244 net.cpp:382] pool1 -> pool1
I0426 20:52:40.432381 32244 net.cpp:124] Setting up pool1
I0426 20:52:40.432386 32244 net.cpp:131] Top shape: 64 25 4 4 (25600)
I0426 20:52:40.432389 32244 net.cpp:139] Memory required for data: 5320960
I0426 20:52:40.432391 32244 layer_factory.hpp:77] Creating layer ip1
I0426 20:52:40.432399 32244 net.cpp:86] Creating Layer ip1
I0426 20:52:40.432401 32244 net.cpp:408] ip1 <- pool1
I0426 20:52:40.432405 32244 net.cpp:382] ip1 -> ip1
I0426 20:52:40.432780 32244 net.cpp:124] Setting up ip1
I0426 20:52:40.432786 32244 net.cpp:131] Top shape: 64 100 (6400)
I0426 20:52:40.432806 32244 net.cpp:139] Memory required for data: 5346560
I0426 20:52:40.432834 32244 layer_factory.hpp:77] Creating layer relu1
I0426 20:52:40.432855 32244 net.cpp:86] Creating Layer relu1
I0426 20:52:40.432858 32244 net.cpp:408] relu1 <- ip1
I0426 20:52:40.432862 32244 net.cpp:369] relu1 -> ip1 (in-place)
I0426 20:52:40.433075 32244 net.cpp:124] Setting up relu1
I0426 20:52:40.433086 32244 net.cpp:131] Top shape: 64 100 (6400)
I0426 20:52:40.433089 32244 net.cpp:139] Memory required for data: 5372160
I0426 20:52:40.433094 32244 layer_factory.hpp:77] Creating layer ip2
I0426 20:52:40.433099 32244 net.cpp:86] Creating Layer ip2
I0426 20:52:40.433102 32244 net.cpp:408] ip2 <- ip1
I0426 20:52:40.433110 32244 net.cpp:382] ip2 -> ip2
I0426 20:52:40.433293 32244 net.cpp:124] Setting up ip2
I0426 20:52:40.433301 32244 net.cpp:131] Top shape: 64 100 (6400)
I0426 20:52:40.433305 32244 net.cpp:139] Memory required for data: 5397760
I0426 20:52:40.433310 32244 layer_factory.hpp:77] Creating layer relu2
I0426 20:52:40.433317 32244 net.cpp:86] Creating Layer relu2
I0426 20:52:40.433320 32244 net.cpp:408] relu2 <- ip2
I0426 20:52:40.433326 32244 net.cpp:369] relu2 -> ip2 (in-place)
I0426 20:52:40.434103 32244 net.cpp:124] Setting up relu2
I0426 20:52:40.434113 32244 net.cpp:131] Top shape: 64 100 (6400)
I0426 20:52:40.434132 32244 net.cpp:139] Memory required for data: 5423360
I0426 20:52:40.434135 32244 layer_factory.hpp:77] Creating layer ip3
I0426 20:52:40.434141 32244 net.cpp:86] Creating Layer ip3
I0426 20:52:40.434144 32244 net.cpp:408] ip3 <- ip2
I0426 20:52:40.434150 32244 net.cpp:382] ip3 -> ip3
I0426 20:52:40.434298 32244 net.cpp:124] Setting up ip3
I0426 20:52:40.434306 32244 net.cpp:131] Top shape: 64 10 (640)
I0426 20:52:40.434309 32244 net.cpp:139] Memory required for data: 5425920
I0426 20:52:40.434317 32244 layer_factory.hpp:77] Creating layer relu3
I0426 20:52:40.434324 32244 net.cpp:86] Creating Layer relu3
I0426 20:52:40.434326 32244 net.cpp:408] relu3 <- ip3
I0426 20:52:40.434330 32244 net.cpp:369] relu3 -> ip3 (in-place)
I0426 20:52:40.434497 32244 net.cpp:124] Setting up relu3
I0426 20:52:40.434506 32244 net.cpp:131] Top shape: 64 10 (640)
I0426 20:52:40.434510 32244 net.cpp:139] Memory required for data: 5428480
I0426 20:52:40.434514 32244 layer_factory.hpp:77] Creating layer loss
I0426 20:52:40.434520 32244 net.cpp:86] Creating Layer loss
I0426 20:52:40.434523 32244 net.cpp:408] loss <- ip3
I0426 20:52:40.434527 32244 net.cpp:408] loss <- label
I0426 20:52:40.434532 32244 net.cpp:382] loss -> loss
I0426 20:52:40.434545 32244 layer_factory.hpp:77] Creating layer loss
I0426 20:52:40.434774 32244 net.cpp:124] Setting up loss
I0426 20:52:40.434783 32244 net.cpp:131] Top shape: (1)
I0426 20:52:40.434787 32244 net.cpp:134]     with loss weight 1
I0426 20:52:40.434801 32244 net.cpp:139] Memory required for data: 5428484
I0426 20:52:40.434804 32244 net.cpp:200] loss needs backward computation.
I0426 20:52:40.434808 32244 net.cpp:200] relu3 needs backward computation.
I0426 20:52:40.434810 32244 net.cpp:200] ip3 needs backward computation.
I0426 20:52:40.434813 32244 net.cpp:200] relu2 needs backward computation.
I0426 20:52:40.434816 32244 net.cpp:200] ip2 needs backward computation.
I0426 20:52:40.434819 32244 net.cpp:200] relu1 needs backward computation.
I0426 20:52:40.434823 32244 net.cpp:200] ip1 needs backward computation.
I0426 20:52:40.434825 32244 net.cpp:200] pool1 needs backward computation.
I0426 20:52:40.434828 32244 net.cpp:200] conv1 needs backward computation.
I0426 20:52:40.434830 32244 net.cpp:200] pool0 needs backward computation.
I0426 20:52:40.434834 32244 net.cpp:200] conv0 needs backward computation.
I0426 20:52:40.434837 32244 net.cpp:202] mnist does not need backward computation.
I0426 20:52:40.434841 32244 net.cpp:244] This network produces output loss
I0426 20:52:40.434850 32244 net.cpp:257] Network initialization done.
I0426 20:52:40.435155 32244 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything/lenet_train_test118.prototxt
I0426 20:52:40.435181 32244 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0426 20:52:40.435272 32244 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0426 20:52:40.435350 32244 layer_factory.hpp:77] Creating layer mnist
I0426 20:52:40.435392 32244 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0426 20:52:40.435405 32244 net.cpp:86] Creating Layer mnist
I0426 20:52:40.435410 32244 net.cpp:382] mnist -> data
I0426 20:52:40.435418 32244 net.cpp:382] mnist -> label
I0426 20:52:40.435503 32244 data_layer.cpp:45] output data size: 100,1,28,28
I0426 20:52:40.437693 32244 net.cpp:124] Setting up mnist
I0426 20:52:40.437705 32244 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0426 20:52:40.437726 32244 net.cpp:131] Top shape: 100 (100)
I0426 20:52:40.437729 32244 net.cpp:139] Memory required for data: 314000
I0426 20:52:40.437733 32244 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0426 20:52:40.437739 32244 net.cpp:86] Creating Layer label_mnist_1_split
I0426 20:52:40.437742 32244 net.cpp:408] label_mnist_1_split <- label
I0426 20:52:40.437783 32244 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0426 20:52:40.437789 32244 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0426 20:52:40.437832 32244 net.cpp:124] Setting up label_mnist_1_split
I0426 20:52:40.437839 32244 net.cpp:131] Top shape: 100 (100)
I0426 20:52:40.437841 32244 net.cpp:131] Top shape: 100 (100)
I0426 20:52:40.437844 32244 net.cpp:139] Memory required for data: 314800
I0426 20:52:40.437849 32244 layer_factory.hpp:77] Creating layer conv0
I0426 20:52:40.437873 32244 net.cpp:86] Creating Layer conv0
I0426 20:52:40.437876 32244 net.cpp:408] conv0 <- data
I0426 20:52:40.437882 32244 net.cpp:382] conv0 -> conv0
I0426 20:52:40.439591 32244 net.cpp:124] Setting up conv0
I0426 20:52:40.439606 32244 net.cpp:131] Top shape: 100 25 24 24 (1440000)
I0426 20:52:40.439626 32244 net.cpp:139] Memory required for data: 6074800
I0426 20:52:40.439635 32244 layer_factory.hpp:77] Creating layer pool0
I0426 20:52:40.439640 32244 net.cpp:86] Creating Layer pool0
I0426 20:52:40.439644 32244 net.cpp:408] pool0 <- conv0
I0426 20:52:40.439652 32244 net.cpp:382] pool0 -> pool0
I0426 20:52:40.439704 32244 net.cpp:124] Setting up pool0
I0426 20:52:40.439709 32244 net.cpp:131] Top shape: 100 25 12 12 (360000)
I0426 20:52:40.439713 32244 net.cpp:139] Memory required for data: 7514800
I0426 20:52:40.439714 32244 layer_factory.hpp:77] Creating layer conv1
I0426 20:52:40.439724 32244 net.cpp:86] Creating Layer conv1
I0426 20:52:40.439728 32244 net.cpp:408] conv1 <- pool0
I0426 20:52:40.439733 32244 net.cpp:382] conv1 -> conv1
I0426 20:52:40.442008 32244 net.cpp:124] Setting up conv1
I0426 20:52:40.442036 32244 net.cpp:131] Top shape: 100 25 8 8 (160000)
I0426 20:52:40.442039 32244 net.cpp:139] Memory required for data: 8154800
I0426 20:52:40.442047 32244 layer_factory.hpp:77] Creating layer pool1
I0426 20:52:40.442054 32244 net.cpp:86] Creating Layer pool1
I0426 20:52:40.442057 32244 net.cpp:408] pool1 <- conv1
I0426 20:52:40.442064 32244 net.cpp:382] pool1 -> pool1
I0426 20:52:40.442137 32244 net.cpp:124] Setting up pool1
I0426 20:52:40.442142 32244 net.cpp:131] Top shape: 100 25 4 4 (40000)
I0426 20:52:40.442145 32244 net.cpp:139] Memory required for data: 8314800
I0426 20:52:40.442148 32244 layer_factory.hpp:77] Creating layer ip1
I0426 20:52:40.442154 32244 net.cpp:86] Creating Layer ip1
I0426 20:52:40.442157 32244 net.cpp:408] ip1 <- pool1
I0426 20:52:40.442178 32244 net.cpp:382] ip1 -> ip1
I0426 20:52:40.442602 32244 net.cpp:124] Setting up ip1
I0426 20:52:40.442610 32244 net.cpp:131] Top shape: 100 100 (10000)
I0426 20:52:40.442638 32244 net.cpp:139] Memory required for data: 8354800
I0426 20:52:40.442646 32244 layer_factory.hpp:77] Creating layer relu1
I0426 20:52:40.442651 32244 net.cpp:86] Creating Layer relu1
I0426 20:52:40.442654 32244 net.cpp:408] relu1 <- ip1
I0426 20:52:40.442661 32244 net.cpp:369] relu1 -> ip1 (in-place)
I0426 20:52:40.442843 32244 net.cpp:124] Setting up relu1
I0426 20:52:40.442853 32244 net.cpp:131] Top shape: 100 100 (10000)
I0426 20:52:40.442857 32244 net.cpp:139] Memory required for data: 8394800
I0426 20:52:40.442860 32244 layer_factory.hpp:77] Creating layer ip2
I0426 20:52:40.442867 32244 net.cpp:86] Creating Layer ip2
I0426 20:52:40.442872 32244 net.cpp:408] ip2 <- ip1
I0426 20:52:40.442875 32244 net.cpp:382] ip2 -> ip2
I0426 20:52:40.443032 32244 net.cpp:124] Setting up ip2
I0426 20:52:40.443040 32244 net.cpp:131] Top shape: 100 100 (10000)
I0426 20:52:40.443043 32244 net.cpp:139] Memory required for data: 8434800
I0426 20:52:40.443048 32244 layer_factory.hpp:77] Creating layer relu2
I0426 20:52:40.443053 32244 net.cpp:86] Creating Layer relu2
I0426 20:52:40.443059 32244 net.cpp:408] relu2 <- ip2
I0426 20:52:40.443063 32244 net.cpp:369] relu2 -> ip2 (in-place)
I0426 20:52:40.443281 32244 net.cpp:124] Setting up relu2
I0426 20:52:40.443290 32244 net.cpp:131] Top shape: 100 100 (10000)
I0426 20:52:40.443294 32244 net.cpp:139] Memory required for data: 8474800
I0426 20:52:40.443297 32244 layer_factory.hpp:77] Creating layer ip3
I0426 20:52:40.443303 32244 net.cpp:86] Creating Layer ip3
I0426 20:52:40.443306 32244 net.cpp:408] ip3 <- ip2
I0426 20:52:40.443311 32244 net.cpp:382] ip3 -> ip3
I0426 20:52:40.443433 32244 net.cpp:124] Setting up ip3
I0426 20:52:40.443440 32244 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:52:40.443444 32244 net.cpp:139] Memory required for data: 8478800
I0426 20:52:40.443450 32244 layer_factory.hpp:77] Creating layer relu3
I0426 20:52:40.443456 32244 net.cpp:86] Creating Layer relu3
I0426 20:52:40.443459 32244 net.cpp:408] relu3 <- ip3
I0426 20:52:40.443464 32244 net.cpp:369] relu3 -> ip3 (in-place)
I0426 20:52:40.444365 32244 net.cpp:124] Setting up relu3
I0426 20:52:40.444378 32244 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:52:40.444381 32244 net.cpp:139] Memory required for data: 8482800
I0426 20:52:40.444386 32244 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0426 20:52:40.444391 32244 net.cpp:86] Creating Layer ip3_relu3_0_split
I0426 20:52:40.444393 32244 net.cpp:408] ip3_relu3_0_split <- ip3
I0426 20:52:40.444398 32244 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0426 20:52:40.444419 32244 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0426 20:52:40.444458 32244 net.cpp:124] Setting up ip3_relu3_0_split
I0426 20:52:40.444463 32244 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:52:40.444468 32244 net.cpp:131] Top shape: 100 10 (1000)
I0426 20:52:40.444469 32244 net.cpp:139] Memory required for data: 8490800
I0426 20:52:40.444473 32244 layer_factory.hpp:77] Creating layer accuracy
I0426 20:52:40.444478 32244 net.cpp:86] Creating Layer accuracy
I0426 20:52:40.444480 32244 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0426 20:52:40.444484 32244 net.cpp:408] accuracy <- label_mnist_1_split_0
I0426 20:52:40.444489 32244 net.cpp:382] accuracy -> accuracy
I0426 20:52:40.444497 32244 net.cpp:124] Setting up accuracy
I0426 20:52:40.444501 32244 net.cpp:131] Top shape: (1)
I0426 20:52:40.444504 32244 net.cpp:139] Memory required for data: 8490804
I0426 20:52:40.444507 32244 layer_factory.hpp:77] Creating layer loss
I0426 20:52:40.444514 32244 net.cpp:86] Creating Layer loss
I0426 20:52:40.444516 32244 net.cpp:408] loss <- ip3_relu3_0_split_1
I0426 20:52:40.444520 32244 net.cpp:408] loss <- label_mnist_1_split_1
I0426 20:52:40.444525 32244 net.cpp:382] loss -> loss
I0426 20:52:40.444530 32244 layer_factory.hpp:77] Creating layer loss
I0426 20:52:40.444766 32244 net.cpp:124] Setting up loss
I0426 20:52:40.444775 32244 net.cpp:131] Top shape: (1)
I0426 20:52:40.444778 32244 net.cpp:134]     with loss weight 1
I0426 20:52:40.444794 32244 net.cpp:139] Memory required for data: 8490808
I0426 20:52:40.444799 32244 net.cpp:200] loss needs backward computation.
I0426 20:52:40.444803 32244 net.cpp:202] accuracy does not need backward computation.
I0426 20:52:40.444806 32244 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0426 20:52:40.444833 32244 net.cpp:200] relu3 needs backward computation.
I0426 20:52:40.444836 32244 net.cpp:200] ip3 needs backward computation.
I0426 20:52:40.444839 32244 net.cpp:200] relu2 needs backward computation.
I0426 20:52:40.444842 32244 net.cpp:200] ip2 needs backward computation.
I0426 20:52:40.444845 32244 net.cpp:200] relu1 needs backward computation.
I0426 20:52:40.444854 32244 net.cpp:200] ip1 needs backward computation.
I0426 20:52:40.444857 32244 net.cpp:200] pool1 needs backward computation.
I0426 20:52:40.444860 32244 net.cpp:200] conv1 needs backward computation.
I0426 20:52:40.444864 32244 net.cpp:200] pool0 needs backward computation.
I0426 20:52:40.444866 32244 net.cpp:200] conv0 needs backward computation.
I0426 20:52:40.444870 32244 net.cpp:202] label_mnist_1_split does not need backward computation.
I0426 20:52:40.444875 32244 net.cpp:202] mnist does not need backward computation.
I0426 20:52:40.444877 32244 net.cpp:244] This network produces output accuracy
I0426 20:52:40.444896 32244 net.cpp:244] This network produces output loss
I0426 20:52:40.444910 32244 net.cpp:257] Network initialization done.
I0426 20:52:40.444953 32244 solver.cpp:56] Solver scaffolding done.
I0426 20:52:40.445343 32244 caffe.cpp:248] Starting Optimization
I0426 20:52:40.445348 32244 solver.cpp:273] Solving LeNet
I0426 20:52:40.445353 32244 solver.cpp:274] Learning Rate Policy: inv
I0426 20:52:40.446208 32244 solver.cpp:331] Iteration 0, Testing net (#0)
I0426 20:52:40.452147 32244 blocking_queue.cpp:49] Waiting for data
I0426 20:52:40.519887 32251 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:52:40.520578 32244 solver.cpp:398]     Test net output #0: accuracy = 0.1206
I0426 20:52:40.520596 32244 solver.cpp:398]     Test net output #1: loss = 2.29914 (* 1 = 2.29914 loss)
I0426 20:52:40.524355 32244 solver.cpp:219] Iteration 0 (-3.19436e-31 iter/s, 0.0789629s/100 iters), loss = 2.3107
I0426 20:52:40.524394 32244 solver.cpp:238]     Train net output #0: loss = 2.3107 (* 1 = 2.3107 loss)
I0426 20:52:40.524416 32244 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0426 20:52:40.633509 32244 solver.cpp:219] Iteration 100 (916.444 iter/s, 0.109117s/100 iters), loss = 1.25588
I0426 20:52:40.633548 32244 solver.cpp:238]     Train net output #0: loss = 1.25588 (* 1 = 1.25588 loss)
I0426 20:52:40.633554 32244 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0426 20:52:40.742210 32244 solver.cpp:219] Iteration 200 (920.374 iter/s, 0.108652s/100 iters), loss = 0.698017
I0426 20:52:40.742249 32244 solver.cpp:238]     Train net output #0: loss = 0.698017 (* 1 = 0.698017 loss)
I0426 20:52:40.742255 32244 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0426 20:52:40.851238 32244 solver.cpp:219] Iteration 300 (917.494 iter/s, 0.108993s/100 iters), loss = 0.558695
I0426 20:52:40.851261 32244 solver.cpp:238]     Train net output #0: loss = 0.558695 (* 1 = 0.558695 loss)
I0426 20:52:40.851267 32244 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0426 20:52:40.959870 32244 solver.cpp:219] Iteration 400 (920.816 iter/s, 0.108599s/100 iters), loss = 0.18652
I0426 20:52:40.959909 32244 solver.cpp:238]     Train net output #0: loss = 0.18652 (* 1 = 0.18652 loss)
I0426 20:52:40.959915 32244 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0426 20:52:41.074596 32244 solver.cpp:331] Iteration 500, Testing net (#0)
I0426 20:52:41.137487 32251 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:52:41.139564 32244 solver.cpp:398]     Test net output #0: accuracy = 0.8724
I0426 20:52:41.139590 32244 solver.cpp:398]     Test net output #1: loss = 0.327805 (* 1 = 0.327805 loss)
I0426 20:52:41.140624 32244 solver.cpp:219] Iteration 500 (553.362 iter/s, 0.180713s/100 iters), loss = 0.224683
I0426 20:52:41.140671 32244 solver.cpp:238]     Train net output #0: loss = 0.224683 (* 1 = 0.224683 loss)
I0426 20:52:41.140679 32244 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0426 20:52:41.251091 32244 solver.cpp:219] Iteration 600 (905.708 iter/s, 0.110411s/100 iters), loss = 0.320932
I0426 20:52:41.251124 32244 solver.cpp:238]     Train net output #0: loss = 0.320932 (* 1 = 0.320932 loss)
I0426 20:52:41.251133 32244 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0426 20:52:41.361259 32244 solver.cpp:219] Iteration 700 (908.067 iter/s, 0.110124s/100 iters), loss = 0.41093
I0426 20:52:41.361291 32244 solver.cpp:238]     Train net output #0: loss = 0.41093 (* 1 = 0.41093 loss)
I0426 20:52:41.361299 32244 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0426 20:52:41.470172 32244 solver.cpp:219] Iteration 800 (918.521 iter/s, 0.108871s/100 iters), loss = 0.495899
I0426 20:52:41.470211 32244 solver.cpp:238]     Train net output #0: loss = 0.495899 (* 1 = 0.495899 loss)
I0426 20:52:41.470219 32244 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0426 20:52:41.578663 32244 solver.cpp:219] Iteration 900 (922.061 iter/s, 0.108453s/100 iters), loss = 0.436636
I0426 20:52:41.578703 32244 solver.cpp:238]     Train net output #0: loss = 0.436636 (* 1 = 0.436636 loss)
I0426 20:52:41.578711 32244 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0426 20:52:41.615203 32250 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:52:41.687362 32244 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0426 20:52:41.689431 32244 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0426 20:52:41.690609 32244 solver.cpp:311] Iteration 1000, loss = 0.394865
I0426 20:52:41.690637 32244 solver.cpp:331] Iteration 1000, Testing net (#0)
I0426 20:52:41.752764 32251 data_layer.cpp:73] Restarting data prefetching from start.
I0426 20:52:41.754973 32244 solver.cpp:398]     Test net output #0: accuracy = 0.881
I0426 20:52:41.754994 32244 solver.cpp:398]     Test net output #1: loss = 0.29447 (* 1 = 0.29447 loss)
I0426 20:52:41.755009 32244 solver.cpp:316] Optimization Done.
I0426 20:52:41.755012 32244 caffe.cpp:259] Optimization Done.
