I0428 20:15:44.936638   512 caffe.cpp:218] Using GPUs 0
I0428 20:15:44.972645   512 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:15:45.483111   512 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1254.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:15:45.483258   512 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1254.prototxt
I0428 20:15:45.483638   512 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:15:45.483656   512 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:15:45.483767   512 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0428 20:15:45.483839   512 layer_factory.hpp:77] Creating layer mnist
I0428 20:15:45.483935   512 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:15:45.483960   512 net.cpp:86] Creating Layer mnist
I0428 20:15:45.483970   512 net.cpp:382] mnist -> data
I0428 20:15:45.483994   512 net.cpp:382] mnist -> label
I0428 20:15:45.485129   512 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:15:45.487645   512 net.cpp:124] Setting up mnist
I0428 20:15:45.487670   512 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:15:45.487681   512 net.cpp:131] Top shape: 64 (64)
I0428 20:15:45.487687   512 net.cpp:139] Memory required for data: 200960
I0428 20:15:45.487700   512 layer_factory.hpp:77] Creating layer conv0
I0428 20:15:45.487718   512 net.cpp:86] Creating Layer conv0
I0428 20:15:45.487725   512 net.cpp:408] conv0 <- data
I0428 20:15:45.487737   512 net.cpp:382] conv0 -> conv0
I0428 20:15:45.768021   512 net.cpp:124] Setting up conv0
I0428 20:15:45.768049   512 net.cpp:131] Top shape: 64 50 24 24 (1843200)
I0428 20:15:45.768052   512 net.cpp:139] Memory required for data: 7573760
I0428 20:15:45.768086   512 layer_factory.hpp:77] Creating layer pool0
I0428 20:15:45.768100   512 net.cpp:86] Creating Layer pool0
I0428 20:15:45.768103   512 net.cpp:408] pool0 <- conv0
I0428 20:15:45.768108   512 net.cpp:382] pool0 -> pool0
I0428 20:15:45.768155   512 net.cpp:124] Setting up pool0
I0428 20:15:45.768162   512 net.cpp:131] Top shape: 64 50 12 12 (460800)
I0428 20:15:45.768163   512 net.cpp:139] Memory required for data: 9416960
I0428 20:15:45.768167   512 layer_factory.hpp:77] Creating layer conv1
I0428 20:15:45.768177   512 net.cpp:86] Creating Layer conv1
I0428 20:15:45.768180   512 net.cpp:408] conv1 <- pool0
I0428 20:15:45.768185   512 net.cpp:382] conv1 -> conv1
I0428 20:15:45.771040   512 net.cpp:124] Setting up conv1
I0428 20:15:45.771055   512 net.cpp:131] Top shape: 64 5 8 8 (20480)
I0428 20:15:45.771059   512 net.cpp:139] Memory required for data: 9498880
I0428 20:15:45.771067   512 layer_factory.hpp:77] Creating layer pool1
I0428 20:15:45.771075   512 net.cpp:86] Creating Layer pool1
I0428 20:15:45.771078   512 net.cpp:408] pool1 <- conv1
I0428 20:15:45.771085   512 net.cpp:382] pool1 -> pool1
I0428 20:15:45.771121   512 net.cpp:124] Setting up pool1
I0428 20:15:45.771124   512 net.cpp:131] Top shape: 64 5 4 4 (5120)
I0428 20:15:45.771127   512 net.cpp:139] Memory required for data: 9519360
I0428 20:15:45.771131   512 layer_factory.hpp:77] Creating layer ip1
I0428 20:15:45.771137   512 net.cpp:86] Creating Layer ip1
I0428 20:15:45.771140   512 net.cpp:408] ip1 <- pool1
I0428 20:15:45.771144   512 net.cpp:382] ip1 -> ip1
I0428 20:15:45.771248   512 net.cpp:124] Setting up ip1
I0428 20:15:45.771256   512 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:15:45.771260   512 net.cpp:139] Memory required for data: 9525760
I0428 20:15:45.771266   512 layer_factory.hpp:77] Creating layer relu1
I0428 20:15:45.771271   512 net.cpp:86] Creating Layer relu1
I0428 20:15:45.771275   512 net.cpp:408] relu1 <- ip1
I0428 20:15:45.771278   512 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:15:45.771435   512 net.cpp:124] Setting up relu1
I0428 20:15:45.771445   512 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:15:45.771447   512 net.cpp:139] Memory required for data: 9532160
I0428 20:15:45.771450   512 layer_factory.hpp:77] Creating layer ip2
I0428 20:15:45.771456   512 net.cpp:86] Creating Layer ip2
I0428 20:15:45.771458   512 net.cpp:408] ip2 <- ip1
I0428 20:15:45.771463   512 net.cpp:382] ip2 -> ip2
I0428 20:15:45.771551   512 net.cpp:124] Setting up ip2
I0428 20:15:45.771559   512 net.cpp:131] Top shape: 64 10 (640)
I0428 20:15:45.771561   512 net.cpp:139] Memory required for data: 9534720
I0428 20:15:45.771566   512 layer_factory.hpp:77] Creating layer relu2
I0428 20:15:45.771571   512 net.cpp:86] Creating Layer relu2
I0428 20:15:45.771574   512 net.cpp:408] relu2 <- ip2
I0428 20:15:45.771579   512 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:15:45.772256   512 net.cpp:124] Setting up relu2
I0428 20:15:45.772269   512 net.cpp:131] Top shape: 64 10 (640)
I0428 20:15:45.772272   512 net.cpp:139] Memory required for data: 9537280
I0428 20:15:45.772275   512 layer_factory.hpp:77] Creating layer loss
I0428 20:15:45.772281   512 net.cpp:86] Creating Layer loss
I0428 20:15:45.772284   512 net.cpp:408] loss <- ip2
I0428 20:15:45.772289   512 net.cpp:408] loss <- label
I0428 20:15:45.772294   512 net.cpp:382] loss -> loss
I0428 20:15:45.772310   512 layer_factory.hpp:77] Creating layer loss
I0428 20:15:45.772521   512 net.cpp:124] Setting up loss
I0428 20:15:45.772531   512 net.cpp:131] Top shape: (1)
I0428 20:15:45.772533   512 net.cpp:134]     with loss weight 1
I0428 20:15:45.772547   512 net.cpp:139] Memory required for data: 9537284
I0428 20:15:45.772550   512 net.cpp:200] loss needs backward computation.
I0428 20:15:45.772554   512 net.cpp:200] relu2 needs backward computation.
I0428 20:15:45.772557   512 net.cpp:200] ip2 needs backward computation.
I0428 20:15:45.772559   512 net.cpp:200] relu1 needs backward computation.
I0428 20:15:45.772562   512 net.cpp:200] ip1 needs backward computation.
I0428 20:15:45.772578   512 net.cpp:200] pool1 needs backward computation.
I0428 20:15:45.772581   512 net.cpp:200] conv1 needs backward computation.
I0428 20:15:45.772584   512 net.cpp:200] pool0 needs backward computation.
I0428 20:15:45.772586   512 net.cpp:200] conv0 needs backward computation.
I0428 20:15:45.772590   512 net.cpp:202] mnist does not need backward computation.
I0428 20:15:45.772593   512 net.cpp:244] This network produces output loss
I0428 20:15:45.772600   512 net.cpp:257] Network initialization done.
I0428 20:15:45.772933   512 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1254.prototxt
I0428 20:15:45.772963   512 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:15:45.773063   512 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0428 20:15:45.773126   512 layer_factory.hpp:77] Creating layer mnist
I0428 20:15:45.773183   512 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:15:45.773211   512 net.cpp:86] Creating Layer mnist
I0428 20:15:45.773214   512 net.cpp:382] mnist -> data
I0428 20:15:45.773221   512 net.cpp:382] mnist -> label
I0428 20:15:45.773299   512 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:15:45.775166   512 net.cpp:124] Setting up mnist
I0428 20:15:45.775193   512 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:15:45.775198   512 net.cpp:131] Top shape: 100 (100)
I0428 20:15:45.775216   512 net.cpp:139] Memory required for data: 314000
I0428 20:15:45.775220   512 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:15:45.775228   512 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:15:45.775230   512 net.cpp:408] label_mnist_1_split <- label
I0428 20:15:45.775235   512 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:15:45.775241   512 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:15:45.775327   512 net.cpp:124] Setting up label_mnist_1_split
I0428 20:15:45.775346   512 net.cpp:131] Top shape: 100 (100)
I0428 20:15:45.775349   512 net.cpp:131] Top shape: 100 (100)
I0428 20:15:45.775352   512 net.cpp:139] Memory required for data: 314800
I0428 20:15:45.775354   512 layer_factory.hpp:77] Creating layer conv0
I0428 20:15:45.775363   512 net.cpp:86] Creating Layer conv0
I0428 20:15:45.775367   512 net.cpp:408] conv0 <- data
I0428 20:15:45.775372   512 net.cpp:382] conv0 -> conv0
I0428 20:15:45.777007   512 net.cpp:124] Setting up conv0
I0428 20:15:45.777032   512 net.cpp:131] Top shape: 100 50 24 24 (2880000)
I0428 20:15:45.777037   512 net.cpp:139] Memory required for data: 11834800
I0428 20:15:45.777046   512 layer_factory.hpp:77] Creating layer pool0
I0428 20:15:45.777052   512 net.cpp:86] Creating Layer pool0
I0428 20:15:45.777056   512 net.cpp:408] pool0 <- conv0
I0428 20:15:45.777077   512 net.cpp:382] pool0 -> pool0
I0428 20:15:45.777113   512 net.cpp:124] Setting up pool0
I0428 20:15:45.777120   512 net.cpp:131] Top shape: 100 50 12 12 (720000)
I0428 20:15:45.777124   512 net.cpp:139] Memory required for data: 14714800
I0428 20:15:45.777127   512 layer_factory.hpp:77] Creating layer conv1
I0428 20:15:45.777135   512 net.cpp:86] Creating Layer conv1
I0428 20:15:45.777139   512 net.cpp:408] conv1 <- pool0
I0428 20:15:45.777144   512 net.cpp:382] conv1 -> conv1
I0428 20:15:45.778648   512 net.cpp:124] Setting up conv1
I0428 20:15:45.778661   512 net.cpp:131] Top shape: 100 5 8 8 (32000)
I0428 20:15:45.778666   512 net.cpp:139] Memory required for data: 14842800
I0428 20:15:45.778673   512 layer_factory.hpp:77] Creating layer pool1
I0428 20:15:45.778679   512 net.cpp:86] Creating Layer pool1
I0428 20:15:45.778682   512 net.cpp:408] pool1 <- conv1
I0428 20:15:45.778687   512 net.cpp:382] pool1 -> pool1
I0428 20:15:45.778728   512 net.cpp:124] Setting up pool1
I0428 20:15:45.778733   512 net.cpp:131] Top shape: 100 5 4 4 (8000)
I0428 20:15:45.778736   512 net.cpp:139] Memory required for data: 14874800
I0428 20:15:45.778739   512 layer_factory.hpp:77] Creating layer ip1
I0428 20:15:45.778745   512 net.cpp:86] Creating Layer ip1
I0428 20:15:45.778748   512 net.cpp:408] ip1 <- pool1
I0428 20:15:45.778753   512 net.cpp:382] ip1 -> ip1
I0428 20:15:45.778851   512 net.cpp:124] Setting up ip1
I0428 20:15:45.778857   512 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:15:45.778861   512 net.cpp:139] Memory required for data: 14884800
I0428 20:15:45.778867   512 layer_factory.hpp:77] Creating layer relu1
I0428 20:15:45.778872   512 net.cpp:86] Creating Layer relu1
I0428 20:15:45.778875   512 net.cpp:408] relu1 <- ip1
I0428 20:15:45.778879   512 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:15:45.779022   512 net.cpp:124] Setting up relu1
I0428 20:15:45.779031   512 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:15:45.779034   512 net.cpp:139] Memory required for data: 14894800
I0428 20:15:45.779037   512 layer_factory.hpp:77] Creating layer ip2
I0428 20:15:45.779043   512 net.cpp:86] Creating Layer ip2
I0428 20:15:45.779047   512 net.cpp:408] ip2 <- ip1
I0428 20:15:45.779052   512 net.cpp:382] ip2 -> ip2
I0428 20:15:45.779139   512 net.cpp:124] Setting up ip2
I0428 20:15:45.779146   512 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:15:45.779150   512 net.cpp:139] Memory required for data: 14898800
I0428 20:15:45.779155   512 layer_factory.hpp:77] Creating layer relu2
I0428 20:15:45.779160   512 net.cpp:86] Creating Layer relu2
I0428 20:15:45.779162   512 net.cpp:408] relu2 <- ip2
I0428 20:15:45.779165   512 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:15:45.779320   512 net.cpp:124] Setting up relu2
I0428 20:15:45.779326   512 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:15:45.779330   512 net.cpp:139] Memory required for data: 14902800
I0428 20:15:45.779333   512 layer_factory.hpp:77] Creating layer ip2_relu2_0_split
I0428 20:15:45.779355   512 net.cpp:86] Creating Layer ip2_relu2_0_split
I0428 20:15:45.779357   512 net.cpp:408] ip2_relu2_0_split <- ip2
I0428 20:15:45.779361   512 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_0
I0428 20:15:45.779377   512 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_1
I0428 20:15:45.779415   512 net.cpp:124] Setting up ip2_relu2_0_split
I0428 20:15:45.779422   512 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:15:45.779427   512 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:15:45.779429   512 net.cpp:139] Memory required for data: 14910800
I0428 20:15:45.779433   512 layer_factory.hpp:77] Creating layer accuracy
I0428 20:15:45.779438   512 net.cpp:86] Creating Layer accuracy
I0428 20:15:45.779440   512 net.cpp:408] accuracy <- ip2_relu2_0_split_0
I0428 20:15:45.779444   512 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:15:45.779448   512 net.cpp:382] accuracy -> accuracy
I0428 20:15:45.779455   512 net.cpp:124] Setting up accuracy
I0428 20:15:45.779459   512 net.cpp:131] Top shape: (1)
I0428 20:15:45.779462   512 net.cpp:139] Memory required for data: 14910804
I0428 20:15:45.779464   512 layer_factory.hpp:77] Creating layer loss
I0428 20:15:45.779469   512 net.cpp:86] Creating Layer loss
I0428 20:15:45.779472   512 net.cpp:408] loss <- ip2_relu2_0_split_1
I0428 20:15:45.779475   512 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:15:45.779479   512 net.cpp:382] loss -> loss
I0428 20:15:45.779485   512 layer_factory.hpp:77] Creating layer loss
I0428 20:15:45.779734   512 net.cpp:124] Setting up loss
I0428 20:15:45.779742   512 net.cpp:131] Top shape: (1)
I0428 20:15:45.779747   512 net.cpp:134]     with loss weight 1
I0428 20:15:45.779752   512 net.cpp:139] Memory required for data: 14910808
I0428 20:15:45.779755   512 net.cpp:200] loss needs backward computation.
I0428 20:15:45.779759   512 net.cpp:202] accuracy does not need backward computation.
I0428 20:15:45.779762   512 net.cpp:200] ip2_relu2_0_split needs backward computation.
I0428 20:15:45.779765   512 net.cpp:200] relu2 needs backward computation.
I0428 20:15:45.779774   512 net.cpp:200] ip2 needs backward computation.
I0428 20:15:45.779777   512 net.cpp:200] relu1 needs backward computation.
I0428 20:15:45.779779   512 net.cpp:200] ip1 needs backward computation.
I0428 20:15:45.779783   512 net.cpp:200] pool1 needs backward computation.
I0428 20:15:45.779785   512 net.cpp:200] conv1 needs backward computation.
I0428 20:15:45.779788   512 net.cpp:200] pool0 needs backward computation.
I0428 20:15:45.779791   512 net.cpp:200] conv0 needs backward computation.
I0428 20:15:45.779795   512 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:15:45.779799   512 net.cpp:202] mnist does not need backward computation.
I0428 20:15:45.779801   512 net.cpp:244] This network produces output accuracy
I0428 20:15:45.779804   512 net.cpp:244] This network produces output loss
I0428 20:15:45.779814   512 net.cpp:257] Network initialization done.
I0428 20:15:45.779850   512 solver.cpp:56] Solver scaffolding done.
I0428 20:15:45.780093   512 caffe.cpp:248] Starting Optimization
I0428 20:15:45.780100   512 solver.cpp:273] Solving LeNet
I0428 20:15:45.780103   512 solver.cpp:274] Learning Rate Policy: inv
I0428 20:15:45.780285   512 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:15:45.788326   512 blocking_queue.cpp:49] Waiting for data
I0428 20:15:45.847962   519 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:15:45.848917   512 solver.cpp:398]     Test net output #0: accuracy = 0.1013
I0428 20:15:45.848935   512 solver.cpp:398]     Test net output #1: loss = 2.40389 (* 1 = 2.40389 loss)
I0428 20:15:45.853207   512 solver.cpp:219] Iteration 0 (-1.11263e-42 iter/s, 0.0730796s/100 iters), loss = 2.41434
I0428 20:15:45.853247   512 solver.cpp:238]     Train net output #0: loss = 2.41434 (* 1 = 2.41434 loss)
I0428 20:15:45.853258   512 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:15:45.973083   512 solver.cpp:219] Iteration 100 (834.577 iter/s, 0.119821s/100 iters), loss = 0.655274
I0428 20:15:45.973109   512 solver.cpp:238]     Train net output #0: loss = 0.655274 (* 1 = 0.655274 loss)
I0428 20:15:45.973115   512 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:15:46.085168   512 solver.cpp:219] Iteration 200 (892.474 iter/s, 0.112048s/100 iters), loss = 0.780585
I0428 20:15:46.085193   512 solver.cpp:238]     Train net output #0: loss = 0.780585 (* 1 = 0.780585 loss)
I0428 20:15:46.085201   512 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:15:46.194262   512 solver.cpp:219] Iteration 300 (916.943 iter/s, 0.109058s/100 iters), loss = 0.449185
I0428 20:15:46.194285   512 solver.cpp:238]     Train net output #0: loss = 0.449185 (* 1 = 0.449185 loss)
I0428 20:15:46.194291   512 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:15:46.305299   512 solver.cpp:219] Iteration 400 (900.888 iter/s, 0.111002s/100 iters), loss = 0.499164
I0428 20:15:46.305323   512 solver.cpp:238]     Train net output #0: loss = 0.499164 (* 1 = 0.499164 loss)
I0428 20:15:46.305346   512 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:15:46.416170   512 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:15:46.477579   519 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:15:46.478478   512 solver.cpp:398]     Test net output #0: accuracy = 0.8629
I0428 20:15:46.478515   512 solver.cpp:398]     Test net output #1: loss = 0.374852 (* 1 = 0.374852 loss)
I0428 20:15:46.479667   512 solver.cpp:219] Iteration 500 (573.625 iter/s, 0.17433s/100 iters), loss = 0.294326
I0428 20:15:46.479708   512 solver.cpp:238]     Train net output #0: loss = 0.294326 (* 1 = 0.294326 loss)
I0428 20:15:46.479717   512 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:15:46.599445   512 solver.cpp:219] Iteration 600 (835.233 iter/s, 0.119727s/100 iters), loss = 0.451806
I0428 20:15:46.599485   512 solver.cpp:238]     Train net output #0: loss = 0.451806 (* 1 = 0.451806 loss)
I0428 20:15:46.599493   512 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:15:46.712915   512 solver.cpp:219] Iteration 700 (881.573 iter/s, 0.113434s/100 iters), loss = 0.24694
I0428 20:15:46.712942   512 solver.cpp:238]     Train net output #0: loss = 0.24694 (* 1 = 0.24694 loss)
I0428 20:15:46.712949   512 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:15:46.831588   512 solver.cpp:219] Iteration 800 (842.962 iter/s, 0.118629s/100 iters), loss = 0.268456
I0428 20:15:46.831614   512 solver.cpp:238]     Train net output #0: loss = 0.268456 (* 1 = 0.268456 loss)
I0428 20:15:46.831619   512 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:15:46.940897   512 solver.cpp:219] Iteration 900 (915.267 iter/s, 0.109258s/100 iters), loss = 0.236588
I0428 20:15:46.940954   512 solver.cpp:238]     Train net output #0: loss = 0.236588 (* 1 = 0.236588 loss)
I0428 20:15:46.940961   512 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:15:46.977623   518 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:15:47.049342   512 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:15:47.050321   512 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:15:47.051079   512 solver.cpp:311] Iteration 1000, loss = 0.200165
I0428 20:15:47.051095   512 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:15:47.117995   519 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:15:47.118736   512 solver.cpp:398]     Test net output #0: accuracy = 0.9686
I0428 20:15:47.118754   512 solver.cpp:398]     Test net output #1: loss = 0.100465 (* 1 = 0.100465 loss)
I0428 20:15:47.118760   512 solver.cpp:316] Optimization Done.
I0428 20:15:47.118763   512 caffe.cpp:259] Optimization Done.
