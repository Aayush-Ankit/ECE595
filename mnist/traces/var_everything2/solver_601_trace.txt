I0428 19:50:19.177871 26986 caffe.cpp:218] Using GPUs 0
I0428 19:50:19.214371 26986 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:50:19.724371 26986 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test601.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:50:19.724509 26986 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test601.prototxt
I0428 19:50:19.724941 26986 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:50:19.724959 26986 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:50:19.725062 26986 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:50:19.725143 26986 layer_factory.hpp:77] Creating layer mnist
I0428 19:50:19.725244 26986 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:50:19.725267 26986 net.cpp:86] Creating Layer mnist
I0428 19:50:19.725276 26986 net.cpp:382] mnist -> data
I0428 19:50:19.725298 26986 net.cpp:382] mnist -> label
I0428 19:50:19.726392 26986 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:50:19.728849 26986 net.cpp:124] Setting up mnist
I0428 19:50:19.728868 26986 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:50:19.728873 26986 net.cpp:131] Top shape: 64 (64)
I0428 19:50:19.728876 26986 net.cpp:139] Memory required for data: 200960
I0428 19:50:19.728883 26986 layer_factory.hpp:77] Creating layer conv0
I0428 19:50:19.728929 26986 net.cpp:86] Creating Layer conv0
I0428 19:50:19.728950 26986 net.cpp:408] conv0 <- data
I0428 19:50:19.728962 26986 net.cpp:382] conv0 -> conv0
I0428 19:50:19.987637 26986 net.cpp:124] Setting up conv0
I0428 19:50:19.987679 26986 net.cpp:131] Top shape: 64 5 24 24 (184320)
I0428 19:50:19.987682 26986 net.cpp:139] Memory required for data: 938240
I0428 19:50:19.987696 26986 layer_factory.hpp:77] Creating layer pool0
I0428 19:50:19.987709 26986 net.cpp:86] Creating Layer pool0
I0428 19:50:19.987712 26986 net.cpp:408] pool0 <- conv0
I0428 19:50:19.987717 26986 net.cpp:382] pool0 -> pool0
I0428 19:50:19.987777 26986 net.cpp:124] Setting up pool0
I0428 19:50:19.987782 26986 net.cpp:131] Top shape: 64 5 12 12 (46080)
I0428 19:50:19.987785 26986 net.cpp:139] Memory required for data: 1122560
I0428 19:50:19.987788 26986 layer_factory.hpp:77] Creating layer conv1
I0428 19:50:19.987798 26986 net.cpp:86] Creating Layer conv1
I0428 19:50:19.987802 26986 net.cpp:408] conv1 <- pool0
I0428 19:50:19.987807 26986 net.cpp:382] conv1 -> conv1
I0428 19:50:19.990689 26986 net.cpp:124] Setting up conv1
I0428 19:50:19.990720 26986 net.cpp:131] Top shape: 64 10 8 8 (40960)
I0428 19:50:19.990723 26986 net.cpp:139] Memory required for data: 1286400
I0428 19:50:19.990747 26986 layer_factory.hpp:77] Creating layer pool1
I0428 19:50:19.990754 26986 net.cpp:86] Creating Layer pool1
I0428 19:50:19.990758 26986 net.cpp:408] pool1 <- conv1
I0428 19:50:19.990763 26986 net.cpp:382] pool1 -> pool1
I0428 19:50:19.990800 26986 net.cpp:124] Setting up pool1
I0428 19:50:19.990806 26986 net.cpp:131] Top shape: 64 10 4 4 (10240)
I0428 19:50:19.990808 26986 net.cpp:139] Memory required for data: 1327360
I0428 19:50:19.990811 26986 layer_factory.hpp:77] Creating layer ip1
I0428 19:50:19.990819 26986 net.cpp:86] Creating Layer ip1
I0428 19:50:19.990823 26986 net.cpp:408] ip1 <- pool1
I0428 19:50:19.990826 26986 net.cpp:382] ip1 -> ip1
I0428 19:50:19.990928 26986 net.cpp:124] Setting up ip1
I0428 19:50:19.990936 26986 net.cpp:131] Top shape: 64 10 (640)
I0428 19:50:19.990939 26986 net.cpp:139] Memory required for data: 1329920
I0428 19:50:19.990947 26986 layer_factory.hpp:77] Creating layer relu1
I0428 19:50:19.990952 26986 net.cpp:86] Creating Layer relu1
I0428 19:50:19.990955 26986 net.cpp:408] relu1 <- ip1
I0428 19:50:19.990959 26986 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:50:19.991119 26986 net.cpp:124] Setting up relu1
I0428 19:50:19.991128 26986 net.cpp:131] Top shape: 64 10 (640)
I0428 19:50:19.991132 26986 net.cpp:139] Memory required for data: 1332480
I0428 19:50:19.991134 26986 layer_factory.hpp:77] Creating layer ip2
I0428 19:50:19.991140 26986 net.cpp:86] Creating Layer ip2
I0428 19:50:19.991143 26986 net.cpp:408] ip2 <- ip1
I0428 19:50:19.991148 26986 net.cpp:382] ip2 -> ip2
I0428 19:50:19.991242 26986 net.cpp:124] Setting up ip2
I0428 19:50:19.991250 26986 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:50:19.991252 26986 net.cpp:139] Memory required for data: 1338880
I0428 19:50:19.991257 26986 layer_factory.hpp:77] Creating layer relu2
I0428 19:50:19.991263 26986 net.cpp:86] Creating Layer relu2
I0428 19:50:19.991266 26986 net.cpp:408] relu2 <- ip2
I0428 19:50:19.991271 26986 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:50:19.992128 26986 net.cpp:124] Setting up relu2
I0428 19:50:19.992141 26986 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:50:19.992159 26986 net.cpp:139] Memory required for data: 1345280
I0428 19:50:19.992162 26986 layer_factory.hpp:77] Creating layer ip3
I0428 19:50:19.992169 26986 net.cpp:86] Creating Layer ip3
I0428 19:50:19.992172 26986 net.cpp:408] ip3 <- ip2
I0428 19:50:19.992178 26986 net.cpp:382] ip3 -> ip3
I0428 19:50:19.992269 26986 net.cpp:124] Setting up ip3
I0428 19:50:19.992277 26986 net.cpp:131] Top shape: 64 10 (640)
I0428 19:50:19.992280 26986 net.cpp:139] Memory required for data: 1347840
I0428 19:50:19.992287 26986 layer_factory.hpp:77] Creating layer relu3
I0428 19:50:19.992292 26986 net.cpp:86] Creating Layer relu3
I0428 19:50:19.992295 26986 net.cpp:408] relu3 <- ip3
I0428 19:50:19.992300 26986 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:50:19.992480 26986 net.cpp:124] Setting up relu3
I0428 19:50:19.992488 26986 net.cpp:131] Top shape: 64 10 (640)
I0428 19:50:19.992491 26986 net.cpp:139] Memory required for data: 1350400
I0428 19:50:19.992494 26986 layer_factory.hpp:77] Creating layer loss
I0428 19:50:19.992503 26986 net.cpp:86] Creating Layer loss
I0428 19:50:19.992507 26986 net.cpp:408] loss <- ip3
I0428 19:50:19.992511 26986 net.cpp:408] loss <- label
I0428 19:50:19.992516 26986 net.cpp:382] loss -> loss
I0428 19:50:19.992528 26986 layer_factory.hpp:77] Creating layer loss
I0428 19:50:19.992781 26986 net.cpp:124] Setting up loss
I0428 19:50:19.992790 26986 net.cpp:131] Top shape: (1)
I0428 19:50:19.992794 26986 net.cpp:134]     with loss weight 1
I0428 19:50:19.992806 26986 net.cpp:139] Memory required for data: 1350404
I0428 19:50:19.992831 26986 net.cpp:200] loss needs backward computation.
I0428 19:50:19.992837 26986 net.cpp:200] relu3 needs backward computation.
I0428 19:50:19.992841 26986 net.cpp:200] ip3 needs backward computation.
I0428 19:50:19.992843 26986 net.cpp:200] relu2 needs backward computation.
I0428 19:50:19.992846 26986 net.cpp:200] ip2 needs backward computation.
I0428 19:50:19.992849 26986 net.cpp:200] relu1 needs backward computation.
I0428 19:50:19.992851 26986 net.cpp:200] ip1 needs backward computation.
I0428 19:50:19.992854 26986 net.cpp:200] pool1 needs backward computation.
I0428 19:50:19.992858 26986 net.cpp:200] conv1 needs backward computation.
I0428 19:50:19.992861 26986 net.cpp:200] pool0 needs backward computation.
I0428 19:50:19.992864 26986 net.cpp:200] conv0 needs backward computation.
I0428 19:50:19.992867 26986 net.cpp:202] mnist does not need backward computation.
I0428 19:50:19.992871 26986 net.cpp:244] This network produces output loss
I0428 19:50:19.992882 26986 net.cpp:257] Network initialization done.
I0428 19:50:19.993269 26986 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test601.prototxt
I0428 19:50:19.993312 26986 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:50:19.993418 26986 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:50:19.993502 26986 layer_factory.hpp:77] Creating layer mnist
I0428 19:50:19.993544 26986 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:50:19.993556 26986 net.cpp:86] Creating Layer mnist
I0428 19:50:19.993561 26986 net.cpp:382] mnist -> data
I0428 19:50:19.993568 26986 net.cpp:382] mnist -> label
I0428 19:50:19.993654 26986 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:50:19.995658 26986 net.cpp:124] Setting up mnist
I0428 19:50:19.995687 26986 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:50:19.995692 26986 net.cpp:131] Top shape: 100 (100)
I0428 19:50:19.995694 26986 net.cpp:139] Memory required for data: 314000
I0428 19:50:19.995698 26986 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:50:19.995704 26986 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:50:19.995707 26986 net.cpp:408] label_mnist_1_split <- label
I0428 19:50:19.995741 26986 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:50:19.995748 26986 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:50:19.995787 26986 net.cpp:124] Setting up label_mnist_1_split
I0428 19:50:19.995792 26986 net.cpp:131] Top shape: 100 (100)
I0428 19:50:19.995796 26986 net.cpp:131] Top shape: 100 (100)
I0428 19:50:19.995798 26986 net.cpp:139] Memory required for data: 314800
I0428 19:50:19.995801 26986 layer_factory.hpp:77] Creating layer conv0
I0428 19:50:19.995811 26986 net.cpp:86] Creating Layer conv0
I0428 19:50:19.995815 26986 net.cpp:408] conv0 <- data
I0428 19:50:19.995820 26986 net.cpp:382] conv0 -> conv0
I0428 19:50:19.997772 26986 net.cpp:124] Setting up conv0
I0428 19:50:19.997787 26986 net.cpp:131] Top shape: 100 5 24 24 (288000)
I0428 19:50:19.997790 26986 net.cpp:139] Memory required for data: 1466800
I0428 19:50:19.997799 26986 layer_factory.hpp:77] Creating layer pool0
I0428 19:50:19.997807 26986 net.cpp:86] Creating Layer pool0
I0428 19:50:19.997810 26986 net.cpp:408] pool0 <- conv0
I0428 19:50:19.997814 26986 net.cpp:382] pool0 -> pool0
I0428 19:50:19.997867 26986 net.cpp:124] Setting up pool0
I0428 19:50:19.997872 26986 net.cpp:131] Top shape: 100 5 12 12 (72000)
I0428 19:50:19.997875 26986 net.cpp:139] Memory required for data: 1754800
I0428 19:50:19.997879 26986 layer_factory.hpp:77] Creating layer conv1
I0428 19:50:19.997886 26986 net.cpp:86] Creating Layer conv1
I0428 19:50:19.997889 26986 net.cpp:408] conv1 <- pool0
I0428 19:50:19.997895 26986 net.cpp:382] conv1 -> conv1
I0428 19:50:20.000229 26986 net.cpp:124] Setting up conv1
I0428 19:50:20.000243 26986 net.cpp:131] Top shape: 100 10 8 8 (64000)
I0428 19:50:20.000247 26986 net.cpp:139] Memory required for data: 2010800
I0428 19:50:20.000257 26986 layer_factory.hpp:77] Creating layer pool1
I0428 19:50:20.000279 26986 net.cpp:86] Creating Layer pool1
I0428 19:50:20.000283 26986 net.cpp:408] pool1 <- conv1
I0428 19:50:20.000288 26986 net.cpp:382] pool1 -> pool1
I0428 19:50:20.000340 26986 net.cpp:124] Setting up pool1
I0428 19:50:20.000346 26986 net.cpp:131] Top shape: 100 10 4 4 (16000)
I0428 19:50:20.000349 26986 net.cpp:139] Memory required for data: 2074800
I0428 19:50:20.000360 26986 layer_factory.hpp:77] Creating layer ip1
I0428 19:50:20.000365 26986 net.cpp:86] Creating Layer ip1
I0428 19:50:20.000372 26986 net.cpp:408] ip1 <- pool1
I0428 19:50:20.000380 26986 net.cpp:382] ip1 -> ip1
I0428 19:50:20.000517 26986 net.cpp:124] Setting up ip1
I0428 19:50:20.000525 26986 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:50:20.000538 26986 net.cpp:139] Memory required for data: 2078800
I0428 19:50:20.000546 26986 layer_factory.hpp:77] Creating layer relu1
I0428 19:50:20.000552 26986 net.cpp:86] Creating Layer relu1
I0428 19:50:20.000556 26986 net.cpp:408] relu1 <- ip1
I0428 19:50:20.000560 26986 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:50:20.000845 26986 net.cpp:124] Setting up relu1
I0428 19:50:20.000855 26986 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:50:20.000859 26986 net.cpp:139] Memory required for data: 2082800
I0428 19:50:20.000861 26986 layer_factory.hpp:77] Creating layer ip2
I0428 19:50:20.000869 26986 net.cpp:86] Creating Layer ip2
I0428 19:50:20.000872 26986 net.cpp:408] ip2 <- ip1
I0428 19:50:20.000879 26986 net.cpp:382] ip2 -> ip2
I0428 19:50:20.000994 26986 net.cpp:124] Setting up ip2
I0428 19:50:20.001001 26986 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:50:20.001004 26986 net.cpp:139] Memory required for data: 2092800
I0428 19:50:20.001021 26986 layer_factory.hpp:77] Creating layer relu2
I0428 19:50:20.001026 26986 net.cpp:86] Creating Layer relu2
I0428 19:50:20.001030 26986 net.cpp:408] relu2 <- ip2
I0428 19:50:20.001034 26986 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:50:20.001237 26986 net.cpp:124] Setting up relu2
I0428 19:50:20.001245 26986 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:50:20.001248 26986 net.cpp:139] Memory required for data: 2102800
I0428 19:50:20.001251 26986 layer_factory.hpp:77] Creating layer ip3
I0428 19:50:20.001263 26986 net.cpp:86] Creating Layer ip3
I0428 19:50:20.001266 26986 net.cpp:408] ip3 <- ip2
I0428 19:50:20.001272 26986 net.cpp:382] ip3 -> ip3
I0428 19:50:20.001392 26986 net.cpp:124] Setting up ip3
I0428 19:50:20.001399 26986 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:50:20.001402 26986 net.cpp:139] Memory required for data: 2106800
I0428 19:50:20.001410 26986 layer_factory.hpp:77] Creating layer relu3
I0428 19:50:20.001415 26986 net.cpp:86] Creating Layer relu3
I0428 19:50:20.001418 26986 net.cpp:408] relu3 <- ip3
I0428 19:50:20.001422 26986 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:50:20.002257 26986 net.cpp:124] Setting up relu3
I0428 19:50:20.002269 26986 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:50:20.002272 26986 net.cpp:139] Memory required for data: 2110800
I0428 19:50:20.002275 26986 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:50:20.002281 26986 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:50:20.002285 26986 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:50:20.002290 26986 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:50:20.002296 26986 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:50:20.002346 26986 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:50:20.002352 26986 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:50:20.002357 26986 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:50:20.002358 26986 net.cpp:139] Memory required for data: 2118800
I0428 19:50:20.002362 26986 layer_factory.hpp:77] Creating layer accuracy
I0428 19:50:20.002372 26986 net.cpp:86] Creating Layer accuracy
I0428 19:50:20.002382 26986 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:50:20.002384 26986 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:50:20.002389 26986 net.cpp:382] accuracy -> accuracy
I0428 19:50:20.002395 26986 net.cpp:124] Setting up accuracy
I0428 19:50:20.002399 26986 net.cpp:131] Top shape: (1)
I0428 19:50:20.002403 26986 net.cpp:139] Memory required for data: 2118804
I0428 19:50:20.002405 26986 layer_factory.hpp:77] Creating layer loss
I0428 19:50:20.002410 26986 net.cpp:86] Creating Layer loss
I0428 19:50:20.002414 26986 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:50:20.002418 26986 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:50:20.002426 26986 net.cpp:382] loss -> loss
I0428 19:50:20.002432 26986 layer_factory.hpp:77] Creating layer loss
I0428 19:50:20.002743 26986 net.cpp:124] Setting up loss
I0428 19:50:20.002753 26986 net.cpp:131] Top shape: (1)
I0428 19:50:20.002761 26986 net.cpp:134]     with loss weight 1
I0428 19:50:20.002768 26986 net.cpp:139] Memory required for data: 2118808
I0428 19:50:20.002797 26986 net.cpp:200] loss needs backward computation.
I0428 19:50:20.002801 26986 net.cpp:202] accuracy does not need backward computation.
I0428 19:50:20.002805 26986 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:50:20.002813 26986 net.cpp:200] relu3 needs backward computation.
I0428 19:50:20.002816 26986 net.cpp:200] ip3 needs backward computation.
I0428 19:50:20.002820 26986 net.cpp:200] relu2 needs backward computation.
I0428 19:50:20.002822 26986 net.cpp:200] ip2 needs backward computation.
I0428 19:50:20.002825 26986 net.cpp:200] relu1 needs backward computation.
I0428 19:50:20.002833 26986 net.cpp:200] ip1 needs backward computation.
I0428 19:50:20.002836 26986 net.cpp:200] pool1 needs backward computation.
I0428 19:50:20.002840 26986 net.cpp:200] conv1 needs backward computation.
I0428 19:50:20.002842 26986 net.cpp:200] pool0 needs backward computation.
I0428 19:50:20.002851 26986 net.cpp:200] conv0 needs backward computation.
I0428 19:50:20.002869 26986 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:50:20.002878 26986 net.cpp:202] mnist does not need backward computation.
I0428 19:50:20.002882 26986 net.cpp:244] This network produces output accuracy
I0428 19:50:20.002884 26986 net.cpp:244] This network produces output loss
I0428 19:50:20.002915 26986 net.cpp:257] Network initialization done.
I0428 19:50:20.002954 26986 solver.cpp:56] Solver scaffolding done.
I0428 19:50:20.003278 26986 caffe.cpp:248] Starting Optimization
I0428 19:50:20.003284 26986 solver.cpp:273] Solving LeNet
I0428 19:50:20.003288 26986 solver.cpp:274] Learning Rate Policy: inv
I0428 19:50:20.003458 26986 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:50:20.007319 26986 blocking_queue.cpp:49] Waiting for data
I0428 19:50:20.079749 26993 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:50:20.080266 26986 solver.cpp:398]     Test net output #0: accuracy = 0.089
I0428 19:50:20.080284 26986 solver.cpp:398]     Test net output #1: loss = 2.30427 (* 1 = 2.30427 loss)
I0428 19:50:20.082291 26986 solver.cpp:219] Iteration 0 (-8.51533e-31 iter/s, 0.078976s/100 iters), loss = 2.29584
I0428 19:50:20.082314 26986 solver.cpp:238]     Train net output #0: loss = 2.29584 (* 1 = 2.29584 loss)
I0428 19:50:20.082326 26986 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:50:20.165287 26986 solver.cpp:219] Iteration 100 (1205.39 iter/s, 0.0829604s/100 iters), loss = 1.38864
I0428 19:50:20.165326 26986 solver.cpp:238]     Train net output #0: loss = 1.38864 (* 1 = 1.38864 loss)
I0428 19:50:20.165333 26986 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:50:20.244086 26986 solver.cpp:219] Iteration 200 (1269.62 iter/s, 0.078764s/100 iters), loss = 1.49861
I0428 19:50:20.244125 26986 solver.cpp:238]     Train net output #0: loss = 1.49861 (* 1 = 1.49861 loss)
I0428 19:50:20.244132 26986 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:50:20.318104 26986 solver.cpp:219] Iteration 300 (1351.92 iter/s, 0.0739686s/100 iters), loss = 0.950318
I0428 19:50:20.318141 26986 solver.cpp:238]     Train net output #0: loss = 0.950318 (* 1 = 0.950318 loss)
I0428 19:50:20.318147 26986 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:50:20.395257 26986 solver.cpp:219] Iteration 400 (1296.65 iter/s, 0.0771215s/100 iters), loss = 1.27152
I0428 19:50:20.395310 26986 solver.cpp:238]     Train net output #0: loss = 1.27152 (* 1 = 1.27152 loss)
I0428 19:50:20.395332 26986 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:50:20.481323 26986 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:50:20.557773 26993 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:50:20.558292 26986 solver.cpp:398]     Test net output #0: accuracy = 0.7229
I0428 19:50:20.558312 26986 solver.cpp:398]     Test net output #1: loss = 0.876005 (* 1 = 0.876005 loss)
I0428 19:50:20.559123 26986 solver.cpp:219] Iteration 500 (610.504 iter/s, 0.163799s/100 iters), loss = 0.909173
I0428 19:50:20.559146 26986 solver.cpp:238]     Train net output #0: loss = 0.909173 (* 1 = 0.909173 loss)
I0428 19:50:20.559168 26986 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:50:20.641881 26986 solver.cpp:219] Iteration 600 (1208.8 iter/s, 0.0827268s/100 iters), loss = 0.745701
I0428 19:50:20.641919 26986 solver.cpp:238]     Train net output #0: loss = 0.745701 (* 1 = 0.745701 loss)
I0428 19:50:20.641926 26986 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:50:20.720722 26986 solver.cpp:219] Iteration 700 (1268.89 iter/s, 0.0788091s/100 iters), loss = 0.755541
I0428 19:50:20.720762 26986 solver.cpp:238]     Train net output #0: loss = 0.755541 (* 1 = 0.755541 loss)
I0428 19:50:20.720767 26986 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:50:20.796169 26986 solver.cpp:219] Iteration 800 (1326.29 iter/s, 0.0753981s/100 iters), loss = 0.837436
I0428 19:50:20.796206 26986 solver.cpp:238]     Train net output #0: loss = 0.837436 (* 1 = 0.837436 loss)
I0428 19:50:20.796212 26986 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:50:20.872217 26986 solver.cpp:219] Iteration 900 (1315.48 iter/s, 0.0760178s/100 iters), loss = 0.804273
I0428 19:50:20.872257 26986 solver.cpp:238]     Train net output #0: loss = 0.804273 (* 1 = 0.804273 loss)
I0428 19:50:20.872263 26986 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:50:20.897008 26992 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:50:20.945574 26986 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:50:20.946315 26986 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:50:20.946854 26986 solver.cpp:311] Iteration 1000, loss = 0.535609
I0428 19:50:20.946868 26986 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:50:21.000305 26993 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:50:21.000849 26986 solver.cpp:398]     Test net output #0: accuracy = 0.7669
I0428 19:50:21.000883 26986 solver.cpp:398]     Test net output #1: loss = 0.619101 (* 1 = 0.619101 loss)
I0428 19:50:21.000888 26986 solver.cpp:316] Optimization Done.
I0428 19:50:21.000891 26986 caffe.cpp:259] Optimization Done.
