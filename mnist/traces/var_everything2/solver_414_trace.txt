I0428 19:43:10.262187 25280 caffe.cpp:218] Using GPUs 0
I0428 19:43:10.289486 25280 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:43:10.744609 25280 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test414.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:43:10.744772 25280 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test414.prototxt
I0428 19:43:10.745157 25280 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:43:10.745177 25280 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:43:10.745281 25280 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:43:10.745368 25280 layer_factory.hpp:77] Creating layer mnist
I0428 19:43:10.745471 25280 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:43:10.745497 25280 net.cpp:86] Creating Layer mnist
I0428 19:43:10.745507 25280 net.cpp:382] mnist -> data
I0428 19:43:10.745529 25280 net.cpp:382] mnist -> label
I0428 19:43:10.746537 25280 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:43:10.748919 25280 net.cpp:124] Setting up mnist
I0428 19:43:10.748935 25280 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:43:10.748945 25280 net.cpp:131] Top shape: 64 (64)
I0428 19:43:10.748950 25280 net.cpp:139] Memory required for data: 200960
I0428 19:43:10.748960 25280 layer_factory.hpp:77] Creating layer conv0
I0428 19:43:10.748977 25280 net.cpp:86] Creating Layer conv0
I0428 19:43:10.748997 25280 net.cpp:408] conv0 <- data
I0428 19:43:10.749016 25280 net.cpp:382] conv0 -> conv0
I0428 19:43:10.984995 25280 net.cpp:124] Setting up conv0
I0428 19:43:10.985025 25280 net.cpp:131] Top shape: 64 2 24 24 (73728)
I0428 19:43:10.985031 25280 net.cpp:139] Memory required for data: 495872
I0428 19:43:10.985051 25280 layer_factory.hpp:77] Creating layer pool0
I0428 19:43:10.985069 25280 net.cpp:86] Creating Layer pool0
I0428 19:43:10.985079 25280 net.cpp:408] pool0 <- conv0
I0428 19:43:10.985087 25280 net.cpp:382] pool0 -> pool0
I0428 19:43:10.985142 25280 net.cpp:124] Setting up pool0
I0428 19:43:10.985152 25280 net.cpp:131] Top shape: 64 2 12 12 (18432)
I0428 19:43:10.985155 25280 net.cpp:139] Memory required for data: 569600
I0428 19:43:10.985160 25280 layer_factory.hpp:77] Creating layer conv1
I0428 19:43:10.985175 25280 net.cpp:86] Creating Layer conv1
I0428 19:43:10.985182 25280 net.cpp:408] conv1 <- pool0
I0428 19:43:10.985189 25280 net.cpp:382] conv1 -> conv1
I0428 19:43:10.987982 25280 net.cpp:124] Setting up conv1
I0428 19:43:10.987998 25280 net.cpp:131] Top shape: 64 25 8 8 (102400)
I0428 19:43:10.988003 25280 net.cpp:139] Memory required for data: 979200
I0428 19:43:10.988032 25280 layer_factory.hpp:77] Creating layer pool1
I0428 19:43:10.988044 25280 net.cpp:86] Creating Layer pool1
I0428 19:43:10.988056 25280 net.cpp:408] pool1 <- conv1
I0428 19:43:10.988066 25280 net.cpp:382] pool1 -> pool1
I0428 19:43:10.988114 25280 net.cpp:124] Setting up pool1
I0428 19:43:10.988123 25280 net.cpp:131] Top shape: 64 25 4 4 (25600)
I0428 19:43:10.988128 25280 net.cpp:139] Memory required for data: 1081600
I0428 19:43:10.988133 25280 layer_factory.hpp:77] Creating layer ip1
I0428 19:43:10.988147 25280 net.cpp:86] Creating Layer ip1
I0428 19:43:10.988154 25280 net.cpp:408] ip1 <- pool1
I0428 19:43:10.988164 25280 net.cpp:382] ip1 -> ip1
I0428 19:43:10.989215 25280 net.cpp:124] Setting up ip1
I0428 19:43:10.989231 25280 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:43:10.989236 25280 net.cpp:139] Memory required for data: 1088000
I0428 19:43:10.989250 25280 layer_factory.hpp:77] Creating layer relu1
I0428 19:43:10.989262 25280 net.cpp:86] Creating Layer relu1
I0428 19:43:10.989269 25280 net.cpp:408] relu1 <- ip1
I0428 19:43:10.989277 25280 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:43:10.989500 25280 net.cpp:124] Setting up relu1
I0428 19:43:10.989511 25280 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:43:10.989516 25280 net.cpp:139] Memory required for data: 1094400
I0428 19:43:10.989521 25280 layer_factory.hpp:77] Creating layer ip2
I0428 19:43:10.989531 25280 net.cpp:86] Creating Layer ip2
I0428 19:43:10.989536 25280 net.cpp:408] ip2 <- ip1
I0428 19:43:10.989545 25280 net.cpp:382] ip2 -> ip2
I0428 19:43:10.989652 25280 net.cpp:124] Setting up ip2
I0428 19:43:10.989660 25280 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:43:10.989665 25280 net.cpp:139] Memory required for data: 1100800
I0428 19:43:10.989675 25280 layer_factory.hpp:77] Creating layer relu2
I0428 19:43:10.989686 25280 net.cpp:86] Creating Layer relu2
I0428 19:43:10.989691 25280 net.cpp:408] relu2 <- ip2
I0428 19:43:10.989699 25280 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:43:10.990514 25280 net.cpp:124] Setting up relu2
I0428 19:43:10.990528 25280 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:43:10.990533 25280 net.cpp:139] Memory required for data: 1107200
I0428 19:43:10.990538 25280 layer_factory.hpp:77] Creating layer ip3
I0428 19:43:10.990548 25280 net.cpp:86] Creating Layer ip3
I0428 19:43:10.990568 25280 net.cpp:408] ip3 <- ip2
I0428 19:43:10.990579 25280 net.cpp:382] ip3 -> ip3
I0428 19:43:10.990687 25280 net.cpp:124] Setting up ip3
I0428 19:43:10.990696 25280 net.cpp:131] Top shape: 64 10 (640)
I0428 19:43:10.990701 25280 net.cpp:139] Memory required for data: 1109760
I0428 19:43:10.990713 25280 layer_factory.hpp:77] Creating layer relu3
I0428 19:43:10.990739 25280 net.cpp:86] Creating Layer relu3
I0428 19:43:10.990746 25280 net.cpp:408] relu3 <- ip3
I0428 19:43:10.990753 25280 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:43:10.990942 25280 net.cpp:124] Setting up relu3
I0428 19:43:10.990952 25280 net.cpp:131] Top shape: 64 10 (640)
I0428 19:43:10.990957 25280 net.cpp:139] Memory required for data: 1112320
I0428 19:43:10.990962 25280 layer_factory.hpp:77] Creating layer loss
I0428 19:43:10.990972 25280 net.cpp:86] Creating Layer loss
I0428 19:43:10.990978 25280 net.cpp:408] loss <- ip3
I0428 19:43:10.990983 25280 net.cpp:408] loss <- label
I0428 19:43:10.990993 25280 net.cpp:382] loss -> loss
I0428 19:43:10.991014 25280 layer_factory.hpp:77] Creating layer loss
I0428 19:43:10.991310 25280 net.cpp:124] Setting up loss
I0428 19:43:10.991322 25280 net.cpp:131] Top shape: (1)
I0428 19:43:10.991328 25280 net.cpp:134]     with loss weight 1
I0428 19:43:10.991348 25280 net.cpp:139] Memory required for data: 1112324
I0428 19:43:10.991354 25280 net.cpp:200] loss needs backward computation.
I0428 19:43:10.991360 25280 net.cpp:200] relu3 needs backward computation.
I0428 19:43:10.991366 25280 net.cpp:200] ip3 needs backward computation.
I0428 19:43:10.991371 25280 net.cpp:200] relu2 needs backward computation.
I0428 19:43:10.991376 25280 net.cpp:200] ip2 needs backward computation.
I0428 19:43:10.991381 25280 net.cpp:200] relu1 needs backward computation.
I0428 19:43:10.991386 25280 net.cpp:200] ip1 needs backward computation.
I0428 19:43:10.991392 25280 net.cpp:200] pool1 needs backward computation.
I0428 19:43:10.991397 25280 net.cpp:200] conv1 needs backward computation.
I0428 19:43:10.991403 25280 net.cpp:200] pool0 needs backward computation.
I0428 19:43:10.991408 25280 net.cpp:200] conv0 needs backward computation.
I0428 19:43:10.991415 25280 net.cpp:202] mnist does not need backward computation.
I0428 19:43:10.991423 25280 net.cpp:244] This network produces output loss
I0428 19:43:10.991438 25280 net.cpp:257] Network initialization done.
I0428 19:43:10.991799 25280 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test414.prototxt
I0428 19:43:10.991832 25280 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:43:10.991930 25280 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:43:10.992064 25280 layer_factory.hpp:77] Creating layer mnist
I0428 19:43:10.992122 25280 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:43:10.992141 25280 net.cpp:86] Creating Layer mnist
I0428 19:43:10.992148 25280 net.cpp:382] mnist -> data
I0428 19:43:10.992161 25280 net.cpp:382] mnist -> label
I0428 19:43:10.992292 25280 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:43:10.994657 25280 net.cpp:124] Setting up mnist
I0428 19:43:10.994673 25280 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:43:10.994679 25280 net.cpp:131] Top shape: 100 (100)
I0428 19:43:10.994699 25280 net.cpp:139] Memory required for data: 314000
I0428 19:43:10.994704 25280 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:43:10.994720 25280 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:43:10.994726 25280 net.cpp:408] label_mnist_1_split <- label
I0428 19:43:10.994735 25280 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:43:10.994746 25280 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:43:10.994858 25280 net.cpp:124] Setting up label_mnist_1_split
I0428 19:43:10.994868 25280 net.cpp:131] Top shape: 100 (100)
I0428 19:43:10.994874 25280 net.cpp:131] Top shape: 100 (100)
I0428 19:43:10.994879 25280 net.cpp:139] Memory required for data: 314800
I0428 19:43:10.994884 25280 layer_factory.hpp:77] Creating layer conv0
I0428 19:43:10.994899 25280 net.cpp:86] Creating Layer conv0
I0428 19:43:10.994904 25280 net.cpp:408] conv0 <- data
I0428 19:43:10.994915 25280 net.cpp:382] conv0 -> conv0
I0428 19:43:10.996480 25280 net.cpp:124] Setting up conv0
I0428 19:43:10.996496 25280 net.cpp:131] Top shape: 100 2 24 24 (115200)
I0428 19:43:10.996502 25280 net.cpp:139] Memory required for data: 775600
I0428 19:43:10.996531 25280 layer_factory.hpp:77] Creating layer pool0
I0428 19:43:10.996541 25280 net.cpp:86] Creating Layer pool0
I0428 19:43:10.996547 25280 net.cpp:408] pool0 <- conv0
I0428 19:43:10.996556 25280 net.cpp:382] pool0 -> pool0
I0428 19:43:10.996600 25280 net.cpp:124] Setting up pool0
I0428 19:43:10.996608 25280 net.cpp:131] Top shape: 100 2 12 12 (28800)
I0428 19:43:10.996613 25280 net.cpp:139] Memory required for data: 890800
I0428 19:43:10.996618 25280 layer_factory.hpp:77] Creating layer conv1
I0428 19:43:10.996632 25280 net.cpp:86] Creating Layer conv1
I0428 19:43:10.996640 25280 net.cpp:408] conv1 <- pool0
I0428 19:43:10.996649 25280 net.cpp:382] conv1 -> conv1
I0428 19:43:10.998325 25280 net.cpp:124] Setting up conv1
I0428 19:43:10.998340 25280 net.cpp:131] Top shape: 100 25 8 8 (160000)
I0428 19:43:10.998347 25280 net.cpp:139] Memory required for data: 1530800
I0428 19:43:10.998364 25280 layer_factory.hpp:77] Creating layer pool1
I0428 19:43:10.998375 25280 net.cpp:86] Creating Layer pool1
I0428 19:43:10.998381 25280 net.cpp:408] pool1 <- conv1
I0428 19:43:10.998389 25280 net.cpp:382] pool1 -> pool1
I0428 19:43:10.998435 25280 net.cpp:124] Setting up pool1
I0428 19:43:10.998443 25280 net.cpp:131] Top shape: 100 25 4 4 (40000)
I0428 19:43:10.998450 25280 net.cpp:139] Memory required for data: 1690800
I0428 19:43:10.998456 25280 layer_factory.hpp:77] Creating layer ip1
I0428 19:43:10.998466 25280 net.cpp:86] Creating Layer ip1
I0428 19:43:10.998471 25280 net.cpp:408] ip1 <- pool1
I0428 19:43:10.998481 25280 net.cpp:382] ip1 -> ip1
I0428 19:43:10.998646 25280 net.cpp:124] Setting up ip1
I0428 19:43:10.998656 25280 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:43:10.998672 25280 net.cpp:139] Memory required for data: 1700800
I0428 19:43:10.998685 25280 layer_factory.hpp:77] Creating layer relu1
I0428 19:43:10.998697 25280 net.cpp:86] Creating Layer relu1
I0428 19:43:10.998703 25280 net.cpp:408] relu1 <- ip1
I0428 19:43:10.998710 25280 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:43:10.998950 25280 net.cpp:124] Setting up relu1
I0428 19:43:10.998960 25280 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:43:10.998965 25280 net.cpp:139] Memory required for data: 1710800
I0428 19:43:10.998970 25280 layer_factory.hpp:77] Creating layer ip2
I0428 19:43:10.998982 25280 net.cpp:86] Creating Layer ip2
I0428 19:43:10.998987 25280 net.cpp:408] ip2 <- ip1
I0428 19:43:10.998996 25280 net.cpp:382] ip2 -> ip2
I0428 19:43:10.999135 25280 net.cpp:124] Setting up ip2
I0428 19:43:10.999145 25280 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:43:10.999150 25280 net.cpp:139] Memory required for data: 1720800
I0428 19:43:10.999160 25280 layer_factory.hpp:77] Creating layer relu2
I0428 19:43:10.999168 25280 net.cpp:86] Creating Layer relu2
I0428 19:43:10.999176 25280 net.cpp:408] relu2 <- ip2
I0428 19:43:10.999182 25280 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:43:10.999372 25280 net.cpp:124] Setting up relu2
I0428 19:43:10.999395 25280 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:43:10.999400 25280 net.cpp:139] Memory required for data: 1730800
I0428 19:43:10.999405 25280 layer_factory.hpp:77] Creating layer ip3
I0428 19:43:10.999415 25280 net.cpp:86] Creating Layer ip3
I0428 19:43:10.999421 25280 net.cpp:408] ip3 <- ip2
I0428 19:43:10.999434 25280 net.cpp:382] ip3 -> ip3
I0428 19:43:10.999562 25280 net.cpp:124] Setting up ip3
I0428 19:43:10.999570 25280 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:43:10.999575 25280 net.cpp:139] Memory required for data: 1734800
I0428 19:43:10.999588 25280 layer_factory.hpp:77] Creating layer relu3
I0428 19:43:10.999595 25280 net.cpp:86] Creating Layer relu3
I0428 19:43:10.999603 25280 net.cpp:408] relu3 <- ip3
I0428 19:43:10.999614 25280 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:43:11.000478 25280 net.cpp:124] Setting up relu3
I0428 19:43:11.000491 25280 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:43:11.000496 25280 net.cpp:139] Memory required for data: 1738800
I0428 19:43:11.000501 25280 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:43:11.000509 25280 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:43:11.000514 25280 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:43:11.000524 25280 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:43:11.000533 25280 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:43:11.000577 25280 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:43:11.000586 25280 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:43:11.000592 25280 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:43:11.000600 25280 net.cpp:139] Memory required for data: 1746800
I0428 19:43:11.000605 25280 layer_factory.hpp:77] Creating layer accuracy
I0428 19:43:11.000615 25280 net.cpp:86] Creating Layer accuracy
I0428 19:43:11.000620 25280 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:43:11.000627 25280 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:43:11.000634 25280 net.cpp:382] accuracy -> accuracy
I0428 19:43:11.000645 25280 net.cpp:124] Setting up accuracy
I0428 19:43:11.000653 25280 net.cpp:131] Top shape: (1)
I0428 19:43:11.000658 25280 net.cpp:139] Memory required for data: 1746804
I0428 19:43:11.000663 25280 layer_factory.hpp:77] Creating layer loss
I0428 19:43:11.000671 25280 net.cpp:86] Creating Layer loss
I0428 19:43:11.000676 25280 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:43:11.000684 25280 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:43:11.000689 25280 net.cpp:382] loss -> loss
I0428 19:43:11.000700 25280 layer_factory.hpp:77] Creating layer loss
I0428 19:43:11.001006 25280 net.cpp:124] Setting up loss
I0428 19:43:11.001019 25280 net.cpp:131] Top shape: (1)
I0428 19:43:11.001024 25280 net.cpp:134]     with loss weight 1
I0428 19:43:11.001034 25280 net.cpp:139] Memory required for data: 1746808
I0428 19:43:11.001066 25280 net.cpp:200] loss needs backward computation.
I0428 19:43:11.001073 25280 net.cpp:202] accuracy does not need backward computation.
I0428 19:43:11.001083 25280 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:43:11.001090 25280 net.cpp:200] relu3 needs backward computation.
I0428 19:43:11.001093 25280 net.cpp:200] ip3 needs backward computation.
I0428 19:43:11.001099 25280 net.cpp:200] relu2 needs backward computation.
I0428 19:43:11.001103 25280 net.cpp:200] ip2 needs backward computation.
I0428 19:43:11.001108 25280 net.cpp:200] relu1 needs backward computation.
I0428 19:43:11.001127 25280 net.cpp:200] ip1 needs backward computation.
I0428 19:43:11.001132 25280 net.cpp:200] pool1 needs backward computation.
I0428 19:43:11.001137 25280 net.cpp:200] conv1 needs backward computation.
I0428 19:43:11.001144 25280 net.cpp:200] pool0 needs backward computation.
I0428 19:43:11.001149 25280 net.cpp:200] conv0 needs backward computation.
I0428 19:43:11.001155 25280 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:43:11.001161 25280 net.cpp:202] mnist does not need backward computation.
I0428 19:43:11.001166 25280 net.cpp:244] This network produces output accuracy
I0428 19:43:11.001171 25280 net.cpp:244] This network produces output loss
I0428 19:43:11.001190 25280 net.cpp:257] Network initialization done.
I0428 19:43:11.001238 25280 solver.cpp:56] Solver scaffolding done.
I0428 19:43:11.001605 25280 caffe.cpp:248] Starting Optimization
I0428 19:43:11.001612 25280 solver.cpp:273] Solving LeNet
I0428 19:43:11.001616 25280 solver.cpp:274] Learning Rate Policy: inv
I0428 19:43:11.002408 25280 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:43:11.006206 25280 blocking_queue.cpp:49] Waiting for data
I0428 19:43:11.064088 25287 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:43:11.064534 25280 solver.cpp:398]     Test net output #0: accuracy = 0.1009
I0428 19:43:11.064573 25280 solver.cpp:398]     Test net output #1: loss = 2.30687 (* 1 = 2.30687 loss)
I0428 19:43:11.065959 25280 solver.cpp:219] Iteration 0 (0 iter/s, 0.0643133s/100 iters), loss = 2.29423
I0428 19:43:11.065986 25280 solver.cpp:238]     Train net output #0: loss = 2.29423 (* 1 = 2.29423 loss)
I0428 19:43:11.066004 25280 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:43:11.153321 25280 solver.cpp:219] Iteration 100 (1145.15 iter/s, 0.0873247s/100 iters), loss = 0.818119
I0428 19:43:11.153344 25280 solver.cpp:238]     Train net output #0: loss = 0.818119 (* 1 = 0.818119 loss)
I0428 19:43:11.153350 25280 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:43:11.240968 25280 solver.cpp:219] Iteration 200 (1141.38 iter/s, 0.0876132s/100 iters), loss = 0.392594
I0428 19:43:11.240993 25280 solver.cpp:238]     Train net output #0: loss = 0.392594 (* 1 = 0.392594 loss)
I0428 19:43:11.240998 25280 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:43:11.328964 25280 solver.cpp:219] Iteration 300 (1137.05 iter/s, 0.0879471s/100 iters), loss = 0.537865
I0428 19:43:11.329004 25280 solver.cpp:238]     Train net output #0: loss = 0.537865 (* 1 = 0.537865 loss)
I0428 19:43:11.329025 25280 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:43:11.416772 25280 solver.cpp:219] Iteration 400 (1139.46 iter/s, 0.0877609s/100 iters), loss = 0.360268
I0428 19:43:11.416795 25280 solver.cpp:238]     Train net output #0: loss = 0.360268 (* 1 = 0.360268 loss)
I0428 19:43:11.416801 25280 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:43:11.504113 25280 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:43:11.577092 25287 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:43:11.577543 25280 solver.cpp:398]     Test net output #0: accuracy = 0.8583
I0428 19:43:11.577564 25280 solver.cpp:398]     Test net output #1: loss = 0.398183 (* 1 = 0.398183 loss)
I0428 19:43:11.578457 25280 solver.cpp:219] Iteration 500 (618.629 iter/s, 0.161648s/100 iters), loss = 0.400843
I0428 19:43:11.578519 25280 solver.cpp:238]     Train net output #0: loss = 0.400843 (* 1 = 0.400843 loss)
I0428 19:43:11.578557 25280 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:43:11.663341 25280 solver.cpp:219] Iteration 600 (1179.11 iter/s, 0.0848099s/100 iters), loss = 0.424844
I0428 19:43:11.663365 25280 solver.cpp:238]     Train net output #0: loss = 0.424844 (* 1 = 0.424844 loss)
I0428 19:43:11.663370 25280 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:43:11.751561 25280 solver.cpp:219] Iteration 700 (1133.96 iter/s, 0.0881868s/100 iters), loss = 0.582227
I0428 19:43:11.751583 25280 solver.cpp:238]     Train net output #0: loss = 0.582228 (* 1 = 0.582228 loss)
I0428 19:43:11.751590 25280 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:43:11.833698 25280 solver.cpp:219] Iteration 800 (1217.96 iter/s, 0.0821043s/100 iters), loss = 0.58587
I0428 19:43:11.833719 25280 solver.cpp:238]     Train net output #0: loss = 0.58587 (* 1 = 0.58587 loss)
I0428 19:43:11.833739 25280 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:43:11.912731 25280 solver.cpp:219] Iteration 900 (1265.78 iter/s, 0.0790026s/100 iters), loss = 0.537715
I0428 19:43:11.912755 25280 solver.cpp:238]     Train net output #0: loss = 0.537715 (* 1 = 0.537715 loss)
I0428 19:43:11.912760 25280 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:43:11.939690 25286 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:43:11.994684 25280 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:43:11.995597 25280 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:43:11.996186 25280 solver.cpp:311] Iteration 1000, loss = 0.430047
I0428 19:43:11.996201 25280 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:43:12.072924 25287 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:43:12.073357 25280 solver.cpp:398]     Test net output #0: accuracy = 0.8752
I0428 19:43:12.073376 25280 solver.cpp:398]     Test net output #1: loss = 0.333633 (* 1 = 0.333633 loss)
I0428 19:43:12.073381 25280 solver.cpp:316] Optimization Done.
I0428 19:43:12.073385 25280 caffe.cpp:259] Optimization Done.
