I0428 19:41:31.055786 24846 caffe.cpp:218] Using GPUs 0
I0428 19:41:31.097474 24846 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:41:31.633396 24846 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test367.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:41:31.633532 24846 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test367.prototxt
I0428 19:41:31.633934 24846 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:41:31.633950 24846 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:41:31.634047 24846 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:41:31.634120 24846 layer_factory.hpp:77] Creating layer mnist
I0428 19:41:31.634219 24846 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:41:31.634244 24846 net.cpp:86] Creating Layer mnist
I0428 19:41:31.634253 24846 net.cpp:382] mnist -> data
I0428 19:41:31.634277 24846 net.cpp:382] mnist -> label
I0428 19:41:31.635357 24846 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:41:31.637830 24846 net.cpp:124] Setting up mnist
I0428 19:41:31.637847 24846 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:41:31.637854 24846 net.cpp:131] Top shape: 64 (64)
I0428 19:41:31.637858 24846 net.cpp:139] Memory required for data: 200960
I0428 19:41:31.637866 24846 layer_factory.hpp:77] Creating layer conv0
I0428 19:41:31.637883 24846 net.cpp:86] Creating Layer conv0
I0428 19:41:31.637905 24846 net.cpp:408] conv0 <- data
I0428 19:41:31.637917 24846 net.cpp:382] conv0 -> conv0
I0428 19:41:31.929080 24846 net.cpp:124] Setting up conv0
I0428 19:41:31.929111 24846 net.cpp:131] Top shape: 64 2 24 24 (73728)
I0428 19:41:31.929116 24846 net.cpp:139] Memory required for data: 495872
I0428 19:41:31.929136 24846 layer_factory.hpp:77] Creating layer pool0
I0428 19:41:31.929150 24846 net.cpp:86] Creating Layer pool0
I0428 19:41:31.929158 24846 net.cpp:408] pool0 <- conv0
I0428 19:41:31.929165 24846 net.cpp:382] pool0 -> pool0
I0428 19:41:31.929220 24846 net.cpp:124] Setting up pool0
I0428 19:41:31.929227 24846 net.cpp:131] Top shape: 64 2 12 12 (18432)
I0428 19:41:31.929230 24846 net.cpp:139] Memory required for data: 569600
I0428 19:41:31.929234 24846 layer_factory.hpp:77] Creating layer conv1
I0428 19:41:31.929246 24846 net.cpp:86] Creating Layer conv1
I0428 19:41:31.929250 24846 net.cpp:408] conv1 <- pool0
I0428 19:41:31.929256 24846 net.cpp:382] conv1 -> conv1
I0428 19:41:31.931381 24846 net.cpp:124] Setting up conv1
I0428 19:41:31.931398 24846 net.cpp:131] Top shape: 64 10 8 8 (40960)
I0428 19:41:31.931402 24846 net.cpp:139] Memory required for data: 733440
I0428 19:41:31.931412 24846 layer_factory.hpp:77] Creating layer pool1
I0428 19:41:31.931421 24846 net.cpp:86] Creating Layer pool1
I0428 19:41:31.931426 24846 net.cpp:408] pool1 <- conv1
I0428 19:41:31.931433 24846 net.cpp:382] pool1 -> pool1
I0428 19:41:31.931478 24846 net.cpp:124] Setting up pool1
I0428 19:41:31.931485 24846 net.cpp:131] Top shape: 64 10 4 4 (10240)
I0428 19:41:31.931488 24846 net.cpp:139] Memory required for data: 774400
I0428 19:41:31.931493 24846 layer_factory.hpp:77] Creating layer ip1
I0428 19:41:31.931500 24846 net.cpp:86] Creating Layer ip1
I0428 19:41:31.931504 24846 net.cpp:408] ip1 <- pool1
I0428 19:41:31.931509 24846 net.cpp:382] ip1 -> ip1
I0428 19:41:31.932606 24846 net.cpp:124] Setting up ip1
I0428 19:41:31.932621 24846 net.cpp:131] Top shape: 64 10 (640)
I0428 19:41:31.932626 24846 net.cpp:139] Memory required for data: 776960
I0428 19:41:31.932636 24846 layer_factory.hpp:77] Creating layer relu1
I0428 19:41:31.932642 24846 net.cpp:86] Creating Layer relu1
I0428 19:41:31.932647 24846 net.cpp:408] relu1 <- ip1
I0428 19:41:31.932654 24846 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:41:31.932867 24846 net.cpp:124] Setting up relu1
I0428 19:41:31.932878 24846 net.cpp:131] Top shape: 64 10 (640)
I0428 19:41:31.932883 24846 net.cpp:139] Memory required for data: 779520
I0428 19:41:31.932886 24846 layer_factory.hpp:77] Creating layer ip2
I0428 19:41:31.932894 24846 net.cpp:86] Creating Layer ip2
I0428 19:41:31.932898 24846 net.cpp:408] ip2 <- ip1
I0428 19:41:31.932905 24846 net.cpp:382] ip2 -> ip2
I0428 19:41:31.933020 24846 net.cpp:124] Setting up ip2
I0428 19:41:31.933028 24846 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:41:31.933032 24846 net.cpp:139] Memory required for data: 785920
I0428 19:41:31.933038 24846 layer_factory.hpp:77] Creating layer relu2
I0428 19:41:31.933045 24846 net.cpp:86] Creating Layer relu2
I0428 19:41:31.933048 24846 net.cpp:408] relu2 <- ip2
I0428 19:41:31.933053 24846 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:41:31.933902 24846 net.cpp:124] Setting up relu2
I0428 19:41:31.933917 24846 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:41:31.933921 24846 net.cpp:139] Memory required for data: 792320
I0428 19:41:31.933925 24846 layer_factory.hpp:77] Creating layer ip3
I0428 19:41:31.933933 24846 net.cpp:86] Creating Layer ip3
I0428 19:41:31.933938 24846 net.cpp:408] ip3 <- ip2
I0428 19:41:31.933944 24846 net.cpp:382] ip3 -> ip3
I0428 19:41:31.934062 24846 net.cpp:124] Setting up ip3
I0428 19:41:31.934070 24846 net.cpp:131] Top shape: 64 10 (640)
I0428 19:41:31.934074 24846 net.cpp:139] Memory required for data: 794880
I0428 19:41:31.934084 24846 layer_factory.hpp:77] Creating layer relu3
I0428 19:41:31.934090 24846 net.cpp:86] Creating Layer relu3
I0428 19:41:31.934093 24846 net.cpp:408] relu3 <- ip3
I0428 19:41:31.934098 24846 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:41:31.934294 24846 net.cpp:124] Setting up relu3
I0428 19:41:31.934304 24846 net.cpp:131] Top shape: 64 10 (640)
I0428 19:41:31.934309 24846 net.cpp:139] Memory required for data: 797440
I0428 19:41:31.934311 24846 layer_factory.hpp:77] Creating layer loss
I0428 19:41:31.934319 24846 net.cpp:86] Creating Layer loss
I0428 19:41:31.934322 24846 net.cpp:408] loss <- ip3
I0428 19:41:31.934327 24846 net.cpp:408] loss <- label
I0428 19:41:31.934334 24846 net.cpp:382] loss -> loss
I0428 19:41:31.934350 24846 layer_factory.hpp:77] Creating layer loss
I0428 19:41:31.934622 24846 net.cpp:124] Setting up loss
I0428 19:41:31.934633 24846 net.cpp:131] Top shape: (1)
I0428 19:41:31.934635 24846 net.cpp:134]     with loss weight 1
I0428 19:41:31.934653 24846 net.cpp:139] Memory required for data: 797444
I0428 19:41:31.934656 24846 net.cpp:200] loss needs backward computation.
I0428 19:41:31.934660 24846 net.cpp:200] relu3 needs backward computation.
I0428 19:41:31.934664 24846 net.cpp:200] ip3 needs backward computation.
I0428 19:41:31.934667 24846 net.cpp:200] relu2 needs backward computation.
I0428 19:41:31.934671 24846 net.cpp:200] ip2 needs backward computation.
I0428 19:41:31.934675 24846 net.cpp:200] relu1 needs backward computation.
I0428 19:41:31.934679 24846 net.cpp:200] ip1 needs backward computation.
I0428 19:41:31.934681 24846 net.cpp:200] pool1 needs backward computation.
I0428 19:41:31.934685 24846 net.cpp:200] conv1 needs backward computation.
I0428 19:41:31.934689 24846 net.cpp:200] pool0 needs backward computation.
I0428 19:41:31.934692 24846 net.cpp:200] conv0 needs backward computation.
I0428 19:41:31.934696 24846 net.cpp:202] mnist does not need backward computation.
I0428 19:41:31.934700 24846 net.cpp:244] This network produces output loss
I0428 19:41:31.934710 24846 net.cpp:257] Network initialization done.
I0428 19:41:31.935092 24846 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test367.prototxt
I0428 19:41:31.935123 24846 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:41:31.935231 24846 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:41:31.935325 24846 layer_factory.hpp:77] Creating layer mnist
I0428 19:41:31.935375 24846 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:41:31.935394 24846 net.cpp:86] Creating Layer mnist
I0428 19:41:31.935398 24846 net.cpp:382] mnist -> data
I0428 19:41:31.935408 24846 net.cpp:382] mnist -> label
I0428 19:41:31.935508 24846 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:41:31.937719 24846 net.cpp:124] Setting up mnist
I0428 19:41:31.937736 24846 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:41:31.937742 24846 net.cpp:131] Top shape: 100 (100)
I0428 19:41:31.937747 24846 net.cpp:139] Memory required for data: 314000
I0428 19:41:31.937750 24846 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:41:31.937759 24846 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:41:31.937763 24846 net.cpp:408] label_mnist_1_split <- label
I0428 19:41:31.937768 24846 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:41:31.937777 24846 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:41:31.937840 24846 net.cpp:124] Setting up label_mnist_1_split
I0428 19:41:31.937846 24846 net.cpp:131] Top shape: 100 (100)
I0428 19:41:31.937851 24846 net.cpp:131] Top shape: 100 (100)
I0428 19:41:31.937855 24846 net.cpp:139] Memory required for data: 314800
I0428 19:41:31.937857 24846 layer_factory.hpp:77] Creating layer conv0
I0428 19:41:31.937867 24846 net.cpp:86] Creating Layer conv0
I0428 19:41:31.937872 24846 net.cpp:408] conv0 <- data
I0428 19:41:31.937877 24846 net.cpp:382] conv0 -> conv0
I0428 19:41:31.939826 24846 net.cpp:124] Setting up conv0
I0428 19:41:31.939842 24846 net.cpp:131] Top shape: 100 2 24 24 (115200)
I0428 19:41:31.939847 24846 net.cpp:139] Memory required for data: 775600
I0428 19:41:31.939858 24846 layer_factory.hpp:77] Creating layer pool0
I0428 19:41:31.939867 24846 net.cpp:86] Creating Layer pool0
I0428 19:41:31.939870 24846 net.cpp:408] pool0 <- conv0
I0428 19:41:31.939877 24846 net.cpp:382] pool0 -> pool0
I0428 19:41:31.939919 24846 net.cpp:124] Setting up pool0
I0428 19:41:31.939926 24846 net.cpp:131] Top shape: 100 2 12 12 (28800)
I0428 19:41:31.939929 24846 net.cpp:139] Memory required for data: 890800
I0428 19:41:31.939932 24846 layer_factory.hpp:77] Creating layer conv1
I0428 19:41:31.939942 24846 net.cpp:86] Creating Layer conv1
I0428 19:41:31.939946 24846 net.cpp:408] conv1 <- pool0
I0428 19:41:31.939952 24846 net.cpp:382] conv1 -> conv1
I0428 19:41:31.941651 24846 net.cpp:124] Setting up conv1
I0428 19:41:31.941669 24846 net.cpp:131] Top shape: 100 10 8 8 (64000)
I0428 19:41:31.941673 24846 net.cpp:139] Memory required for data: 1146800
I0428 19:41:31.941692 24846 layer_factory.hpp:77] Creating layer pool1
I0428 19:41:31.941700 24846 net.cpp:86] Creating Layer pool1
I0428 19:41:31.941705 24846 net.cpp:408] pool1 <- conv1
I0428 19:41:31.941711 24846 net.cpp:382] pool1 -> pool1
I0428 19:41:31.941756 24846 net.cpp:124] Setting up pool1
I0428 19:41:31.941774 24846 net.cpp:131] Top shape: 100 10 4 4 (16000)
I0428 19:41:31.941786 24846 net.cpp:139] Memory required for data: 1210800
I0428 19:41:31.941789 24846 layer_factory.hpp:77] Creating layer ip1
I0428 19:41:31.941797 24846 net.cpp:86] Creating Layer ip1
I0428 19:41:31.941802 24846 net.cpp:408] ip1 <- pool1
I0428 19:41:31.941807 24846 net.cpp:382] ip1 -> ip1
I0428 19:41:31.941936 24846 net.cpp:124] Setting up ip1
I0428 19:41:31.941946 24846 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:41:31.941962 24846 net.cpp:139] Memory required for data: 1214800
I0428 19:41:31.941969 24846 layer_factory.hpp:77] Creating layer relu1
I0428 19:41:31.941975 24846 net.cpp:86] Creating Layer relu1
I0428 19:41:31.941979 24846 net.cpp:408] relu1 <- ip1
I0428 19:41:31.941984 24846 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:41:31.942175 24846 net.cpp:124] Setting up relu1
I0428 19:41:31.942186 24846 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:41:31.942190 24846 net.cpp:139] Memory required for data: 1218800
I0428 19:41:31.942194 24846 layer_factory.hpp:77] Creating layer ip2
I0428 19:41:31.942203 24846 net.cpp:86] Creating Layer ip2
I0428 19:41:31.942206 24846 net.cpp:408] ip2 <- ip1
I0428 19:41:31.942212 24846 net.cpp:382] ip2 -> ip2
I0428 19:41:31.942324 24846 net.cpp:124] Setting up ip2
I0428 19:41:31.942332 24846 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:41:31.942337 24846 net.cpp:139] Memory required for data: 1228800
I0428 19:41:31.942343 24846 layer_factory.hpp:77] Creating layer relu2
I0428 19:41:31.942348 24846 net.cpp:86] Creating Layer relu2
I0428 19:41:31.942353 24846 net.cpp:408] relu2 <- ip2
I0428 19:41:31.942358 24846 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:41:31.942533 24846 net.cpp:124] Setting up relu2
I0428 19:41:31.942543 24846 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:41:31.942545 24846 net.cpp:139] Memory required for data: 1238800
I0428 19:41:31.942549 24846 layer_factory.hpp:77] Creating layer ip3
I0428 19:41:31.942555 24846 net.cpp:86] Creating Layer ip3
I0428 19:41:31.942559 24846 net.cpp:408] ip3 <- ip2
I0428 19:41:31.942566 24846 net.cpp:382] ip3 -> ip3
I0428 19:41:31.942699 24846 net.cpp:124] Setting up ip3
I0428 19:41:31.942708 24846 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:41:31.942710 24846 net.cpp:139] Memory required for data: 1242800
I0428 19:41:31.942719 24846 layer_factory.hpp:77] Creating layer relu3
I0428 19:41:31.942726 24846 net.cpp:86] Creating Layer relu3
I0428 19:41:31.942730 24846 net.cpp:408] relu3 <- ip3
I0428 19:41:31.942734 24846 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:41:31.943696 24846 net.cpp:124] Setting up relu3
I0428 19:41:31.943712 24846 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:41:31.943717 24846 net.cpp:139] Memory required for data: 1246800
I0428 19:41:31.943720 24846 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:41:31.943727 24846 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:41:31.943730 24846 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:41:31.943737 24846 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:41:31.943744 24846 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:41:31.943792 24846 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:41:31.943799 24846 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:41:31.943802 24846 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:41:31.943806 24846 net.cpp:139] Memory required for data: 1254800
I0428 19:41:31.943809 24846 layer_factory.hpp:77] Creating layer accuracy
I0428 19:41:31.943815 24846 net.cpp:86] Creating Layer accuracy
I0428 19:41:31.943819 24846 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:41:31.943823 24846 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:41:31.943830 24846 net.cpp:382] accuracy -> accuracy
I0428 19:41:31.943838 24846 net.cpp:124] Setting up accuracy
I0428 19:41:31.943842 24846 net.cpp:131] Top shape: (1)
I0428 19:41:31.943845 24846 net.cpp:139] Memory required for data: 1254804
I0428 19:41:31.943850 24846 layer_factory.hpp:77] Creating layer loss
I0428 19:41:31.943856 24846 net.cpp:86] Creating Layer loss
I0428 19:41:31.943859 24846 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:41:31.943864 24846 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:41:31.943877 24846 net.cpp:382] loss -> loss
I0428 19:41:31.943884 24846 layer_factory.hpp:77] Creating layer loss
I0428 19:41:31.944174 24846 net.cpp:124] Setting up loss
I0428 19:41:31.944185 24846 net.cpp:131] Top shape: (1)
I0428 19:41:31.944190 24846 net.cpp:134]     with loss weight 1
I0428 19:41:31.944196 24846 net.cpp:139] Memory required for data: 1254808
I0428 19:41:31.944211 24846 net.cpp:200] loss needs backward computation.
I0428 19:41:31.944224 24846 net.cpp:202] accuracy does not need backward computation.
I0428 19:41:31.944229 24846 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:41:31.944232 24846 net.cpp:200] relu3 needs backward computation.
I0428 19:41:31.944236 24846 net.cpp:200] ip3 needs backward computation.
I0428 19:41:31.944239 24846 net.cpp:200] relu2 needs backward computation.
I0428 19:41:31.944242 24846 net.cpp:200] ip2 needs backward computation.
I0428 19:41:31.944245 24846 net.cpp:200] relu1 needs backward computation.
I0428 19:41:31.944249 24846 net.cpp:200] ip1 needs backward computation.
I0428 19:41:31.944259 24846 net.cpp:200] pool1 needs backward computation.
I0428 19:41:31.944262 24846 net.cpp:200] conv1 needs backward computation.
I0428 19:41:31.944267 24846 net.cpp:200] pool0 needs backward computation.
I0428 19:41:31.944270 24846 net.cpp:200] conv0 needs backward computation.
I0428 19:41:31.944275 24846 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:41:31.944279 24846 net.cpp:202] mnist does not need backward computation.
I0428 19:41:31.944283 24846 net.cpp:244] This network produces output accuracy
I0428 19:41:31.944286 24846 net.cpp:244] This network produces output loss
I0428 19:41:31.944300 24846 net.cpp:257] Network initialization done.
I0428 19:41:31.944358 24846 solver.cpp:56] Solver scaffolding done.
I0428 19:41:31.944753 24846 caffe.cpp:248] Starting Optimization
I0428 19:41:31.944761 24846 solver.cpp:273] Solving LeNet
I0428 19:41:31.944763 24846 solver.cpp:274] Learning Rate Policy: inv
I0428 19:41:31.945758 24846 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:41:31.948232 24846 blocking_queue.cpp:49] Waiting for data
I0428 19:41:32.021641 24853 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:41:32.022136 24846 solver.cpp:398]     Test net output #0: accuracy = 0.1065
I0428 19:41:32.022161 24846 solver.cpp:398]     Test net output #1: loss = 2.2994 (* 1 = 2.2994 loss)
I0428 19:41:32.024230 24846 solver.cpp:219] Iteration 0 (0 iter/s, 0.0794225s/100 iters), loss = 2.29441
I0428 19:41:32.024261 24846 solver.cpp:238]     Train net output #0: loss = 2.29441 (* 1 = 2.29441 loss)
I0428 19:41:32.024281 24846 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:41:32.103698 24846 solver.cpp:219] Iteration 100 (1259.06 iter/s, 0.0794245s/100 iters), loss = 1.0819
I0428 19:41:32.103729 24846 solver.cpp:238]     Train net output #0: loss = 1.0819 (* 1 = 1.0819 loss)
I0428 19:41:32.103736 24846 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:41:32.182199 24846 solver.cpp:219] Iteration 200 (1274.54 iter/s, 0.0784599s/100 iters), loss = 0.666415
I0428 19:41:32.182229 24846 solver.cpp:238]     Train net output #0: loss = 0.666415 (* 1 = 0.666415 loss)
I0428 19:41:32.182236 24846 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:41:32.265159 24846 solver.cpp:219] Iteration 300 (1206.11 iter/s, 0.0829112s/100 iters), loss = 0.505495
I0428 19:41:32.265211 24846 solver.cpp:238]     Train net output #0: loss = 0.505495 (* 1 = 0.505495 loss)
I0428 19:41:32.265225 24846 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:41:32.357041 24846 solver.cpp:219] Iteration 400 (1089.05 iter/s, 0.0918233s/100 iters), loss = 0.677243
I0428 19:41:32.357072 24846 solver.cpp:238]     Train net output #0: loss = 0.677243 (* 1 = 0.677243 loss)
I0428 19:41:32.357080 24846 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:41:32.433120 24846 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:41:32.485363 24853 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:41:32.485831 24846 solver.cpp:398]     Test net output #0: accuracy = 0.8344
I0428 19:41:32.485854 24846 solver.cpp:398]     Test net output #1: loss = 0.477315 (* 1 = 0.477315 loss)
I0428 19:41:32.486677 24846 solver.cpp:219] Iteration 500 (771.675 iter/s, 0.129588s/100 iters), loss = 0.650414
I0428 19:41:32.486704 24846 solver.cpp:238]     Train net output #0: loss = 0.650414 (* 1 = 0.650414 loss)
I0428 19:41:32.486732 24846 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:41:32.565548 24846 solver.cpp:219] Iteration 600 (1268.54 iter/s, 0.0788306s/100 iters), loss = 0.269331
I0428 19:41:32.565579 24846 solver.cpp:238]     Train net output #0: loss = 0.269331 (* 1 = 0.269331 loss)
I0428 19:41:32.565587 24846 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:41:32.644080 24846 solver.cpp:219] Iteration 700 (1274.07 iter/s, 0.0784886s/100 iters), loss = 0.433219
I0428 19:41:32.644111 24846 solver.cpp:238]     Train net output #0: loss = 0.433219 (* 1 = 0.433219 loss)
I0428 19:41:32.644120 24846 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:41:32.724112 24846 solver.cpp:219] Iteration 800 (1250.19 iter/s, 0.0799877s/100 iters), loss = 0.522667
I0428 19:41:32.724141 24846 solver.cpp:238]     Train net output #0: loss = 0.522667 (* 1 = 0.522667 loss)
I0428 19:41:32.724151 24846 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:41:32.803221 24846 solver.cpp:219] Iteration 900 (1264.83 iter/s, 0.079062s/100 iters), loss = 0.279623
I0428 19:41:32.803252 24846 solver.cpp:238]     Train net output #0: loss = 0.279623 (* 1 = 0.279623 loss)
I0428 19:41:32.803264 24846 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:41:32.830174 24852 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:41:32.882411 24846 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:41:32.883128 24846 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:41:32.883581 24846 solver.cpp:311] Iteration 1000, loss = 0.374916
I0428 19:41:32.883600 24846 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:41:32.958070 24853 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:41:32.958540 24846 solver.cpp:398]     Test net output #0: accuracy = 0.857
I0428 19:41:32.958564 24846 solver.cpp:398]     Test net output #1: loss = 0.400173 (* 1 = 0.400173 loss)
I0428 19:41:32.958571 24846 solver.cpp:316] Optimization Done.
I0428 19:41:32.958575 24846 caffe.cpp:259] Optimization Done.
