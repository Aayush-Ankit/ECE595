I0428 19:30:25.125705 22284 caffe.cpp:218] Using GPUs 0
I0428 19:30:25.164433 22284 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:30:25.680979 22284 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test90.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:30:25.681149 22284 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test90.prototxt
I0428 19:30:25.681516 22284 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:30:25.681537 22284 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:30:25.681630 22284 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool0"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:30:25.681727 22284 layer_factory.hpp:77] Creating layer mnist
I0428 19:30:25.681869 22284 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:30:25.681902 22284 net.cpp:86] Creating Layer mnist
I0428 19:30:25.681915 22284 net.cpp:382] mnist -> data
I0428 19:30:25.681946 22284 net.cpp:382] mnist -> label
I0428 19:30:25.683181 22284 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:30:25.685665 22284 net.cpp:124] Setting up mnist
I0428 19:30:25.685685 22284 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:30:25.685695 22284 net.cpp:131] Top shape: 64 (64)
I0428 19:30:25.685703 22284 net.cpp:139] Memory required for data: 200960
I0428 19:30:25.685714 22284 layer_factory.hpp:77] Creating layer conv0
I0428 19:30:25.685736 22284 net.cpp:86] Creating Layer conv0
I0428 19:30:25.685746 22284 net.cpp:408] conv0 <- data
I0428 19:30:25.685763 22284 net.cpp:382] conv0 -> conv0
I0428 19:30:25.975080 22284 net.cpp:124] Setting up conv0
I0428 19:30:25.975113 22284 net.cpp:131] Top shape: 64 5 24 24 (184320)
I0428 19:30:25.975121 22284 net.cpp:139] Memory required for data: 938240
I0428 19:30:25.975143 22284 layer_factory.hpp:77] Creating layer pool0
I0428 19:30:25.975163 22284 net.cpp:86] Creating Layer pool0
I0428 19:30:25.975195 22284 net.cpp:408] pool0 <- conv0
I0428 19:30:25.975208 22284 net.cpp:382] pool0 -> pool0
I0428 19:30:25.975275 22284 net.cpp:124] Setting up pool0
I0428 19:30:25.975286 22284 net.cpp:131] Top shape: 64 5 12 12 (46080)
I0428 19:30:25.975291 22284 net.cpp:139] Memory required for data: 1122560
I0428 19:30:25.975298 22284 layer_factory.hpp:77] Creating layer ip1
I0428 19:30:25.975311 22284 net.cpp:86] Creating Layer ip1
I0428 19:30:25.975318 22284 net.cpp:408] ip1 <- pool0
I0428 19:30:25.975327 22284 net.cpp:382] ip1 -> ip1
I0428 19:30:25.976444 22284 net.cpp:124] Setting up ip1
I0428 19:30:25.976461 22284 net.cpp:131] Top shape: 64 10 (640)
I0428 19:30:25.976467 22284 net.cpp:139] Memory required for data: 1125120
I0428 19:30:25.976483 22284 layer_factory.hpp:77] Creating layer relu1
I0428 19:30:25.976496 22284 net.cpp:86] Creating Layer relu1
I0428 19:30:25.976502 22284 net.cpp:408] relu1 <- ip1
I0428 19:30:25.976512 22284 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:30:25.976714 22284 net.cpp:124] Setting up relu1
I0428 19:30:25.976727 22284 net.cpp:131] Top shape: 64 10 (640)
I0428 19:30:25.976732 22284 net.cpp:139] Memory required for data: 1127680
I0428 19:30:25.976738 22284 layer_factory.hpp:77] Creating layer ip2
I0428 19:30:25.976750 22284 net.cpp:86] Creating Layer ip2
I0428 19:30:25.976757 22284 net.cpp:408] ip2 <- ip1
I0428 19:30:25.976768 22284 net.cpp:382] ip2 -> ip2
I0428 19:30:25.976891 22284 net.cpp:124] Setting up ip2
I0428 19:30:25.976903 22284 net.cpp:131] Top shape: 64 10 (640)
I0428 19:30:25.976909 22284 net.cpp:139] Memory required for data: 1130240
I0428 19:30:25.976924 22284 layer_factory.hpp:77] Creating layer relu2
I0428 19:30:25.976940 22284 net.cpp:86] Creating Layer relu2
I0428 19:30:25.976951 22284 net.cpp:408] relu2 <- ip2
I0428 19:30:25.976960 22284 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:30:25.977794 22284 net.cpp:124] Setting up relu2
I0428 19:30:25.977810 22284 net.cpp:131] Top shape: 64 10 (640)
I0428 19:30:25.977818 22284 net.cpp:139] Memory required for data: 1132800
I0428 19:30:25.977823 22284 layer_factory.hpp:77] Creating layer ip3
I0428 19:30:25.977835 22284 net.cpp:86] Creating Layer ip3
I0428 19:30:25.977843 22284 net.cpp:408] ip3 <- ip2
I0428 19:30:25.977852 22284 net.cpp:382] ip3 -> ip3
I0428 19:30:25.977974 22284 net.cpp:124] Setting up ip3
I0428 19:30:25.977985 22284 net.cpp:131] Top shape: 64 10 (640)
I0428 19:30:25.977991 22284 net.cpp:139] Memory required for data: 1135360
I0428 19:30:25.978003 22284 layer_factory.hpp:77] Creating layer relu3
I0428 19:30:25.978013 22284 net.cpp:86] Creating Layer relu3
I0428 19:30:25.978021 22284 net.cpp:408] relu3 <- ip3
I0428 19:30:25.978030 22284 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:30:25.978211 22284 net.cpp:124] Setting up relu3
I0428 19:30:25.978224 22284 net.cpp:131] Top shape: 64 10 (640)
I0428 19:30:25.978229 22284 net.cpp:139] Memory required for data: 1137920
I0428 19:30:25.978235 22284 layer_factory.hpp:77] Creating layer loss
I0428 19:30:25.978245 22284 net.cpp:86] Creating Layer loss
I0428 19:30:25.978250 22284 net.cpp:408] loss <- ip3
I0428 19:30:25.978258 22284 net.cpp:408] loss <- label
I0428 19:30:25.978268 22284 net.cpp:382] loss -> loss
I0428 19:30:25.978296 22284 layer_factory.hpp:77] Creating layer loss
I0428 19:30:25.978566 22284 net.cpp:124] Setting up loss
I0428 19:30:25.978579 22284 net.cpp:131] Top shape: (1)
I0428 19:30:25.978585 22284 net.cpp:134]     with loss weight 1
I0428 19:30:25.978608 22284 net.cpp:139] Memory required for data: 1137924
I0428 19:30:25.978615 22284 net.cpp:200] loss needs backward computation.
I0428 19:30:25.978622 22284 net.cpp:200] relu3 needs backward computation.
I0428 19:30:25.978628 22284 net.cpp:200] ip3 needs backward computation.
I0428 19:30:25.978634 22284 net.cpp:200] relu2 needs backward computation.
I0428 19:30:25.978639 22284 net.cpp:200] ip2 needs backward computation.
I0428 19:30:25.978646 22284 net.cpp:200] relu1 needs backward computation.
I0428 19:30:25.978652 22284 net.cpp:200] ip1 needs backward computation.
I0428 19:30:25.978673 22284 net.cpp:200] pool0 needs backward computation.
I0428 19:30:25.978680 22284 net.cpp:200] conv0 needs backward computation.
I0428 19:30:25.978687 22284 net.cpp:202] mnist does not need backward computation.
I0428 19:30:25.978693 22284 net.cpp:244] This network produces output loss
I0428 19:30:25.978708 22284 net.cpp:257] Network initialization done.
I0428 19:30:25.979053 22284 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test90.prototxt
I0428 19:30:25.979089 22284 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:30:25.979189 22284 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool0"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:30:25.979296 22284 layer_factory.hpp:77] Creating layer mnist
I0428 19:30:25.979364 22284 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:30:25.979383 22284 net.cpp:86] Creating Layer mnist
I0428 19:30:25.979393 22284 net.cpp:382] mnist -> data
I0428 19:30:25.979406 22284 net.cpp:382] mnist -> label
I0428 19:30:25.979542 22284 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:30:25.981788 22284 net.cpp:124] Setting up mnist
I0428 19:30:25.981806 22284 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:30:25.981814 22284 net.cpp:131] Top shape: 100 (100)
I0428 19:30:25.981822 22284 net.cpp:139] Memory required for data: 314000
I0428 19:30:25.981827 22284 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:30:25.981839 22284 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:30:25.981845 22284 net.cpp:408] label_mnist_1_split <- label
I0428 19:30:25.981855 22284 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:30:25.981868 22284 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:30:25.981923 22284 net.cpp:124] Setting up label_mnist_1_split
I0428 19:30:25.981933 22284 net.cpp:131] Top shape: 100 (100)
I0428 19:30:25.981940 22284 net.cpp:131] Top shape: 100 (100)
I0428 19:30:25.981946 22284 net.cpp:139] Memory required for data: 314800
I0428 19:30:25.981966 22284 layer_factory.hpp:77] Creating layer conv0
I0428 19:30:25.981983 22284 net.cpp:86] Creating Layer conv0
I0428 19:30:25.981992 22284 net.cpp:408] conv0 <- data
I0428 19:30:25.982002 22284 net.cpp:382] conv0 -> conv0
I0428 19:30:25.983685 22284 net.cpp:124] Setting up conv0
I0428 19:30:25.983705 22284 net.cpp:131] Top shape: 100 5 24 24 (288000)
I0428 19:30:25.983713 22284 net.cpp:139] Memory required for data: 1466800
I0428 19:30:25.983731 22284 layer_factory.hpp:77] Creating layer pool0
I0428 19:30:25.983742 22284 net.cpp:86] Creating Layer pool0
I0428 19:30:25.983752 22284 net.cpp:408] pool0 <- conv0
I0428 19:30:25.983764 22284 net.cpp:382] pool0 -> pool0
I0428 19:30:25.983816 22284 net.cpp:124] Setting up pool0
I0428 19:30:25.983826 22284 net.cpp:131] Top shape: 100 5 12 12 (72000)
I0428 19:30:25.983832 22284 net.cpp:139] Memory required for data: 1754800
I0428 19:30:25.983841 22284 layer_factory.hpp:77] Creating layer ip1
I0428 19:30:25.983855 22284 net.cpp:86] Creating Layer ip1
I0428 19:30:25.983866 22284 net.cpp:408] ip1 <- pool0
I0428 19:30:25.983877 22284 net.cpp:382] ip1 -> ip1
I0428 19:30:25.984058 22284 net.cpp:124] Setting up ip1
I0428 19:30:25.984068 22284 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:30:25.984074 22284 net.cpp:139] Memory required for data: 1758800
I0428 19:30:25.984088 22284 layer_factory.hpp:77] Creating layer relu1
I0428 19:30:25.984100 22284 net.cpp:86] Creating Layer relu1
I0428 19:30:25.984107 22284 net.cpp:408] relu1 <- ip1
I0428 19:30:25.984115 22284 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:30:25.984364 22284 net.cpp:124] Setting up relu1
I0428 19:30:25.984377 22284 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:30:25.984383 22284 net.cpp:139] Memory required for data: 1762800
I0428 19:30:25.984390 22284 layer_factory.hpp:77] Creating layer ip2
I0428 19:30:25.984400 22284 net.cpp:86] Creating Layer ip2
I0428 19:30:25.984407 22284 net.cpp:408] ip2 <- ip1
I0428 19:30:25.984419 22284 net.cpp:382] ip2 -> ip2
I0428 19:30:25.984549 22284 net.cpp:124] Setting up ip2
I0428 19:30:25.984561 22284 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:30:25.984567 22284 net.cpp:139] Memory required for data: 1766800
I0428 19:30:25.984582 22284 layer_factory.hpp:77] Creating layer relu2
I0428 19:30:25.984593 22284 net.cpp:86] Creating Layer relu2
I0428 19:30:25.984601 22284 net.cpp:408] relu2 <- ip2
I0428 19:30:25.984611 22284 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:30:25.985496 22284 net.cpp:124] Setting up relu2
I0428 19:30:25.985512 22284 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:30:25.985517 22284 net.cpp:139] Memory required for data: 1770800
I0428 19:30:25.985524 22284 layer_factory.hpp:77] Creating layer ip3
I0428 19:30:25.985538 22284 net.cpp:86] Creating Layer ip3
I0428 19:30:25.985544 22284 net.cpp:408] ip3 <- ip2
I0428 19:30:25.985556 22284 net.cpp:382] ip3 -> ip3
I0428 19:30:25.985698 22284 net.cpp:124] Setting up ip3
I0428 19:30:25.985713 22284 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:30:25.985720 22284 net.cpp:139] Memory required for data: 1774800
I0428 19:30:25.985731 22284 layer_factory.hpp:77] Creating layer relu3
I0428 19:30:25.985743 22284 net.cpp:86] Creating Layer relu3
I0428 19:30:25.985751 22284 net.cpp:408] relu3 <- ip3
I0428 19:30:25.985759 22284 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:30:25.985952 22284 net.cpp:124] Setting up relu3
I0428 19:30:25.985963 22284 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:30:25.985970 22284 net.cpp:139] Memory required for data: 1778800
I0428 19:30:25.985978 22284 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:30:25.985990 22284 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:30:25.985998 22284 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:30:25.986008 22284 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:30:25.986019 22284 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:30:25.986083 22284 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:30:25.986091 22284 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:30:25.986099 22284 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:30:25.986119 22284 net.cpp:139] Memory required for data: 1786800
I0428 19:30:25.986125 22284 layer_factory.hpp:77] Creating layer accuracy
I0428 19:30:25.986140 22284 net.cpp:86] Creating Layer accuracy
I0428 19:30:25.986147 22284 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:30:25.986155 22284 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:30:25.986166 22284 net.cpp:382] accuracy -> accuracy
I0428 19:30:25.986181 22284 net.cpp:124] Setting up accuracy
I0428 19:30:25.986189 22284 net.cpp:131] Top shape: (1)
I0428 19:30:25.986196 22284 net.cpp:139] Memory required for data: 1786804
I0428 19:30:25.986202 22284 layer_factory.hpp:77] Creating layer loss
I0428 19:30:25.986212 22284 net.cpp:86] Creating Layer loss
I0428 19:30:25.986217 22284 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:30:25.986228 22284 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:30:25.986238 22284 net.cpp:382] loss -> loss
I0428 19:30:25.986250 22284 layer_factory.hpp:77] Creating layer loss
I0428 19:30:25.986541 22284 net.cpp:124] Setting up loss
I0428 19:30:25.986553 22284 net.cpp:131] Top shape: (1)
I0428 19:30:25.986562 22284 net.cpp:134]     with loss weight 1
I0428 19:30:25.986573 22284 net.cpp:139] Memory required for data: 1786808
I0428 19:30:25.986580 22284 net.cpp:200] loss needs backward computation.
I0428 19:30:25.986588 22284 net.cpp:202] accuracy does not need backward computation.
I0428 19:30:25.986598 22284 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:30:25.986603 22284 net.cpp:200] relu3 needs backward computation.
I0428 19:30:25.986609 22284 net.cpp:200] ip3 needs backward computation.
I0428 19:30:25.986614 22284 net.cpp:200] relu2 needs backward computation.
I0428 19:30:25.986620 22284 net.cpp:200] ip2 needs backward computation.
I0428 19:30:25.986625 22284 net.cpp:200] relu1 needs backward computation.
I0428 19:30:25.986631 22284 net.cpp:200] ip1 needs backward computation.
I0428 19:30:25.986637 22284 net.cpp:200] pool0 needs backward computation.
I0428 19:30:25.986644 22284 net.cpp:200] conv0 needs backward computation.
I0428 19:30:25.986649 22284 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:30:25.986657 22284 net.cpp:202] mnist does not need backward computation.
I0428 19:30:25.986662 22284 net.cpp:244] This network produces output accuracy
I0428 19:30:25.986668 22284 net.cpp:244] This network produces output loss
I0428 19:30:25.986687 22284 net.cpp:257] Network initialization done.
I0428 19:30:25.986737 22284 solver.cpp:56] Solver scaffolding done.
I0428 19:30:25.987049 22284 caffe.cpp:248] Starting Optimization
I0428 19:30:25.987057 22284 solver.cpp:273] Solving LeNet
I0428 19:30:25.987062 22284 solver.cpp:274] Learning Rate Policy: inv
I0428 19:30:25.987221 22284 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:30:25.991704 22284 blocking_queue.cpp:49] Waiting for data
I0428 19:30:26.041746 22291 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:30:26.042160 22284 solver.cpp:398]     Test net output #0: accuracy = 0.093
I0428 19:30:26.042179 22284 solver.cpp:398]     Test net output #1: loss = 2.31446 (* 1 = 2.31446 loss)
I0428 19:30:26.043740 22284 solver.cpp:219] Iteration 0 (0 iter/s, 0.0566456s/100 iters), loss = 2.30884
I0428 19:30:26.043778 22284 solver.cpp:238]     Train net output #0: loss = 2.30884 (* 1 = 2.30884 loss)
I0428 19:30:26.043799 22284 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:30:26.102535 22284 solver.cpp:219] Iteration 100 (1702.07 iter/s, 0.0587522s/100 iters), loss = 1.43488
I0428 19:30:26.102597 22284 solver.cpp:238]     Train net output #0: loss = 1.43488 (* 1 = 1.43488 loss)
I0428 19:30:26.102608 22284 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:30:26.160986 22284 solver.cpp:219] Iteration 200 (1712.68 iter/s, 0.0583879s/100 iters), loss = 0.558339
I0428 19:30:26.161015 22284 solver.cpp:238]     Train net output #0: loss = 0.558339 (* 1 = 0.558339 loss)
I0428 19:30:26.161041 22284 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:30:26.218179 22284 solver.cpp:219] Iteration 300 (1749.48 iter/s, 0.0571597s/100 iters), loss = 0.422838
I0428 19:30:26.218207 22284 solver.cpp:238]     Train net output #0: loss = 0.422838 (* 1 = 0.422838 loss)
I0428 19:30:26.218233 22284 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:30:26.275375 22284 solver.cpp:219] Iteration 400 (1749.4 iter/s, 0.0571626s/100 iters), loss = 0.267124
I0428 19:30:26.275403 22284 solver.cpp:238]     Train net output #0: loss = 0.267124 (* 1 = 0.267124 loss)
I0428 19:30:26.275429 22284 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:30:26.331442 22284 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:30:26.406553 22291 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:30:26.407102 22284 solver.cpp:398]     Test net output #0: accuracy = 0.9014
I0428 19:30:26.407138 22284 solver.cpp:398]     Test net output #1: loss = 0.325117 (* 1 = 0.325117 loss)
I0428 19:30:26.407984 22284 solver.cpp:219] Iteration 500 (754.329 iter/s, 0.132568s/100 iters), loss = 0.338616
I0428 19:30:26.408028 22284 solver.cpp:238]     Train net output #0: loss = 0.338616 (* 1 = 0.338616 loss)
I0428 19:30:26.408053 22284 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:30:26.480150 22284 solver.cpp:219] Iteration 600 (1386.61 iter/s, 0.0721185s/100 iters), loss = 0.192284
I0428 19:30:26.480186 22284 solver.cpp:238]     Train net output #0: loss = 0.192284 (* 1 = 0.192284 loss)
I0428 19:30:26.480199 22284 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:30:26.541157 22284 solver.cpp:219] Iteration 700 (1640.29 iter/s, 0.0609649s/100 iters), loss = 0.275295
I0428 19:30:26.541187 22284 solver.cpp:238]     Train net output #0: loss = 0.275295 (* 1 = 0.275295 loss)
I0428 19:30:26.541198 22284 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:30:26.598291 22284 solver.cpp:219] Iteration 800 (1751.34 iter/s, 0.0570993s/100 iters), loss = 0.263087
I0428 19:30:26.598335 22284 solver.cpp:238]     Train net output #0: loss = 0.263087 (* 1 = 0.263087 loss)
I0428 19:30:26.598345 22284 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:30:26.655930 22284 solver.cpp:219] Iteration 900 (1735.96 iter/s, 0.0576051s/100 iters), loss = 0.271177
I0428 19:30:26.655957 22284 solver.cpp:238]     Train net output #0: loss = 0.271178 (* 1 = 0.271178 loss)
I0428 19:30:26.655982 22284 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:30:26.675184 22290 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:30:26.712730 22284 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:30:26.713476 22284 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:30:26.713966 22284 solver.cpp:311] Iteration 1000, loss = 0.219894
I0428 19:30:26.713997 22284 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:30:26.788259 22291 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:30:26.788785 22284 solver.cpp:398]     Test net output #0: accuracy = 0.9371
I0428 19:30:26.788830 22284 solver.cpp:398]     Test net output #1: loss = 0.205063 (* 1 = 0.205063 loss)
I0428 19:30:26.788844 22284 solver.cpp:316] Optimization Done.
I0428 19:30:26.788853 22284 caffe.cpp:259] Optimization Done.
