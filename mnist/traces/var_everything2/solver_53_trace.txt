I0428 19:29:15.921785 21973 caffe.cpp:218] Using GPUs 0
I0428 19:29:15.962010 21973 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:29:16.478543 21973 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test53.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:29:16.478713 21973 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test53.prototxt
I0428 19:29:16.479089 21973 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:29:16.479113 21973 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:29:16.479218 21973 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool0"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:29:16.479326 21973 layer_factory.hpp:77] Creating layer mnist
I0428 19:29:16.479454 21973 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:29:16.479485 21973 net.cpp:86] Creating Layer mnist
I0428 19:29:16.479499 21973 net.cpp:382] mnist -> data
I0428 19:29:16.479529 21973 net.cpp:382] mnist -> label
I0428 19:29:16.480758 21973 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:29:16.483250 21973 net.cpp:124] Setting up mnist
I0428 19:29:16.483271 21973 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:29:16.483280 21973 net.cpp:131] Top shape: 64 (64)
I0428 19:29:16.483286 21973 net.cpp:139] Memory required for data: 200960
I0428 19:29:16.483297 21973 layer_factory.hpp:77] Creating layer conv0
I0428 19:29:16.483321 21973 net.cpp:86] Creating Layer conv0
I0428 19:29:16.483332 21973 net.cpp:408] conv0 <- data
I0428 19:29:16.483352 21973 net.cpp:382] conv0 -> conv0
I0428 19:29:16.765813 21973 net.cpp:124] Setting up conv0
I0428 19:29:16.765843 21973 net.cpp:131] Top shape: 64 2 24 24 (73728)
I0428 19:29:16.765849 21973 net.cpp:139] Memory required for data: 495872
I0428 19:29:16.765871 21973 layer_factory.hpp:77] Creating layer pool0
I0428 19:29:16.765888 21973 net.cpp:86] Creating Layer pool0
I0428 19:29:16.765914 21973 net.cpp:408] pool0 <- conv0
I0428 19:29:16.765924 21973 net.cpp:382] pool0 -> pool0
I0428 19:29:16.765983 21973 net.cpp:124] Setting up pool0
I0428 19:29:16.765992 21973 net.cpp:131] Top shape: 64 2 12 12 (18432)
I0428 19:29:16.765997 21973 net.cpp:139] Memory required for data: 569600
I0428 19:29:16.766003 21973 layer_factory.hpp:77] Creating layer ip1
I0428 19:29:16.766016 21973 net.cpp:86] Creating Layer ip1
I0428 19:29:16.766024 21973 net.cpp:408] ip1 <- pool0
I0428 19:29:16.766033 21973 net.cpp:382] ip1 -> ip1
I0428 19:29:16.766995 21973 net.cpp:124] Setting up ip1
I0428 19:29:16.767011 21973 net.cpp:131] Top shape: 64 10 (640)
I0428 19:29:16.767016 21973 net.cpp:139] Memory required for data: 572160
I0428 19:29:16.767030 21973 layer_factory.hpp:77] Creating layer relu1
I0428 19:29:16.767040 21973 net.cpp:86] Creating Layer relu1
I0428 19:29:16.767052 21973 net.cpp:408] relu1 <- ip1
I0428 19:29:16.767060 21973 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:29:16.767235 21973 net.cpp:124] Setting up relu1
I0428 19:29:16.767246 21973 net.cpp:131] Top shape: 64 10 (640)
I0428 19:29:16.767251 21973 net.cpp:139] Memory required for data: 574720
I0428 19:29:16.767256 21973 layer_factory.hpp:77] Creating layer ip2
I0428 19:29:16.767266 21973 net.cpp:86] Creating Layer ip2
I0428 19:29:16.767272 21973 net.cpp:408] ip2 <- ip1
I0428 19:29:16.767280 21973 net.cpp:382] ip2 -> ip2
I0428 19:29:16.767382 21973 net.cpp:124] Setting up ip2
I0428 19:29:16.767391 21973 net.cpp:131] Top shape: 64 10 (640)
I0428 19:29:16.767396 21973 net.cpp:139] Memory required for data: 577280
I0428 19:29:16.767408 21973 layer_factory.hpp:77] Creating layer relu2
I0428 19:29:16.767417 21973 net.cpp:86] Creating Layer relu2
I0428 19:29:16.767423 21973 net.cpp:408] relu2 <- ip2
I0428 19:29:16.767431 21973 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:29:16.768168 21973 net.cpp:124] Setting up relu2
I0428 19:29:16.768182 21973 net.cpp:131] Top shape: 64 10 (640)
I0428 19:29:16.768188 21973 net.cpp:139] Memory required for data: 579840
I0428 19:29:16.768193 21973 layer_factory.hpp:77] Creating layer ip3
I0428 19:29:16.768204 21973 net.cpp:86] Creating Layer ip3
I0428 19:29:16.768210 21973 net.cpp:408] ip3 <- ip2
I0428 19:29:16.768219 21973 net.cpp:382] ip3 -> ip3
I0428 19:29:16.768324 21973 net.cpp:124] Setting up ip3
I0428 19:29:16.768333 21973 net.cpp:131] Top shape: 64 10 (640)
I0428 19:29:16.768338 21973 net.cpp:139] Memory required for data: 582400
I0428 19:29:16.768348 21973 layer_factory.hpp:77] Creating layer relu3
I0428 19:29:16.768358 21973 net.cpp:86] Creating Layer relu3
I0428 19:29:16.768364 21973 net.cpp:408] relu3 <- ip3
I0428 19:29:16.768373 21973 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:29:16.768530 21973 net.cpp:124] Setting up relu3
I0428 19:29:16.768540 21973 net.cpp:131] Top shape: 64 10 (640)
I0428 19:29:16.768545 21973 net.cpp:139] Memory required for data: 584960
I0428 19:29:16.768551 21973 layer_factory.hpp:77] Creating layer loss
I0428 19:29:16.768560 21973 net.cpp:86] Creating Layer loss
I0428 19:29:16.768566 21973 net.cpp:408] loss <- ip3
I0428 19:29:16.768573 21973 net.cpp:408] loss <- label
I0428 19:29:16.768582 21973 net.cpp:382] loss -> loss
I0428 19:29:16.768602 21973 layer_factory.hpp:77] Creating layer loss
I0428 19:29:16.768890 21973 net.cpp:124] Setting up loss
I0428 19:29:16.768901 21973 net.cpp:131] Top shape: (1)
I0428 19:29:16.768906 21973 net.cpp:134]     with loss weight 1
I0428 19:29:16.768928 21973 net.cpp:139] Memory required for data: 584964
I0428 19:29:16.768934 21973 net.cpp:200] loss needs backward computation.
I0428 19:29:16.768940 21973 net.cpp:200] relu3 needs backward computation.
I0428 19:29:16.768946 21973 net.cpp:200] ip3 needs backward computation.
I0428 19:29:16.768951 21973 net.cpp:200] relu2 needs backward computation.
I0428 19:29:16.768957 21973 net.cpp:200] ip2 needs backward computation.
I0428 19:29:16.768962 21973 net.cpp:200] relu1 needs backward computation.
I0428 19:29:16.768967 21973 net.cpp:200] ip1 needs backward computation.
I0428 19:29:16.768986 21973 net.cpp:200] pool0 needs backward computation.
I0428 19:29:16.768992 21973 net.cpp:200] conv0 needs backward computation.
I0428 19:29:16.768998 21973 net.cpp:202] mnist does not need backward computation.
I0428 19:29:16.769003 21973 net.cpp:244] This network produces output loss
I0428 19:29:16.769017 21973 net.cpp:257] Network initialization done.
I0428 19:29:16.769330 21973 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test53.prototxt
I0428 19:29:16.769362 21973 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:29:16.769448 21973 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool0"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:29:16.769543 21973 layer_factory.hpp:77] Creating layer mnist
I0428 19:29:16.769601 21973 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:29:16.769618 21973 net.cpp:86] Creating Layer mnist
I0428 19:29:16.769625 21973 net.cpp:382] mnist -> data
I0428 19:29:16.769637 21973 net.cpp:382] mnist -> label
I0428 19:29:16.769752 21973 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:29:16.772048 21973 net.cpp:124] Setting up mnist
I0428 19:29:16.772080 21973 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:29:16.772089 21973 net.cpp:131] Top shape: 100 (100)
I0428 19:29:16.772094 21973 net.cpp:139] Memory required for data: 314000
I0428 19:29:16.772101 21973 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:29:16.772111 21973 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:29:16.772117 21973 net.cpp:408] label_mnist_1_split <- label
I0428 19:29:16.772126 21973 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:29:16.772137 21973 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:29:16.772254 21973 net.cpp:124] Setting up label_mnist_1_split
I0428 19:29:16.772264 21973 net.cpp:131] Top shape: 100 (100)
I0428 19:29:16.772269 21973 net.cpp:131] Top shape: 100 (100)
I0428 19:29:16.772275 21973 net.cpp:139] Memory required for data: 314800
I0428 19:29:16.772292 21973 layer_factory.hpp:77] Creating layer conv0
I0428 19:29:16.772306 21973 net.cpp:86] Creating Layer conv0
I0428 19:29:16.772312 21973 net.cpp:408] conv0 <- data
I0428 19:29:16.772321 21973 net.cpp:382] conv0 -> conv0
I0428 19:29:16.773756 21973 net.cpp:124] Setting up conv0
I0428 19:29:16.773772 21973 net.cpp:131] Top shape: 100 2 24 24 (115200)
I0428 19:29:16.773777 21973 net.cpp:139] Memory required for data: 775600
I0428 19:29:16.773792 21973 layer_factory.hpp:77] Creating layer pool0
I0428 19:29:16.773802 21973 net.cpp:86] Creating Layer pool0
I0428 19:29:16.773808 21973 net.cpp:408] pool0 <- conv0
I0428 19:29:16.773815 21973 net.cpp:382] pool0 -> pool0
I0428 19:29:16.773855 21973 net.cpp:124] Setting up pool0
I0428 19:29:16.773864 21973 net.cpp:131] Top shape: 100 2 12 12 (28800)
I0428 19:29:16.773869 21973 net.cpp:139] Memory required for data: 890800
I0428 19:29:16.773874 21973 layer_factory.hpp:77] Creating layer ip1
I0428 19:29:16.773885 21973 net.cpp:86] Creating Layer ip1
I0428 19:29:16.773890 21973 net.cpp:408] ip1 <- pool0
I0428 19:29:16.773897 21973 net.cpp:382] ip1 -> ip1
I0428 19:29:16.774009 21973 net.cpp:124] Setting up ip1
I0428 19:29:16.774018 21973 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:29:16.774022 21973 net.cpp:139] Memory required for data: 894800
I0428 19:29:16.774034 21973 layer_factory.hpp:77] Creating layer relu1
I0428 19:29:16.774042 21973 net.cpp:86] Creating Layer relu1
I0428 19:29:16.774049 21973 net.cpp:408] relu1 <- ip1
I0428 19:29:16.774055 21973 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:29:16.774217 21973 net.cpp:124] Setting up relu1
I0428 19:29:16.774243 21973 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:29:16.774248 21973 net.cpp:139] Memory required for data: 898800
I0428 19:29:16.774253 21973 layer_factory.hpp:77] Creating layer ip2
I0428 19:29:16.774262 21973 net.cpp:86] Creating Layer ip2
I0428 19:29:16.774268 21973 net.cpp:408] ip2 <- ip1
I0428 19:29:16.774292 21973 net.cpp:382] ip2 -> ip2
I0428 19:29:16.774447 21973 net.cpp:124] Setting up ip2
I0428 19:29:16.774456 21973 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:29:16.774461 21973 net.cpp:139] Memory required for data: 902800
I0428 19:29:16.774474 21973 layer_factory.hpp:77] Creating layer relu2
I0428 19:29:16.774484 21973 net.cpp:86] Creating Layer relu2
I0428 19:29:16.774492 21973 net.cpp:408] relu2 <- ip2
I0428 19:29:16.774499 21973 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:29:16.775310 21973 net.cpp:124] Setting up relu2
I0428 19:29:16.775341 21973 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:29:16.775348 21973 net.cpp:139] Memory required for data: 906800
I0428 19:29:16.775354 21973 layer_factory.hpp:77] Creating layer ip3
I0428 19:29:16.775367 21973 net.cpp:86] Creating Layer ip3
I0428 19:29:16.775373 21973 net.cpp:408] ip3 <- ip2
I0428 19:29:16.775382 21973 net.cpp:382] ip3 -> ip3
I0428 19:29:16.775530 21973 net.cpp:124] Setting up ip3
I0428 19:29:16.775539 21973 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:29:16.775544 21973 net.cpp:139] Memory required for data: 910800
I0428 19:29:16.775554 21973 layer_factory.hpp:77] Creating layer relu3
I0428 19:29:16.775563 21973 net.cpp:86] Creating Layer relu3
I0428 19:29:16.775568 21973 net.cpp:408] relu3 <- ip3
I0428 19:29:16.775575 21973 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:29:16.775756 21973 net.cpp:124] Setting up relu3
I0428 19:29:16.775766 21973 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:29:16.775771 21973 net.cpp:139] Memory required for data: 914800
I0428 19:29:16.775776 21973 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:29:16.775784 21973 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:29:16.775790 21973 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:29:16.775797 21973 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:29:16.775806 21973 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:29:16.775846 21973 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:29:16.775856 21973 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:29:16.775862 21973 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:29:16.775878 21973 net.cpp:139] Memory required for data: 922800
I0428 19:29:16.775884 21973 layer_factory.hpp:77] Creating layer accuracy
I0428 19:29:16.775892 21973 net.cpp:86] Creating Layer accuracy
I0428 19:29:16.775898 21973 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:29:16.775905 21973 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:29:16.775913 21973 net.cpp:382] accuracy -> accuracy
I0428 19:29:16.775925 21973 net.cpp:124] Setting up accuracy
I0428 19:29:16.775949 21973 net.cpp:131] Top shape: (1)
I0428 19:29:16.775954 21973 net.cpp:139] Memory required for data: 922804
I0428 19:29:16.775959 21973 layer_factory.hpp:77] Creating layer loss
I0428 19:29:16.775967 21973 net.cpp:86] Creating Layer loss
I0428 19:29:16.775972 21973 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:29:16.775979 21973 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:29:16.776001 21973 net.cpp:382] loss -> loss
I0428 19:29:16.776011 21973 layer_factory.hpp:77] Creating layer loss
I0428 19:29:16.776265 21973 net.cpp:124] Setting up loss
I0428 19:29:16.776276 21973 net.cpp:131] Top shape: (1)
I0428 19:29:16.776283 21973 net.cpp:134]     with loss weight 1
I0428 19:29:16.776291 21973 net.cpp:139] Memory required for data: 922808
I0428 19:29:16.776301 21973 net.cpp:200] loss needs backward computation.
I0428 19:29:16.776309 21973 net.cpp:202] accuracy does not need backward computation.
I0428 19:29:16.776315 21973 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:29:16.776322 21973 net.cpp:200] relu3 needs backward computation.
I0428 19:29:16.776329 21973 net.cpp:200] ip3 needs backward computation.
I0428 19:29:16.776334 21973 net.cpp:200] relu2 needs backward computation.
I0428 19:29:16.776340 21973 net.cpp:200] ip2 needs backward computation.
I0428 19:29:16.776345 21973 net.cpp:200] relu1 needs backward computation.
I0428 19:29:16.776365 21973 net.cpp:200] ip1 needs backward computation.
I0428 19:29:16.776370 21973 net.cpp:200] pool0 needs backward computation.
I0428 19:29:16.776374 21973 net.cpp:200] conv0 needs backward computation.
I0428 19:29:16.776381 21973 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:29:16.776387 21973 net.cpp:202] mnist does not need backward computation.
I0428 19:29:16.776392 21973 net.cpp:244] This network produces output accuracy
I0428 19:29:16.776398 21973 net.cpp:244] This network produces output loss
I0428 19:29:16.776424 21973 net.cpp:257] Network initialization done.
I0428 19:29:16.776468 21973 solver.cpp:56] Solver scaffolding done.
I0428 19:29:16.776823 21973 caffe.cpp:248] Starting Optimization
I0428 19:29:16.776831 21973 solver.cpp:273] Solving LeNet
I0428 19:29:16.776835 21973 solver.cpp:274] Learning Rate Policy: inv
I0428 19:29:16.777717 21973 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:29:16.780861 21973 blocking_queue.cpp:49] Waiting for data
I0428 19:29:16.851855 21980 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:29:16.852309 21973 solver.cpp:398]     Test net output #0: accuracy = 0.0834
I0428 19:29:16.852344 21973 solver.cpp:398]     Test net output #1: loss = 2.31902 (* 1 = 2.31902 loss)
I0428 19:29:16.853955 21973 solver.cpp:219] Iteration 0 (-1.05518e-42 iter/s, 0.0770277s/100 iters), loss = 2.31682
I0428 19:29:16.853998 21973 solver.cpp:238]     Train net output #0: loss = 2.31682 (* 1 = 2.31682 loss)
I0428 19:29:16.854017 21973 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:29:16.911828 21973 solver.cpp:219] Iteration 100 (1729.4 iter/s, 0.0578236s/100 iters), loss = 1.62065
I0428 19:29:16.911897 21973 solver.cpp:238]     Train net output #0: loss = 1.62065 (* 1 = 1.62065 loss)
I0428 19:29:16.911909 21973 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:29:16.967319 21973 solver.cpp:219] Iteration 200 (1804.36 iter/s, 0.0554214s/100 iters), loss = 0.927099
I0428 19:29:16.967348 21973 solver.cpp:238]     Train net output #0: loss = 0.927099 (* 1 = 0.927099 loss)
I0428 19:29:16.967375 21973 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:29:17.021651 21973 solver.cpp:219] Iteration 300 (1841.72 iter/s, 0.0542971s/100 iters), loss = 1.04983
I0428 19:29:17.021677 21973 solver.cpp:238]     Train net output #0: loss = 1.04983 (* 1 = 1.04983 loss)
I0428 19:29:17.021703 21973 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:29:17.076284 21973 solver.cpp:219] Iteration 400 (1831.48 iter/s, 0.0546007s/100 iters), loss = 0.972639
I0428 19:29:17.076310 21973 solver.cpp:238]     Train net output #0: loss = 0.972639 (* 1 = 0.972639 loss)
I0428 19:29:17.076336 21973 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:29:17.130988 21973 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:29:17.182189 21980 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:29:17.182584 21973 solver.cpp:398]     Test net output #0: accuracy = 0.7223
I0428 19:29:17.182605 21973 solver.cpp:398]     Test net output #1: loss = 0.955098 (* 1 = 0.955098 loss)
I0428 19:29:17.183290 21973 solver.cpp:219] Iteration 500 (934.849 iter/s, 0.106969s/100 iters), loss = 0.925678
I0428 19:29:17.183316 21973 solver.cpp:238]     Train net output #0: loss = 0.925678 (* 1 = 0.925678 loss)
I0428 19:29:17.183327 21973 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:29:17.254823 21973 solver.cpp:219] Iteration 600 (1398.63 iter/s, 0.0714988s/100 iters), loss = 0.96579
I0428 19:29:17.254860 21973 solver.cpp:238]     Train net output #0: loss = 0.96579 (* 1 = 0.96579 loss)
I0428 19:29:17.254873 21973 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:29:17.314537 21973 solver.cpp:219] Iteration 700 (1675.81 iter/s, 0.0596726s/100 iters), loss = 1.13701
I0428 19:29:17.314566 21973 solver.cpp:238]     Train net output #0: loss = 1.13701 (* 1 = 1.13701 loss)
I0428 19:29:17.314576 21973 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:29:17.370401 21973 solver.cpp:219] Iteration 800 (1791.17 iter/s, 0.0558295s/100 iters), loss = 0.905801
I0428 19:29:17.370429 21973 solver.cpp:238]     Train net output #0: loss = 0.905801 (* 1 = 0.905801 loss)
I0428 19:29:17.370438 21973 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:29:17.426496 21973 solver.cpp:219] Iteration 900 (1783.74 iter/s, 0.0560619s/100 iters), loss = 1.12577
I0428 19:29:17.426525 21973 solver.cpp:238]     Train net output #0: loss = 1.12577 (* 1 = 1.12577 loss)
I0428 19:29:17.426535 21973 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:29:17.445493 21979 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:29:17.481753 21973 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:29:17.482287 21973 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:29:17.482676 21973 solver.cpp:311] Iteration 1000, loss = 1.04054
I0428 19:29:17.482692 21973 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:29:17.536154 21980 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:29:17.536547 21973 solver.cpp:398]     Test net output #0: accuracy = 0.7323
I0428 19:29:17.536571 21973 solver.cpp:398]     Test net output #1: loss = 0.863697 (* 1 = 0.863697 loss)
I0428 19:29:17.536578 21973 solver.cpp:316] Optimization Done.
I0428 19:29:17.536583 21973 caffe.cpp:259] Optimization Done.
