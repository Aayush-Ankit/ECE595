I0428 19:55:58.523722 28318 caffe.cpp:218] Using GPUs 0
I0428 19:55:58.561585 28318 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:55:59.074036 28318 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test745.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:55:59.074196 28318 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test745.prototxt
I0428 19:55:59.074790 28318 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:55:59.074816 28318 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:55:59.074955 28318 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0428 19:55:59.075064 28318 layer_factory.hpp:77] Creating layer mnist
I0428 19:55:59.075197 28318 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:55:59.075229 28318 net.cpp:86] Creating Layer mnist
I0428 19:55:59.075242 28318 net.cpp:382] mnist -> data
I0428 19:55:59.075273 28318 net.cpp:382] mnist -> label
I0428 19:55:59.076846 28318 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:55:59.079699 28318 net.cpp:124] Setting up mnist
I0428 19:55:59.079718 28318 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:55:59.079725 28318 net.cpp:131] Top shape: 64 (64)
I0428 19:55:59.079728 28318 net.cpp:139] Memory required for data: 200960
I0428 19:55:59.079735 28318 layer_factory.hpp:77] Creating layer conv0
I0428 19:55:59.079773 28318 net.cpp:86] Creating Layer conv0
I0428 19:55:59.079784 28318 net.cpp:408] conv0 <- data
I0428 19:55:59.079805 28318 net.cpp:382] conv0 -> conv0
I0428 19:55:59.350821 28318 net.cpp:124] Setting up conv0
I0428 19:55:59.350864 28318 net.cpp:131] Top shape: 64 10 24 24 (368640)
I0428 19:55:59.350883 28318 net.cpp:139] Memory required for data: 1675520
I0428 19:55:59.350917 28318 layer_factory.hpp:77] Creating layer pool0
I0428 19:55:59.350934 28318 net.cpp:86] Creating Layer pool0
I0428 19:55:59.350939 28318 net.cpp:408] pool0 <- conv0
I0428 19:55:59.350947 28318 net.cpp:382] pool0 -> pool0
I0428 19:55:59.351006 28318 net.cpp:124] Setting up pool0
I0428 19:55:59.351017 28318 net.cpp:131] Top shape: 64 10 12 12 (92160)
I0428 19:55:59.351022 28318 net.cpp:139] Memory required for data: 2044160
I0428 19:55:59.351027 28318 layer_factory.hpp:77] Creating layer conv1
I0428 19:55:59.351040 28318 net.cpp:86] Creating Layer conv1
I0428 19:55:59.351045 28318 net.cpp:408] conv1 <- pool0
I0428 19:55:59.351053 28318 net.cpp:382] conv1 -> conv1
I0428 19:55:59.353258 28318 net.cpp:124] Setting up conv1
I0428 19:55:59.353287 28318 net.cpp:131] Top shape: 64 2 8 8 (8192)
I0428 19:55:59.353291 28318 net.cpp:139] Memory required for data: 2076928
I0428 19:55:59.353301 28318 layer_factory.hpp:77] Creating layer pool1
I0428 19:55:59.353309 28318 net.cpp:86] Creating Layer pool1
I0428 19:55:59.353312 28318 net.cpp:408] pool1 <- conv1
I0428 19:55:59.353317 28318 net.cpp:382] pool1 -> pool1
I0428 19:55:59.353364 28318 net.cpp:124] Setting up pool1
I0428 19:55:59.353374 28318 net.cpp:131] Top shape: 64 2 4 4 (2048)
I0428 19:55:59.353379 28318 net.cpp:139] Memory required for data: 2085120
I0428 19:55:59.353384 28318 layer_factory.hpp:77] Creating layer ip1
I0428 19:55:59.353395 28318 net.cpp:86] Creating Layer ip1
I0428 19:55:59.353400 28318 net.cpp:408] ip1 <- pool1
I0428 19:55:59.353408 28318 net.cpp:382] ip1 -> ip1
I0428 19:55:59.353546 28318 net.cpp:124] Setting up ip1
I0428 19:55:59.353557 28318 net.cpp:131] Top shape: 64 10 (640)
I0428 19:55:59.353562 28318 net.cpp:139] Memory required for data: 2087680
I0428 19:55:59.353574 28318 layer_factory.hpp:77] Creating layer relu1
I0428 19:55:59.353581 28318 net.cpp:86] Creating Layer relu1
I0428 19:55:59.353586 28318 net.cpp:408] relu1 <- ip1
I0428 19:55:59.353593 28318 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:55:59.353832 28318 net.cpp:124] Setting up relu1
I0428 19:55:59.353844 28318 net.cpp:131] Top shape: 64 10 (640)
I0428 19:55:59.353850 28318 net.cpp:139] Memory required for data: 2090240
I0428 19:55:59.353869 28318 layer_factory.hpp:77] Creating layer ip2
I0428 19:55:59.353878 28318 net.cpp:86] Creating Layer ip2
I0428 19:55:59.353883 28318 net.cpp:408] ip2 <- ip1
I0428 19:55:59.353890 28318 net.cpp:382] ip2 -> ip2
I0428 19:55:59.354038 28318 net.cpp:124] Setting up ip2
I0428 19:55:59.354050 28318 net.cpp:131] Top shape: 64 10 (640)
I0428 19:55:59.354055 28318 net.cpp:139] Memory required for data: 2092800
I0428 19:55:59.354063 28318 layer_factory.hpp:77] Creating layer relu2
I0428 19:55:59.354074 28318 net.cpp:86] Creating Layer relu2
I0428 19:55:59.354080 28318 net.cpp:408] relu2 <- ip2
I0428 19:55:59.354087 28318 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:55:59.355152 28318 net.cpp:124] Setting up relu2
I0428 19:55:59.355183 28318 net.cpp:131] Top shape: 64 10 (640)
I0428 19:55:59.355190 28318 net.cpp:139] Memory required for data: 2095360
I0428 19:55:59.355208 28318 layer_factory.hpp:77] Creating layer loss
I0428 19:55:59.355219 28318 net.cpp:86] Creating Layer loss
I0428 19:55:59.355224 28318 net.cpp:408] loss <- ip2
I0428 19:55:59.355232 28318 net.cpp:408] loss <- label
I0428 19:55:59.355239 28318 net.cpp:382] loss -> loss
I0428 19:55:59.355275 28318 layer_factory.hpp:77] Creating layer loss
I0428 19:55:59.355691 28318 net.cpp:124] Setting up loss
I0428 19:55:59.355715 28318 net.cpp:131] Top shape: (1)
I0428 19:55:59.355736 28318 net.cpp:134]     with loss weight 1
I0428 19:55:59.355782 28318 net.cpp:139] Memory required for data: 2095364
I0428 19:55:59.355787 28318 net.cpp:200] loss needs backward computation.
I0428 19:55:59.355792 28318 net.cpp:200] relu2 needs backward computation.
I0428 19:55:59.355798 28318 net.cpp:200] ip2 needs backward computation.
I0428 19:55:59.355801 28318 net.cpp:200] relu1 needs backward computation.
I0428 19:55:59.355805 28318 net.cpp:200] ip1 needs backward computation.
I0428 19:55:59.355823 28318 net.cpp:200] pool1 needs backward computation.
I0428 19:55:59.355828 28318 net.cpp:200] conv1 needs backward computation.
I0428 19:55:59.355831 28318 net.cpp:200] pool0 needs backward computation.
I0428 19:55:59.355836 28318 net.cpp:200] conv0 needs backward computation.
I0428 19:55:59.355844 28318 net.cpp:202] mnist does not need backward computation.
I0428 19:55:59.355847 28318 net.cpp:244] This network produces output loss
I0428 19:55:59.355859 28318 net.cpp:257] Network initialization done.
I0428 19:55:59.356353 28318 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test745.prototxt
I0428 19:55:59.356407 28318 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:55:59.356556 28318 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0428 19:55:59.356657 28318 layer_factory.hpp:77] Creating layer mnist
I0428 19:55:59.356717 28318 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:55:59.356739 28318 net.cpp:86] Creating Layer mnist
I0428 19:55:59.356747 28318 net.cpp:382] mnist -> data
I0428 19:55:59.356758 28318 net.cpp:382] mnist -> label
I0428 19:55:59.356912 28318 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:55:59.359648 28318 net.cpp:124] Setting up mnist
I0428 19:55:59.359691 28318 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:55:59.359699 28318 net.cpp:131] Top shape: 100 (100)
I0428 19:55:59.359701 28318 net.cpp:139] Memory required for data: 314000
I0428 19:55:59.359704 28318 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:55:59.359726 28318 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:55:59.359730 28318 net.cpp:408] label_mnist_1_split <- label
I0428 19:55:59.359735 28318 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:55:59.359743 28318 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:55:59.359853 28318 net.cpp:124] Setting up label_mnist_1_split
I0428 19:55:59.359877 28318 net.cpp:131] Top shape: 100 (100)
I0428 19:55:59.359884 28318 net.cpp:131] Top shape: 100 (100)
I0428 19:55:59.359889 28318 net.cpp:139] Memory required for data: 314800
I0428 19:55:59.359894 28318 layer_factory.hpp:77] Creating layer conv0
I0428 19:55:59.359911 28318 net.cpp:86] Creating Layer conv0
I0428 19:55:59.359918 28318 net.cpp:408] conv0 <- data
I0428 19:55:59.359928 28318 net.cpp:382] conv0 -> conv0
I0428 19:55:59.361969 28318 net.cpp:124] Setting up conv0
I0428 19:55:59.361984 28318 net.cpp:131] Top shape: 100 10 24 24 (576000)
I0428 19:55:59.362004 28318 net.cpp:139] Memory required for data: 2618800
I0428 19:55:59.362013 28318 layer_factory.hpp:77] Creating layer pool0
I0428 19:55:59.362022 28318 net.cpp:86] Creating Layer pool0
I0428 19:55:59.362026 28318 net.cpp:408] pool0 <- conv0
I0428 19:55:59.362031 28318 net.cpp:382] pool0 -> pool0
I0428 19:55:59.362084 28318 net.cpp:124] Setting up pool0
I0428 19:55:59.362103 28318 net.cpp:131] Top shape: 100 10 12 12 (144000)
I0428 19:55:59.362108 28318 net.cpp:139] Memory required for data: 3194800
I0428 19:55:59.362114 28318 layer_factory.hpp:77] Creating layer conv1
I0428 19:55:59.362130 28318 net.cpp:86] Creating Layer conv1
I0428 19:55:59.362136 28318 net.cpp:408] conv1 <- pool0
I0428 19:55:59.362146 28318 net.cpp:382] conv1 -> conv1
I0428 19:55:59.364861 28318 net.cpp:124] Setting up conv1
I0428 19:55:59.364876 28318 net.cpp:131] Top shape: 100 2 8 8 (12800)
I0428 19:55:59.364881 28318 net.cpp:139] Memory required for data: 3246000
I0428 19:55:59.364892 28318 layer_factory.hpp:77] Creating layer pool1
I0428 19:55:59.364899 28318 net.cpp:86] Creating Layer pool1
I0428 19:55:59.364905 28318 net.cpp:408] pool1 <- conv1
I0428 19:55:59.364915 28318 net.cpp:382] pool1 -> pool1
I0428 19:55:59.364972 28318 net.cpp:124] Setting up pool1
I0428 19:55:59.364986 28318 net.cpp:131] Top shape: 100 2 4 4 (3200)
I0428 19:55:59.365015 28318 net.cpp:139] Memory required for data: 3258800
I0428 19:55:59.365020 28318 layer_factory.hpp:77] Creating layer ip1
I0428 19:55:59.365031 28318 net.cpp:86] Creating Layer ip1
I0428 19:55:59.365051 28318 net.cpp:408] ip1 <- pool1
I0428 19:55:59.365058 28318 net.cpp:382] ip1 -> ip1
I0428 19:55:59.365228 28318 net.cpp:124] Setting up ip1
I0428 19:55:59.365239 28318 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:55:59.365243 28318 net.cpp:139] Memory required for data: 3262800
I0428 19:55:59.365254 28318 layer_factory.hpp:77] Creating layer relu1
I0428 19:55:59.365263 28318 net.cpp:86] Creating Layer relu1
I0428 19:55:59.365268 28318 net.cpp:408] relu1 <- ip1
I0428 19:55:59.365276 28318 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:55:59.365504 28318 net.cpp:124] Setting up relu1
I0428 19:55:59.365517 28318 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:55:59.365532 28318 net.cpp:139] Memory required for data: 3266800
I0428 19:55:59.365537 28318 layer_factory.hpp:77] Creating layer ip2
I0428 19:55:59.365569 28318 net.cpp:86] Creating Layer ip2
I0428 19:55:59.365579 28318 net.cpp:408] ip2 <- ip1
I0428 19:55:59.365588 28318 net.cpp:382] ip2 -> ip2
I0428 19:55:59.365780 28318 net.cpp:124] Setting up ip2
I0428 19:55:59.365794 28318 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:55:59.365799 28318 net.cpp:139] Memory required for data: 3270800
I0428 19:55:59.365809 28318 layer_factory.hpp:77] Creating layer relu2
I0428 19:55:59.365833 28318 net.cpp:86] Creating Layer relu2
I0428 19:55:59.365839 28318 net.cpp:408] relu2 <- ip2
I0428 19:55:59.365847 28318 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:55:59.366127 28318 net.cpp:124] Setting up relu2
I0428 19:55:59.366153 28318 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:55:59.366158 28318 net.cpp:139] Memory required for data: 3274800
I0428 19:55:59.366179 28318 layer_factory.hpp:77] Creating layer ip2_relu2_0_split
I0428 19:55:59.366188 28318 net.cpp:86] Creating Layer ip2_relu2_0_split
I0428 19:55:59.366192 28318 net.cpp:408] ip2_relu2_0_split <- ip2
I0428 19:55:59.366201 28318 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_0
I0428 19:55:59.366222 28318 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_1
I0428 19:55:59.366281 28318 net.cpp:124] Setting up ip2_relu2_0_split
I0428 19:55:59.366292 28318 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:55:59.366298 28318 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:55:59.366303 28318 net.cpp:139] Memory required for data: 3282800
I0428 19:55:59.366308 28318 layer_factory.hpp:77] Creating layer accuracy
I0428 19:55:59.366317 28318 net.cpp:86] Creating Layer accuracy
I0428 19:55:59.366322 28318 net.cpp:408] accuracy <- ip2_relu2_0_split_0
I0428 19:55:59.366328 28318 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:55:59.366335 28318 net.cpp:382] accuracy -> accuracy
I0428 19:55:59.366346 28318 net.cpp:124] Setting up accuracy
I0428 19:55:59.366353 28318 net.cpp:131] Top shape: (1)
I0428 19:55:59.366372 28318 net.cpp:139] Memory required for data: 3282804
I0428 19:55:59.366376 28318 layer_factory.hpp:77] Creating layer loss
I0428 19:55:59.366384 28318 net.cpp:86] Creating Layer loss
I0428 19:55:59.366389 28318 net.cpp:408] loss <- ip2_relu2_0_split_1
I0428 19:55:59.366394 28318 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:55:59.366400 28318 net.cpp:382] loss -> loss
I0428 19:55:59.366410 28318 layer_factory.hpp:77] Creating layer loss
I0428 19:55:59.366788 28318 net.cpp:124] Setting up loss
I0428 19:55:59.366830 28318 net.cpp:131] Top shape: (1)
I0428 19:55:59.366835 28318 net.cpp:134]     with loss weight 1
I0428 19:55:59.366844 28318 net.cpp:139] Memory required for data: 3282808
I0428 19:55:59.366849 28318 net.cpp:200] loss needs backward computation.
I0428 19:55:59.366854 28318 net.cpp:202] accuracy does not need backward computation.
I0428 19:55:59.366860 28318 net.cpp:200] ip2_relu2_0_split needs backward computation.
I0428 19:55:59.366864 28318 net.cpp:200] relu2 needs backward computation.
I0428 19:55:59.366869 28318 net.cpp:200] ip2 needs backward computation.
I0428 19:55:59.366873 28318 net.cpp:200] relu1 needs backward computation.
I0428 19:55:59.366878 28318 net.cpp:200] ip1 needs backward computation.
I0428 19:55:59.366883 28318 net.cpp:200] pool1 needs backward computation.
I0428 19:55:59.366888 28318 net.cpp:200] conv1 needs backward computation.
I0428 19:55:59.366892 28318 net.cpp:200] pool0 needs backward computation.
I0428 19:55:59.366897 28318 net.cpp:200] conv0 needs backward computation.
I0428 19:55:59.366902 28318 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:55:59.366924 28318 net.cpp:202] mnist does not need backward computation.
I0428 19:55:59.366927 28318 net.cpp:244] This network produces output accuracy
I0428 19:55:59.366933 28318 net.cpp:244] This network produces output loss
I0428 19:55:59.366947 28318 net.cpp:257] Network initialization done.
I0428 19:55:59.366998 28318 solver.cpp:56] Solver scaffolding done.
I0428 19:55:59.367411 28318 caffe.cpp:248] Starting Optimization
I0428 19:55:59.367435 28318 solver.cpp:273] Solving LeNet
I0428 19:55:59.367455 28318 solver.cpp:274] Learning Rate Policy: inv
I0428 19:55:59.368513 28318 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:55:59.372182 28318 blocking_queue.cpp:49] Waiting for data
I0428 19:55:59.442787 28325 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:55:59.443356 28318 solver.cpp:398]     Test net output #0: accuracy = 0.1332
I0428 19:55:59.443395 28318 solver.cpp:398]     Test net output #1: loss = 2.29784 (* 1 = 2.29784 loss)
I0428 19:55:59.446089 28318 solver.cpp:219] Iteration 0 (0 iter/s, 0.0785919s/100 iters), loss = 2.29456
I0428 19:55:59.446125 28318 solver.cpp:238]     Train net output #0: loss = 2.29456 (* 1 = 2.29456 loss)
I0428 19:55:59.446136 28318 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:55:59.514485 28318 solver.cpp:219] Iteration 100 (1462.91 iter/s, 0.068357s/100 iters), loss = 1.15322
I0428 19:55:59.514526 28318 solver.cpp:238]     Train net output #0: loss = 1.15322 (* 1 = 1.15322 loss)
I0428 19:55:59.514538 28318 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:55:59.592638 28318 solver.cpp:219] Iteration 200 (1280.33 iter/s, 0.0781047s/100 iters), loss = 0.805565
I0428 19:55:59.592686 28318 solver.cpp:238]     Train net output #0: loss = 0.805565 (* 1 = 0.805565 loss)
I0428 19:55:59.592695 28318 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:55:59.661911 28318 solver.cpp:219] Iteration 300 (1444.7 iter/s, 0.0692185s/100 iters), loss = 0.691759
I0428 19:55:59.661949 28318 solver.cpp:238]     Train net output #0: loss = 0.691759 (* 1 = 0.691759 loss)
I0428 19:55:59.661955 28318 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:55:59.729485 28318 solver.cpp:219] Iteration 400 (1480.58 iter/s, 0.067541s/100 iters), loss = 0.63596
I0428 19:55:59.729521 28318 solver.cpp:238]     Train net output #0: loss = 0.63596 (* 1 = 0.63596 loss)
I0428 19:55:59.729527 28318 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:55:59.796183 28318 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:55:59.871323 28325 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:55:59.871922 28318 solver.cpp:398]     Test net output #0: accuracy = 0.7222
I0428 19:55:59.871960 28318 solver.cpp:398]     Test net output #1: loss = 0.766266 (* 1 = 0.766266 loss)
I0428 19:55:59.872773 28318 solver.cpp:219] Iteration 500 (698.053 iter/s, 0.143256s/100 iters), loss = 0.757049
I0428 19:55:59.872818 28318 solver.cpp:238]     Train net output #0: loss = 0.757049 (* 1 = 0.757049 loss)
I0428 19:55:59.872825 28318 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:55:59.951977 28318 solver.cpp:219] Iteration 600 (1263.1 iter/s, 0.0791706s/100 iters), loss = 0.937581
I0428 19:55:59.952016 28318 solver.cpp:238]     Train net output #0: loss = 0.937581 (* 1 = 0.937581 loss)
I0428 19:55:59.952023 28318 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:56:00.020193 28318 solver.cpp:219] Iteration 700 (1466.62 iter/s, 0.0681841s/100 iters), loss = 0.777062
I0428 19:56:00.020232 28318 solver.cpp:238]     Train net output #0: loss = 0.777062 (* 1 = 0.777062 loss)
I0428 19:56:00.020238 28318 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:56:00.089694 28318 solver.cpp:219] Iteration 800 (1439.51 iter/s, 0.0694682s/100 iters), loss = 0.681581
I0428 19:56:00.089732 28318 solver.cpp:238]     Train net output #0: loss = 0.681581 (* 1 = 0.681581 loss)
I0428 19:56:00.089740 28318 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:56:00.165608 28318 solver.cpp:219] Iteration 900 (1318.11 iter/s, 0.0758661s/100 iters), loss = 0.874786
I0428 19:56:00.165647 28318 solver.cpp:238]     Train net output #0: loss = 0.874786 (* 1 = 0.874786 loss)
I0428 19:56:00.165653 28318 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:56:00.189098 28324 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:56:00.233623 28318 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:56:00.234432 28318 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:56:00.235103 28318 solver.cpp:311] Iteration 1000, loss = 0.715605
I0428 19:56:00.235122 28318 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:56:00.309483 28325 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:56:00.310078 28318 solver.cpp:398]     Test net output #0: accuracy = 0.7346
I0428 19:56:00.310104 28318 solver.cpp:398]     Test net output #1: loss = 0.701101 (* 1 = 0.701101 loss)
I0428 19:56:00.310111 28318 solver.cpp:316] Optimization Done.
I0428 19:56:00.310117 28318 caffe.cpp:259] Optimization Done.
