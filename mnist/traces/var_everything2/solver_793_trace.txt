I0428 19:57:34.893208 28718 caffe.cpp:218] Using GPUs 0
I0428 19:57:34.928647 28718 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:57:35.381868 28718 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test793.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:57:35.382000 28718 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test793.prototxt
I0428 19:57:35.382316 28718 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:57:35.382330 28718 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:57:35.382408 28718 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:57:35.382465 28718 layer_factory.hpp:77] Creating layer mnist
I0428 19:57:35.382546 28718 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:57:35.382565 28718 net.cpp:86] Creating Layer mnist
I0428 19:57:35.382571 28718 net.cpp:382] mnist -> data
I0428 19:57:35.382589 28718 net.cpp:382] mnist -> label
I0428 19:57:35.383507 28718 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:57:35.385706 28718 net.cpp:124] Setting up mnist
I0428 19:57:35.385754 28718 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:57:35.385759 28718 net.cpp:131] Top shape: 64 (64)
I0428 19:57:35.385762 28718 net.cpp:139] Memory required for data: 200960
I0428 19:57:35.385769 28718 layer_factory.hpp:77] Creating layer conv0
I0428 19:57:35.385782 28718 net.cpp:86] Creating Layer conv0
I0428 19:57:35.385815 28718 net.cpp:408] conv0 <- data
I0428 19:57:35.385826 28718 net.cpp:382] conv0 -> conv0
I0428 19:57:35.619740 28718 net.cpp:124] Setting up conv0
I0428 19:57:35.619784 28718 net.cpp:131] Top shape: 64 10 24 24 (368640)
I0428 19:57:35.619789 28718 net.cpp:139] Memory required for data: 1675520
I0428 19:57:35.619804 28718 layer_factory.hpp:77] Creating layer pool0
I0428 19:57:35.619817 28718 net.cpp:86] Creating Layer pool0
I0428 19:57:35.619822 28718 net.cpp:408] pool0 <- conv0
I0428 19:57:35.619827 28718 net.cpp:382] pool0 -> pool0
I0428 19:57:35.619871 28718 net.cpp:124] Setting up pool0
I0428 19:57:35.619879 28718 net.cpp:131] Top shape: 64 10 12 12 (92160)
I0428 19:57:35.619881 28718 net.cpp:139] Memory required for data: 2044160
I0428 19:57:35.619884 28718 layer_factory.hpp:77] Creating layer conv1
I0428 19:57:35.619895 28718 net.cpp:86] Creating Layer conv1
I0428 19:57:35.619899 28718 net.cpp:408] conv1 <- pool0
I0428 19:57:35.619904 28718 net.cpp:382] conv1 -> conv1
I0428 19:57:35.622748 28718 net.cpp:124] Setting up conv1
I0428 19:57:35.622777 28718 net.cpp:131] Top shape: 64 5 8 8 (20480)
I0428 19:57:35.622781 28718 net.cpp:139] Memory required for data: 2126080
I0428 19:57:35.622792 28718 layer_factory.hpp:77] Creating layer pool1
I0428 19:57:35.622798 28718 net.cpp:86] Creating Layer pool1
I0428 19:57:35.622802 28718 net.cpp:408] pool1 <- conv1
I0428 19:57:35.622807 28718 net.cpp:382] pool1 -> pool1
I0428 19:57:35.622843 28718 net.cpp:124] Setting up pool1
I0428 19:57:35.622849 28718 net.cpp:131] Top shape: 64 5 4 4 (5120)
I0428 19:57:35.622851 28718 net.cpp:139] Memory required for data: 2146560
I0428 19:57:35.622854 28718 layer_factory.hpp:77] Creating layer ip1
I0428 19:57:35.622862 28718 net.cpp:86] Creating Layer ip1
I0428 19:57:35.622865 28718 net.cpp:408] ip1 <- pool1
I0428 19:57:35.622870 28718 net.cpp:382] ip1 -> ip1
I0428 19:57:35.622977 28718 net.cpp:124] Setting up ip1
I0428 19:57:35.622985 28718 net.cpp:131] Top shape: 64 10 (640)
I0428 19:57:35.622987 28718 net.cpp:139] Memory required for data: 2149120
I0428 19:57:35.622994 28718 layer_factory.hpp:77] Creating layer relu1
I0428 19:57:35.623001 28718 net.cpp:86] Creating Layer relu1
I0428 19:57:35.623005 28718 net.cpp:408] relu1 <- ip1
I0428 19:57:35.623009 28718 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:57:35.623191 28718 net.cpp:124] Setting up relu1
I0428 19:57:35.623199 28718 net.cpp:131] Top shape: 64 10 (640)
I0428 19:57:35.623203 28718 net.cpp:139] Memory required for data: 2151680
I0428 19:57:35.623206 28718 layer_factory.hpp:77] Creating layer ip2
I0428 19:57:35.623212 28718 net.cpp:86] Creating Layer ip2
I0428 19:57:35.623215 28718 net.cpp:408] ip2 <- ip1
I0428 19:57:35.623221 28718 net.cpp:382] ip2 -> ip2
I0428 19:57:35.623320 28718 net.cpp:124] Setting up ip2
I0428 19:57:35.623327 28718 net.cpp:131] Top shape: 64 10 (640)
I0428 19:57:35.623330 28718 net.cpp:139] Memory required for data: 2154240
I0428 19:57:35.623335 28718 layer_factory.hpp:77] Creating layer relu2
I0428 19:57:35.623340 28718 net.cpp:86] Creating Layer relu2
I0428 19:57:35.623343 28718 net.cpp:408] relu2 <- ip2
I0428 19:57:35.623347 28718 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:57:35.624131 28718 net.cpp:124] Setting up relu2
I0428 19:57:35.624143 28718 net.cpp:131] Top shape: 64 10 (640)
I0428 19:57:35.624163 28718 net.cpp:139] Memory required for data: 2156800
I0428 19:57:35.624166 28718 layer_factory.hpp:77] Creating layer ip3
I0428 19:57:35.624174 28718 net.cpp:86] Creating Layer ip3
I0428 19:57:35.624177 28718 net.cpp:408] ip3 <- ip2
I0428 19:57:35.624184 28718 net.cpp:382] ip3 -> ip3
I0428 19:57:35.624295 28718 net.cpp:124] Setting up ip3
I0428 19:57:35.624303 28718 net.cpp:131] Top shape: 64 10 (640)
I0428 19:57:35.624305 28718 net.cpp:139] Memory required for data: 2159360
I0428 19:57:35.624313 28718 layer_factory.hpp:77] Creating layer relu3
I0428 19:57:35.624318 28718 net.cpp:86] Creating Layer relu3
I0428 19:57:35.624321 28718 net.cpp:408] relu3 <- ip3
I0428 19:57:35.624326 28718 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:57:35.624511 28718 net.cpp:124] Setting up relu3
I0428 19:57:35.624519 28718 net.cpp:131] Top shape: 64 10 (640)
I0428 19:57:35.624522 28718 net.cpp:139] Memory required for data: 2161920
I0428 19:57:35.624526 28718 layer_factory.hpp:77] Creating layer loss
I0428 19:57:35.624531 28718 net.cpp:86] Creating Layer loss
I0428 19:57:35.624534 28718 net.cpp:408] loss <- ip3
I0428 19:57:35.624538 28718 net.cpp:408] loss <- label
I0428 19:57:35.624544 28718 net.cpp:382] loss -> loss
I0428 19:57:35.624575 28718 layer_factory.hpp:77] Creating layer loss
I0428 19:57:35.624805 28718 net.cpp:124] Setting up loss
I0428 19:57:35.624835 28718 net.cpp:131] Top shape: (1)
I0428 19:57:35.624838 28718 net.cpp:134]     with loss weight 1
I0428 19:57:35.624853 28718 net.cpp:139] Memory required for data: 2161924
I0428 19:57:35.624856 28718 net.cpp:200] loss needs backward computation.
I0428 19:57:35.624861 28718 net.cpp:200] relu3 needs backward computation.
I0428 19:57:35.624879 28718 net.cpp:200] ip3 needs backward computation.
I0428 19:57:35.624882 28718 net.cpp:200] relu2 needs backward computation.
I0428 19:57:35.624886 28718 net.cpp:200] ip2 needs backward computation.
I0428 19:57:35.624896 28718 net.cpp:200] relu1 needs backward computation.
I0428 19:57:35.624898 28718 net.cpp:200] ip1 needs backward computation.
I0428 19:57:35.624902 28718 net.cpp:200] pool1 needs backward computation.
I0428 19:57:35.624904 28718 net.cpp:200] conv1 needs backward computation.
I0428 19:57:35.624907 28718 net.cpp:200] pool0 needs backward computation.
I0428 19:57:35.624910 28718 net.cpp:200] conv0 needs backward computation.
I0428 19:57:35.624914 28718 net.cpp:202] mnist does not need backward computation.
I0428 19:57:35.624917 28718 net.cpp:244] This network produces output loss
I0428 19:57:35.624928 28718 net.cpp:257] Network initialization done.
I0428 19:57:35.625319 28718 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test793.prototxt
I0428 19:57:35.625377 28718 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:57:35.625484 28718 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:57:35.625563 28718 layer_factory.hpp:77] Creating layer mnist
I0428 19:57:35.625605 28718 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:57:35.625619 28718 net.cpp:86] Creating Layer mnist
I0428 19:57:35.625623 28718 net.cpp:382] mnist -> data
I0428 19:57:35.625632 28718 net.cpp:382] mnist -> label
I0428 19:57:35.625713 28718 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:57:35.627692 28718 net.cpp:124] Setting up mnist
I0428 19:57:35.627732 28718 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:57:35.627737 28718 net.cpp:131] Top shape: 100 (100)
I0428 19:57:35.627740 28718 net.cpp:139] Memory required for data: 314000
I0428 19:57:35.627743 28718 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:57:35.627784 28718 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:57:35.627791 28718 net.cpp:408] label_mnist_1_split <- label
I0428 19:57:35.627796 28718 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:57:35.627804 28718 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:57:35.627846 28718 net.cpp:124] Setting up label_mnist_1_split
I0428 19:57:35.627853 28718 net.cpp:131] Top shape: 100 (100)
I0428 19:57:35.627857 28718 net.cpp:131] Top shape: 100 (100)
I0428 19:57:35.627861 28718 net.cpp:139] Memory required for data: 314800
I0428 19:57:35.627863 28718 layer_factory.hpp:77] Creating layer conv0
I0428 19:57:35.627873 28718 net.cpp:86] Creating Layer conv0
I0428 19:57:35.627876 28718 net.cpp:408] conv0 <- data
I0428 19:57:35.627882 28718 net.cpp:382] conv0 -> conv0
I0428 19:57:35.629694 28718 net.cpp:124] Setting up conv0
I0428 19:57:35.629724 28718 net.cpp:131] Top shape: 100 10 24 24 (576000)
I0428 19:57:35.629729 28718 net.cpp:139] Memory required for data: 2618800
I0428 19:57:35.629737 28718 layer_factory.hpp:77] Creating layer pool0
I0428 19:57:35.629745 28718 net.cpp:86] Creating Layer pool0
I0428 19:57:35.629750 28718 net.cpp:408] pool0 <- conv0
I0428 19:57:35.629755 28718 net.cpp:382] pool0 -> pool0
I0428 19:57:35.629806 28718 net.cpp:124] Setting up pool0
I0428 19:57:35.629812 28718 net.cpp:131] Top shape: 100 10 12 12 (144000)
I0428 19:57:35.629814 28718 net.cpp:139] Memory required for data: 3194800
I0428 19:57:35.629817 28718 layer_factory.hpp:77] Creating layer conv1
I0428 19:57:35.629827 28718 net.cpp:86] Creating Layer conv1
I0428 19:57:35.629830 28718 net.cpp:408] conv1 <- pool0
I0428 19:57:35.629837 28718 net.cpp:382] conv1 -> conv1
I0428 19:57:35.632062 28718 net.cpp:124] Setting up conv1
I0428 19:57:35.632091 28718 net.cpp:131] Top shape: 100 5 8 8 (32000)
I0428 19:57:35.632094 28718 net.cpp:139] Memory required for data: 3322800
I0428 19:57:35.632104 28718 layer_factory.hpp:77] Creating layer pool1
I0428 19:57:35.632112 28718 net.cpp:86] Creating Layer pool1
I0428 19:57:35.632117 28718 net.cpp:408] pool1 <- conv1
I0428 19:57:35.632122 28718 net.cpp:382] pool1 -> pool1
I0428 19:57:35.632160 28718 net.cpp:124] Setting up pool1
I0428 19:57:35.632185 28718 net.cpp:131] Top shape: 100 5 4 4 (8000)
I0428 19:57:35.632189 28718 net.cpp:139] Memory required for data: 3354800
I0428 19:57:35.632192 28718 layer_factory.hpp:77] Creating layer ip1
I0428 19:57:35.632200 28718 net.cpp:86] Creating Layer ip1
I0428 19:57:35.632202 28718 net.cpp:408] ip1 <- pool1
I0428 19:57:35.632207 28718 net.cpp:382] ip1 -> ip1
I0428 19:57:35.632355 28718 net.cpp:124] Setting up ip1
I0428 19:57:35.632364 28718 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:57:35.632378 28718 net.cpp:139] Memory required for data: 3358800
I0428 19:57:35.632386 28718 layer_factory.hpp:77] Creating layer relu1
I0428 19:57:35.632393 28718 net.cpp:86] Creating Layer relu1
I0428 19:57:35.632397 28718 net.cpp:408] relu1 <- ip1
I0428 19:57:35.632402 28718 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:57:35.632644 28718 net.cpp:124] Setting up relu1
I0428 19:57:35.632654 28718 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:57:35.632658 28718 net.cpp:139] Memory required for data: 3362800
I0428 19:57:35.632663 28718 layer_factory.hpp:77] Creating layer ip2
I0428 19:57:35.632670 28718 net.cpp:86] Creating Layer ip2
I0428 19:57:35.632674 28718 net.cpp:408] ip2 <- ip1
I0428 19:57:35.632680 28718 net.cpp:382] ip2 -> ip2
I0428 19:57:35.632797 28718 net.cpp:124] Setting up ip2
I0428 19:57:35.632803 28718 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:57:35.632807 28718 net.cpp:139] Memory required for data: 3366800
I0428 19:57:35.632835 28718 layer_factory.hpp:77] Creating layer relu2
I0428 19:57:35.632841 28718 net.cpp:86] Creating Layer relu2
I0428 19:57:35.632844 28718 net.cpp:408] relu2 <- ip2
I0428 19:57:35.632850 28718 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:57:35.633031 28718 net.cpp:124] Setting up relu2
I0428 19:57:35.633040 28718 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:57:35.633044 28718 net.cpp:139] Memory required for data: 3370800
I0428 19:57:35.633047 28718 layer_factory.hpp:77] Creating layer ip3
I0428 19:57:35.633054 28718 net.cpp:86] Creating Layer ip3
I0428 19:57:35.633056 28718 net.cpp:408] ip3 <- ip2
I0428 19:57:35.633064 28718 net.cpp:382] ip3 -> ip3
I0428 19:57:35.633185 28718 net.cpp:124] Setting up ip3
I0428 19:57:35.633205 28718 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:57:35.633220 28718 net.cpp:139] Memory required for data: 3374800
I0428 19:57:35.633229 28718 layer_factory.hpp:77] Creating layer relu3
I0428 19:57:35.633234 28718 net.cpp:86] Creating Layer relu3
I0428 19:57:35.633239 28718 net.cpp:408] relu3 <- ip3
I0428 19:57:35.633244 28718 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:57:35.634114 28718 net.cpp:124] Setting up relu3
I0428 19:57:35.634126 28718 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:57:35.634147 28718 net.cpp:139] Memory required for data: 3378800
I0428 19:57:35.634150 28718 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:57:35.634155 28718 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:57:35.634160 28718 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:57:35.634166 28718 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:57:35.634172 28718 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:57:35.634224 28718 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:57:35.634230 28718 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:57:35.634233 28718 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:57:35.634237 28718 net.cpp:139] Memory required for data: 3386800
I0428 19:57:35.634239 28718 layer_factory.hpp:77] Creating layer accuracy
I0428 19:57:35.634244 28718 net.cpp:86] Creating Layer accuracy
I0428 19:57:35.634254 28718 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:57:35.634258 28718 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:57:35.634263 28718 net.cpp:382] accuracy -> accuracy
I0428 19:57:35.634270 28718 net.cpp:124] Setting up accuracy
I0428 19:57:35.634274 28718 net.cpp:131] Top shape: (1)
I0428 19:57:35.634277 28718 net.cpp:139] Memory required for data: 3386804
I0428 19:57:35.634280 28718 layer_factory.hpp:77] Creating layer loss
I0428 19:57:35.634284 28718 net.cpp:86] Creating Layer loss
I0428 19:57:35.634287 28718 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:57:35.634292 28718 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:57:35.634296 28718 net.cpp:382] loss -> loss
I0428 19:57:35.634302 28718 layer_factory.hpp:77] Creating layer loss
I0428 19:57:35.634552 28718 net.cpp:124] Setting up loss
I0428 19:57:35.634562 28718 net.cpp:131] Top shape: (1)
I0428 19:57:35.634565 28718 net.cpp:134]     with loss weight 1
I0428 19:57:35.634572 28718 net.cpp:139] Memory required for data: 3386808
I0428 19:57:35.634585 28718 net.cpp:200] loss needs backward computation.
I0428 19:57:35.634589 28718 net.cpp:202] accuracy does not need backward computation.
I0428 19:57:35.634593 28718 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:57:35.634598 28718 net.cpp:200] relu3 needs backward computation.
I0428 19:57:35.634600 28718 net.cpp:200] ip3 needs backward computation.
I0428 19:57:35.634603 28718 net.cpp:200] relu2 needs backward computation.
I0428 19:57:35.634606 28718 net.cpp:200] ip2 needs backward computation.
I0428 19:57:35.634614 28718 net.cpp:200] relu1 needs backward computation.
I0428 19:57:35.634618 28718 net.cpp:200] ip1 needs backward computation.
I0428 19:57:35.634621 28718 net.cpp:200] pool1 needs backward computation.
I0428 19:57:35.634624 28718 net.cpp:200] conv1 needs backward computation.
I0428 19:57:35.634632 28718 net.cpp:200] pool0 needs backward computation.
I0428 19:57:35.634635 28718 net.cpp:200] conv0 needs backward computation.
I0428 19:57:35.634644 28718 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:57:35.634646 28718 net.cpp:202] mnist does not need backward computation.
I0428 19:57:35.634650 28718 net.cpp:244] This network produces output accuracy
I0428 19:57:35.634654 28718 net.cpp:244] This network produces output loss
I0428 19:57:35.634665 28718 net.cpp:257] Network initialization done.
I0428 19:57:35.634721 28718 solver.cpp:56] Solver scaffolding done.
I0428 19:57:35.635056 28718 caffe.cpp:248] Starting Optimization
I0428 19:57:35.635061 28718 solver.cpp:273] Solving LeNet
I0428 19:57:35.635063 28718 solver.cpp:274] Learning Rate Policy: inv
I0428 19:57:35.635967 28718 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:57:35.639686 28718 blocking_queue.cpp:49] Waiting for data
I0428 19:57:35.711374 28725 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:57:35.711942 28718 solver.cpp:398]     Test net output #0: accuracy = 0.1
I0428 19:57:35.711961 28718 solver.cpp:398]     Test net output #1: loss = 2.30102 (* 1 = 2.30102 loss)
I0428 19:57:35.714596 28718 solver.cpp:219] Iteration 0 (-1.02295e-42 iter/s, 0.079487s/100 iters), loss = 2.32189
I0428 19:57:35.714635 28718 solver.cpp:238]     Train net output #0: loss = 2.32189 (* 1 = 2.32189 loss)
I0428 19:57:35.714648 28718 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:57:35.791949 28718 solver.cpp:219] Iteration 100 (1293.36 iter/s, 0.0773181s/100 iters), loss = 1.3709
I0428 19:57:35.791990 28718 solver.cpp:238]     Train net output #0: loss = 1.3709 (* 1 = 1.3709 loss)
I0428 19:57:35.791996 28718 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:57:35.866597 28718 solver.cpp:219] Iteration 200 (1340.2 iter/s, 0.0746158s/100 iters), loss = 0.875095
I0428 19:57:35.866636 28718 solver.cpp:238]     Train net output #0: loss = 0.875095 (* 1 = 0.875095 loss)
I0428 19:57:35.866641 28718 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:57:35.941345 28718 solver.cpp:219] Iteration 300 (1338.38 iter/s, 0.074717s/100 iters), loss = 0.418153
I0428 19:57:35.941382 28718 solver.cpp:238]     Train net output #0: loss = 0.418153 (* 1 = 0.418153 loss)
I0428 19:57:35.941388 28718 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:57:36.015579 28718 solver.cpp:219] Iteration 400 (1347.65 iter/s, 0.0742031s/100 iters), loss = 0.367142
I0428 19:57:36.015617 28718 solver.cpp:238]     Train net output #0: loss = 0.367142 (* 1 = 0.367142 loss)
I0428 19:57:36.015624 28718 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:57:36.090143 28718 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:57:36.135520 28725 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:57:36.136093 28718 solver.cpp:398]     Test net output #0: accuracy = 0.8809
I0428 19:57:36.136126 28718 solver.cpp:398]     Test net output #1: loss = 0.377503 (* 1 = 0.377503 loss)
I0428 19:57:36.136942 28718 solver.cpp:219] Iteration 500 (824.215 iter/s, 0.121328s/100 iters), loss = 0.408616
I0428 19:57:36.136976 28718 solver.cpp:238]     Train net output #0: loss = 0.408617 (* 1 = 0.408617 loss)
I0428 19:57:36.137001 28718 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:57:36.218528 28718 solver.cpp:219] Iteration 600 (1226.32 iter/s, 0.081545s/100 iters), loss = 0.199442
I0428 19:57:36.218569 28718 solver.cpp:238]     Train net output #0: loss = 0.199442 (* 1 = 0.199442 loss)
I0428 19:57:36.218575 28718 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:57:36.299785 28718 solver.cpp:219] Iteration 700 (1231.17 iter/s, 0.0812237s/100 iters), loss = 0.470149
I0428 19:57:36.299840 28718 solver.cpp:238]     Train net output #0: loss = 0.470149 (* 1 = 0.470149 loss)
I0428 19:57:36.299846 28718 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:57:36.378351 28718 solver.cpp:219] Iteration 800 (1273.59 iter/s, 0.0785183s/100 iters), loss = 0.342142
I0428 19:57:36.378391 28718 solver.cpp:238]     Train net output #0: loss = 0.342142 (* 1 = 0.342142 loss)
I0428 19:57:36.378396 28718 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:57:36.457482 28718 solver.cpp:219] Iteration 900 (1264.27 iter/s, 0.0790968s/100 iters), loss = 0.332418
I0428 19:57:36.457512 28718 solver.cpp:238]     Train net output #0: loss = 0.332418 (* 1 = 0.332418 loss)
I0428 19:57:36.457520 28718 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:57:36.483395 28724 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:57:36.536039 28718 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:57:36.536819 28718 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:57:36.537349 28718 solver.cpp:311] Iteration 1000, loss = 0.146316
I0428 19:57:36.537365 28718 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:57:36.611999 28725 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:57:36.612540 28718 solver.cpp:398]     Test net output #0: accuracy = 0.9505
I0428 19:57:36.612574 28718 solver.cpp:398]     Test net output #1: loss = 0.162734 (* 1 = 0.162734 loss)
I0428 19:57:36.612579 28718 solver.cpp:316] Optimization Done.
I0428 19:57:36.612582 28718 caffe.cpp:259] Optimization Done.
