I0428 19:51:52.540115 27383 caffe.cpp:218] Using GPUs 0
I0428 19:51:52.577102 27383 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:51:53.091205 27383 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test646.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:51:53.091351 27383 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test646.prototxt
I0428 19:51:53.091773 27383 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:51:53.091792 27383 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:51:53.091894 27383 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:51:53.091974 27383 layer_factory.hpp:77] Creating layer mnist
I0428 19:51:53.092074 27383 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:51:53.092099 27383 net.cpp:86] Creating Layer mnist
I0428 19:51:53.092108 27383 net.cpp:382] mnist -> data
I0428 19:51:53.092128 27383 net.cpp:382] mnist -> label
I0428 19:51:53.093220 27383 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:51:53.095660 27383 net.cpp:124] Setting up mnist
I0428 19:51:53.095679 27383 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:51:53.095686 27383 net.cpp:131] Top shape: 64 (64)
I0428 19:51:53.095690 27383 net.cpp:139] Memory required for data: 200960
I0428 19:51:53.095696 27383 layer_factory.hpp:77] Creating layer conv0
I0428 19:51:53.095712 27383 net.cpp:86] Creating Layer conv0
I0428 19:51:53.095732 27383 net.cpp:408] conv0 <- data
I0428 19:51:53.095746 27383 net.cpp:382] conv0 -> conv0
I0428 19:51:53.381367 27383 net.cpp:124] Setting up conv0
I0428 19:51:53.381395 27383 net.cpp:131] Top shape: 64 5 24 24 (184320)
I0428 19:51:53.381400 27383 net.cpp:139] Memory required for data: 938240
I0428 19:51:53.381415 27383 layer_factory.hpp:77] Creating layer pool0
I0428 19:51:53.381428 27383 net.cpp:86] Creating Layer pool0
I0428 19:51:53.381433 27383 net.cpp:408] pool0 <- conv0
I0428 19:51:53.381438 27383 net.cpp:382] pool0 -> pool0
I0428 19:51:53.381487 27383 net.cpp:124] Setting up pool0
I0428 19:51:53.381492 27383 net.cpp:131] Top shape: 64 5 12 12 (46080)
I0428 19:51:53.381495 27383 net.cpp:139] Memory required for data: 1122560
I0428 19:51:53.381498 27383 layer_factory.hpp:77] Creating layer conv1
I0428 19:51:53.381510 27383 net.cpp:86] Creating Layer conv1
I0428 19:51:53.381513 27383 net.cpp:408] conv1 <- pool0
I0428 19:51:53.381517 27383 net.cpp:382] conv1 -> conv1
I0428 19:51:53.384212 27383 net.cpp:124] Setting up conv1
I0428 19:51:53.384227 27383 net.cpp:131] Top shape: 64 25 8 8 (102400)
I0428 19:51:53.384232 27383 net.cpp:139] Memory required for data: 1532160
I0428 19:51:53.384239 27383 layer_factory.hpp:77] Creating layer pool1
I0428 19:51:53.384248 27383 net.cpp:86] Creating Layer pool1
I0428 19:51:53.384251 27383 net.cpp:408] pool1 <- conv1
I0428 19:51:53.384256 27383 net.cpp:382] pool1 -> pool1
I0428 19:51:53.384294 27383 net.cpp:124] Setting up pool1
I0428 19:51:53.384299 27383 net.cpp:131] Top shape: 64 25 4 4 (25600)
I0428 19:51:53.384302 27383 net.cpp:139] Memory required for data: 1634560
I0428 19:51:53.384305 27383 layer_factory.hpp:77] Creating layer ip1
I0428 19:51:53.384312 27383 net.cpp:86] Creating Layer ip1
I0428 19:51:53.384315 27383 net.cpp:408] ip1 <- pool1
I0428 19:51:53.384320 27383 net.cpp:382] ip1 -> ip1
I0428 19:51:53.385305 27383 net.cpp:124] Setting up ip1
I0428 19:51:53.385318 27383 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:51:53.385323 27383 net.cpp:139] Memory required for data: 1640960
I0428 19:51:53.385330 27383 layer_factory.hpp:77] Creating layer relu1
I0428 19:51:53.385337 27383 net.cpp:86] Creating Layer relu1
I0428 19:51:53.385340 27383 net.cpp:408] relu1 <- ip1
I0428 19:51:53.385345 27383 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:51:53.385514 27383 net.cpp:124] Setting up relu1
I0428 19:51:53.385522 27383 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:51:53.385526 27383 net.cpp:139] Memory required for data: 1647360
I0428 19:51:53.385529 27383 layer_factory.hpp:77] Creating layer ip2
I0428 19:51:53.385536 27383 net.cpp:86] Creating Layer ip2
I0428 19:51:53.385540 27383 net.cpp:408] ip2 <- ip1
I0428 19:51:53.385545 27383 net.cpp:382] ip2 -> ip2
I0428 19:51:53.385641 27383 net.cpp:124] Setting up ip2
I0428 19:51:53.385648 27383 net.cpp:131] Top shape: 64 10 (640)
I0428 19:51:53.385651 27383 net.cpp:139] Memory required for data: 1649920
I0428 19:51:53.385656 27383 layer_factory.hpp:77] Creating layer relu2
I0428 19:51:53.385663 27383 net.cpp:86] Creating Layer relu2
I0428 19:51:53.385665 27383 net.cpp:408] relu2 <- ip2
I0428 19:51:53.385669 27383 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:51:53.386392 27383 net.cpp:124] Setting up relu2
I0428 19:51:53.386405 27383 net.cpp:131] Top shape: 64 10 (640)
I0428 19:51:53.386409 27383 net.cpp:139] Memory required for data: 1652480
I0428 19:51:53.386412 27383 layer_factory.hpp:77] Creating layer ip3
I0428 19:51:53.386420 27383 net.cpp:86] Creating Layer ip3
I0428 19:51:53.386422 27383 net.cpp:408] ip3 <- ip2
I0428 19:51:53.386428 27383 net.cpp:382] ip3 -> ip3
I0428 19:51:53.386528 27383 net.cpp:124] Setting up ip3
I0428 19:51:53.386536 27383 net.cpp:131] Top shape: 64 10 (640)
I0428 19:51:53.386539 27383 net.cpp:139] Memory required for data: 1655040
I0428 19:51:53.386546 27383 layer_factory.hpp:77] Creating layer relu3
I0428 19:51:53.386553 27383 net.cpp:86] Creating Layer relu3
I0428 19:51:53.386555 27383 net.cpp:408] relu3 <- ip3
I0428 19:51:53.386559 27383 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:51:53.386724 27383 net.cpp:124] Setting up relu3
I0428 19:51:53.386732 27383 net.cpp:131] Top shape: 64 10 (640)
I0428 19:51:53.386735 27383 net.cpp:139] Memory required for data: 1657600
I0428 19:51:53.386739 27383 layer_factory.hpp:77] Creating layer loss
I0428 19:51:53.386745 27383 net.cpp:86] Creating Layer loss
I0428 19:51:53.386749 27383 net.cpp:408] loss <- ip3
I0428 19:51:53.386752 27383 net.cpp:408] loss <- label
I0428 19:51:53.386757 27383 net.cpp:382] loss -> loss
I0428 19:51:53.386775 27383 layer_factory.hpp:77] Creating layer loss
I0428 19:51:53.387006 27383 net.cpp:124] Setting up loss
I0428 19:51:53.387017 27383 net.cpp:131] Top shape: (1)
I0428 19:51:53.387022 27383 net.cpp:134]     with loss weight 1
I0428 19:51:53.387035 27383 net.cpp:139] Memory required for data: 1657604
I0428 19:51:53.387039 27383 net.cpp:200] loss needs backward computation.
I0428 19:51:53.387043 27383 net.cpp:200] relu3 needs backward computation.
I0428 19:51:53.387046 27383 net.cpp:200] ip3 needs backward computation.
I0428 19:51:53.387049 27383 net.cpp:200] relu2 needs backward computation.
I0428 19:51:53.387051 27383 net.cpp:200] ip2 needs backward computation.
I0428 19:51:53.387054 27383 net.cpp:200] relu1 needs backward computation.
I0428 19:51:53.387058 27383 net.cpp:200] ip1 needs backward computation.
I0428 19:51:53.387060 27383 net.cpp:200] pool1 needs backward computation.
I0428 19:51:53.387063 27383 net.cpp:200] conv1 needs backward computation.
I0428 19:51:53.387066 27383 net.cpp:200] pool0 needs backward computation.
I0428 19:51:53.387069 27383 net.cpp:200] conv0 needs backward computation.
I0428 19:51:53.387073 27383 net.cpp:202] mnist does not need backward computation.
I0428 19:51:53.387075 27383 net.cpp:244] This network produces output loss
I0428 19:51:53.387085 27383 net.cpp:257] Network initialization done.
I0428 19:51:53.387413 27383 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test646.prototxt
I0428 19:51:53.387439 27383 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:51:53.387531 27383 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:51:53.387612 27383 layer_factory.hpp:77] Creating layer mnist
I0428 19:51:53.387657 27383 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:51:53.387670 27383 net.cpp:86] Creating Layer mnist
I0428 19:51:53.387673 27383 net.cpp:382] mnist -> data
I0428 19:51:53.387681 27383 net.cpp:382] mnist -> label
I0428 19:51:53.387765 27383 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:51:53.389660 27383 net.cpp:124] Setting up mnist
I0428 19:51:53.389674 27383 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:51:53.389679 27383 net.cpp:131] Top shape: 100 (100)
I0428 19:51:53.389683 27383 net.cpp:139] Memory required for data: 314000
I0428 19:51:53.389686 27383 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:51:53.389706 27383 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:51:53.389710 27383 net.cpp:408] label_mnist_1_split <- label
I0428 19:51:53.389715 27383 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:51:53.389721 27383 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:51:53.389816 27383 net.cpp:124] Setting up label_mnist_1_split
I0428 19:51:53.389823 27383 net.cpp:131] Top shape: 100 (100)
I0428 19:51:53.389827 27383 net.cpp:131] Top shape: 100 (100)
I0428 19:51:53.389830 27383 net.cpp:139] Memory required for data: 314800
I0428 19:51:53.389833 27383 layer_factory.hpp:77] Creating layer conv0
I0428 19:51:53.389842 27383 net.cpp:86] Creating Layer conv0
I0428 19:51:53.389844 27383 net.cpp:408] conv0 <- data
I0428 19:51:53.389849 27383 net.cpp:382] conv0 -> conv0
I0428 19:51:53.391393 27383 net.cpp:124] Setting up conv0
I0428 19:51:53.391407 27383 net.cpp:131] Top shape: 100 5 24 24 (288000)
I0428 19:51:53.391412 27383 net.cpp:139] Memory required for data: 1466800
I0428 19:51:53.391420 27383 layer_factory.hpp:77] Creating layer pool0
I0428 19:51:53.391427 27383 net.cpp:86] Creating Layer pool0
I0428 19:51:53.391430 27383 net.cpp:408] pool0 <- conv0
I0428 19:51:53.391434 27383 net.cpp:382] pool0 -> pool0
I0428 19:51:53.391470 27383 net.cpp:124] Setting up pool0
I0428 19:51:53.391475 27383 net.cpp:131] Top shape: 100 5 12 12 (72000)
I0428 19:51:53.391479 27383 net.cpp:139] Memory required for data: 1754800
I0428 19:51:53.391481 27383 layer_factory.hpp:77] Creating layer conv1
I0428 19:51:53.391489 27383 net.cpp:86] Creating Layer conv1
I0428 19:51:53.391492 27383 net.cpp:408] conv1 <- pool0
I0428 19:51:53.391496 27383 net.cpp:382] conv1 -> conv1
I0428 19:51:53.393586 27383 net.cpp:124] Setting up conv1
I0428 19:51:53.393599 27383 net.cpp:131] Top shape: 100 25 8 8 (160000)
I0428 19:51:53.393604 27383 net.cpp:139] Memory required for data: 2394800
I0428 19:51:53.393612 27383 layer_factory.hpp:77] Creating layer pool1
I0428 19:51:53.393620 27383 net.cpp:86] Creating Layer pool1
I0428 19:51:53.393622 27383 net.cpp:408] pool1 <- conv1
I0428 19:51:53.393627 27383 net.cpp:382] pool1 -> pool1
I0428 19:51:53.393666 27383 net.cpp:124] Setting up pool1
I0428 19:51:53.393671 27383 net.cpp:131] Top shape: 100 25 4 4 (40000)
I0428 19:51:53.393674 27383 net.cpp:139] Memory required for data: 2554800
I0428 19:51:53.393677 27383 layer_factory.hpp:77] Creating layer ip1
I0428 19:51:53.393683 27383 net.cpp:86] Creating Layer ip1
I0428 19:51:53.393687 27383 net.cpp:408] ip1 <- pool1
I0428 19:51:53.393692 27383 net.cpp:382] ip1 -> ip1
I0428 19:51:53.393843 27383 net.cpp:124] Setting up ip1
I0428 19:51:53.393851 27383 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:51:53.393864 27383 net.cpp:139] Memory required for data: 2564800
I0428 19:51:53.393872 27383 layer_factory.hpp:77] Creating layer relu1
I0428 19:51:53.393877 27383 net.cpp:86] Creating Layer relu1
I0428 19:51:53.393880 27383 net.cpp:408] relu1 <- ip1
I0428 19:51:53.393901 27383 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:51:53.394075 27383 net.cpp:124] Setting up relu1
I0428 19:51:53.394084 27383 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:51:53.394088 27383 net.cpp:139] Memory required for data: 2574800
I0428 19:51:53.394090 27383 layer_factory.hpp:77] Creating layer ip2
I0428 19:51:53.394098 27383 net.cpp:86] Creating Layer ip2
I0428 19:51:53.394101 27383 net.cpp:408] ip2 <- ip1
I0428 19:51:53.394106 27383 net.cpp:382] ip2 -> ip2
I0428 19:51:53.394202 27383 net.cpp:124] Setting up ip2
I0428 19:51:53.394209 27383 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:51:53.394212 27383 net.cpp:139] Memory required for data: 2578800
I0428 19:51:53.394217 27383 layer_factory.hpp:77] Creating layer relu2
I0428 19:51:53.394223 27383 net.cpp:86] Creating Layer relu2
I0428 19:51:53.394227 27383 net.cpp:408] relu2 <- ip2
I0428 19:51:53.394230 27383 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:51:53.394374 27383 net.cpp:124] Setting up relu2
I0428 19:51:53.394382 27383 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:51:53.394387 27383 net.cpp:139] Memory required for data: 2582800
I0428 19:51:53.394389 27383 layer_factory.hpp:77] Creating layer ip3
I0428 19:51:53.394394 27383 net.cpp:86] Creating Layer ip3
I0428 19:51:53.394398 27383 net.cpp:408] ip3 <- ip2
I0428 19:51:53.394402 27383 net.cpp:382] ip3 -> ip3
I0428 19:51:53.394520 27383 net.cpp:124] Setting up ip3
I0428 19:51:53.394526 27383 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:51:53.394529 27383 net.cpp:139] Memory required for data: 2586800
I0428 19:51:53.394537 27383 layer_factory.hpp:77] Creating layer relu3
I0428 19:51:53.394542 27383 net.cpp:86] Creating Layer relu3
I0428 19:51:53.394546 27383 net.cpp:408] relu3 <- ip3
I0428 19:51:53.394549 27383 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:51:53.395364 27383 net.cpp:124] Setting up relu3
I0428 19:51:53.395376 27383 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:51:53.395380 27383 net.cpp:139] Memory required for data: 2590800
I0428 19:51:53.395385 27383 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:51:53.395390 27383 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:51:53.395393 27383 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:51:53.395398 27383 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:51:53.395404 27383 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:51:53.395445 27383 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:51:53.395452 27383 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:51:53.395454 27383 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:51:53.395457 27383 net.cpp:139] Memory required for data: 2598800
I0428 19:51:53.395460 27383 layer_factory.hpp:77] Creating layer accuracy
I0428 19:51:53.395465 27383 net.cpp:86] Creating Layer accuracy
I0428 19:51:53.395469 27383 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:51:53.395474 27383 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:51:53.395478 27383 net.cpp:382] accuracy -> accuracy
I0428 19:51:53.395485 27383 net.cpp:124] Setting up accuracy
I0428 19:51:53.395490 27383 net.cpp:131] Top shape: (1)
I0428 19:51:53.395493 27383 net.cpp:139] Memory required for data: 2598804
I0428 19:51:53.395495 27383 layer_factory.hpp:77] Creating layer loss
I0428 19:51:53.395501 27383 net.cpp:86] Creating Layer loss
I0428 19:51:53.395504 27383 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:51:53.395514 27383 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:51:53.395519 27383 net.cpp:382] loss -> loss
I0428 19:51:53.395524 27383 layer_factory.hpp:77] Creating layer loss
I0428 19:51:53.395764 27383 net.cpp:124] Setting up loss
I0428 19:51:53.395774 27383 net.cpp:131] Top shape: (1)
I0428 19:51:53.395777 27383 net.cpp:134]     with loss weight 1
I0428 19:51:53.395783 27383 net.cpp:139] Memory required for data: 2598808
I0428 19:51:53.395795 27383 net.cpp:200] loss needs backward computation.
I0428 19:51:53.395799 27383 net.cpp:202] accuracy does not need backward computation.
I0428 19:51:53.395803 27383 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:51:53.395807 27383 net.cpp:200] relu3 needs backward computation.
I0428 19:51:53.395809 27383 net.cpp:200] ip3 needs backward computation.
I0428 19:51:53.395812 27383 net.cpp:200] relu2 needs backward computation.
I0428 19:51:53.395815 27383 net.cpp:200] ip2 needs backward computation.
I0428 19:51:53.395817 27383 net.cpp:200] relu1 needs backward computation.
I0428 19:51:53.395820 27383 net.cpp:200] ip1 needs backward computation.
I0428 19:51:53.395823 27383 net.cpp:200] pool1 needs backward computation.
I0428 19:51:53.395826 27383 net.cpp:200] conv1 needs backward computation.
I0428 19:51:53.395829 27383 net.cpp:200] pool0 needs backward computation.
I0428 19:51:53.395833 27383 net.cpp:200] conv0 needs backward computation.
I0428 19:51:53.395836 27383 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:51:53.395840 27383 net.cpp:202] mnist does not need backward computation.
I0428 19:51:53.395843 27383 net.cpp:244] This network produces output accuracy
I0428 19:51:53.395846 27383 net.cpp:244] This network produces output loss
I0428 19:51:53.395858 27383 net.cpp:257] Network initialization done.
I0428 19:51:53.395900 27383 solver.cpp:56] Solver scaffolding done.
I0428 19:51:53.396241 27383 caffe.cpp:248] Starting Optimization
I0428 19:51:53.396246 27383 solver.cpp:273] Solving LeNet
I0428 19:51:53.396250 27383 solver.cpp:274] Learning Rate Policy: inv
I0428 19:51:53.396459 27383 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:51:53.400625 27383 blocking_queue.cpp:49] Waiting for data
I0428 19:51:53.471758 27390 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:51:53.472301 27383 solver.cpp:398]     Test net output #0: accuracy = 0.1685
I0428 19:51:53.472349 27383 solver.cpp:398]     Test net output #1: loss = 2.2855 (* 1 = 2.2855 loss)
I0428 19:51:53.474282 27383 solver.cpp:219] Iteration 0 (-4.26241e-31 iter/s, 0.077995s/100 iters), loss = 2.26456
I0428 19:51:53.474306 27383 solver.cpp:238]     Train net output #0: loss = 2.26456 (* 1 = 2.26456 loss)
I0428 19:51:53.474318 27383 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:51:53.571871 27383 solver.cpp:219] Iteration 100 (1025.11 iter/s, 0.0975503s/100 iters), loss = 1.10413
I0428 19:51:53.571895 27383 solver.cpp:238]     Train net output #0: loss = 1.10413 (* 1 = 1.10413 loss)
I0428 19:51:53.571900 27383 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:51:53.656833 27383 solver.cpp:219] Iteration 200 (1177.46 iter/s, 0.0849285s/100 iters), loss = 0.791794
I0428 19:51:53.656858 27383 solver.cpp:238]     Train net output #0: loss = 0.791794 (* 1 = 0.791794 loss)
I0428 19:51:53.656864 27383 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:51:53.739498 27383 solver.cpp:219] Iteration 300 (1210.2 iter/s, 0.0826307s/100 iters), loss = 0.471233
I0428 19:51:53.739521 27383 solver.cpp:238]     Train net output #0: loss = 0.471233 (* 1 = 0.471233 loss)
I0428 19:51:53.739527 27383 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:51:53.822273 27383 solver.cpp:219] Iteration 400 (1208.59 iter/s, 0.0827412s/100 iters), loss = 0.684609
I0428 19:51:53.822296 27383 solver.cpp:238]     Train net output #0: loss = 0.684609 (* 1 = 0.684609 loss)
I0428 19:51:53.822319 27383 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:51:53.905135 27383 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:51:53.979818 27390 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:51:53.980363 27383 solver.cpp:398]     Test net output #0: accuracy = 0.8426
I0428 19:51:53.980399 27383 solver.cpp:398]     Test net output #1: loss = 0.59539 (* 1 = 0.59539 loss)
I0428 19:51:53.981329 27383 solver.cpp:219] Iteration 500 (628.909 iter/s, 0.159006s/100 iters), loss = 0.609229
I0428 19:51:53.981369 27383 solver.cpp:238]     Train net output #0: loss = 0.609229 (* 1 = 0.609229 loss)
I0428 19:51:53.981391 27383 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:51:54.076303 27383 solver.cpp:219] Iteration 600 (1053.49 iter/s, 0.0949228s/100 iters), loss = 0.423129
I0428 19:51:54.076346 27383 solver.cpp:238]     Train net output #0: loss = 0.423129 (* 1 = 0.423129 loss)
I0428 19:51:54.076354 27383 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:51:54.173435 27383 solver.cpp:219] Iteration 700 (1030.06 iter/s, 0.0970813s/100 iters), loss = 0.930332
I0428 19:51:54.173478 27383 solver.cpp:238]     Train net output #0: loss = 0.930332 (* 1 = 0.930332 loss)
I0428 19:51:54.173485 27383 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:51:54.258895 27383 solver.cpp:219] Iteration 800 (1170.85 iter/s, 0.0854077s/100 iters), loss = 0.619754
I0428 19:51:54.258936 27383 solver.cpp:238]     Train net output #0: loss = 0.619754 (* 1 = 0.619754 loss)
I0428 19:51:54.258942 27383 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:51:54.342522 27383 solver.cpp:219] Iteration 900 (1196.5 iter/s, 0.0835768s/100 iters), loss = 0.508784
I0428 19:51:54.342563 27383 solver.cpp:238]     Train net output #0: loss = 0.508784 (* 1 = 0.508784 loss)
I0428 19:51:54.342584 27383 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:51:54.370862 27389 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:51:54.428143 27383 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:51:54.429162 27383 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:51:54.429780 27383 solver.cpp:311] Iteration 1000, loss = 0.72851
I0428 19:51:54.429800 27383 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:51:54.483538 27390 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:51:54.484081 27383 solver.cpp:398]     Test net output #0: accuracy = 0.8755
I0428 19:51:54.484102 27383 solver.cpp:398]     Test net output #1: loss = 0.538558 (* 1 = 0.538558 loss)
I0428 19:51:54.484107 27383 solver.cpp:316] Optimization Done.
I0428 19:51:54.484110 27383 caffe.cpp:259] Optimization Done.
