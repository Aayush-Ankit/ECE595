I0428 20:19:25.107518  1359 caffe.cpp:218] Using GPUs 0
I0428 20:19:25.139092  1359 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:19:25.599503  1359 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1341.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:19:25.599630  1359 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1341.prototxt
I0428 20:19:25.599958  1359 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:19:25.599973  1359 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:19:25.600054  1359 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:19:25.600117  1359 layer_factory.hpp:77] Creating layer mnist
I0428 20:19:25.600214  1359 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:19:25.600234  1359 net.cpp:86] Creating Layer mnist
I0428 20:19:25.600240  1359 net.cpp:382] mnist -> data
I0428 20:19:25.600257  1359 net.cpp:382] mnist -> label
I0428 20:19:25.601341  1359 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:19:25.603536  1359 net.cpp:124] Setting up mnist
I0428 20:19:25.603584  1359 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:19:25.603590  1359 net.cpp:131] Top shape: 64 (64)
I0428 20:19:25.603592  1359 net.cpp:139] Memory required for data: 200960
I0428 20:19:25.603600  1359 layer_factory.hpp:77] Creating layer conv0
I0428 20:19:25.603615  1359 net.cpp:86] Creating Layer conv0
I0428 20:19:25.603636  1359 net.cpp:408] conv0 <- data
I0428 20:19:25.603646  1359 net.cpp:382] conv0 -> conv0
I0428 20:19:25.832883  1359 net.cpp:124] Setting up conv0
I0428 20:19:25.832926  1359 net.cpp:131] Top shape: 64 50 24 24 (1843200)
I0428 20:19:25.832928  1359 net.cpp:139] Memory required for data: 7573760
I0428 20:19:25.832959  1359 layer_factory.hpp:77] Creating layer pool0
I0428 20:19:25.832972  1359 net.cpp:86] Creating Layer pool0
I0428 20:19:25.832975  1359 net.cpp:408] pool0 <- conv0
I0428 20:19:25.832996  1359 net.cpp:382] pool0 -> pool0
I0428 20:19:25.833043  1359 net.cpp:124] Setting up pool0
I0428 20:19:25.833050  1359 net.cpp:131] Top shape: 64 50 12 12 (460800)
I0428 20:19:25.833052  1359 net.cpp:139] Memory required for data: 9416960
I0428 20:19:25.833055  1359 layer_factory.hpp:77] Creating layer conv1
I0428 20:19:25.833066  1359 net.cpp:86] Creating Layer conv1
I0428 20:19:25.833070  1359 net.cpp:408] conv1 <- pool0
I0428 20:19:25.833075  1359 net.cpp:382] conv1 -> conv1
I0428 20:19:25.835085  1359 net.cpp:124] Setting up conv1
I0428 20:19:25.835116  1359 net.cpp:131] Top shape: 64 25 8 8 (102400)
I0428 20:19:25.835120  1359 net.cpp:139] Memory required for data: 9826560
I0428 20:19:25.835129  1359 layer_factory.hpp:77] Creating layer pool1
I0428 20:19:25.835152  1359 net.cpp:86] Creating Layer pool1
I0428 20:19:25.835155  1359 net.cpp:408] pool1 <- conv1
I0428 20:19:25.835160  1359 net.cpp:382] pool1 -> pool1
I0428 20:19:25.835213  1359 net.cpp:124] Setting up pool1
I0428 20:19:25.835218  1359 net.cpp:131] Top shape: 64 25 4 4 (25600)
I0428 20:19:25.835222  1359 net.cpp:139] Memory required for data: 9928960
I0428 20:19:25.835225  1359 layer_factory.hpp:77] Creating layer ip1
I0428 20:19:25.835233  1359 net.cpp:86] Creating Layer ip1
I0428 20:19:25.835237  1359 net.cpp:408] ip1 <- pool1
I0428 20:19:25.835240  1359 net.cpp:382] ip1 -> ip1
I0428 20:19:25.835374  1359 net.cpp:124] Setting up ip1
I0428 20:19:25.835397  1359 net.cpp:131] Top shape: 64 10 (640)
I0428 20:19:25.835400  1359 net.cpp:139] Memory required for data: 9931520
I0428 20:19:25.835407  1359 layer_factory.hpp:77] Creating layer relu1
I0428 20:19:25.835413  1359 net.cpp:86] Creating Layer relu1
I0428 20:19:25.835417  1359 net.cpp:408] relu1 <- ip1
I0428 20:19:25.835422  1359 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:19:25.835594  1359 net.cpp:124] Setting up relu1
I0428 20:19:25.835604  1359 net.cpp:131] Top shape: 64 10 (640)
I0428 20:19:25.835608  1359 net.cpp:139] Memory required for data: 9934080
I0428 20:19:25.835611  1359 layer_factory.hpp:77] Creating layer ip2
I0428 20:19:25.835618  1359 net.cpp:86] Creating Layer ip2
I0428 20:19:25.835620  1359 net.cpp:408] ip2 <- ip1
I0428 20:19:25.835628  1359 net.cpp:382] ip2 -> ip2
I0428 20:19:25.835724  1359 net.cpp:124] Setting up ip2
I0428 20:19:25.835731  1359 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:19:25.835734  1359 net.cpp:139] Memory required for data: 9940480
I0428 20:19:25.835739  1359 layer_factory.hpp:77] Creating layer relu2
I0428 20:19:25.835747  1359 net.cpp:86] Creating Layer relu2
I0428 20:19:25.835750  1359 net.cpp:408] relu2 <- ip2
I0428 20:19:25.835754  1359 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:19:25.836524  1359 net.cpp:124] Setting up relu2
I0428 20:19:25.836536  1359 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:19:25.836555  1359 net.cpp:139] Memory required for data: 9946880
I0428 20:19:25.836558  1359 layer_factory.hpp:77] Creating layer ip3
I0428 20:19:25.836565  1359 net.cpp:86] Creating Layer ip3
I0428 20:19:25.836570  1359 net.cpp:408] ip3 <- ip2
I0428 20:19:25.836576  1359 net.cpp:382] ip3 -> ip3
I0428 20:19:25.836676  1359 net.cpp:124] Setting up ip3
I0428 20:19:25.836683  1359 net.cpp:131] Top shape: 64 10 (640)
I0428 20:19:25.836686  1359 net.cpp:139] Memory required for data: 9949440
I0428 20:19:25.836694  1359 layer_factory.hpp:77] Creating layer relu3
I0428 20:19:25.836699  1359 net.cpp:86] Creating Layer relu3
I0428 20:19:25.836701  1359 net.cpp:408] relu3 <- ip3
I0428 20:19:25.836706  1359 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:19:25.836948  1359 net.cpp:124] Setting up relu3
I0428 20:19:25.836958  1359 net.cpp:131] Top shape: 64 10 (640)
I0428 20:19:25.836961  1359 net.cpp:139] Memory required for data: 9952000
I0428 20:19:25.836966  1359 layer_factory.hpp:77] Creating layer loss
I0428 20:19:25.836972  1359 net.cpp:86] Creating Layer loss
I0428 20:19:25.836977  1359 net.cpp:408] loss <- ip3
I0428 20:19:25.836980  1359 net.cpp:408] loss <- label
I0428 20:19:25.836985  1359 net.cpp:382] loss -> loss
I0428 20:19:25.837000  1359 layer_factory.hpp:77] Creating layer loss
I0428 20:19:25.837319  1359 net.cpp:124] Setting up loss
I0428 20:19:25.837327  1359 net.cpp:131] Top shape: (1)
I0428 20:19:25.837347  1359 net.cpp:134]     with loss weight 1
I0428 20:19:25.837360  1359 net.cpp:139] Memory required for data: 9952004
I0428 20:19:25.837364  1359 net.cpp:200] loss needs backward computation.
I0428 20:19:25.837368  1359 net.cpp:200] relu3 needs backward computation.
I0428 20:19:25.837370  1359 net.cpp:200] ip3 needs backward computation.
I0428 20:19:25.837373  1359 net.cpp:200] relu2 needs backward computation.
I0428 20:19:25.837375  1359 net.cpp:200] ip2 needs backward computation.
I0428 20:19:25.837378  1359 net.cpp:200] relu1 needs backward computation.
I0428 20:19:25.837380  1359 net.cpp:200] ip1 needs backward computation.
I0428 20:19:25.837383  1359 net.cpp:200] pool1 needs backward computation.
I0428 20:19:25.837386  1359 net.cpp:200] conv1 needs backward computation.
I0428 20:19:25.837389  1359 net.cpp:200] pool0 needs backward computation.
I0428 20:19:25.837393  1359 net.cpp:200] conv0 needs backward computation.
I0428 20:19:25.837395  1359 net.cpp:202] mnist does not need backward computation.
I0428 20:19:25.837397  1359 net.cpp:244] This network produces output loss
I0428 20:19:25.837409  1359 net.cpp:257] Network initialization done.
I0428 20:19:25.837741  1359 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1341.prototxt
I0428 20:19:25.837769  1359 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:19:25.837882  1359 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:19:25.837962  1359 layer_factory.hpp:77] Creating layer mnist
I0428 20:19:25.838003  1359 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:19:25.838017  1359 net.cpp:86] Creating Layer mnist
I0428 20:19:25.838022  1359 net.cpp:382] mnist -> data
I0428 20:19:25.838030  1359 net.cpp:382] mnist -> label
I0428 20:19:25.838124  1359 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:19:25.840203  1359 net.cpp:124] Setting up mnist
I0428 20:19:25.840246  1359 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:19:25.840251  1359 net.cpp:131] Top shape: 100 (100)
I0428 20:19:25.840255  1359 net.cpp:139] Memory required for data: 314000
I0428 20:19:25.840258  1359 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:19:25.840288  1359 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:19:25.840292  1359 net.cpp:408] label_mnist_1_split <- label
I0428 20:19:25.840297  1359 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:19:25.840306  1359 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:19:25.840353  1359 net.cpp:124] Setting up label_mnist_1_split
I0428 20:19:25.840359  1359 net.cpp:131] Top shape: 100 (100)
I0428 20:19:25.840363  1359 net.cpp:131] Top shape: 100 (100)
I0428 20:19:25.840381  1359 net.cpp:139] Memory required for data: 314800
I0428 20:19:25.840384  1359 layer_factory.hpp:77] Creating layer conv0
I0428 20:19:25.840394  1359 net.cpp:86] Creating Layer conv0
I0428 20:19:25.840396  1359 net.cpp:408] conv0 <- data
I0428 20:19:25.840402  1359 net.cpp:382] conv0 -> conv0
I0428 20:19:25.842069  1359 net.cpp:124] Setting up conv0
I0428 20:19:25.842083  1359 net.cpp:131] Top shape: 100 50 24 24 (2880000)
I0428 20:19:25.842103  1359 net.cpp:139] Memory required for data: 11834800
I0428 20:19:25.842110  1359 layer_factory.hpp:77] Creating layer pool0
I0428 20:19:25.842116  1359 net.cpp:86] Creating Layer pool0
I0428 20:19:25.842120  1359 net.cpp:408] pool0 <- conv0
I0428 20:19:25.842133  1359 net.cpp:382] pool0 -> pool0
I0428 20:19:25.842183  1359 net.cpp:124] Setting up pool0
I0428 20:19:25.842188  1359 net.cpp:131] Top shape: 100 50 12 12 (720000)
I0428 20:19:25.842191  1359 net.cpp:139] Memory required for data: 14714800
I0428 20:19:25.842195  1359 layer_factory.hpp:77] Creating layer conv1
I0428 20:19:25.842205  1359 net.cpp:86] Creating Layer conv1
I0428 20:19:25.842207  1359 net.cpp:408] conv1 <- pool0
I0428 20:19:25.842212  1359 net.cpp:382] conv1 -> conv1
I0428 20:19:25.843972  1359 net.cpp:124] Setting up conv1
I0428 20:19:25.843986  1359 net.cpp:131] Top shape: 100 25 8 8 (160000)
I0428 20:19:25.844007  1359 net.cpp:139] Memory required for data: 15354800
I0428 20:19:25.844032  1359 layer_factory.hpp:77] Creating layer pool1
I0428 20:19:25.844039  1359 net.cpp:86] Creating Layer pool1
I0428 20:19:25.844043  1359 net.cpp:408] pool1 <- conv1
I0428 20:19:25.844050  1359 net.cpp:382] pool1 -> pool1
I0428 20:19:25.844089  1359 net.cpp:124] Setting up pool1
I0428 20:19:25.844095  1359 net.cpp:131] Top shape: 100 25 4 4 (40000)
I0428 20:19:25.844099  1359 net.cpp:139] Memory required for data: 15514800
I0428 20:19:25.844102  1359 layer_factory.hpp:77] Creating layer ip1
I0428 20:19:25.844108  1359 net.cpp:86] Creating Layer ip1
I0428 20:19:25.844111  1359 net.cpp:408] ip1 <- pool1
I0428 20:19:25.844118  1359 net.cpp:382] ip1 -> ip1
I0428 20:19:25.844266  1359 net.cpp:124] Setting up ip1
I0428 20:19:25.844275  1359 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:19:25.844288  1359 net.cpp:139] Memory required for data: 15518800
I0428 20:19:25.844297  1359 layer_factory.hpp:77] Creating layer relu1
I0428 20:19:25.844305  1359 net.cpp:86] Creating Layer relu1
I0428 20:19:25.844308  1359 net.cpp:408] relu1 <- ip1
I0428 20:19:25.844314  1359 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:19:25.844501  1359 net.cpp:124] Setting up relu1
I0428 20:19:25.844511  1359 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:19:25.844513  1359 net.cpp:139] Memory required for data: 15522800
I0428 20:19:25.844517  1359 layer_factory.hpp:77] Creating layer ip2
I0428 20:19:25.844525  1359 net.cpp:86] Creating Layer ip2
I0428 20:19:25.844529  1359 net.cpp:408] ip2 <- ip1
I0428 20:19:25.844549  1359 net.cpp:382] ip2 -> ip2
I0428 20:19:25.844650  1359 net.cpp:124] Setting up ip2
I0428 20:19:25.844657  1359 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:19:25.844660  1359 net.cpp:139] Memory required for data: 15532800
I0428 20:19:25.844666  1359 layer_factory.hpp:77] Creating layer relu2
I0428 20:19:25.844671  1359 net.cpp:86] Creating Layer relu2
I0428 20:19:25.844676  1359 net.cpp:408] relu2 <- ip2
I0428 20:19:25.844681  1359 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:19:25.844921  1359 net.cpp:124] Setting up relu2
I0428 20:19:25.844931  1359 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:19:25.844934  1359 net.cpp:139] Memory required for data: 15542800
I0428 20:19:25.844938  1359 layer_factory.hpp:77] Creating layer ip3
I0428 20:19:25.844944  1359 net.cpp:86] Creating Layer ip3
I0428 20:19:25.844947  1359 net.cpp:408] ip3 <- ip2
I0428 20:19:25.844959  1359 net.cpp:382] ip3 -> ip3
I0428 20:19:25.845113  1359 net.cpp:124] Setting up ip3
I0428 20:19:25.845121  1359 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:19:25.845124  1359 net.cpp:139] Memory required for data: 15546800
I0428 20:19:25.845132  1359 layer_factory.hpp:77] Creating layer relu3
I0428 20:19:25.845139  1359 net.cpp:86] Creating Layer relu3
I0428 20:19:25.845141  1359 net.cpp:408] relu3 <- ip3
I0428 20:19:25.845145  1359 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:19:25.846050  1359 net.cpp:124] Setting up relu3
I0428 20:19:25.846061  1359 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:19:25.846081  1359 net.cpp:139] Memory required for data: 15550800
I0428 20:19:25.846084  1359 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:19:25.846091  1359 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:19:25.846096  1359 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:19:25.846101  1359 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:19:25.846107  1359 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:19:25.846160  1359 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:19:25.846173  1359 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:19:25.846176  1359 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:19:25.846179  1359 net.cpp:139] Memory required for data: 15558800
I0428 20:19:25.846182  1359 layer_factory.hpp:77] Creating layer accuracy
I0428 20:19:25.846187  1359 net.cpp:86] Creating Layer accuracy
I0428 20:19:25.846190  1359 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:19:25.846195  1359 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:19:25.846200  1359 net.cpp:382] accuracy -> accuracy
I0428 20:19:25.846207  1359 net.cpp:124] Setting up accuracy
I0428 20:19:25.846211  1359 net.cpp:131] Top shape: (1)
I0428 20:19:25.846215  1359 net.cpp:139] Memory required for data: 15558804
I0428 20:19:25.846217  1359 layer_factory.hpp:77] Creating layer loss
I0428 20:19:25.846221  1359 net.cpp:86] Creating Layer loss
I0428 20:19:25.846225  1359 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:19:25.846235  1359 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:19:25.846240  1359 net.cpp:382] loss -> loss
I0428 20:19:25.846246  1359 layer_factory.hpp:77] Creating layer loss
I0428 20:19:25.846541  1359 net.cpp:124] Setting up loss
I0428 20:19:25.846566  1359 net.cpp:131] Top shape: (1)
I0428 20:19:25.846570  1359 net.cpp:134]     with loss weight 1
I0428 20:19:25.846601  1359 net.cpp:139] Memory required for data: 15558808
I0428 20:19:25.846606  1359 net.cpp:200] loss needs backward computation.
I0428 20:19:25.846616  1359 net.cpp:202] accuracy does not need backward computation.
I0428 20:19:25.846619  1359 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:19:25.846622  1359 net.cpp:200] relu3 needs backward computation.
I0428 20:19:25.846626  1359 net.cpp:200] ip3 needs backward computation.
I0428 20:19:25.846629  1359 net.cpp:200] relu2 needs backward computation.
I0428 20:19:25.846632  1359 net.cpp:200] ip2 needs backward computation.
I0428 20:19:25.846635  1359 net.cpp:200] relu1 needs backward computation.
I0428 20:19:25.846637  1359 net.cpp:200] ip1 needs backward computation.
I0428 20:19:25.846640  1359 net.cpp:200] pool1 needs backward computation.
I0428 20:19:25.846643  1359 net.cpp:200] conv1 needs backward computation.
I0428 20:19:25.846647  1359 net.cpp:200] pool0 needs backward computation.
I0428 20:19:25.846650  1359 net.cpp:200] conv0 needs backward computation.
I0428 20:19:25.846654  1359 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:19:25.846663  1359 net.cpp:202] mnist does not need backward computation.
I0428 20:19:25.846667  1359 net.cpp:244] This network produces output accuracy
I0428 20:19:25.846670  1359 net.cpp:244] This network produces output loss
I0428 20:19:25.846683  1359 net.cpp:257] Network initialization done.
I0428 20:19:25.846741  1359 solver.cpp:56] Solver scaffolding done.
I0428 20:19:25.847115  1359 caffe.cpp:248] Starting Optimization
I0428 20:19:25.847121  1359 solver.cpp:273] Solving LeNet
I0428 20:19:25.847124  1359 solver.cpp:274] Learning Rate Policy: inv
I0428 20:19:25.847431  1359 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:19:25.854286  1359 blocking_queue.cpp:49] Waiting for data
I0428 20:19:25.924528  1366 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:19:25.925381  1359 solver.cpp:398]     Test net output #0: accuracy = 0.0731
I0428 20:19:25.925400  1359 solver.cpp:398]     Test net output #1: loss = 2.32088 (* 1 = 2.32088 loss)
I0428 20:19:25.929638  1359 solver.cpp:219] Iteration 0 (0 iter/s, 0.0824668s/100 iters), loss = 2.32302
I0428 20:19:25.929692  1359 solver.cpp:238]     Train net output #0: loss = 2.32302 (* 1 = 2.32302 loss)
I0428 20:19:25.929703  1359 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:19:26.063642  1359 solver.cpp:219] Iteration 100 (746.573 iter/s, 0.133945s/100 iters), loss = 0.994139
I0428 20:19:26.063678  1359 solver.cpp:238]     Train net output #0: loss = 0.994139 (* 1 = 0.994139 loss)
I0428 20:19:26.063689  1359 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:19:26.213407  1359 solver.cpp:219] Iteration 200 (667.931 iter/s, 0.149716s/100 iters), loss = 0.520191
I0428 20:19:26.213443  1359 solver.cpp:238]     Train net output #0: loss = 0.520191 (* 1 = 0.520191 loss)
I0428 20:19:26.213452  1359 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:19:26.354212  1359 solver.cpp:219] Iteration 300 (710.449 iter/s, 0.140756s/100 iters), loss = 0.451729
I0428 20:19:26.354238  1359 solver.cpp:238]     Train net output #0: loss = 0.451729 (* 1 = 0.451729 loss)
I0428 20:19:26.354245  1359 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:19:26.507640  1359 solver.cpp:219] Iteration 400 (651.954 iter/s, 0.153385s/100 iters), loss = 0.102433
I0428 20:19:26.507685  1359 solver.cpp:238]     Train net output #0: loss = 0.102433 (* 1 = 0.102433 loss)
I0428 20:19:26.507696  1359 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:19:26.661485  1359 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:19:26.737123  1366 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:19:26.739720  1359 solver.cpp:398]     Test net output #0: accuracy = 0.9619
I0428 20:19:26.739758  1359 solver.cpp:398]     Test net output #1: loss = 0.126894 (* 1 = 0.126894 loss)
I0428 20:19:26.741088  1359 solver.cpp:219] Iteration 500 (428.471 iter/s, 0.233388s/100 iters), loss = 0.130695
I0428 20:19:26.741154  1359 solver.cpp:238]     Train net output #0: loss = 0.130695 (* 1 = 0.130695 loss)
I0428 20:19:26.741163  1359 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:19:26.883195  1359 solver.cpp:219] Iteration 600 (704.032 iter/s, 0.142039s/100 iters), loss = 0.128612
I0428 20:19:26.883225  1359 solver.cpp:238]     Train net output #0: loss = 0.128612 (* 1 = 0.128612 loss)
I0428 20:19:26.883232  1359 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:19:27.023553  1359 solver.cpp:219] Iteration 700 (712.674 iter/s, 0.140317s/100 iters), loss = 0.140657
I0428 20:19:27.023582  1359 solver.cpp:238]     Train net output #0: loss = 0.140657 (* 1 = 0.140657 loss)
I0428 20:19:27.023589  1359 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:19:27.160001  1359 solver.cpp:219] Iteration 800 (733.105 iter/s, 0.136406s/100 iters), loss = 0.194664
I0428 20:19:27.160042  1359 solver.cpp:238]     Train net output #0: loss = 0.194664 (* 1 = 0.194664 loss)
I0428 20:19:27.160048  1359 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:19:27.301303  1359 solver.cpp:219] Iteration 900 (707.921 iter/s, 0.141259s/100 iters), loss = 0.13017
I0428 20:19:27.301354  1359 solver.cpp:238]     Train net output #0: loss = 0.13017 (* 1 = 0.13017 loss)
I0428 20:19:27.301367  1359 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:19:27.351795  1365 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:19:27.450700  1359 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:19:27.452510  1359 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:19:27.453708  1359 solver.cpp:311] Iteration 1000, loss = 0.111076
I0428 20:19:27.453743  1359 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:19:27.527956  1366 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:19:27.530002  1359 solver.cpp:398]     Test net output #0: accuracy = 0.9708
I0428 20:19:27.530040  1359 solver.cpp:398]     Test net output #1: loss = 0.0900584 (* 1 = 0.0900584 loss)
I0428 20:19:27.530046  1359 solver.cpp:316] Optimization Done.
I0428 20:19:27.530050  1359 caffe.cpp:259] Optimization Done.
