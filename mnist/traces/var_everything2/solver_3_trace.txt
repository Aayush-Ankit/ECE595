I0428 19:27:46.790875 21446 caffe.cpp:218] Using GPUs 0
I0428 19:27:46.831288 21446 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:27:47.296061 21446 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test3.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:27:47.296217 21446 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test3.prototxt
I0428 19:27:47.296454 21446 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:27:47.296466 21446 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:27:47.296526 21446 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0428 19:27:47.296596 21446 layer_factory.hpp:77] Creating layer mnist
I0428 19:27:47.296701 21446 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:27:47.296728 21446 net.cpp:86] Creating Layer mnist
I0428 19:27:47.296739 21446 net.cpp:382] mnist -> data
I0428 19:27:47.296778 21446 net.cpp:382] mnist -> label
I0428 19:27:47.297917 21446 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:27:47.300238 21446 net.cpp:124] Setting up mnist
I0428 19:27:47.300273 21446 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:27:47.300297 21446 net.cpp:131] Top shape: 64 (64)
I0428 19:27:47.300302 21446 net.cpp:139] Memory required for data: 200960
I0428 19:27:47.300326 21446 layer_factory.hpp:77] Creating layer ip1
I0428 19:27:47.300354 21446 net.cpp:86] Creating Layer ip1
I0428 19:27:47.300369 21446 net.cpp:408] ip1 <- data
I0428 19:27:47.300386 21446 net.cpp:382] ip1 -> ip1
I0428 19:27:47.301647 21446 net.cpp:124] Setting up ip1
I0428 19:27:47.301661 21446 net.cpp:131] Top shape: 64 10 (640)
I0428 19:27:47.301667 21446 net.cpp:139] Memory required for data: 203520
I0428 19:27:47.301686 21446 layer_factory.hpp:77] Creating layer relu1
I0428 19:27:47.301697 21446 net.cpp:86] Creating Layer relu1
I0428 19:27:47.301704 21446 net.cpp:408] relu1 <- ip1
I0428 19:27:47.301714 21446 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:27:47.533931 21446 net.cpp:124] Setting up relu1
I0428 19:27:47.533959 21446 net.cpp:131] Top shape: 64 10 (640)
I0428 19:27:47.533965 21446 net.cpp:139] Memory required for data: 206080
I0428 19:27:47.533972 21446 layer_factory.hpp:77] Creating layer ip2
I0428 19:27:47.533987 21446 net.cpp:86] Creating Layer ip2
I0428 19:27:47.533993 21446 net.cpp:408] ip2 <- ip1
I0428 19:27:47.534004 21446 net.cpp:382] ip2 -> ip2
I0428 19:27:47.534113 21446 net.cpp:124] Setting up ip2
I0428 19:27:47.534122 21446 net.cpp:131] Top shape: 64 10 (640)
I0428 19:27:47.534127 21446 net.cpp:139] Memory required for data: 208640
I0428 19:27:47.534140 21446 layer_factory.hpp:77] Creating layer relu2
I0428 19:27:47.534164 21446 net.cpp:86] Creating Layer relu2
I0428 19:27:47.534171 21446 net.cpp:408] relu2 <- ip2
I0428 19:27:47.534178 21446 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:27:47.534909 21446 net.cpp:124] Setting up relu2
I0428 19:27:47.534922 21446 net.cpp:131] Top shape: 64 10 (640)
I0428 19:27:47.534927 21446 net.cpp:139] Memory required for data: 211200
I0428 19:27:47.534934 21446 layer_factory.hpp:77] Creating layer loss
I0428 19:27:47.534942 21446 net.cpp:86] Creating Layer loss
I0428 19:27:47.534947 21446 net.cpp:408] loss <- ip2
I0428 19:27:47.534955 21446 net.cpp:408] loss <- label
I0428 19:27:47.534976 21446 net.cpp:382] loss -> loss
I0428 19:27:47.534999 21446 layer_factory.hpp:77] Creating layer loss
I0428 19:27:47.536037 21446 net.cpp:124] Setting up loss
I0428 19:27:47.536051 21446 net.cpp:131] Top shape: (1)
I0428 19:27:47.536070 21446 net.cpp:134]     with loss weight 1
I0428 19:27:47.536093 21446 net.cpp:139] Memory required for data: 211204
I0428 19:27:47.536098 21446 net.cpp:200] loss needs backward computation.
I0428 19:27:47.536104 21446 net.cpp:200] relu2 needs backward computation.
I0428 19:27:47.536109 21446 net.cpp:200] ip2 needs backward computation.
I0428 19:27:47.536113 21446 net.cpp:200] relu1 needs backward computation.
I0428 19:27:47.536118 21446 net.cpp:200] ip1 needs backward computation.
I0428 19:27:47.536123 21446 net.cpp:202] mnist does not need backward computation.
I0428 19:27:47.536128 21446 net.cpp:244] This network produces output loss
I0428 19:27:47.536137 21446 net.cpp:257] Network initialization done.
I0428 19:27:47.536326 21446 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test3.prototxt
I0428 19:27:47.536351 21446 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:27:47.536412 21446 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0428 19:27:47.536481 21446 layer_factory.hpp:77] Creating layer mnist
I0428 19:27:47.536536 21446 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:27:47.536551 21446 net.cpp:86] Creating Layer mnist
I0428 19:27:47.536558 21446 net.cpp:382] mnist -> data
I0428 19:27:47.536569 21446 net.cpp:382] mnist -> label
I0428 19:27:47.536679 21446 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:27:47.538900 21446 net.cpp:124] Setting up mnist
I0428 19:27:47.538915 21446 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:27:47.538923 21446 net.cpp:131] Top shape: 100 (100)
I0428 19:27:47.538944 21446 net.cpp:139] Memory required for data: 314000
I0428 19:27:47.538949 21446 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:27:47.538965 21446 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:27:47.538971 21446 net.cpp:408] label_mnist_1_split <- label
I0428 19:27:47.538978 21446 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:27:47.539005 21446 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:27:47.539057 21446 net.cpp:124] Setting up label_mnist_1_split
I0428 19:27:47.539065 21446 net.cpp:131] Top shape: 100 (100)
I0428 19:27:47.539072 21446 net.cpp:131] Top shape: 100 (100)
I0428 19:27:47.539077 21446 net.cpp:139] Memory required for data: 314800
I0428 19:27:47.539083 21446 layer_factory.hpp:77] Creating layer ip1
I0428 19:27:47.539108 21446 net.cpp:86] Creating Layer ip1
I0428 19:27:47.539114 21446 net.cpp:408] ip1 <- data
I0428 19:27:47.539124 21446 net.cpp:382] ip1 -> ip1
I0428 19:27:47.539301 21446 net.cpp:124] Setting up ip1
I0428 19:27:47.539311 21446 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:27:47.539316 21446 net.cpp:139] Memory required for data: 318800
I0428 19:27:47.539330 21446 layer_factory.hpp:77] Creating layer relu1
I0428 19:27:47.539340 21446 net.cpp:86] Creating Layer relu1
I0428 19:27:47.539346 21446 net.cpp:408] relu1 <- ip1
I0428 19:27:47.539355 21446 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:27:47.539541 21446 net.cpp:124] Setting up relu1
I0428 19:27:47.539551 21446 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:27:47.539556 21446 net.cpp:139] Memory required for data: 322800
I0428 19:27:47.539561 21446 layer_factory.hpp:77] Creating layer ip2
I0428 19:27:47.539572 21446 net.cpp:86] Creating Layer ip2
I0428 19:27:47.539577 21446 net.cpp:408] ip2 <- ip1
I0428 19:27:47.539585 21446 net.cpp:382] ip2 -> ip2
I0428 19:27:47.539685 21446 net.cpp:124] Setting up ip2
I0428 19:27:47.539693 21446 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:27:47.539698 21446 net.cpp:139] Memory required for data: 326800
I0428 19:27:47.539710 21446 layer_factory.hpp:77] Creating layer relu2
I0428 19:27:47.539718 21446 net.cpp:86] Creating Layer relu2
I0428 19:27:47.539724 21446 net.cpp:408] relu2 <- ip2
I0428 19:27:47.539731 21446 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:27:47.540478 21446 net.cpp:124] Setting up relu2
I0428 19:27:47.540493 21446 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:27:47.540513 21446 net.cpp:139] Memory required for data: 330800
I0428 19:27:47.540518 21446 layer_factory.hpp:77] Creating layer ip2_relu2_0_split
I0428 19:27:47.540525 21446 net.cpp:86] Creating Layer ip2_relu2_0_split
I0428 19:27:47.540531 21446 net.cpp:408] ip2_relu2_0_split <- ip2
I0428 19:27:47.540539 21446 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_0
I0428 19:27:47.540549 21446 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_1
I0428 19:27:47.540614 21446 net.cpp:124] Setting up ip2_relu2_0_split
I0428 19:27:47.540622 21446 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:27:47.540630 21446 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:27:47.540635 21446 net.cpp:139] Memory required for data: 338800
I0428 19:27:47.540640 21446 layer_factory.hpp:77] Creating layer accuracy
I0428 19:27:47.540649 21446 net.cpp:86] Creating Layer accuracy
I0428 19:27:47.540654 21446 net.cpp:408] accuracy <- ip2_relu2_0_split_0
I0428 19:27:47.540662 21446 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:27:47.540669 21446 net.cpp:382] accuracy -> accuracy
I0428 19:27:47.540688 21446 net.cpp:124] Setting up accuracy
I0428 19:27:47.540695 21446 net.cpp:131] Top shape: (1)
I0428 19:27:47.540700 21446 net.cpp:139] Memory required for data: 338804
I0428 19:27:47.540705 21446 layer_factory.hpp:77] Creating layer loss
I0428 19:27:47.540715 21446 net.cpp:86] Creating Layer loss
I0428 19:27:47.540720 21446 net.cpp:408] loss <- ip2_relu2_0_split_1
I0428 19:27:47.540727 21446 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:27:47.540735 21446 net.cpp:382] loss -> loss
I0428 19:27:47.540745 21446 layer_factory.hpp:77] Creating layer loss
I0428 19:27:47.541043 21446 net.cpp:124] Setting up loss
I0428 19:27:47.541054 21446 net.cpp:131] Top shape: (1)
I0428 19:27:47.541059 21446 net.cpp:134]     with loss weight 1
I0428 19:27:47.541069 21446 net.cpp:139] Memory required for data: 338808
I0428 19:27:47.541075 21446 net.cpp:200] loss needs backward computation.
I0428 19:27:47.541100 21446 net.cpp:202] accuracy does not need backward computation.
I0428 19:27:47.541107 21446 net.cpp:200] ip2_relu2_0_split needs backward computation.
I0428 19:27:47.541112 21446 net.cpp:200] relu2 needs backward computation.
I0428 19:27:47.541117 21446 net.cpp:200] ip2 needs backward computation.
I0428 19:27:47.541123 21446 net.cpp:200] relu1 needs backward computation.
I0428 19:27:47.541128 21446 net.cpp:200] ip1 needs backward computation.
I0428 19:27:47.541134 21446 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:27:47.541141 21446 net.cpp:202] mnist does not need backward computation.
I0428 19:27:47.541146 21446 net.cpp:244] This network produces output accuracy
I0428 19:27:47.541169 21446 net.cpp:244] This network produces output loss
I0428 19:27:47.541198 21446 net.cpp:257] Network initialization done.
I0428 19:27:47.541231 21446 solver.cpp:56] Solver scaffolding done.
I0428 19:27:47.541394 21446 caffe.cpp:248] Starting Optimization
I0428 19:27:47.541400 21446 solver.cpp:273] Solving LeNet
I0428 19:27:47.541405 21446 solver.cpp:274] Learning Rate Policy: inv
I0428 19:27:47.541483 21446 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:27:47.541559 21446 blocking_queue.cpp:49] Waiting for data
I0428 19:27:47.622320 21453 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:27:47.622805 21446 solver.cpp:398]     Test net output #0: accuracy = 0.104
I0428 19:27:47.622843 21446 solver.cpp:398]     Test net output #1: loss = 2.31214 (* 1 = 2.31214 loss)
I0428 19:27:47.623610 21446 solver.cpp:219] Iteration 0 (-5.72928e-36 iter/s, 0.0821738s/100 iters), loss = 2.31617
I0428 19:27:47.623656 21446 solver.cpp:238]     Train net output #0: loss = 2.31617 (* 1 = 2.31617 loss)
I0428 19:27:47.623694 21446 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:27:47.678189 21446 solver.cpp:219] Iteration 100 (1833.89 iter/s, 0.054529s/100 iters), loss = 1.32968
I0428 19:27:47.678231 21446 solver.cpp:238]     Train net output #0: loss = 1.32968 (* 1 = 1.32968 loss)
I0428 19:27:47.678244 21446 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:27:47.720306 21446 solver.cpp:219] Iteration 200 (2376.82 iter/s, 0.0420731s/100 iters), loss = 1.02345
I0428 19:27:47.720341 21446 solver.cpp:238]     Train net output #0: loss = 1.02345 (* 1 = 1.02345 loss)
I0428 19:27:47.720357 21446 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:27:47.759361 21446 solver.cpp:219] Iteration 300 (2563.02 iter/s, 0.0390164s/100 iters), loss = 1.08817
I0428 19:27:47.759392 21446 solver.cpp:238]     Train net output #0: loss = 1.08817 (* 1 = 1.08817 loss)
I0428 19:27:47.759402 21446 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:27:47.798677 21446 solver.cpp:219] Iteration 400 (2545.65 iter/s, 0.0392827s/100 iters), loss = 0.927157
I0428 19:27:47.798707 21446 solver.cpp:238]     Train net output #0: loss = 0.927157 (* 1 = 0.927157 loss)
I0428 19:27:47.798717 21446 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:27:47.840251 21446 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:27:47.895519 21453 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:27:47.896019 21446 solver.cpp:398]     Test net output #0: accuracy = 0.762
I0428 19:27:47.896055 21446 solver.cpp:398]     Test net output #1: loss = 0.788248 (* 1 = 0.788248 loss)
I0428 19:27:47.896612 21446 solver.cpp:219] Iteration 500 (1021.51 iter/s, 0.0978944s/100 iters), loss = 0.843394
I0428 19:27:47.896658 21446 solver.cpp:238]     Train net output #0: loss = 0.843394 (* 1 = 0.843394 loss)
I0428 19:27:47.896687 21446 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:27:47.945211 21446 solver.cpp:219] Iteration 600 (2059.69 iter/s, 0.0485509s/100 iters), loss = 0.621146
I0428 19:27:47.945250 21446 solver.cpp:238]     Train net output #0: loss = 0.621146 (* 1 = 0.621146 loss)
I0428 19:27:47.945263 21446 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:27:47.991202 21446 solver.cpp:219] Iteration 700 (2176.23 iter/s, 0.045951s/100 iters), loss = 0.75849
I0428 19:27:47.991253 21446 solver.cpp:238]     Train net output #0: loss = 0.75849 (* 1 = 0.75849 loss)
I0428 19:27:47.991266 21446 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:27:48.046756 21446 solver.cpp:219] Iteration 800 (1801.84 iter/s, 0.0554988s/100 iters), loss = 0.968137
I0428 19:27:48.046787 21446 solver.cpp:238]     Train net output #0: loss = 0.968137 (* 1 = 0.968137 loss)
I0428 19:27:48.046799 21446 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:27:48.092541 21446 solver.cpp:219] Iteration 900 (2185.78 iter/s, 0.0457503s/100 iters), loss = 0.831405
I0428 19:27:48.092567 21446 solver.cpp:238]     Train net output #0: loss = 0.831405 (* 1 = 0.831405 loss)
I0428 19:27:48.092578 21446 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:27:48.107383 21452 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:27:48.132704 21446 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:27:48.133265 21446 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:27:48.133587 21446 solver.cpp:311] Iteration 1000, loss = 0.749127
I0428 19:27:48.133604 21446 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:27:48.190737 21453 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:27:48.191099 21446 solver.cpp:398]     Test net output #0: accuracy = 0.7927
I0428 19:27:48.191120 21446 solver.cpp:398]     Test net output #1: loss = 0.73496 (* 1 = 0.73496 loss)
I0428 19:27:48.191129 21446 solver.cpp:316] Optimization Done.
I0428 19:27:48.191134 21446 caffe.cpp:259] Optimization Done.
