I0428 20:35:12.335552  4585 caffe.cpp:218] Using GPUs 0
I0428 20:35:12.372570  4585 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:35:12.889977  4585 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1617.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:35:12.890139  4585 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1617.prototxt
I0428 20:35:12.890564  4585 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:35:12.890584  4585 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:35:12.890687  4585 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:35:12.890769  4585 layer_factory.hpp:77] Creating layer mnist
I0428 20:35:12.890874  4585 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:35:12.890897  4585 net.cpp:86] Creating Layer mnist
I0428 20:35:12.890908  4585 net.cpp:382] mnist -> data
I0428 20:35:12.890931  4585 net.cpp:382] mnist -> label
I0428 20:35:12.892038  4585 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:35:12.894498  4585 net.cpp:124] Setting up mnist
I0428 20:35:12.894516  4585 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:35:12.894522  4585 net.cpp:131] Top shape: 64 (64)
I0428 20:35:12.894526  4585 net.cpp:139] Memory required for data: 200960
I0428 20:35:12.894532  4585 layer_factory.hpp:77] Creating layer conv0
I0428 20:35:12.894562  4585 net.cpp:86] Creating Layer conv0
I0428 20:35:12.894593  4585 net.cpp:408] conv0 <- data
I0428 20:35:12.894608  4585 net.cpp:382] conv0 -> conv0
I0428 20:35:13.200108  4585 net.cpp:124] Setting up conv0
I0428 20:35:13.200139  4585 net.cpp:131] Top shape: 64 100 24 24 (3686400)
I0428 20:35:13.200143  4585 net.cpp:139] Memory required for data: 14946560
I0428 20:35:13.200161  4585 layer_factory.hpp:77] Creating layer pool0
I0428 20:35:13.200177  4585 net.cpp:86] Creating Layer pool0
I0428 20:35:13.200193  4585 net.cpp:408] pool0 <- conv0
I0428 20:35:13.200201  4585 net.cpp:382] pool0 -> pool0
I0428 20:35:13.200256  4585 net.cpp:124] Setting up pool0
I0428 20:35:13.200265  4585 net.cpp:131] Top shape: 64 100 12 12 (921600)
I0428 20:35:13.200269  4585 net.cpp:139] Memory required for data: 18632960
I0428 20:35:13.200273  4585 layer_factory.hpp:77] Creating layer conv1
I0428 20:35:13.200286  4585 net.cpp:86] Creating Layer conv1
I0428 20:35:13.200292  4585 net.cpp:408] conv1 <- pool0
I0428 20:35:13.200299  4585 net.cpp:382] conv1 -> conv1
I0428 20:35:13.204252  4585 net.cpp:124] Setting up conv1
I0428 20:35:13.204269  4585 net.cpp:131] Top shape: 64 50 8 8 (204800)
I0428 20:35:13.204273  4585 net.cpp:139] Memory required for data: 19452160
I0428 20:35:13.204284  4585 layer_factory.hpp:77] Creating layer pool1
I0428 20:35:13.204293  4585 net.cpp:86] Creating Layer pool1
I0428 20:35:13.204298  4585 net.cpp:408] pool1 <- conv1
I0428 20:35:13.204304  4585 net.cpp:382] pool1 -> pool1
I0428 20:35:13.204357  4585 net.cpp:124] Setting up pool1
I0428 20:35:13.204366  4585 net.cpp:131] Top shape: 64 50 4 4 (51200)
I0428 20:35:13.204370  4585 net.cpp:139] Memory required for data: 19656960
I0428 20:35:13.204373  4585 layer_factory.hpp:77] Creating layer ip1
I0428 20:35:13.204381  4585 net.cpp:86] Creating Layer ip1
I0428 20:35:13.204388  4585 net.cpp:408] ip1 <- pool1
I0428 20:35:13.204394  4585 net.cpp:382] ip1 -> ip1
I0428 20:35:13.205548  4585 net.cpp:124] Setting up ip1
I0428 20:35:13.205562  4585 net.cpp:131] Top shape: 64 10 (640)
I0428 20:35:13.205567  4585 net.cpp:139] Memory required for data: 19659520
I0428 20:35:13.205576  4585 layer_factory.hpp:77] Creating layer relu1
I0428 20:35:13.205585  4585 net.cpp:86] Creating Layer relu1
I0428 20:35:13.205601  4585 net.cpp:408] relu1 <- ip1
I0428 20:35:13.205606  4585 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:35:13.205824  4585 net.cpp:124] Setting up relu1
I0428 20:35:13.205838  4585 net.cpp:131] Top shape: 64 10 (640)
I0428 20:35:13.205842  4585 net.cpp:139] Memory required for data: 19662080
I0428 20:35:13.205858  4585 layer_factory.hpp:77] Creating layer ip2
I0428 20:35:13.205866  4585 net.cpp:86] Creating Layer ip2
I0428 20:35:13.205869  4585 net.cpp:408] ip2 <- ip1
I0428 20:35:13.205875  4585 net.cpp:382] ip2 -> ip2
I0428 20:35:13.205991  4585 net.cpp:124] Setting up ip2
I0428 20:35:13.205999  4585 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:35:13.206002  4585 net.cpp:139] Memory required for data: 19674880
I0428 20:35:13.206009  4585 layer_factory.hpp:77] Creating layer relu2
I0428 20:35:13.206019  4585 net.cpp:86] Creating Layer relu2
I0428 20:35:13.206025  4585 net.cpp:408] relu2 <- ip2
I0428 20:35:13.206030  4585 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:35:13.206914  4585 net.cpp:124] Setting up relu2
I0428 20:35:13.206929  4585 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:35:13.206933  4585 net.cpp:139] Memory required for data: 19687680
I0428 20:35:13.206938  4585 layer_factory.hpp:77] Creating layer ip3
I0428 20:35:13.206946  4585 net.cpp:86] Creating Layer ip3
I0428 20:35:13.206951  4585 net.cpp:408] ip3 <- ip2
I0428 20:35:13.206957  4585 net.cpp:382] ip3 -> ip3
I0428 20:35:13.207078  4585 net.cpp:124] Setting up ip3
I0428 20:35:13.207087  4585 net.cpp:131] Top shape: 64 10 (640)
I0428 20:35:13.207092  4585 net.cpp:139] Memory required for data: 19690240
I0428 20:35:13.207100  4585 layer_factory.hpp:77] Creating layer relu3
I0428 20:35:13.207108  4585 net.cpp:86] Creating Layer relu3
I0428 20:35:13.207111  4585 net.cpp:408] relu3 <- ip3
I0428 20:35:13.207116  4585 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:35:13.207325  4585 net.cpp:124] Setting up relu3
I0428 20:35:13.207335  4585 net.cpp:131] Top shape: 64 10 (640)
I0428 20:35:13.207351  4585 net.cpp:139] Memory required for data: 19692800
I0428 20:35:13.207355  4585 layer_factory.hpp:77] Creating layer loss
I0428 20:35:13.207361  4585 net.cpp:86] Creating Layer loss
I0428 20:35:13.207365  4585 net.cpp:408] loss <- ip3
I0428 20:35:13.207370  4585 net.cpp:408] loss <- label
I0428 20:35:13.207376  4585 net.cpp:382] loss -> loss
I0428 20:35:13.207397  4585 layer_factory.hpp:77] Creating layer loss
I0428 20:35:13.207669  4585 net.cpp:124] Setting up loss
I0428 20:35:13.207680  4585 net.cpp:131] Top shape: (1)
I0428 20:35:13.207684  4585 net.cpp:134]     with loss weight 1
I0428 20:35:13.207700  4585 net.cpp:139] Memory required for data: 19692804
I0428 20:35:13.207703  4585 net.cpp:200] loss needs backward computation.
I0428 20:35:13.207707  4585 net.cpp:200] relu3 needs backward computation.
I0428 20:35:13.207725  4585 net.cpp:200] ip3 needs backward computation.
I0428 20:35:13.207727  4585 net.cpp:200] relu2 needs backward computation.
I0428 20:35:13.207731  4585 net.cpp:200] ip2 needs backward computation.
I0428 20:35:13.207734  4585 net.cpp:200] relu1 needs backward computation.
I0428 20:35:13.207747  4585 net.cpp:200] ip1 needs backward computation.
I0428 20:35:13.207751  4585 net.cpp:200] pool1 needs backward computation.
I0428 20:35:13.207754  4585 net.cpp:200] conv1 needs backward computation.
I0428 20:35:13.207758  4585 net.cpp:200] pool0 needs backward computation.
I0428 20:35:13.207762  4585 net.cpp:200] conv0 needs backward computation.
I0428 20:35:13.207765  4585 net.cpp:202] mnist does not need backward computation.
I0428 20:35:13.207769  4585 net.cpp:244] This network produces output loss
I0428 20:35:13.207779  4585 net.cpp:257] Network initialization done.
I0428 20:35:13.208165  4585 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1617.prototxt
I0428 20:35:13.208199  4585 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:35:13.208304  4585 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:35:13.208398  4585 layer_factory.hpp:77] Creating layer mnist
I0428 20:35:13.208458  4585 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:35:13.208474  4585 net.cpp:86] Creating Layer mnist
I0428 20:35:13.208480  4585 net.cpp:382] mnist -> data
I0428 20:35:13.208490  4585 net.cpp:382] mnist -> label
I0428 20:35:13.208595  4585 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:35:13.209883  4585 net.cpp:124] Setting up mnist
I0428 20:35:13.209898  4585 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:35:13.209905  4585 net.cpp:131] Top shape: 100 (100)
I0428 20:35:13.209908  4585 net.cpp:139] Memory required for data: 314000
I0428 20:35:13.209913  4585 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:35:13.209920  4585 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:35:13.209925  4585 net.cpp:408] label_mnist_1_split <- label
I0428 20:35:13.209930  4585 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:35:13.209939  4585 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:35:13.209988  4585 net.cpp:124] Setting up label_mnist_1_split
I0428 20:35:13.209996  4585 net.cpp:131] Top shape: 100 (100)
I0428 20:35:13.210000  4585 net.cpp:131] Top shape: 100 (100)
I0428 20:35:13.210013  4585 net.cpp:139] Memory required for data: 314800
I0428 20:35:13.210016  4585 layer_factory.hpp:77] Creating layer conv0
I0428 20:35:13.210026  4585 net.cpp:86] Creating Layer conv0
I0428 20:35:13.210031  4585 net.cpp:408] conv0 <- data
I0428 20:35:13.210036  4585 net.cpp:382] conv0 -> conv0
I0428 20:35:13.211995  4585 net.cpp:124] Setting up conv0
I0428 20:35:13.212013  4585 net.cpp:131] Top shape: 100 100 24 24 (5760000)
I0428 20:35:13.212016  4585 net.cpp:139] Memory required for data: 23354800
I0428 20:35:13.212028  4585 layer_factory.hpp:77] Creating layer pool0
I0428 20:35:13.212035  4585 net.cpp:86] Creating Layer pool0
I0428 20:35:13.212039  4585 net.cpp:408] pool0 <- conv0
I0428 20:35:13.212045  4585 net.cpp:382] pool0 -> pool0
I0428 20:35:13.212090  4585 net.cpp:124] Setting up pool0
I0428 20:35:13.212098  4585 net.cpp:131] Top shape: 100 100 12 12 (1440000)
I0428 20:35:13.212101  4585 net.cpp:139] Memory required for data: 29114800
I0428 20:35:13.212105  4585 layer_factory.hpp:77] Creating layer conv1
I0428 20:35:13.212116  4585 net.cpp:86] Creating Layer conv1
I0428 20:35:13.212121  4585 net.cpp:408] conv1 <- pool0
I0428 20:35:13.212127  4585 net.cpp:382] conv1 -> conv1
I0428 20:35:13.215687  4585 net.cpp:124] Setting up conv1
I0428 20:35:13.215703  4585 net.cpp:131] Top shape: 100 50 8 8 (320000)
I0428 20:35:13.215708  4585 net.cpp:139] Memory required for data: 30394800
I0428 20:35:13.215719  4585 layer_factory.hpp:77] Creating layer pool1
I0428 20:35:13.215728  4585 net.cpp:86] Creating Layer pool1
I0428 20:35:13.215733  4585 net.cpp:408] pool1 <- conv1
I0428 20:35:13.215739  4585 net.cpp:382] pool1 -> pool1
I0428 20:35:13.215802  4585 net.cpp:124] Setting up pool1
I0428 20:35:13.215811  4585 net.cpp:131] Top shape: 100 50 4 4 (80000)
I0428 20:35:13.215816  4585 net.cpp:139] Memory required for data: 30714800
I0428 20:35:13.215819  4585 layer_factory.hpp:77] Creating layer ip1
I0428 20:35:13.215827  4585 net.cpp:86] Creating Layer ip1
I0428 20:35:13.215833  4585 net.cpp:408] ip1 <- pool1
I0428 20:35:13.215840  4585 net.cpp:382] ip1 -> ip1
I0428 20:35:13.216007  4585 net.cpp:124] Setting up ip1
I0428 20:35:13.216029  4585 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:35:13.216037  4585 net.cpp:139] Memory required for data: 30718800
I0428 20:35:13.216045  4585 layer_factory.hpp:77] Creating layer relu1
I0428 20:35:13.216053  4585 net.cpp:86] Creating Layer relu1
I0428 20:35:13.216056  4585 net.cpp:408] relu1 <- ip1
I0428 20:35:13.216063  4585 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:35:13.216318  4585 net.cpp:124] Setting up relu1
I0428 20:35:13.216331  4585 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:35:13.216334  4585 net.cpp:139] Memory required for data: 30722800
I0428 20:35:13.216337  4585 layer_factory.hpp:77] Creating layer ip2
I0428 20:35:13.216347  4585 net.cpp:86] Creating Layer ip2
I0428 20:35:13.216351  4585 net.cpp:408] ip2 <- ip1
I0428 20:35:13.216358  4585 net.cpp:382] ip2 -> ip2
I0428 20:35:13.216497  4585 net.cpp:124] Setting up ip2
I0428 20:35:13.216506  4585 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:35:13.216511  4585 net.cpp:139] Memory required for data: 30742800
I0428 20:35:13.216517  4585 layer_factory.hpp:77] Creating layer relu2
I0428 20:35:13.216523  4585 net.cpp:86] Creating Layer relu2
I0428 20:35:13.216527  4585 net.cpp:408] relu2 <- ip2
I0428 20:35:13.216533  4585 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:35:13.216727  4585 net.cpp:124] Setting up relu2
I0428 20:35:13.216737  4585 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:35:13.216742  4585 net.cpp:139] Memory required for data: 30762800
I0428 20:35:13.216744  4585 layer_factory.hpp:77] Creating layer ip3
I0428 20:35:13.216753  4585 net.cpp:86] Creating Layer ip3
I0428 20:35:13.216758  4585 net.cpp:408] ip3 <- ip2
I0428 20:35:13.216763  4585 net.cpp:382] ip3 -> ip3
I0428 20:35:13.216897  4585 net.cpp:124] Setting up ip3
I0428 20:35:13.216907  4585 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:35:13.216910  4585 net.cpp:139] Memory required for data: 30766800
I0428 20:35:13.216922  4585 layer_factory.hpp:77] Creating layer relu3
I0428 20:35:13.216928  4585 net.cpp:86] Creating Layer relu3
I0428 20:35:13.216930  4585 net.cpp:408] relu3 <- ip3
I0428 20:35:13.216935  4585 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:35:13.217870  4585 net.cpp:124] Setting up relu3
I0428 20:35:13.217886  4585 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:35:13.217890  4585 net.cpp:139] Memory required for data: 30770800
I0428 20:35:13.217895  4585 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:35:13.217902  4585 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:35:13.217907  4585 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:35:13.217912  4585 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:35:13.217919  4585 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:35:13.217979  4585 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:35:13.217988  4585 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:35:13.217993  4585 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:35:13.217995  4585 net.cpp:139] Memory required for data: 30778800
I0428 20:35:13.217999  4585 layer_factory.hpp:77] Creating layer accuracy
I0428 20:35:13.218006  4585 net.cpp:86] Creating Layer accuracy
I0428 20:35:13.218010  4585 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:35:13.218015  4585 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:35:13.218020  4585 net.cpp:382] accuracy -> accuracy
I0428 20:35:13.218029  4585 net.cpp:124] Setting up accuracy
I0428 20:35:13.218034  4585 net.cpp:131] Top shape: (1)
I0428 20:35:13.218039  4585 net.cpp:139] Memory required for data: 30778804
I0428 20:35:13.218041  4585 layer_factory.hpp:77] Creating layer loss
I0428 20:35:13.218046  4585 net.cpp:86] Creating Layer loss
I0428 20:35:13.218051  4585 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:35:13.218055  4585 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:35:13.218062  4585 net.cpp:382] loss -> loss
I0428 20:35:13.218070  4585 layer_factory.hpp:77] Creating layer loss
I0428 20:35:13.218363  4585 net.cpp:124] Setting up loss
I0428 20:35:13.218374  4585 net.cpp:131] Top shape: (1)
I0428 20:35:13.218377  4585 net.cpp:134]     with loss weight 1
I0428 20:35:13.218396  4585 net.cpp:139] Memory required for data: 30778808
I0428 20:35:13.218400  4585 net.cpp:200] loss needs backward computation.
I0428 20:35:13.218406  4585 net.cpp:202] accuracy does not need backward computation.
I0428 20:35:13.218410  4585 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:35:13.218415  4585 net.cpp:200] relu3 needs backward computation.
I0428 20:35:13.218417  4585 net.cpp:200] ip3 needs backward computation.
I0428 20:35:13.218421  4585 net.cpp:200] relu2 needs backward computation.
I0428 20:35:13.218425  4585 net.cpp:200] ip2 needs backward computation.
I0428 20:35:13.218427  4585 net.cpp:200] relu1 needs backward computation.
I0428 20:35:13.218431  4585 net.cpp:200] ip1 needs backward computation.
I0428 20:35:13.218436  4585 net.cpp:200] pool1 needs backward computation.
I0428 20:35:13.218438  4585 net.cpp:200] conv1 needs backward computation.
I0428 20:35:13.218442  4585 net.cpp:200] pool0 needs backward computation.
I0428 20:35:13.218446  4585 net.cpp:200] conv0 needs backward computation.
I0428 20:35:13.218451  4585 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:35:13.218456  4585 net.cpp:202] mnist does not need backward computation.
I0428 20:35:13.218458  4585 net.cpp:244] This network produces output accuracy
I0428 20:35:13.218462  4585 net.cpp:244] This network produces output loss
I0428 20:35:13.218475  4585 net.cpp:257] Network initialization done.
I0428 20:35:13.218528  4585 solver.cpp:56] Solver scaffolding done.
I0428 20:35:13.218969  4585 caffe.cpp:248] Starting Optimization
I0428 20:35:13.218976  4585 solver.cpp:273] Solving LeNet
I0428 20:35:13.218979  4585 solver.cpp:274] Learning Rate Policy: inv
I0428 20:35:13.219877  4585 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:35:13.331785  4592 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:35:13.334890  4585 solver.cpp:398]     Test net output #0: accuracy = 0.0687
I0428 20:35:13.334913  4585 solver.cpp:398]     Test net output #1: loss = 2.30858 (* 1 = 2.30858 loss)
I0428 20:35:13.341547  4585 solver.cpp:219] Iteration 0 (-5.98998e-31 iter/s, 0.122531s/100 iters), loss = 2.32301
I0428 20:35:13.341578  4585 solver.cpp:238]     Train net output #0: loss = 2.32301 (* 1 = 2.32301 loss)
I0428 20:35:13.341591  4585 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:35:13.571388  4585 solver.cpp:219] Iteration 100 (435.179 iter/s, 0.229791s/100 iters), loss = 0.675474
I0428 20:35:13.571420  4585 solver.cpp:238]     Train net output #0: loss = 0.675474 (* 1 = 0.675474 loss)
I0428 20:35:13.571429  4585 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:35:13.802764  4585 solver.cpp:219] Iteration 200 (432.287 iter/s, 0.231328s/100 iters), loss = 0.403003
I0428 20:35:13.802801  4585 solver.cpp:238]     Train net output #0: loss = 0.403003 (* 1 = 0.403003 loss)
I0428 20:35:13.802810  4585 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:35:14.043743  4585 solver.cpp:219] Iteration 300 (415.062 iter/s, 0.240928s/100 iters), loss = 0.595977
I0428 20:35:14.043783  4585 solver.cpp:238]     Train net output #0: loss = 0.595977 (* 1 = 0.595977 loss)
I0428 20:35:14.043793  4585 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:35:14.289729  4585 solver.cpp:219] Iteration 400 (406.625 iter/s, 0.245927s/100 iters), loss = 0.126754
I0428 20:35:14.289772  4585 solver.cpp:238]     Train net output #0: loss = 0.126754 (* 1 = 0.126754 loss)
I0428 20:35:14.289783  4585 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:35:14.529060  4585 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:35:14.640383  4592 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:35:14.643486  4585 solver.cpp:398]     Test net output #0: accuracy = 0.9624
I0428 20:35:14.643512  4585 solver.cpp:398]     Test net output #1: loss = 0.119119 (* 1 = 0.119119 loss)
I0428 20:35:14.645712  4585 solver.cpp:219] Iteration 500 (280.961 iter/s, 0.355921s/100 iters), loss = 0.130815
I0428 20:35:14.645741  4585 solver.cpp:238]     Train net output #0: loss = 0.130815 (* 1 = 0.130815 loss)
I0428 20:35:14.645764  4585 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:35:14.881003  4585 solver.cpp:219] Iteration 600 (425.088 iter/s, 0.235246s/100 iters), loss = 0.104923
I0428 20:35:14.881036  4585 solver.cpp:238]     Train net output #0: loss = 0.104923 (* 1 = 0.104923 loss)
I0428 20:35:14.881044  4585 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:35:15.118335  4585 solver.cpp:219] Iteration 700 (421.437 iter/s, 0.237283s/100 iters), loss = 0.129674
I0428 20:35:15.118367  4585 solver.cpp:238]     Train net output #0: loss = 0.129675 (* 1 = 0.129675 loss)
I0428 20:35:15.118374  4585 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:35:15.356243  4585 solver.cpp:219] Iteration 800 (420.414 iter/s, 0.237861s/100 iters), loss = 0.392279
I0428 20:35:15.356283  4585 solver.cpp:238]     Train net output #0: loss = 0.39228 (* 1 = 0.39228 loss)
I0428 20:35:15.356290  4585 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:35:15.590139  4585 solver.cpp:219] Iteration 900 (427.627 iter/s, 0.233849s/100 iters), loss = 0.137582
I0428 20:35:15.590186  4585 solver.cpp:238]     Train net output #0: loss = 0.137582 (* 1 = 0.137582 loss)
I0428 20:35:15.590193  4585 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:35:15.667871  4591 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:35:15.822129  4585 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:35:15.826859  4585 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:35:15.828739  4585 solver.cpp:311] Iteration 1000, loss = 0.0845561
I0428 20:35:15.828755  4585 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:35:15.937075  4592 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:35:15.940011  4585 solver.cpp:398]     Test net output #0: accuracy = 0.976
I0428 20:35:15.940029  4585 solver.cpp:398]     Test net output #1: loss = 0.0730147 (* 1 = 0.0730147 loss)
I0428 20:35:15.940049  4585 solver.cpp:316] Optimization Done.
I0428 20:35:15.940052  4585 caffe.cpp:259] Optimization Done.
