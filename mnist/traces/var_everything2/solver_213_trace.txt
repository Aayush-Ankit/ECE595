I0428 19:34:52.498785 23405 caffe.cpp:218] Using GPUs 0
I0428 19:34:52.540460 23405 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:34:53.064540 23405 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test213.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:34:53.064709 23405 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test213.prototxt
I0428 19:34:53.065090 23405 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:34:53.065112 23405 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:34:53.065209 23405 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool0"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:34:53.065317 23405 layer_factory.hpp:77] Creating layer mnist
I0428 19:34:53.065448 23405 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:34:53.065480 23405 net.cpp:86] Creating Layer mnist
I0428 19:34:53.065492 23405 net.cpp:382] mnist -> data
I0428 19:34:53.065526 23405 net.cpp:382] mnist -> label
I0428 19:34:53.066758 23405 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:34:53.069227 23405 net.cpp:124] Setting up mnist
I0428 19:34:53.069248 23405 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:34:53.069283 23405 net.cpp:131] Top shape: 64 (64)
I0428 19:34:53.069290 23405 net.cpp:139] Memory required for data: 200960
I0428 19:34:53.069300 23405 layer_factory.hpp:77] Creating layer conv0
I0428 19:34:53.069322 23405 net.cpp:86] Creating Layer conv0
I0428 19:34:53.069332 23405 net.cpp:408] conv0 <- data
I0428 19:34:53.069352 23405 net.cpp:382] conv0 -> conv0
I0428 19:34:53.361253 23405 net.cpp:124] Setting up conv0
I0428 19:34:53.361286 23405 net.cpp:131] Top shape: 64 50 24 24 (1843200)
I0428 19:34:53.361294 23405 net.cpp:139] Memory required for data: 7573760
I0428 19:34:53.361318 23405 layer_factory.hpp:77] Creating layer pool0
I0428 19:34:53.361340 23405 net.cpp:86] Creating Layer pool0
I0428 19:34:53.361371 23405 net.cpp:408] pool0 <- conv0
I0428 19:34:53.361385 23405 net.cpp:382] pool0 -> pool0
I0428 19:34:53.361456 23405 net.cpp:124] Setting up pool0
I0428 19:34:53.361469 23405 net.cpp:131] Top shape: 64 50 12 12 (460800)
I0428 19:34:53.361474 23405 net.cpp:139] Memory required for data: 9416960
I0428 19:34:53.361479 23405 layer_factory.hpp:77] Creating layer ip1
I0428 19:34:53.361496 23405 net.cpp:86] Creating Layer ip1
I0428 19:34:53.361508 23405 net.cpp:408] ip1 <- pool0
I0428 19:34:53.361522 23405 net.cpp:382] ip1 -> ip1
I0428 19:34:53.363118 23405 net.cpp:124] Setting up ip1
I0428 19:34:53.363137 23405 net.cpp:131] Top shape: 64 10 (640)
I0428 19:34:53.363143 23405 net.cpp:139] Memory required for data: 9419520
I0428 19:34:53.363160 23405 layer_factory.hpp:77] Creating layer relu1
I0428 19:34:53.363173 23405 net.cpp:86] Creating Layer relu1
I0428 19:34:53.363180 23405 net.cpp:408] relu1 <- ip1
I0428 19:34:53.363191 23405 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:34:53.363418 23405 net.cpp:124] Setting up relu1
I0428 19:34:53.363431 23405 net.cpp:131] Top shape: 64 10 (640)
I0428 19:34:53.363437 23405 net.cpp:139] Memory required for data: 9422080
I0428 19:34:53.363443 23405 layer_factory.hpp:77] Creating layer ip2
I0428 19:34:53.363458 23405 net.cpp:86] Creating Layer ip2
I0428 19:34:53.363466 23405 net.cpp:408] ip2 <- ip1
I0428 19:34:53.363476 23405 net.cpp:382] ip2 -> ip2
I0428 19:34:53.363615 23405 net.cpp:124] Setting up ip2
I0428 19:34:53.363627 23405 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:34:53.363633 23405 net.cpp:139] Memory required for data: 9434880
I0428 19:34:53.363648 23405 layer_factory.hpp:77] Creating layer relu2
I0428 19:34:53.363665 23405 net.cpp:86] Creating Layer relu2
I0428 19:34:53.363674 23405 net.cpp:408] relu2 <- ip2
I0428 19:34:53.363683 23405 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:34:53.364562 23405 net.cpp:124] Setting up relu2
I0428 19:34:53.364579 23405 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:34:53.364586 23405 net.cpp:139] Memory required for data: 9447680
I0428 19:34:53.364593 23405 layer_factory.hpp:77] Creating layer ip3
I0428 19:34:53.364608 23405 net.cpp:86] Creating Layer ip3
I0428 19:34:53.364614 23405 net.cpp:408] ip3 <- ip2
I0428 19:34:53.364624 23405 net.cpp:382] ip3 -> ip3
I0428 19:34:53.364761 23405 net.cpp:124] Setting up ip3
I0428 19:34:53.364773 23405 net.cpp:131] Top shape: 64 10 (640)
I0428 19:34:53.364779 23405 net.cpp:139] Memory required for data: 9450240
I0428 19:34:53.364791 23405 layer_factory.hpp:77] Creating layer relu3
I0428 19:34:53.364805 23405 net.cpp:86] Creating Layer relu3
I0428 19:34:53.364825 23405 net.cpp:408] relu3 <- ip3
I0428 19:34:53.364835 23405 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:34:53.365041 23405 net.cpp:124] Setting up relu3
I0428 19:34:53.365053 23405 net.cpp:131] Top shape: 64 10 (640)
I0428 19:34:53.365059 23405 net.cpp:139] Memory required for data: 9452800
I0428 19:34:53.365066 23405 layer_factory.hpp:77] Creating layer loss
I0428 19:34:53.365078 23405 net.cpp:86] Creating Layer loss
I0428 19:34:53.365084 23405 net.cpp:408] loss <- ip3
I0428 19:34:53.365092 23405 net.cpp:408] loss <- label
I0428 19:34:53.365103 23405 net.cpp:382] loss -> loss
I0428 19:34:53.365133 23405 layer_factory.hpp:77] Creating layer loss
I0428 19:34:53.365444 23405 net.cpp:124] Setting up loss
I0428 19:34:53.365458 23405 net.cpp:131] Top shape: (1)
I0428 19:34:53.365464 23405 net.cpp:134]     with loss weight 1
I0428 19:34:53.365489 23405 net.cpp:139] Memory required for data: 9452804
I0428 19:34:53.365495 23405 net.cpp:200] loss needs backward computation.
I0428 19:34:53.365502 23405 net.cpp:200] relu3 needs backward computation.
I0428 19:34:53.365509 23405 net.cpp:200] ip3 needs backward computation.
I0428 19:34:53.365514 23405 net.cpp:200] relu2 needs backward computation.
I0428 19:34:53.365520 23405 net.cpp:200] ip2 needs backward computation.
I0428 19:34:53.365525 23405 net.cpp:200] relu1 needs backward computation.
I0428 19:34:53.365531 23405 net.cpp:200] ip1 needs backward computation.
I0428 19:34:53.365551 23405 net.cpp:200] pool0 needs backward computation.
I0428 19:34:53.365558 23405 net.cpp:200] conv0 needs backward computation.
I0428 19:34:53.365566 23405 net.cpp:202] mnist does not need backward computation.
I0428 19:34:53.365571 23405 net.cpp:244] This network produces output loss
I0428 19:34:53.365587 23405 net.cpp:257] Network initialization done.
I0428 19:34:53.365932 23405 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test213.prototxt
I0428 19:34:53.365973 23405 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:34:53.366080 23405 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool0"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:34:53.366205 23405 layer_factory.hpp:77] Creating layer mnist
I0428 19:34:53.366276 23405 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:34:53.366295 23405 net.cpp:86] Creating Layer mnist
I0428 19:34:53.366307 23405 net.cpp:382] mnist -> data
I0428 19:34:53.366320 23405 net.cpp:382] mnist -> label
I0428 19:34:53.366477 23405 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:34:53.368031 23405 net.cpp:124] Setting up mnist
I0428 19:34:53.368047 23405 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:34:53.368059 23405 net.cpp:131] Top shape: 100 (100)
I0428 19:34:53.368065 23405 net.cpp:139] Memory required for data: 314000
I0428 19:34:53.368072 23405 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:34:53.368084 23405 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:34:53.368091 23405 net.cpp:408] label_mnist_1_split <- label
I0428 19:34:53.368100 23405 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:34:53.368113 23405 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:34:53.368242 23405 net.cpp:124] Setting up label_mnist_1_split
I0428 19:34:53.368254 23405 net.cpp:131] Top shape: 100 (100)
I0428 19:34:53.368263 23405 net.cpp:131] Top shape: 100 (100)
I0428 19:34:53.368268 23405 net.cpp:139] Memory required for data: 314800
I0428 19:34:53.368286 23405 layer_factory.hpp:77] Creating layer conv0
I0428 19:34:53.368316 23405 net.cpp:86] Creating Layer conv0
I0428 19:34:53.368324 23405 net.cpp:408] conv0 <- data
I0428 19:34:53.368336 23405 net.cpp:382] conv0 -> conv0
I0428 19:34:53.370038 23405 net.cpp:124] Setting up conv0
I0428 19:34:53.370056 23405 net.cpp:131] Top shape: 100 50 24 24 (2880000)
I0428 19:34:53.370064 23405 net.cpp:139] Memory required for data: 11834800
I0428 19:34:53.370080 23405 layer_factory.hpp:77] Creating layer pool0
I0428 19:34:53.370097 23405 net.cpp:86] Creating Layer pool0
I0428 19:34:53.370105 23405 net.cpp:408] pool0 <- conv0
I0428 19:34:53.370115 23405 net.cpp:382] pool0 -> pool0
I0428 19:34:53.370167 23405 net.cpp:124] Setting up pool0
I0428 19:34:53.370177 23405 net.cpp:131] Top shape: 100 50 12 12 (720000)
I0428 19:34:53.370183 23405 net.cpp:139] Memory required for data: 14714800
I0428 19:34:53.370189 23405 layer_factory.hpp:77] Creating layer ip1
I0428 19:34:53.370203 23405 net.cpp:86] Creating Layer ip1
I0428 19:34:53.370210 23405 net.cpp:408] ip1 <- pool0
I0428 19:34:53.370220 23405 net.cpp:382] ip1 -> ip1
I0428 19:34:53.370821 23405 net.cpp:124] Setting up ip1
I0428 19:34:53.370832 23405 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:34:53.370838 23405 net.cpp:139] Memory required for data: 14718800
I0428 19:34:53.370852 23405 layer_factory.hpp:77] Creating layer relu1
I0428 19:34:53.370862 23405 net.cpp:86] Creating Layer relu1
I0428 19:34:53.370869 23405 net.cpp:408] relu1 <- ip1
I0428 19:34:53.370879 23405 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:34:53.371088 23405 net.cpp:124] Setting up relu1
I0428 19:34:53.371100 23405 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:34:53.371107 23405 net.cpp:139] Memory required for data: 14722800
I0428 19:34:53.371114 23405 layer_factory.hpp:77] Creating layer ip2
I0428 19:34:53.371126 23405 net.cpp:86] Creating Layer ip2
I0428 19:34:53.371134 23405 net.cpp:408] ip2 <- ip1
I0428 19:34:53.371145 23405 net.cpp:382] ip2 -> ip2
I0428 19:34:53.371287 23405 net.cpp:124] Setting up ip2
I0428 19:34:53.371297 23405 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:34:53.371302 23405 net.cpp:139] Memory required for data: 14742800
I0428 19:34:53.371320 23405 layer_factory.hpp:77] Creating layer relu2
I0428 19:34:53.371330 23405 net.cpp:86] Creating Layer relu2
I0428 19:34:53.371337 23405 net.cpp:408] relu2 <- ip2
I0428 19:34:53.371346 23405 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:34:53.372287 23405 net.cpp:124] Setting up relu2
I0428 19:34:53.372303 23405 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:34:53.372308 23405 net.cpp:139] Memory required for data: 14762800
I0428 19:34:53.372315 23405 layer_factory.hpp:77] Creating layer ip3
I0428 19:34:53.372331 23405 net.cpp:86] Creating Layer ip3
I0428 19:34:53.372339 23405 net.cpp:408] ip3 <- ip2
I0428 19:34:53.372350 23405 net.cpp:382] ip3 -> ip3
I0428 19:34:53.372488 23405 net.cpp:124] Setting up ip3
I0428 19:34:53.372500 23405 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:34:53.372505 23405 net.cpp:139] Memory required for data: 14766800
I0428 19:34:53.372516 23405 layer_factory.hpp:77] Creating layer relu3
I0428 19:34:53.372526 23405 net.cpp:86] Creating Layer relu3
I0428 19:34:53.372535 23405 net.cpp:408] relu3 <- ip3
I0428 19:34:53.372545 23405 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:34:53.372740 23405 net.cpp:124] Setting up relu3
I0428 19:34:53.372753 23405 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:34:53.372759 23405 net.cpp:139] Memory required for data: 14770800
I0428 19:34:53.372766 23405 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:34:53.372774 23405 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:34:53.372781 23405 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:34:53.372794 23405 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:34:53.372805 23405 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:34:53.372865 23405 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:34:53.372876 23405 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:34:53.372898 23405 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:34:53.372905 23405 net.cpp:139] Memory required for data: 14778800
I0428 19:34:53.372911 23405 layer_factory.hpp:77] Creating layer accuracy
I0428 19:34:53.372923 23405 net.cpp:86] Creating Layer accuracy
I0428 19:34:53.372930 23405 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:34:53.372938 23405 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:34:53.372951 23405 net.cpp:382] accuracy -> accuracy
I0428 19:34:53.372966 23405 net.cpp:124] Setting up accuracy
I0428 19:34:53.372975 23405 net.cpp:131] Top shape: (1)
I0428 19:34:53.372982 23405 net.cpp:139] Memory required for data: 14778804
I0428 19:34:53.372987 23405 layer_factory.hpp:77] Creating layer loss
I0428 19:34:53.372997 23405 net.cpp:86] Creating Layer loss
I0428 19:34:53.373004 23405 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:34:53.373011 23405 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:34:53.373019 23405 net.cpp:382] loss -> loss
I0428 19:34:53.373033 23405 layer_factory.hpp:77] Creating layer loss
I0428 19:34:53.373381 23405 net.cpp:124] Setting up loss
I0428 19:34:53.373395 23405 net.cpp:131] Top shape: (1)
I0428 19:34:53.373401 23405 net.cpp:134]     with loss weight 1
I0428 19:34:53.373411 23405 net.cpp:139] Memory required for data: 14778808
I0428 19:34:53.373420 23405 net.cpp:200] loss needs backward computation.
I0428 19:34:53.373426 23405 net.cpp:202] accuracy does not need backward computation.
I0428 19:34:53.373433 23405 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:34:53.373440 23405 net.cpp:200] relu3 needs backward computation.
I0428 19:34:53.373445 23405 net.cpp:200] ip3 needs backward computation.
I0428 19:34:53.373450 23405 net.cpp:200] relu2 needs backward computation.
I0428 19:34:53.373456 23405 net.cpp:200] ip2 needs backward computation.
I0428 19:34:53.373462 23405 net.cpp:200] relu1 needs backward computation.
I0428 19:34:53.373467 23405 net.cpp:200] ip1 needs backward computation.
I0428 19:34:53.373473 23405 net.cpp:200] pool0 needs backward computation.
I0428 19:34:53.373479 23405 net.cpp:200] conv0 needs backward computation.
I0428 19:34:53.373488 23405 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:34:53.373495 23405 net.cpp:202] mnist does not need backward computation.
I0428 19:34:53.373502 23405 net.cpp:244] This network produces output accuracy
I0428 19:34:53.373508 23405 net.cpp:244] This network produces output loss
I0428 19:34:53.373525 23405 net.cpp:257] Network initialization done.
I0428 19:34:53.373576 23405 solver.cpp:56] Solver scaffolding done.
I0428 19:34:53.373895 23405 caffe.cpp:248] Starting Optimization
I0428 19:34:53.373904 23405 solver.cpp:273] Solving LeNet
I0428 19:34:53.373910 23405 solver.cpp:274] Learning Rate Policy: inv
I0428 19:34:53.375471 23405 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:34:53.473728 23412 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:34:53.476323 23405 solver.cpp:398]     Test net output #0: accuracy = 0.1147
I0428 19:34:53.476343 23405 solver.cpp:398]     Test net output #1: loss = 2.29929 (* 1 = 2.29929 loss)
I0428 19:34:53.480598 23405 solver.cpp:219] Iteration 0 (0 iter/s, 0.106654s/100 iters), loss = 2.27912
I0428 19:34:53.480625 23405 solver.cpp:238]     Train net output #0: loss = 2.27912 (* 1 = 2.27912 loss)
I0428 19:34:53.480643 23405 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:34:53.623522 23405 solver.cpp:219] Iteration 100 (699.874 iter/s, 0.142883s/100 iters), loss = 0.410878
I0428 19:34:53.623551 23405 solver.cpp:238]     Train net output #0: loss = 0.410878 (* 1 = 0.410878 loss)
I0428 19:34:53.623576 23405 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:34:53.766901 23405 solver.cpp:219] Iteration 200 (697.649 iter/s, 0.143339s/100 iters), loss = 0.252127
I0428 19:34:53.766927 23405 solver.cpp:238]     Train net output #0: loss = 0.252127 (* 1 = 0.252127 loss)
I0428 19:34:53.766937 23405 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:34:53.907395 23405 solver.cpp:219] Iteration 300 (711.966 iter/s, 0.140456s/100 iters), loss = 0.256865
I0428 19:34:53.907436 23405 solver.cpp:238]     Train net output #0: loss = 0.256865 (* 1 = 0.256865 loss)
I0428 19:34:53.907446 23405 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:34:54.047511 23405 solver.cpp:219] Iteration 400 (713.95 iter/s, 0.140066s/100 iters), loss = 0.148005
I0428 19:34:54.047557 23405 solver.cpp:238]     Train net output #0: loss = 0.148005 (* 1 = 0.148005 loss)
I0428 19:34:54.047582 23405 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:34:54.190023 23405 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:34:54.286677 23412 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:34:54.290092 23405 solver.cpp:398]     Test net output #0: accuracy = 0.9449
I0428 19:34:54.290115 23405 solver.cpp:398]     Test net output #1: loss = 0.181006 (* 1 = 0.181006 loss)
I0428 19:34:54.291455 23405 solver.cpp:219] Iteration 500 (410.033 iter/s, 0.243883s/100 iters), loss = 0.221417
I0428 19:34:54.291481 23405 solver.cpp:238]     Train net output #0: loss = 0.221417 (* 1 = 0.221417 loss)
I0428 19:34:54.291491 23405 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:34:54.437628 23405 solver.cpp:219] Iteration 600 (684.294 iter/s, 0.146136s/100 iters), loss = 0.114304
I0428 19:34:54.437654 23405 solver.cpp:238]     Train net output #0: loss = 0.114304 (* 1 = 0.114304 loss)
I0428 19:34:54.437680 23405 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:34:54.579622 23405 solver.cpp:219] Iteration 700 (704.438 iter/s, 0.141957s/100 iters), loss = 0.214778
I0428 19:34:54.579649 23405 solver.cpp:238]     Train net output #0: loss = 0.214778 (* 1 = 0.214778 loss)
I0428 19:34:54.579674 23405 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:34:54.722136 23405 solver.cpp:219] Iteration 800 (701.866 iter/s, 0.142477s/100 iters), loss = 0.338625
I0428 19:34:54.722163 23405 solver.cpp:238]     Train net output #0: loss = 0.338625 (* 1 = 0.338625 loss)
I0428 19:34:54.722188 23405 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:34:54.865192 23405 solver.cpp:219] Iteration 900 (699.213 iter/s, 0.143018s/100 iters), loss = 0.178658
I0428 19:34:54.865218 23405 solver.cpp:238]     Train net output #0: loss = 0.178658 (* 1 = 0.178658 loss)
I0428 19:34:54.865243 23405 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:34:54.912389 23411 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:34:55.004730 23405 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:34:55.006551 23405 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:34:55.008132 23405 solver.cpp:311] Iteration 1000, loss = 0.12181
I0428 19:34:55.008173 23405 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:34:55.103485 23412 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:34:55.106981 23405 solver.cpp:398]     Test net output #0: accuracy = 0.9642
I0428 19:34:55.107017 23405 solver.cpp:398]     Test net output #1: loss = 0.117498 (* 1 = 0.117498 loss)
I0428 19:34:55.107039 23405 solver.cpp:316] Optimization Done.
I0428 19:34:55.107044 23405 caffe.cpp:259] Optimization Done.
