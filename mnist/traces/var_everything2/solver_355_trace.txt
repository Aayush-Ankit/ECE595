I0428 19:41:07.122686 24745 caffe.cpp:218] Using GPUs 0
I0428 19:41:07.164027 24745 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:41:07.686326 24745 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test355.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:41:07.686502 24745 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test355.prototxt
I0428 19:41:07.686889 24745 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:41:07.686913 24745 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:41:07.687019 24745 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0428 19:41:07.687129 24745 layer_factory.hpp:77] Creating layer mnist
I0428 19:41:07.687261 24745 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:41:07.687297 24745 net.cpp:86] Creating Layer mnist
I0428 19:41:07.687309 24745 net.cpp:382] mnist -> data
I0428 19:41:07.687341 24745 net.cpp:382] mnist -> label
I0428 19:41:07.688573 24745 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:41:07.691081 24745 net.cpp:124] Setting up mnist
I0428 19:41:07.691102 24745 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:41:07.691113 24745 net.cpp:131] Top shape: 64 (64)
I0428 19:41:07.691120 24745 net.cpp:139] Memory required for data: 200960
I0428 19:41:07.691131 24745 layer_factory.hpp:77] Creating layer conv0
I0428 19:41:07.691154 24745 net.cpp:86] Creating Layer conv0
I0428 19:41:07.691164 24745 net.cpp:408] conv0 <- data
I0428 19:41:07.691184 24745 net.cpp:382] conv0 -> conv0
I0428 19:41:07.982915 24745 net.cpp:124] Setting up conv0
I0428 19:41:07.982947 24745 net.cpp:131] Top shape: 64 2 24 24 (73728)
I0428 19:41:07.982954 24745 net.cpp:139] Memory required for data: 495872
I0428 19:41:07.983006 24745 layer_factory.hpp:77] Creating layer pool0
I0428 19:41:07.983026 24745 net.cpp:86] Creating Layer pool0
I0428 19:41:07.983036 24745 net.cpp:408] pool0 <- conv0
I0428 19:41:07.983045 24745 net.cpp:382] pool0 -> pool0
I0428 19:41:07.983108 24745 net.cpp:124] Setting up pool0
I0428 19:41:07.983119 24745 net.cpp:131] Top shape: 64 2 12 12 (18432)
I0428 19:41:07.983124 24745 net.cpp:139] Memory required for data: 569600
I0428 19:41:07.983131 24745 layer_factory.hpp:77] Creating layer conv1
I0428 19:41:07.983150 24745 net.cpp:86] Creating Layer conv1
I0428 19:41:07.983157 24745 net.cpp:408] conv1 <- pool0
I0428 19:41:07.983168 24745 net.cpp:382] conv1 -> conv1
I0428 19:41:07.985296 24745 net.cpp:124] Setting up conv1
I0428 19:41:07.985314 24745 net.cpp:131] Top shape: 64 10 8 8 (40960)
I0428 19:41:07.985321 24745 net.cpp:139] Memory required for data: 733440
I0428 19:41:07.985339 24745 layer_factory.hpp:77] Creating layer pool1
I0428 19:41:07.985352 24745 net.cpp:86] Creating Layer pool1
I0428 19:41:07.985368 24745 net.cpp:408] pool1 <- conv1
I0428 19:41:07.985379 24745 net.cpp:382] pool1 -> pool1
I0428 19:41:07.985431 24745 net.cpp:124] Setting up pool1
I0428 19:41:07.985445 24745 net.cpp:131] Top shape: 64 10 4 4 (10240)
I0428 19:41:07.985450 24745 net.cpp:139] Memory required for data: 774400
I0428 19:41:07.985455 24745 layer_factory.hpp:77] Creating layer ip1
I0428 19:41:07.985471 24745 net.cpp:86] Creating Layer ip1
I0428 19:41:07.985481 24745 net.cpp:408] ip1 <- pool1
I0428 19:41:07.985489 24745 net.cpp:382] ip1 -> ip1
I0428 19:41:07.986572 24745 net.cpp:124] Setting up ip1
I0428 19:41:07.986588 24745 net.cpp:131] Top shape: 64 10 (640)
I0428 19:41:07.986594 24745 net.cpp:139] Memory required for data: 776960
I0428 19:41:07.986613 24745 layer_factory.hpp:77] Creating layer relu1
I0428 19:41:07.986624 24745 net.cpp:86] Creating Layer relu1
I0428 19:41:07.986635 24745 net.cpp:408] relu1 <- ip1
I0428 19:41:07.986644 24745 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:41:07.986855 24745 net.cpp:124] Setting up relu1
I0428 19:41:07.986868 24745 net.cpp:131] Top shape: 64 10 (640)
I0428 19:41:07.986874 24745 net.cpp:139] Memory required for data: 779520
I0428 19:41:07.986881 24745 layer_factory.hpp:77] Creating layer ip2
I0428 19:41:07.986894 24745 net.cpp:86] Creating Layer ip2
I0428 19:41:07.986901 24745 net.cpp:408] ip2 <- ip1
I0428 19:41:07.986912 24745 net.cpp:382] ip2 -> ip2
I0428 19:41:07.987038 24745 net.cpp:124] Setting up ip2
I0428 19:41:07.987049 24745 net.cpp:131] Top shape: 64 10 (640)
I0428 19:41:07.987054 24745 net.cpp:139] Memory required for data: 782080
I0428 19:41:07.987066 24745 layer_factory.hpp:77] Creating layer relu2
I0428 19:41:07.987076 24745 net.cpp:86] Creating Layer relu2
I0428 19:41:07.987083 24745 net.cpp:408] relu2 <- ip2
I0428 19:41:07.987092 24745 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:41:07.987929 24745 net.cpp:124] Setting up relu2
I0428 19:41:07.987946 24745 net.cpp:131] Top shape: 64 10 (640)
I0428 19:41:07.987952 24745 net.cpp:139] Memory required for data: 784640
I0428 19:41:07.987958 24745 layer_factory.hpp:77] Creating layer loss
I0428 19:41:07.987968 24745 net.cpp:86] Creating Layer loss
I0428 19:41:07.987974 24745 net.cpp:408] loss <- ip2
I0428 19:41:07.987982 24745 net.cpp:408] loss <- label
I0428 19:41:07.987994 24745 net.cpp:382] loss -> loss
I0428 19:41:07.988018 24745 layer_factory.hpp:77] Creating layer loss
I0428 19:41:07.988318 24745 net.cpp:124] Setting up loss
I0428 19:41:07.988332 24745 net.cpp:131] Top shape: (1)
I0428 19:41:07.988337 24745 net.cpp:134]     with loss weight 1
I0428 19:41:07.988359 24745 net.cpp:139] Memory required for data: 784644
I0428 19:41:07.988365 24745 net.cpp:200] loss needs backward computation.
I0428 19:41:07.988373 24745 net.cpp:200] relu2 needs backward computation.
I0428 19:41:07.988379 24745 net.cpp:200] ip2 needs backward computation.
I0428 19:41:07.988384 24745 net.cpp:200] relu1 needs backward computation.
I0428 19:41:07.988389 24745 net.cpp:200] ip1 needs backward computation.
I0428 19:41:07.988395 24745 net.cpp:200] pool1 needs backward computation.
I0428 19:41:07.988415 24745 net.cpp:200] conv1 needs backward computation.
I0428 19:41:07.988422 24745 net.cpp:200] pool0 needs backward computation.
I0428 19:41:07.988427 24745 net.cpp:200] conv0 needs backward computation.
I0428 19:41:07.988433 24745 net.cpp:202] mnist does not need backward computation.
I0428 19:41:07.988440 24745 net.cpp:244] This network produces output loss
I0428 19:41:07.988457 24745 net.cpp:257] Network initialization done.
I0428 19:41:07.988795 24745 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test355.prototxt
I0428 19:41:07.988842 24745 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:41:07.988950 24745 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0428 19:41:07.989069 24745 layer_factory.hpp:77] Creating layer mnist
I0428 19:41:07.989156 24745 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:41:07.989179 24745 net.cpp:86] Creating Layer mnist
I0428 19:41:07.989188 24745 net.cpp:382] mnist -> data
I0428 19:41:07.989202 24745 net.cpp:382] mnist -> label
I0428 19:41:07.989337 24745 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:41:07.991546 24745 net.cpp:124] Setting up mnist
I0428 19:41:07.991564 24745 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:41:07.991574 24745 net.cpp:131] Top shape: 100 (100)
I0428 19:41:07.991580 24745 net.cpp:139] Memory required for data: 314000
I0428 19:41:07.991585 24745 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:41:07.991596 24745 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:41:07.991602 24745 net.cpp:408] label_mnist_1_split <- label
I0428 19:41:07.991611 24745 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:41:07.991622 24745 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:41:07.991750 24745 net.cpp:124] Setting up label_mnist_1_split
I0428 19:41:07.991770 24745 net.cpp:131] Top shape: 100 (100)
I0428 19:41:07.991778 24745 net.cpp:131] Top shape: 100 (100)
I0428 19:41:07.991785 24745 net.cpp:139] Memory required for data: 314800
I0428 19:41:07.991791 24745 layer_factory.hpp:77] Creating layer conv0
I0428 19:41:07.991807 24745 net.cpp:86] Creating Layer conv0
I0428 19:41:07.991816 24745 net.cpp:408] conv0 <- data
I0428 19:41:07.991827 24745 net.cpp:382] conv0 -> conv0
I0428 19:41:07.993396 24745 net.cpp:124] Setting up conv0
I0428 19:41:07.993419 24745 net.cpp:131] Top shape: 100 2 24 24 (115200)
I0428 19:41:07.993427 24745 net.cpp:139] Memory required for data: 775600
I0428 19:41:07.993441 24745 layer_factory.hpp:77] Creating layer pool0
I0428 19:41:07.993458 24745 net.cpp:86] Creating Layer pool0
I0428 19:41:07.993464 24745 net.cpp:408] pool0 <- conv0
I0428 19:41:07.993475 24745 net.cpp:382] pool0 -> pool0
I0428 19:41:07.993525 24745 net.cpp:124] Setting up pool0
I0428 19:41:07.993535 24745 net.cpp:131] Top shape: 100 2 12 12 (28800)
I0428 19:41:07.993541 24745 net.cpp:139] Memory required for data: 890800
I0428 19:41:07.993546 24745 layer_factory.hpp:77] Creating layer conv1
I0428 19:41:07.993566 24745 net.cpp:86] Creating Layer conv1
I0428 19:41:07.993573 24745 net.cpp:408] conv1 <- pool0
I0428 19:41:07.993585 24745 net.cpp:382] conv1 -> conv1
I0428 19:41:07.995096 24745 net.cpp:124] Setting up conv1
I0428 19:41:07.995121 24745 net.cpp:131] Top shape: 100 10 8 8 (64000)
I0428 19:41:07.995128 24745 net.cpp:139] Memory required for data: 1146800
I0428 19:41:07.995142 24745 layer_factory.hpp:77] Creating layer pool1
I0428 19:41:07.995157 24745 net.cpp:86] Creating Layer pool1
I0428 19:41:07.995163 24745 net.cpp:408] pool1 <- conv1
I0428 19:41:07.995172 24745 net.cpp:382] pool1 -> pool1
I0428 19:41:07.995223 24745 net.cpp:124] Setting up pool1
I0428 19:41:07.995232 24745 net.cpp:131] Top shape: 100 10 4 4 (16000)
I0428 19:41:07.995240 24745 net.cpp:139] Memory required for data: 1210800
I0428 19:41:07.995245 24745 layer_factory.hpp:77] Creating layer ip1
I0428 19:41:07.995256 24745 net.cpp:86] Creating Layer ip1
I0428 19:41:07.995262 24745 net.cpp:408] ip1 <- pool1
I0428 19:41:07.995273 24745 net.cpp:382] ip1 -> ip1
I0428 19:41:07.995407 24745 net.cpp:124] Setting up ip1
I0428 19:41:07.995417 24745 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:41:07.995424 24745 net.cpp:139] Memory required for data: 1214800
I0428 19:41:07.995437 24745 layer_factory.hpp:77] Creating layer relu1
I0428 19:41:07.995446 24745 net.cpp:86] Creating Layer relu1
I0428 19:41:07.995455 24745 net.cpp:408] relu1 <- ip1
I0428 19:41:07.995465 24745 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:41:07.995652 24745 net.cpp:124] Setting up relu1
I0428 19:41:07.995666 24745 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:41:07.995671 24745 net.cpp:139] Memory required for data: 1218800
I0428 19:41:07.995676 24745 layer_factory.hpp:77] Creating layer ip2
I0428 19:41:07.995693 24745 net.cpp:86] Creating Layer ip2
I0428 19:41:07.995702 24745 net.cpp:408] ip2 <- ip1
I0428 19:41:07.995713 24745 net.cpp:382] ip2 -> ip2
I0428 19:41:07.995834 24745 net.cpp:124] Setting up ip2
I0428 19:41:07.995846 24745 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:41:07.995851 24745 net.cpp:139] Memory required for data: 1222800
I0428 19:41:07.995860 24745 layer_factory.hpp:77] Creating layer relu2
I0428 19:41:07.995872 24745 net.cpp:86] Creating Layer relu2
I0428 19:41:07.995879 24745 net.cpp:408] relu2 <- ip2
I0428 19:41:07.995887 24745 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:41:07.996166 24745 net.cpp:124] Setting up relu2
I0428 19:41:07.996178 24745 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:41:07.996183 24745 net.cpp:139] Memory required for data: 1226800
I0428 19:41:07.996189 24745 layer_factory.hpp:77] Creating layer ip2_relu2_0_split
I0428 19:41:07.996201 24745 net.cpp:86] Creating Layer ip2_relu2_0_split
I0428 19:41:07.996206 24745 net.cpp:408] ip2_relu2_0_split <- ip2
I0428 19:41:07.996215 24745 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_0
I0428 19:41:07.996239 24745 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_1
I0428 19:41:07.996291 24745 net.cpp:124] Setting up ip2_relu2_0_split
I0428 19:41:07.996301 24745 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:41:07.996309 24745 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:41:07.996314 24745 net.cpp:139] Memory required for data: 1234800
I0428 19:41:07.996320 24745 layer_factory.hpp:77] Creating layer accuracy
I0428 19:41:07.996328 24745 net.cpp:86] Creating Layer accuracy
I0428 19:41:07.996336 24745 net.cpp:408] accuracy <- ip2_relu2_0_split_0
I0428 19:41:07.996345 24745 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:41:07.996354 24745 net.cpp:382] accuracy -> accuracy
I0428 19:41:07.996366 24745 net.cpp:124] Setting up accuracy
I0428 19:41:07.996376 24745 net.cpp:131] Top shape: (1)
I0428 19:41:07.996381 24745 net.cpp:139] Memory required for data: 1234804
I0428 19:41:07.996387 24745 layer_factory.hpp:77] Creating layer loss
I0428 19:41:07.996395 24745 net.cpp:86] Creating Layer loss
I0428 19:41:07.996402 24745 net.cpp:408] loss <- ip2_relu2_0_split_1
I0428 19:41:07.996408 24745 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:41:07.996418 24745 net.cpp:382] loss -> loss
I0428 19:41:07.996429 24745 layer_factory.hpp:77] Creating layer loss
I0428 19:41:07.996708 24745 net.cpp:124] Setting up loss
I0428 19:41:07.996721 24745 net.cpp:131] Top shape: (1)
I0428 19:41:07.996726 24745 net.cpp:134]     with loss weight 1
I0428 19:41:07.996737 24745 net.cpp:139] Memory required for data: 1234808
I0428 19:41:07.996742 24745 net.cpp:200] loss needs backward computation.
I0428 19:41:07.996750 24745 net.cpp:202] accuracy does not need backward computation.
I0428 19:41:07.996757 24745 net.cpp:200] ip2_relu2_0_split needs backward computation.
I0428 19:41:07.996763 24745 net.cpp:200] relu2 needs backward computation.
I0428 19:41:07.996768 24745 net.cpp:200] ip2 needs backward computation.
I0428 19:41:07.996783 24745 net.cpp:200] relu1 needs backward computation.
I0428 19:41:07.996789 24745 net.cpp:200] ip1 needs backward computation.
I0428 19:41:07.996795 24745 net.cpp:200] pool1 needs backward computation.
I0428 19:41:07.996805 24745 net.cpp:200] conv1 needs backward computation.
I0428 19:41:07.996819 24745 net.cpp:200] pool0 needs backward computation.
I0428 19:41:07.996824 24745 net.cpp:200] conv0 needs backward computation.
I0428 19:41:07.996830 24745 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:41:07.996837 24745 net.cpp:202] mnist does not need backward computation.
I0428 19:41:07.996842 24745 net.cpp:244] This network produces output accuracy
I0428 19:41:07.996848 24745 net.cpp:244] This network produces output loss
I0428 19:41:07.996868 24745 net.cpp:257] Network initialization done.
I0428 19:41:07.996917 24745 solver.cpp:56] Solver scaffolding done.
I0428 19:41:07.997261 24745 caffe.cpp:248] Starting Optimization
I0428 19:41:07.997268 24745 solver.cpp:273] Solving LeNet
I0428 19:41:07.997273 24745 solver.cpp:274] Learning Rate Policy: inv
I0428 19:41:07.998167 24745 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:41:08.000649 24745 blocking_queue.cpp:49] Waiting for data
I0428 19:41:08.071373 24752 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:41:08.071781 24745 solver.cpp:398]     Test net output #0: accuracy = 0.1198
I0428 19:41:08.071805 24745 solver.cpp:398]     Test net output #1: loss = 2.34087 (* 1 = 2.34087 loss)
I0428 19:41:08.073631 24745 solver.cpp:219] Iteration 0 (0 iter/s, 0.0763025s/100 iters), loss = 2.35389
I0428 19:41:08.073657 24745 solver.cpp:238]     Train net output #0: loss = 2.35389 (* 1 = 2.35389 loss)
I0428 19:41:08.073690 24745 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:41:08.138108 24745 solver.cpp:219] Iteration 100 (1551.86 iter/s, 0.0644389s/100 iters), loss = 1.35896
I0428 19:41:08.138137 24745 solver.cpp:238]     Train net output #0: loss = 1.35896 (* 1 = 1.35896 loss)
I0428 19:41:08.138149 24745 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:41:08.201949 24745 solver.cpp:219] Iteration 200 (1567.24 iter/s, 0.0638065s/100 iters), loss = 0.574229
I0428 19:41:08.201989 24745 solver.cpp:238]     Train net output #0: loss = 0.574229 (* 1 = 0.574229 loss)
I0428 19:41:08.202015 24745 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:41:08.266409 24745 solver.cpp:219] Iteration 300 (1552.49 iter/s, 0.0644127s/100 iters), loss = 0.316962
I0428 19:41:08.266449 24745 solver.cpp:238]     Train net output #0: loss = 0.316962 (* 1 = 0.316962 loss)
I0428 19:41:08.266474 24745 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:41:08.330718 24745 solver.cpp:219] Iteration 400 (1556.25 iter/s, 0.064257s/100 iters), loss = 0.28026
I0428 19:41:08.330760 24745 solver.cpp:238]     Train net output #0: loss = 0.28026 (* 1 = 0.28026 loss)
I0428 19:41:08.330786 24745 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:41:08.394580 24745 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:41:08.447404 24752 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:41:08.447801 24745 solver.cpp:398]     Test net output #0: accuracy = 0.9332
I0428 19:41:08.447821 24745 solver.cpp:398]     Test net output #1: loss = 0.225894 (* 1 = 0.225894 loss)
I0428 19:41:08.448571 24745 solver.cpp:219] Iteration 500 (848.883 iter/s, 0.117802s/100 iters), loss = 0.248607
I0428 19:41:08.448599 24745 solver.cpp:238]     Train net output #0: loss = 0.248607 (* 1 = 0.248607 loss)
I0428 19:41:08.448616 24745 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:41:08.512938 24745 solver.cpp:219] Iteration 600 (1554.47 iter/s, 0.0643307s/100 iters), loss = 0.211571
I0428 19:41:08.512964 24745 solver.cpp:238]     Train net output #0: loss = 0.211571 (* 1 = 0.211571 loss)
I0428 19:41:08.512990 24745 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:41:08.577450 24745 solver.cpp:219] Iteration 700 (1551.04 iter/s, 0.064473s/100 iters), loss = 0.375182
I0428 19:41:08.577477 24745 solver.cpp:238]     Train net output #0: loss = 0.375182 (* 1 = 0.375182 loss)
I0428 19:41:08.577502 24745 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:41:08.642725 24745 solver.cpp:219] Iteration 800 (1532.85 iter/s, 0.065238s/100 iters), loss = 0.417267
I0428 19:41:08.642753 24745 solver.cpp:238]     Train net output #0: loss = 0.417267 (* 1 = 0.417267 loss)
I0428 19:41:08.642779 24745 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:41:08.707309 24745 solver.cpp:219] Iteration 900 (1549.26 iter/s, 0.064547s/100 iters), loss = 0.221316
I0428 19:41:08.707335 24745 solver.cpp:238]     Train net output #0: loss = 0.221315 (* 1 = 0.221315 loss)
I0428 19:41:08.707360 24745 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:41:08.728943 24751 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:41:08.771754 24745 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:41:08.772433 24745 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:41:08.772891 24745 solver.cpp:311] Iteration 1000, loss = 0.238266
I0428 19:41:08.772908 24745 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:41:08.818610 24752 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:41:08.819023 24745 solver.cpp:398]     Test net output #0: accuracy = 0.9493
I0428 19:41:08.819044 24745 solver.cpp:398]     Test net output #1: loss = 0.169017 (* 1 = 0.169017 loss)
I0428 19:41:08.819056 24745 solver.cpp:316] Optimization Done.
I0428 19:41:08.819062 24745 caffe.cpp:259] Optimization Done.
