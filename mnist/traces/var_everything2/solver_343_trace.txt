I0428 19:40:43.316994 24619 caffe.cpp:218] Using GPUs 0
I0428 19:40:43.358947 24619 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:40:43.897548 24619 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test343.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:40:43.897693 24619 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test343.prototxt
I0428 19:40:43.898113 24619 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:40:43.898133 24619 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:40:43.898236 24619 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:40:43.898317 24619 layer_factory.hpp:77] Creating layer mnist
I0428 19:40:43.898416 24619 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:40:43.898447 24619 net.cpp:86] Creating Layer mnist
I0428 19:40:43.898457 24619 net.cpp:382] mnist -> data
I0428 19:40:43.898478 24619 net.cpp:382] mnist -> label
I0428 19:40:43.899586 24619 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:40:43.902066 24619 net.cpp:124] Setting up mnist
I0428 19:40:43.902086 24619 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:40:43.902092 24619 net.cpp:131] Top shape: 64 (64)
I0428 19:40:43.902096 24619 net.cpp:139] Memory required for data: 200960
I0428 19:40:43.902102 24619 layer_factory.hpp:77] Creating layer conv0
I0428 19:40:43.902119 24619 net.cpp:86] Creating Layer conv0
I0428 19:40:43.902139 24619 net.cpp:408] conv0 <- data
I0428 19:40:43.902153 24619 net.cpp:382] conv0 -> conv0
I0428 19:40:44.195202 24619 net.cpp:124] Setting up conv0
I0428 19:40:44.195235 24619 net.cpp:131] Top shape: 64 2 24 24 (73728)
I0428 19:40:44.195240 24619 net.cpp:139] Memory required for data: 495872
I0428 19:40:44.195257 24619 layer_factory.hpp:77] Creating layer pool0
I0428 19:40:44.195272 24619 net.cpp:86] Creating Layer pool0
I0428 19:40:44.195277 24619 net.cpp:408] pool0 <- conv0
I0428 19:40:44.195284 24619 net.cpp:382] pool0 -> pool0
I0428 19:40:44.195339 24619 net.cpp:124] Setting up pool0
I0428 19:40:44.195348 24619 net.cpp:131] Top shape: 64 2 12 12 (18432)
I0428 19:40:44.195353 24619 net.cpp:139] Memory required for data: 569600
I0428 19:40:44.195356 24619 layer_factory.hpp:77] Creating layer conv1
I0428 19:40:44.195369 24619 net.cpp:86] Creating Layer conv1
I0428 19:40:44.195375 24619 net.cpp:408] conv1 <- pool0
I0428 19:40:44.195381 24619 net.cpp:382] conv1 -> conv1
I0428 19:40:44.197531 24619 net.cpp:124] Setting up conv1
I0428 19:40:44.197549 24619 net.cpp:131] Top shape: 64 5 8 8 (20480)
I0428 19:40:44.197554 24619 net.cpp:139] Memory required for data: 651520
I0428 19:40:44.197564 24619 layer_factory.hpp:77] Creating layer pool1
I0428 19:40:44.197573 24619 net.cpp:86] Creating Layer pool1
I0428 19:40:44.197578 24619 net.cpp:408] pool1 <- conv1
I0428 19:40:44.197584 24619 net.cpp:382] pool1 -> pool1
I0428 19:40:44.197628 24619 net.cpp:124] Setting up pool1
I0428 19:40:44.197635 24619 net.cpp:131] Top shape: 64 5 4 4 (5120)
I0428 19:40:44.197638 24619 net.cpp:139] Memory required for data: 672000
I0428 19:40:44.197643 24619 layer_factory.hpp:77] Creating layer ip1
I0428 19:40:44.197651 24619 net.cpp:86] Creating Layer ip1
I0428 19:40:44.197654 24619 net.cpp:408] ip1 <- pool1
I0428 19:40:44.197660 24619 net.cpp:382] ip1 -> ip1
I0428 19:40:44.198772 24619 net.cpp:124] Setting up ip1
I0428 19:40:44.198788 24619 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:40:44.198792 24619 net.cpp:139] Memory required for data: 684800
I0428 19:40:44.198802 24619 layer_factory.hpp:77] Creating layer relu1
I0428 19:40:44.198809 24619 net.cpp:86] Creating Layer relu1
I0428 19:40:44.198813 24619 net.cpp:408] relu1 <- ip1
I0428 19:40:44.198820 24619 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:40:44.199026 24619 net.cpp:124] Setting up relu1
I0428 19:40:44.199038 24619 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:40:44.199041 24619 net.cpp:139] Memory required for data: 697600
I0428 19:40:44.199045 24619 layer_factory.hpp:77] Creating layer ip2
I0428 19:40:44.199054 24619 net.cpp:86] Creating Layer ip2
I0428 19:40:44.199057 24619 net.cpp:408] ip2 <- ip1
I0428 19:40:44.199064 24619 net.cpp:382] ip2 -> ip2
I0428 19:40:44.199183 24619 net.cpp:124] Setting up ip2
I0428 19:40:44.199192 24619 net.cpp:131] Top shape: 64 10 (640)
I0428 19:40:44.199195 24619 net.cpp:139] Memory required for data: 700160
I0428 19:40:44.199203 24619 layer_factory.hpp:77] Creating layer relu2
I0428 19:40:44.199208 24619 net.cpp:86] Creating Layer relu2
I0428 19:40:44.199213 24619 net.cpp:408] relu2 <- ip2
I0428 19:40:44.199218 24619 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:40:44.200083 24619 net.cpp:124] Setting up relu2
I0428 19:40:44.200098 24619 net.cpp:131] Top shape: 64 10 (640)
I0428 19:40:44.200103 24619 net.cpp:139] Memory required for data: 702720
I0428 19:40:44.200108 24619 layer_factory.hpp:77] Creating layer ip3
I0428 19:40:44.200115 24619 net.cpp:86] Creating Layer ip3
I0428 19:40:44.200119 24619 net.cpp:408] ip3 <- ip2
I0428 19:40:44.200126 24619 net.cpp:382] ip3 -> ip3
I0428 19:40:44.200245 24619 net.cpp:124] Setting up ip3
I0428 19:40:44.200254 24619 net.cpp:131] Top shape: 64 10 (640)
I0428 19:40:44.200258 24619 net.cpp:139] Memory required for data: 705280
I0428 19:40:44.200268 24619 layer_factory.hpp:77] Creating layer relu3
I0428 19:40:44.200278 24619 net.cpp:86] Creating Layer relu3
I0428 19:40:44.200281 24619 net.cpp:408] relu3 <- ip3
I0428 19:40:44.200286 24619 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:40:44.200486 24619 net.cpp:124] Setting up relu3
I0428 19:40:44.200496 24619 net.cpp:131] Top shape: 64 10 (640)
I0428 19:40:44.200500 24619 net.cpp:139] Memory required for data: 707840
I0428 19:40:44.200503 24619 layer_factory.hpp:77] Creating layer loss
I0428 19:40:44.200510 24619 net.cpp:86] Creating Layer loss
I0428 19:40:44.200515 24619 net.cpp:408] loss <- ip3
I0428 19:40:44.200520 24619 net.cpp:408] loss <- label
I0428 19:40:44.200526 24619 net.cpp:382] loss -> loss
I0428 19:40:44.200547 24619 layer_factory.hpp:77] Creating layer loss
I0428 19:40:44.200834 24619 net.cpp:124] Setting up loss
I0428 19:40:44.200845 24619 net.cpp:131] Top shape: (1)
I0428 19:40:44.200850 24619 net.cpp:134]     with loss weight 1
I0428 19:40:44.200866 24619 net.cpp:139] Memory required for data: 707844
I0428 19:40:44.200870 24619 net.cpp:200] loss needs backward computation.
I0428 19:40:44.200875 24619 net.cpp:200] relu3 needs backward computation.
I0428 19:40:44.200878 24619 net.cpp:200] ip3 needs backward computation.
I0428 19:40:44.200881 24619 net.cpp:200] relu2 needs backward computation.
I0428 19:40:44.200886 24619 net.cpp:200] ip2 needs backward computation.
I0428 19:40:44.200888 24619 net.cpp:200] relu1 needs backward computation.
I0428 19:40:44.200891 24619 net.cpp:200] ip1 needs backward computation.
I0428 19:40:44.200896 24619 net.cpp:200] pool1 needs backward computation.
I0428 19:40:44.200898 24619 net.cpp:200] conv1 needs backward computation.
I0428 19:40:44.200902 24619 net.cpp:200] pool0 needs backward computation.
I0428 19:40:44.200906 24619 net.cpp:200] conv0 needs backward computation.
I0428 19:40:44.200911 24619 net.cpp:202] mnist does not need backward computation.
I0428 19:40:44.200913 24619 net.cpp:244] This network produces output loss
I0428 19:40:44.200924 24619 net.cpp:257] Network initialization done.
I0428 19:40:44.201306 24619 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test343.prototxt
I0428 19:40:44.201339 24619 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:40:44.201444 24619 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:40:44.201541 24619 layer_factory.hpp:77] Creating layer mnist
I0428 19:40:44.201592 24619 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:40:44.201607 24619 net.cpp:86] Creating Layer mnist
I0428 19:40:44.201613 24619 net.cpp:382] mnist -> data
I0428 19:40:44.201622 24619 net.cpp:382] mnist -> label
I0428 19:40:44.201722 24619 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:40:44.203930 24619 net.cpp:124] Setting up mnist
I0428 19:40:44.203946 24619 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:40:44.203953 24619 net.cpp:131] Top shape: 100 (100)
I0428 19:40:44.203958 24619 net.cpp:139] Memory required for data: 314000
I0428 19:40:44.203961 24619 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:40:44.203986 24619 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:40:44.203991 24619 net.cpp:408] label_mnist_1_split <- label
I0428 19:40:44.203997 24619 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:40:44.204005 24619 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:40:44.204120 24619 net.cpp:124] Setting up label_mnist_1_split
I0428 19:40:44.204129 24619 net.cpp:131] Top shape: 100 (100)
I0428 19:40:44.204133 24619 net.cpp:131] Top shape: 100 (100)
I0428 19:40:44.204138 24619 net.cpp:139] Memory required for data: 314800
I0428 19:40:44.204141 24619 layer_factory.hpp:77] Creating layer conv0
I0428 19:40:44.204151 24619 net.cpp:86] Creating Layer conv0
I0428 19:40:44.204155 24619 net.cpp:408] conv0 <- data
I0428 19:40:44.204161 24619 net.cpp:382] conv0 -> conv0
I0428 19:40:44.205828 24619 net.cpp:124] Setting up conv0
I0428 19:40:44.205847 24619 net.cpp:131] Top shape: 100 2 24 24 (115200)
I0428 19:40:44.205852 24619 net.cpp:139] Memory required for data: 775600
I0428 19:40:44.205864 24619 layer_factory.hpp:77] Creating layer pool0
I0428 19:40:44.205873 24619 net.cpp:86] Creating Layer pool0
I0428 19:40:44.205876 24619 net.cpp:408] pool0 <- conv0
I0428 19:40:44.205883 24619 net.cpp:382] pool0 -> pool0
I0428 19:40:44.205965 24619 net.cpp:124] Setting up pool0
I0428 19:40:44.205972 24619 net.cpp:131] Top shape: 100 2 12 12 (28800)
I0428 19:40:44.205976 24619 net.cpp:139] Memory required for data: 890800
I0428 19:40:44.205981 24619 layer_factory.hpp:77] Creating layer conv1
I0428 19:40:44.205991 24619 net.cpp:86] Creating Layer conv1
I0428 19:40:44.205994 24619 net.cpp:408] conv1 <- pool0
I0428 19:40:44.206001 24619 net.cpp:382] conv1 -> conv1
I0428 19:40:44.207666 24619 net.cpp:124] Setting up conv1
I0428 19:40:44.207682 24619 net.cpp:131] Top shape: 100 5 8 8 (32000)
I0428 19:40:44.207690 24619 net.cpp:139] Memory required for data: 1018800
I0428 19:40:44.207702 24619 layer_factory.hpp:77] Creating layer pool1
I0428 19:40:44.207710 24619 net.cpp:86] Creating Layer pool1
I0428 19:40:44.207725 24619 net.cpp:408] pool1 <- conv1
I0428 19:40:44.207736 24619 net.cpp:382] pool1 -> pool1
I0428 19:40:44.207779 24619 net.cpp:124] Setting up pool1
I0428 19:40:44.207787 24619 net.cpp:131] Top shape: 100 5 4 4 (8000)
I0428 19:40:44.207792 24619 net.cpp:139] Memory required for data: 1050800
I0428 19:40:44.207794 24619 layer_factory.hpp:77] Creating layer ip1
I0428 19:40:44.207803 24619 net.cpp:86] Creating Layer ip1
I0428 19:40:44.207808 24619 net.cpp:408] ip1 <- pool1
I0428 19:40:44.207813 24619 net.cpp:382] ip1 -> ip1
I0428 19:40:44.207957 24619 net.cpp:124] Setting up ip1
I0428 19:40:44.207967 24619 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:40:44.207984 24619 net.cpp:139] Memory required for data: 1070800
I0428 19:40:44.207993 24619 layer_factory.hpp:77] Creating layer relu1
I0428 19:40:44.207999 24619 net.cpp:86] Creating Layer relu1
I0428 19:40:44.208003 24619 net.cpp:408] relu1 <- ip1
I0428 19:40:44.208009 24619 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:40:44.208235 24619 net.cpp:124] Setting up relu1
I0428 19:40:44.208245 24619 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:40:44.208250 24619 net.cpp:139] Memory required for data: 1090800
I0428 19:40:44.208253 24619 layer_factory.hpp:77] Creating layer ip2
I0428 19:40:44.208261 24619 net.cpp:86] Creating Layer ip2
I0428 19:40:44.208266 24619 net.cpp:408] ip2 <- ip1
I0428 19:40:44.208271 24619 net.cpp:382] ip2 -> ip2
I0428 19:40:44.208415 24619 net.cpp:124] Setting up ip2
I0428 19:40:44.208423 24619 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:40:44.208427 24619 net.cpp:139] Memory required for data: 1094800
I0428 19:40:44.208434 24619 layer_factory.hpp:77] Creating layer relu2
I0428 19:40:44.208446 24619 net.cpp:86] Creating Layer relu2
I0428 19:40:44.208451 24619 net.cpp:408] relu2 <- ip2
I0428 19:40:44.208456 24619 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:40:44.208643 24619 net.cpp:124] Setting up relu2
I0428 19:40:44.208653 24619 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:40:44.208657 24619 net.cpp:139] Memory required for data: 1098800
I0428 19:40:44.208662 24619 layer_factory.hpp:77] Creating layer ip3
I0428 19:40:44.208668 24619 net.cpp:86] Creating Layer ip3
I0428 19:40:44.208673 24619 net.cpp:408] ip3 <- ip2
I0428 19:40:44.208678 24619 net.cpp:382] ip3 -> ip3
I0428 19:40:44.208794 24619 net.cpp:124] Setting up ip3
I0428 19:40:44.208802 24619 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:40:44.208806 24619 net.cpp:139] Memory required for data: 1102800
I0428 19:40:44.208822 24619 layer_factory.hpp:77] Creating layer relu3
I0428 19:40:44.208827 24619 net.cpp:86] Creating Layer relu3
I0428 19:40:44.208832 24619 net.cpp:408] relu3 <- ip3
I0428 19:40:44.208837 24619 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:40:44.209735 24619 net.cpp:124] Setting up relu3
I0428 19:40:44.209750 24619 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:40:44.209754 24619 net.cpp:139] Memory required for data: 1106800
I0428 19:40:44.209758 24619 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:40:44.209765 24619 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:40:44.209769 24619 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:40:44.209775 24619 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:40:44.209784 24619 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:40:44.209870 24619 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:40:44.209878 24619 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:40:44.209883 24619 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:40:44.209887 24619 net.cpp:139] Memory required for data: 1114800
I0428 19:40:44.209892 24619 layer_factory.hpp:77] Creating layer accuracy
I0428 19:40:44.209903 24619 net.cpp:86] Creating Layer accuracy
I0428 19:40:44.209908 24619 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:40:44.209913 24619 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:40:44.209918 24619 net.cpp:382] accuracy -> accuracy
I0428 19:40:44.209928 24619 net.cpp:124] Setting up accuracy
I0428 19:40:44.209939 24619 net.cpp:131] Top shape: (1)
I0428 19:40:44.209941 24619 net.cpp:139] Memory required for data: 1114804
I0428 19:40:44.209945 24619 layer_factory.hpp:77] Creating layer loss
I0428 19:40:44.209957 24619 net.cpp:86] Creating Layer loss
I0428 19:40:44.209960 24619 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:40:44.209965 24619 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:40:44.209971 24619 net.cpp:382] loss -> loss
I0428 19:40:44.209978 24619 layer_factory.hpp:77] Creating layer loss
I0428 19:40:44.210242 24619 net.cpp:124] Setting up loss
I0428 19:40:44.210253 24619 net.cpp:131] Top shape: (1)
I0428 19:40:44.210258 24619 net.cpp:134]     with loss weight 1
I0428 19:40:44.210266 24619 net.cpp:139] Memory required for data: 1114808
I0428 19:40:44.210283 24619 net.cpp:200] loss needs backward computation.
I0428 19:40:44.210288 24619 net.cpp:202] accuracy does not need backward computation.
I0428 19:40:44.210291 24619 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:40:44.210295 24619 net.cpp:200] relu3 needs backward computation.
I0428 19:40:44.210299 24619 net.cpp:200] ip3 needs backward computation.
I0428 19:40:44.210302 24619 net.cpp:200] relu2 needs backward computation.
I0428 19:40:44.210305 24619 net.cpp:200] ip2 needs backward computation.
I0428 19:40:44.210309 24619 net.cpp:200] relu1 needs backward computation.
I0428 19:40:44.210312 24619 net.cpp:200] ip1 needs backward computation.
I0428 19:40:44.210315 24619 net.cpp:200] pool1 needs backward computation.
I0428 19:40:44.210319 24619 net.cpp:200] conv1 needs backward computation.
I0428 19:40:44.210332 24619 net.cpp:200] pool0 needs backward computation.
I0428 19:40:44.210335 24619 net.cpp:200] conv0 needs backward computation.
I0428 19:40:44.210345 24619 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:40:44.210350 24619 net.cpp:202] mnist does not need backward computation.
I0428 19:40:44.210353 24619 net.cpp:244] This network produces output accuracy
I0428 19:40:44.210357 24619 net.cpp:244] This network produces output loss
I0428 19:40:44.210382 24619 net.cpp:257] Network initialization done.
I0428 19:40:44.210434 24619 solver.cpp:56] Solver scaffolding done.
I0428 19:40:44.210819 24619 caffe.cpp:248] Starting Optimization
I0428 19:40:44.210826 24619 solver.cpp:273] Solving LeNet
I0428 19:40:44.210831 24619 solver.cpp:274] Learning Rate Policy: inv
I0428 19:40:44.211740 24619 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:40:44.214658 24619 blocking_queue.cpp:49] Waiting for data
I0428 19:40:44.286378 24626 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:40:44.286870 24619 solver.cpp:398]     Test net output #0: accuracy = 0.124
I0428 19:40:44.286895 24619 solver.cpp:398]     Test net output #1: loss = 2.29967 (* 1 = 2.29967 loss)
I0428 19:40:44.288928 24619 solver.cpp:219] Iteration 0 (-2.07235e-31 iter/s, 0.0780688s/100 iters), loss = 2.29723
I0428 19:40:44.288960 24619 solver.cpp:238]     Train net output #0: loss = 2.29723 (* 1 = 2.29723 loss)
I0428 19:40:44.288975 24619 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:40:44.365185 24619 solver.cpp:219] Iteration 100 (1312.07 iter/s, 0.0762153s/100 iters), loss = 0.946069
I0428 19:40:44.365217 24619 solver.cpp:238]     Train net output #0: loss = 0.946069 (* 1 = 0.946069 loss)
I0428 19:40:44.365226 24619 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:40:44.440667 24619 solver.cpp:219] Iteration 200 (1325.55 iter/s, 0.0754403s/100 iters), loss = 0.555273
I0428 19:40:44.440698 24619 solver.cpp:238]     Train net output #0: loss = 0.555273 (* 1 = 0.555273 loss)
I0428 19:40:44.440706 24619 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:40:44.515592 24619 solver.cpp:219] Iteration 300 (1335.4 iter/s, 0.0748841s/100 iters), loss = 0.368574
I0428 19:40:44.515624 24619 solver.cpp:238]     Train net output #0: loss = 0.368574 (* 1 = 0.368574 loss)
I0428 19:40:44.515630 24619 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:40:44.587177 24619 solver.cpp:219] Iteration 400 (1397.69 iter/s, 0.0715468s/100 iters), loss = 0.336479
I0428 19:40:44.587203 24619 solver.cpp:238]     Train net output #0: loss = 0.336479 (* 1 = 0.336479 loss)
I0428 19:40:44.587209 24619 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:40:44.654948 24619 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:40:44.707260 24626 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:40:44.707670 24619 solver.cpp:398]     Test net output #0: accuracy = 0.9145
I0428 19:40:44.707690 24619 solver.cpp:398]     Test net output #1: loss = 0.269846 (* 1 = 0.269846 loss)
I0428 19:40:44.708477 24619 solver.cpp:219] Iteration 500 (824.653 iter/s, 0.121263s/100 iters), loss = 0.447976
I0428 19:40:44.708499 24619 solver.cpp:238]     Train net output #0: loss = 0.447976 (* 1 = 0.447976 loss)
I0428 19:40:44.708520 24619 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:40:44.781790 24619 solver.cpp:219] Iteration 600 (1364.6 iter/s, 0.0732816s/100 iters), loss = 0.244669
I0428 19:40:44.781813 24619 solver.cpp:238]     Train net output #0: loss = 0.244669 (* 1 = 0.244669 loss)
I0428 19:40:44.781834 24619 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:40:44.851845 24619 solver.cpp:219] Iteration 700 (1428.14 iter/s, 0.0700211s/100 iters), loss = 0.380961
I0428 19:40:44.851868 24619 solver.cpp:238]     Train net output #0: loss = 0.380961 (* 1 = 0.380961 loss)
I0428 19:40:44.851889 24619 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:40:44.919857 24619 solver.cpp:219] Iteration 800 (1471.01 iter/s, 0.0679803s/100 iters), loss = 0.377361
I0428 19:40:44.919883 24619 solver.cpp:238]     Train net output #0: loss = 0.377361 (* 1 = 0.377361 loss)
I0428 19:40:44.919904 24619 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:40:44.988657 24619 solver.cpp:219] Iteration 900 (1454.19 iter/s, 0.0687667s/100 iters), loss = 0.227884
I0428 19:40:44.988682 24619 solver.cpp:238]     Train net output #0: loss = 0.227884 (* 1 = 0.227884 loss)
I0428 19:40:44.988703 24619 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:40:45.011656 24625 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:40:45.056475 24619 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:40:45.057253 24619 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:40:45.057673 24619 solver.cpp:311] Iteration 1000, loss = 0.156123
I0428 19:40:45.057693 24619 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:40:45.131983 24626 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:40:45.132455 24619 solver.cpp:398]     Test net output #0: accuracy = 0.9469
I0428 19:40:45.132472 24619 solver.cpp:398]     Test net output #1: loss = 0.167198 (* 1 = 0.167198 loss)
I0428 19:40:45.132477 24619 solver.cpp:316] Optimization Done.
I0428 19:40:45.132480 24619 caffe.cpp:259] Optimization Done.
