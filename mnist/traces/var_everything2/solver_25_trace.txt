I0428 19:28:25.762753 21624 caffe.cpp:218] Using GPUs 0
I0428 19:28:25.803050 21624 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:28:26.320102 21624 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test25.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:28:26.320276 21624 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test25.prototxt
I0428 19:28:26.320597 21624 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:28:26.320618 21624 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:28:26.320701 21624 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:28:26.320796 21624 layer_factory.hpp:77] Creating layer mnist
I0428 19:28:26.320932 21624 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:28:26.320967 21624 net.cpp:86] Creating Layer mnist
I0428 19:28:26.320979 21624 net.cpp:382] mnist -> data
I0428 19:28:26.321009 21624 net.cpp:382] mnist -> label
I0428 19:28:26.322197 21624 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:28:26.324677 21624 net.cpp:124] Setting up mnist
I0428 19:28:26.324698 21624 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:28:26.324708 21624 net.cpp:131] Top shape: 64 (64)
I0428 19:28:26.324714 21624 net.cpp:139] Memory required for data: 200960
I0428 19:28:26.324725 21624 layer_factory.hpp:77] Creating layer ip1
I0428 19:28:26.324743 21624 net.cpp:86] Creating Layer ip1
I0428 19:28:26.324755 21624 net.cpp:408] ip1 <- data
I0428 19:28:26.324774 21624 net.cpp:382] ip1 -> ip1
I0428 19:28:26.325132 21624 net.cpp:124] Setting up ip1
I0428 19:28:26.325145 21624 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:28:26.325151 21624 net.cpp:139] Memory required for data: 207360
I0428 19:28:26.325172 21624 layer_factory.hpp:77] Creating layer relu1
I0428 19:28:26.325189 21624 net.cpp:86] Creating Layer relu1
I0428 19:28:26.325197 21624 net.cpp:408] relu1 <- ip1
I0428 19:28:26.325206 21624 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:28:26.607604 21624 net.cpp:124] Setting up relu1
I0428 19:28:26.607633 21624 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:28:26.607638 21624 net.cpp:139] Memory required for data: 213760
I0428 19:28:26.607645 21624 layer_factory.hpp:77] Creating layer ip2
I0428 19:28:26.607662 21624 net.cpp:86] Creating Layer ip2
I0428 19:28:26.607686 21624 net.cpp:408] ip2 <- ip1
I0428 19:28:26.607698 21624 net.cpp:382] ip2 -> ip2
I0428 19:28:26.608738 21624 net.cpp:124] Setting up ip2
I0428 19:28:26.608753 21624 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:28:26.608759 21624 net.cpp:139] Memory required for data: 220160
I0428 19:28:26.608775 21624 layer_factory.hpp:77] Creating layer relu2
I0428 19:28:26.608793 21624 net.cpp:86] Creating Layer relu2
I0428 19:28:26.608798 21624 net.cpp:408] relu2 <- ip2
I0428 19:28:26.608808 21624 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:28:26.609650 21624 net.cpp:124] Setting up relu2
I0428 19:28:26.609665 21624 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:28:26.609671 21624 net.cpp:139] Memory required for data: 226560
I0428 19:28:26.609676 21624 layer_factory.hpp:77] Creating layer ip3
I0428 19:28:26.609688 21624 net.cpp:86] Creating Layer ip3
I0428 19:28:26.609694 21624 net.cpp:408] ip3 <- ip2
I0428 19:28:26.609702 21624 net.cpp:382] ip3 -> ip3
I0428 19:28:26.609807 21624 net.cpp:124] Setting up ip3
I0428 19:28:26.609817 21624 net.cpp:131] Top shape: 64 10 (640)
I0428 19:28:26.609822 21624 net.cpp:139] Memory required for data: 229120
I0428 19:28:26.609834 21624 layer_factory.hpp:77] Creating layer relu3
I0428 19:28:26.609845 21624 net.cpp:86] Creating Layer relu3
I0428 19:28:26.609851 21624 net.cpp:408] relu3 <- ip3
I0428 19:28:26.609858 21624 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:28:26.610013 21624 net.cpp:124] Setting up relu3
I0428 19:28:26.610023 21624 net.cpp:131] Top shape: 64 10 (640)
I0428 19:28:26.610028 21624 net.cpp:139] Memory required for data: 231680
I0428 19:28:26.610033 21624 layer_factory.hpp:77] Creating layer loss
I0428 19:28:26.610043 21624 net.cpp:86] Creating Layer loss
I0428 19:28:26.610047 21624 net.cpp:408] loss <- ip3
I0428 19:28:26.610054 21624 net.cpp:408] loss <- label
I0428 19:28:26.610062 21624 net.cpp:382] loss -> loss
I0428 19:28:26.610086 21624 layer_factory.hpp:77] Creating layer loss
I0428 19:28:26.610321 21624 net.cpp:124] Setting up loss
I0428 19:28:26.610332 21624 net.cpp:131] Top shape: (1)
I0428 19:28:26.610337 21624 net.cpp:134]     with loss weight 1
I0428 19:28:26.610357 21624 net.cpp:139] Memory required for data: 231684
I0428 19:28:26.610378 21624 net.cpp:200] loss needs backward computation.
I0428 19:28:26.610385 21624 net.cpp:200] relu3 needs backward computation.
I0428 19:28:26.610390 21624 net.cpp:200] ip3 needs backward computation.
I0428 19:28:26.610395 21624 net.cpp:200] relu2 needs backward computation.
I0428 19:28:26.610400 21624 net.cpp:200] ip2 needs backward computation.
I0428 19:28:26.610405 21624 net.cpp:200] relu1 needs backward computation.
I0428 19:28:26.610411 21624 net.cpp:200] ip1 needs backward computation.
I0428 19:28:26.610417 21624 net.cpp:202] mnist does not need backward computation.
I0428 19:28:26.610422 21624 net.cpp:244] This network produces output loss
I0428 19:28:26.610433 21624 net.cpp:257] Network initialization done.
I0428 19:28:26.610690 21624 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test25.prototxt
I0428 19:28:26.610718 21624 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:28:26.610790 21624 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:28:26.610885 21624 layer_factory.hpp:77] Creating layer mnist
I0428 19:28:26.610944 21624 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:28:26.610961 21624 net.cpp:86] Creating Layer mnist
I0428 19:28:26.610968 21624 net.cpp:382] mnist -> data
I0428 19:28:26.610980 21624 net.cpp:382] mnist -> label
I0428 19:28:26.611098 21624 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:28:26.613046 21624 net.cpp:124] Setting up mnist
I0428 19:28:26.613061 21624 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:28:26.613070 21624 net.cpp:131] Top shape: 100 (100)
I0428 19:28:26.613075 21624 net.cpp:139] Memory required for data: 314000
I0428 19:28:26.613081 21624 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:28:26.613095 21624 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:28:26.613101 21624 net.cpp:408] label_mnist_1_split <- label
I0428 19:28:26.613109 21624 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:28:26.613121 21624 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:28:26.613191 21624 net.cpp:124] Setting up label_mnist_1_split
I0428 19:28:26.613200 21624 net.cpp:131] Top shape: 100 (100)
I0428 19:28:26.613206 21624 net.cpp:131] Top shape: 100 (100)
I0428 19:28:26.613211 21624 net.cpp:139] Memory required for data: 314800
I0428 19:28:26.613216 21624 layer_factory.hpp:77] Creating layer ip1
I0428 19:28:26.613227 21624 net.cpp:86] Creating Layer ip1
I0428 19:28:26.613232 21624 net.cpp:408] ip1 <- data
I0428 19:28:26.613240 21624 net.cpp:382] ip1 -> ip1
I0428 19:28:26.613500 21624 net.cpp:124] Setting up ip1
I0428 19:28:26.613510 21624 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:28:26.613517 21624 net.cpp:139] Memory required for data: 324800
I0428 19:28:26.613530 21624 layer_factory.hpp:77] Creating layer relu1
I0428 19:28:26.613538 21624 net.cpp:86] Creating Layer relu1
I0428 19:28:26.613560 21624 net.cpp:408] relu1 <- ip1
I0428 19:28:26.613569 21624 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:28:26.614478 21624 net.cpp:124] Setting up relu1
I0428 19:28:26.614493 21624 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:28:26.614498 21624 net.cpp:139] Memory required for data: 334800
I0428 19:28:26.614504 21624 layer_factory.hpp:77] Creating layer ip2
I0428 19:28:26.614516 21624 net.cpp:86] Creating Layer ip2
I0428 19:28:26.614521 21624 net.cpp:408] ip2 <- ip1
I0428 19:28:26.614529 21624 net.cpp:382] ip2 -> ip2
I0428 19:28:26.614630 21624 net.cpp:124] Setting up ip2
I0428 19:28:26.614640 21624 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:28:26.614645 21624 net.cpp:139] Memory required for data: 344800
I0428 19:28:26.614657 21624 layer_factory.hpp:77] Creating layer relu2
I0428 19:28:26.614665 21624 net.cpp:86] Creating Layer relu2
I0428 19:28:26.614672 21624 net.cpp:408] relu2 <- ip2
I0428 19:28:26.614681 21624 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:28:26.614838 21624 net.cpp:124] Setting up relu2
I0428 19:28:26.614848 21624 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:28:26.614853 21624 net.cpp:139] Memory required for data: 354800
I0428 19:28:26.614858 21624 layer_factory.hpp:77] Creating layer ip3
I0428 19:28:26.614867 21624 net.cpp:86] Creating Layer ip3
I0428 19:28:26.614872 21624 net.cpp:408] ip3 <- ip2
I0428 19:28:26.614881 21624 net.cpp:382] ip3 -> ip3
I0428 19:28:26.614989 21624 net.cpp:124] Setting up ip3
I0428 19:28:26.614997 21624 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:28:26.615002 21624 net.cpp:139] Memory required for data: 358800
I0428 19:28:26.615015 21624 layer_factory.hpp:77] Creating layer relu3
I0428 19:28:26.615022 21624 net.cpp:86] Creating Layer relu3
I0428 19:28:26.615027 21624 net.cpp:408] relu3 <- ip3
I0428 19:28:26.615034 21624 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:28:26.615186 21624 net.cpp:124] Setting up relu3
I0428 19:28:26.615196 21624 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:28:26.615201 21624 net.cpp:139] Memory required for data: 362800
I0428 19:28:26.615206 21624 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:28:26.615216 21624 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:28:26.615222 21624 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:28:26.615231 21624 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:28:26.615239 21624 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:28:26.615329 21624 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:28:26.615337 21624 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:28:26.615345 21624 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:28:26.615350 21624 net.cpp:139] Memory required for data: 370800
I0428 19:28:26.615355 21624 layer_factory.hpp:77] Creating layer accuracy
I0428 19:28:26.615367 21624 net.cpp:86] Creating Layer accuracy
I0428 19:28:26.615373 21624 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:28:26.615381 21624 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:28:26.615388 21624 net.cpp:382] accuracy -> accuracy
I0428 19:28:26.615399 21624 net.cpp:124] Setting up accuracy
I0428 19:28:26.615407 21624 net.cpp:131] Top shape: (1)
I0428 19:28:26.615412 21624 net.cpp:139] Memory required for data: 370804
I0428 19:28:26.615417 21624 layer_factory.hpp:77] Creating layer loss
I0428 19:28:26.615424 21624 net.cpp:86] Creating Layer loss
I0428 19:28:26.615430 21624 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:28:26.615437 21624 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:28:26.615453 21624 net.cpp:382] loss -> loss
I0428 19:28:26.615463 21624 layer_factory.hpp:77] Creating layer loss
I0428 19:28:26.615700 21624 net.cpp:124] Setting up loss
I0428 19:28:26.615711 21624 net.cpp:131] Top shape: (1)
I0428 19:28:26.615716 21624 net.cpp:134]     with loss weight 1
I0428 19:28:26.615726 21624 net.cpp:139] Memory required for data: 370808
I0428 19:28:26.615732 21624 net.cpp:200] loss needs backward computation.
I0428 19:28:26.615739 21624 net.cpp:202] accuracy does not need backward computation.
I0428 19:28:26.615746 21624 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:28:26.615751 21624 net.cpp:200] relu3 needs backward computation.
I0428 19:28:26.615756 21624 net.cpp:200] ip3 needs backward computation.
I0428 19:28:26.615762 21624 net.cpp:200] relu2 needs backward computation.
I0428 19:28:26.615767 21624 net.cpp:200] ip2 needs backward computation.
I0428 19:28:26.615772 21624 net.cpp:200] relu1 needs backward computation.
I0428 19:28:26.615792 21624 net.cpp:200] ip1 needs backward computation.
I0428 19:28:26.615798 21624 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:28:26.615805 21624 net.cpp:202] mnist does not need backward computation.
I0428 19:28:26.615810 21624 net.cpp:244] This network produces output accuracy
I0428 19:28:26.615816 21624 net.cpp:244] This network produces output loss
I0428 19:28:26.615845 21624 net.cpp:257] Network initialization done.
I0428 19:28:26.615898 21624 solver.cpp:56] Solver scaffolding done.
I0428 19:28:26.616137 21624 caffe.cpp:248] Starting Optimization
I0428 19:28:26.616144 21624 solver.cpp:273] Solving LeNet
I0428 19:28:26.616149 21624 solver.cpp:274] Learning Rate Policy: inv
I0428 19:28:26.616901 21624 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:28:26.618376 21624 blocking_queue.cpp:49] Waiting for data
I0428 19:28:26.695436 21631 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:28:26.695888 21624 solver.cpp:398]     Test net output #0: accuracy = 0.0986
I0428 19:28:26.695915 21624 solver.cpp:398]     Test net output #1: loss = 2.30252 (* 1 = 2.30252 loss)
I0428 19:28:26.696614 21624 solver.cpp:219] Iteration 0 (0 iter/s, 0.080422s/100 iters), loss = 2.3068
I0428 19:28:26.696646 21624 solver.cpp:238]     Train net output #0: loss = 2.3068 (* 1 = 2.3068 loss)
I0428 19:28:26.696671 21624 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:28:26.756227 21624 solver.cpp:219] Iteration 100 (1678.52 iter/s, 0.0595763s/100 iters), loss = 1.08413
I0428 19:28:26.756255 21624 solver.cpp:238]     Train net output #0: loss = 1.08413 (* 1 = 1.08413 loss)
I0428 19:28:26.756266 21624 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:28:26.805152 21624 solver.cpp:219] Iteration 200 (2045.33 iter/s, 0.0488919s/100 iters), loss = 1.06395
I0428 19:28:26.805181 21624 solver.cpp:238]     Train net output #0: loss = 1.06395 (* 1 = 1.06395 loss)
I0428 19:28:26.805191 21624 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:28:26.848839 21624 solver.cpp:219] Iteration 300 (2290.78 iter/s, 0.0436533s/100 iters), loss = 0.632131
I0428 19:28:26.848865 21624 solver.cpp:238]     Train net output #0: loss = 0.632131 (* 1 = 0.632131 loss)
I0428 19:28:26.848889 21624 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:28:26.892983 21624 solver.cpp:219] Iteration 400 (2266.86 iter/s, 0.0441139s/100 iters), loss = 0.623063
I0428 19:28:26.893010 21624 solver.cpp:238]     Train net output #0: loss = 0.623063 (* 1 = 0.623063 loss)
I0428 19:28:26.893035 21624 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:28:26.936306 21624 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:28:27.014155 21631 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:28:27.014533 21624 solver.cpp:398]     Test net output #0: accuracy = 0.7174
I0428 19:28:27.014555 21624 solver.cpp:398]     Test net output #1: loss = 0.758394 (* 1 = 0.758394 loss)
I0428 19:28:27.014953 21624 solver.cpp:219] Iteration 500 (820.117 iter/s, 0.121934s/100 iters), loss = 0.811209
I0428 19:28:27.015005 21624 solver.cpp:238]     Train net output #0: loss = 0.811209 (* 1 = 0.811209 loss)
I0428 19:28:27.015015 21624 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:28:27.072088 21624 solver.cpp:219] Iteration 600 (1752.08 iter/s, 0.0570749s/100 iters), loss = 0.935177
I0428 19:28:27.072118 21624 solver.cpp:238]     Train net output #0: loss = 0.935177 (* 1 = 0.935177 loss)
I0428 19:28:27.072126 21624 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:28:27.139463 21624 solver.cpp:219] Iteration 700 (1485.2 iter/s, 0.0673311s/100 iters), loss = 0.654299
I0428 19:28:27.139515 21624 solver.cpp:238]     Train net output #0: loss = 0.654299 (* 1 = 0.654299 loss)
I0428 19:28:27.139529 21624 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:28:27.200076 21624 solver.cpp:219] Iteration 800 (1651.38 iter/s, 0.0605553s/100 iters), loss = 0.81759
I0428 19:28:27.200112 21624 solver.cpp:238]     Train net output #0: loss = 0.81759 (* 1 = 0.81759 loss)
I0428 19:28:27.200122 21624 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:28:27.259459 21624 solver.cpp:219] Iteration 900 (1685.41 iter/s, 0.0593326s/100 iters), loss = 1.08916
I0428 19:28:27.259516 21624 solver.cpp:238]     Train net output #0: loss = 1.08916 (* 1 = 1.08916 loss)
I0428 19:28:27.259529 21624 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:28:27.281432 21630 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:28:27.319911 21624 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:28:27.320884 21624 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:28:27.321547 21624 solver.cpp:311] Iteration 1000, loss = 0.683334
I0428 19:28:27.321573 21624 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:28:27.377007 21631 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:28:27.377454 21624 solver.cpp:398]     Test net output #0: accuracy = 0.7228
I0428 19:28:27.377477 21624 solver.cpp:398]     Test net output #1: loss = 0.726007 (* 1 = 0.726007 loss)
I0428 19:28:27.377483 21624 solver.cpp:316] Optimization Done.
I0428 19:28:27.377487 21624 caffe.cpp:259] Optimization Done.
