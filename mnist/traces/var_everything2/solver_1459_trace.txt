I0428 20:25:43.376019  2655 caffe.cpp:218] Using GPUs 0
I0428 20:25:43.405840  2655 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:25:43.852221  2655 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1459.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:25:43.852751  2655 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1459.prototxt
I0428 20:25:43.853162  2655 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:25:43.853178  2655 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:25:43.853260  2655 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:25:43.853376  2655 layer_factory.hpp:77] Creating layer mnist
I0428 20:25:43.853469  2655 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:25:43.853489  2655 net.cpp:86] Creating Layer mnist
I0428 20:25:43.853497  2655 net.cpp:382] mnist -> data
I0428 20:25:43.853516  2655 net.cpp:382] mnist -> label
I0428 20:25:43.854511  2655 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:25:43.856868  2655 net.cpp:124] Setting up mnist
I0428 20:25:43.856883  2655 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:25:43.856905  2655 net.cpp:131] Top shape: 64 (64)
I0428 20:25:43.856909  2655 net.cpp:139] Memory required for data: 200960
I0428 20:25:43.856914  2655 layer_factory.hpp:77] Creating layer conv0
I0428 20:25:43.856927  2655 net.cpp:86] Creating Layer conv0
I0428 20:25:43.856945  2655 net.cpp:408] conv0 <- data
I0428 20:25:43.856956  2655 net.cpp:382] conv0 -> conv0
I0428 20:25:44.089802  2655 net.cpp:124] Setting up conv0
I0428 20:25:44.089843  2655 net.cpp:131] Top shape: 64 100 24 24 (3686400)
I0428 20:25:44.089848  2655 net.cpp:139] Memory required for data: 14946560
I0428 20:25:44.089862  2655 layer_factory.hpp:77] Creating layer pool0
I0428 20:25:44.089877  2655 net.cpp:86] Creating Layer pool0
I0428 20:25:44.089880  2655 net.cpp:408] pool0 <- conv0
I0428 20:25:44.089885  2655 net.cpp:382] pool0 -> pool0
I0428 20:25:44.089934  2655 net.cpp:124] Setting up pool0
I0428 20:25:44.089939  2655 net.cpp:131] Top shape: 64 100 12 12 (921600)
I0428 20:25:44.089942  2655 net.cpp:139] Memory required for data: 18632960
I0428 20:25:44.089946  2655 layer_factory.hpp:77] Creating layer conv1
I0428 20:25:44.089957  2655 net.cpp:86] Creating Layer conv1
I0428 20:25:44.089960  2655 net.cpp:408] conv1 <- pool0
I0428 20:25:44.089967  2655 net.cpp:382] conv1 -> conv1
I0428 20:25:44.092658  2655 net.cpp:124] Setting up conv1
I0428 20:25:44.092674  2655 net.cpp:131] Top shape: 64 2 8 8 (8192)
I0428 20:25:44.092677  2655 net.cpp:139] Memory required for data: 18665728
I0428 20:25:44.092685  2655 layer_factory.hpp:77] Creating layer pool1
I0428 20:25:44.092694  2655 net.cpp:86] Creating Layer pool1
I0428 20:25:44.092696  2655 net.cpp:408] pool1 <- conv1
I0428 20:25:44.092706  2655 net.cpp:382] pool1 -> pool1
I0428 20:25:44.092742  2655 net.cpp:124] Setting up pool1
I0428 20:25:44.092759  2655 net.cpp:131] Top shape: 64 2 4 4 (2048)
I0428 20:25:44.092761  2655 net.cpp:139] Memory required for data: 18673920
I0428 20:25:44.092764  2655 layer_factory.hpp:77] Creating layer ip1
I0428 20:25:44.092772  2655 net.cpp:86] Creating Layer ip1
I0428 20:25:44.092774  2655 net.cpp:408] ip1 <- pool1
I0428 20:25:44.092779  2655 net.cpp:382] ip1 -> ip1
I0428 20:25:44.092921  2655 net.cpp:124] Setting up ip1
I0428 20:25:44.092929  2655 net.cpp:131] Top shape: 64 10 (640)
I0428 20:25:44.092932  2655 net.cpp:139] Memory required for data: 18676480
I0428 20:25:44.092939  2655 layer_factory.hpp:77] Creating layer relu1
I0428 20:25:44.092947  2655 net.cpp:86] Creating Layer relu1
I0428 20:25:44.092950  2655 net.cpp:408] relu1 <- ip1
I0428 20:25:44.092957  2655 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:25:44.093128  2655 net.cpp:124] Setting up relu1
I0428 20:25:44.093152  2655 net.cpp:131] Top shape: 64 10 (640)
I0428 20:25:44.093154  2655 net.cpp:139] Memory required for data: 18679040
I0428 20:25:44.093158  2655 layer_factory.hpp:77] Creating layer ip2
I0428 20:25:44.093178  2655 net.cpp:86] Creating Layer ip2
I0428 20:25:44.093181  2655 net.cpp:408] ip2 <- ip1
I0428 20:25:44.093188  2655 net.cpp:382] ip2 -> ip2
I0428 20:25:44.093281  2655 net.cpp:124] Setting up ip2
I0428 20:25:44.093288  2655 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:25:44.093291  2655 net.cpp:139] Memory required for data: 18685440
I0428 20:25:44.093297  2655 layer_factory.hpp:77] Creating layer relu2
I0428 20:25:44.093302  2655 net.cpp:86] Creating Layer relu2
I0428 20:25:44.093304  2655 net.cpp:408] relu2 <- ip2
I0428 20:25:44.093309  2655 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:25:44.093998  2655 net.cpp:124] Setting up relu2
I0428 20:25:44.094009  2655 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:25:44.094028  2655 net.cpp:139] Memory required for data: 18691840
I0428 20:25:44.094032  2655 layer_factory.hpp:77] Creating layer ip3
I0428 20:25:44.094038  2655 net.cpp:86] Creating Layer ip3
I0428 20:25:44.094058  2655 net.cpp:408] ip3 <- ip2
I0428 20:25:44.094064  2655 net.cpp:382] ip3 -> ip3
I0428 20:25:44.094197  2655 net.cpp:124] Setting up ip3
I0428 20:25:44.094205  2655 net.cpp:131] Top shape: 64 10 (640)
I0428 20:25:44.094208  2655 net.cpp:139] Memory required for data: 18694400
I0428 20:25:44.094215  2655 layer_factory.hpp:77] Creating layer relu3
I0428 20:25:44.094221  2655 net.cpp:86] Creating Layer relu3
I0428 20:25:44.094225  2655 net.cpp:408] relu3 <- ip3
I0428 20:25:44.094229  2655 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:25:44.094415  2655 net.cpp:124] Setting up relu3
I0428 20:25:44.094425  2655 net.cpp:131] Top shape: 64 10 (640)
I0428 20:25:44.094429  2655 net.cpp:139] Memory required for data: 18696960
I0428 20:25:44.094431  2655 layer_factory.hpp:77] Creating layer loss
I0428 20:25:44.094436  2655 net.cpp:86] Creating Layer loss
I0428 20:25:44.094439  2655 net.cpp:408] loss <- ip3
I0428 20:25:44.094444  2655 net.cpp:408] loss <- label
I0428 20:25:44.094449  2655 net.cpp:382] loss -> loss
I0428 20:25:44.094480  2655 layer_factory.hpp:77] Creating layer loss
I0428 20:25:44.094703  2655 net.cpp:124] Setting up loss
I0428 20:25:44.094713  2655 net.cpp:131] Top shape: (1)
I0428 20:25:44.094717  2655 net.cpp:134]     with loss weight 1
I0428 20:25:44.094730  2655 net.cpp:139] Memory required for data: 18696964
I0428 20:25:44.094733  2655 net.cpp:200] loss needs backward computation.
I0428 20:25:44.094738  2655 net.cpp:200] relu3 needs backward computation.
I0428 20:25:44.094739  2655 net.cpp:200] ip3 needs backward computation.
I0428 20:25:44.094743  2655 net.cpp:200] relu2 needs backward computation.
I0428 20:25:44.094745  2655 net.cpp:200] ip2 needs backward computation.
I0428 20:25:44.094748  2655 net.cpp:200] relu1 needs backward computation.
I0428 20:25:44.094750  2655 net.cpp:200] ip1 needs backward computation.
I0428 20:25:44.094753  2655 net.cpp:200] pool1 needs backward computation.
I0428 20:25:44.094756  2655 net.cpp:200] conv1 needs backward computation.
I0428 20:25:44.094758  2655 net.cpp:200] pool0 needs backward computation.
I0428 20:25:44.094761  2655 net.cpp:200] conv0 needs backward computation.
I0428 20:25:44.094765  2655 net.cpp:202] mnist does not need backward computation.
I0428 20:25:44.094768  2655 net.cpp:244] This network produces output loss
I0428 20:25:44.094776  2655 net.cpp:257] Network initialization done.
I0428 20:25:44.095165  2655 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1459.prototxt
I0428 20:25:44.095206  2655 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:25:44.095295  2655 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:25:44.095376  2655 layer_factory.hpp:77] Creating layer mnist
I0428 20:25:44.095440  2655 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:25:44.095453  2655 net.cpp:86] Creating Layer mnist
I0428 20:25:44.095459  2655 net.cpp:382] mnist -> data
I0428 20:25:44.095466  2655 net.cpp:382] mnist -> label
I0428 20:25:44.095554  2655 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:25:44.097580  2655 net.cpp:124] Setting up mnist
I0428 20:25:44.097594  2655 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:25:44.097599  2655 net.cpp:131] Top shape: 100 (100)
I0428 20:25:44.097600  2655 net.cpp:139] Memory required for data: 314000
I0428 20:25:44.097604  2655 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:25:44.097622  2655 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:25:44.097626  2655 net.cpp:408] label_mnist_1_split <- label
I0428 20:25:44.097633  2655 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:25:44.097640  2655 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:25:44.097723  2655 net.cpp:124] Setting up label_mnist_1_split
I0428 20:25:44.097733  2655 net.cpp:131] Top shape: 100 (100)
I0428 20:25:44.097736  2655 net.cpp:131] Top shape: 100 (100)
I0428 20:25:44.097739  2655 net.cpp:139] Memory required for data: 314800
I0428 20:25:44.097743  2655 layer_factory.hpp:77] Creating layer conv0
I0428 20:25:44.097751  2655 net.cpp:86] Creating Layer conv0
I0428 20:25:44.097754  2655 net.cpp:408] conv0 <- data
I0428 20:25:44.097759  2655 net.cpp:382] conv0 -> conv0
I0428 20:25:44.099467  2655 net.cpp:124] Setting up conv0
I0428 20:25:44.099479  2655 net.cpp:131] Top shape: 100 100 24 24 (5760000)
I0428 20:25:44.099483  2655 net.cpp:139] Memory required for data: 23354800
I0428 20:25:44.099491  2655 layer_factory.hpp:77] Creating layer pool0
I0428 20:25:44.099498  2655 net.cpp:86] Creating Layer pool0
I0428 20:25:44.099500  2655 net.cpp:408] pool0 <- conv0
I0428 20:25:44.099506  2655 net.cpp:382] pool0 -> pool0
I0428 20:25:44.099541  2655 net.cpp:124] Setting up pool0
I0428 20:25:44.099546  2655 net.cpp:131] Top shape: 100 100 12 12 (1440000)
I0428 20:25:44.099548  2655 net.cpp:139] Memory required for data: 29114800
I0428 20:25:44.099551  2655 layer_factory.hpp:77] Creating layer conv1
I0428 20:25:44.099561  2655 net.cpp:86] Creating Layer conv1
I0428 20:25:44.099565  2655 net.cpp:408] conv1 <- pool0
I0428 20:25:44.099568  2655 net.cpp:382] conv1 -> conv1
I0428 20:25:44.101308  2655 net.cpp:124] Setting up conv1
I0428 20:25:44.101337  2655 net.cpp:131] Top shape: 100 2 8 8 (12800)
I0428 20:25:44.101341  2655 net.cpp:139] Memory required for data: 29166000
I0428 20:25:44.101351  2655 layer_factory.hpp:77] Creating layer pool1
I0428 20:25:44.101356  2655 net.cpp:86] Creating Layer pool1
I0428 20:25:44.101361  2655 net.cpp:408] pool1 <- conv1
I0428 20:25:44.101379  2655 net.cpp:382] pool1 -> pool1
I0428 20:25:44.101431  2655 net.cpp:124] Setting up pool1
I0428 20:25:44.101438  2655 net.cpp:131] Top shape: 100 2 4 4 (3200)
I0428 20:25:44.101439  2655 net.cpp:139] Memory required for data: 29178800
I0428 20:25:44.101442  2655 layer_factory.hpp:77] Creating layer ip1
I0428 20:25:44.101449  2655 net.cpp:86] Creating Layer ip1
I0428 20:25:44.101452  2655 net.cpp:408] ip1 <- pool1
I0428 20:25:44.101457  2655 net.cpp:382] ip1 -> ip1
I0428 20:25:44.101557  2655 net.cpp:124] Setting up ip1
I0428 20:25:44.101564  2655 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:25:44.101577  2655 net.cpp:139] Memory required for data: 29182800
I0428 20:25:44.101584  2655 layer_factory.hpp:77] Creating layer relu1
I0428 20:25:44.101590  2655 net.cpp:86] Creating Layer relu1
I0428 20:25:44.101594  2655 net.cpp:408] relu1 <- ip1
I0428 20:25:44.101598  2655 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:25:44.101771  2655 net.cpp:124] Setting up relu1
I0428 20:25:44.101780  2655 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:25:44.101783  2655 net.cpp:139] Memory required for data: 29186800
I0428 20:25:44.101788  2655 layer_factory.hpp:77] Creating layer ip2
I0428 20:25:44.101794  2655 net.cpp:86] Creating Layer ip2
I0428 20:25:44.101797  2655 net.cpp:408] ip2 <- ip1
I0428 20:25:44.101809  2655 net.cpp:382] ip2 -> ip2
I0428 20:25:44.101908  2655 net.cpp:124] Setting up ip2
I0428 20:25:44.101915  2655 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:25:44.101918  2655 net.cpp:139] Memory required for data: 29196800
I0428 20:25:44.101924  2655 layer_factory.hpp:77] Creating layer relu2
I0428 20:25:44.101929  2655 net.cpp:86] Creating Layer relu2
I0428 20:25:44.101933  2655 net.cpp:408] relu2 <- ip2
I0428 20:25:44.101938  2655 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:25:44.102144  2655 net.cpp:124] Setting up relu2
I0428 20:25:44.102167  2655 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:25:44.102171  2655 net.cpp:139] Memory required for data: 29206800
I0428 20:25:44.102174  2655 layer_factory.hpp:77] Creating layer ip3
I0428 20:25:44.102180  2655 net.cpp:86] Creating Layer ip3
I0428 20:25:44.102183  2655 net.cpp:408] ip3 <- ip2
I0428 20:25:44.102190  2655 net.cpp:382] ip3 -> ip3
I0428 20:25:44.102367  2655 net.cpp:124] Setting up ip3
I0428 20:25:44.102391  2655 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:25:44.102396  2655 net.cpp:139] Memory required for data: 29210800
I0428 20:25:44.102403  2655 layer_factory.hpp:77] Creating layer relu3
I0428 20:25:44.102407  2655 net.cpp:86] Creating Layer relu3
I0428 20:25:44.102411  2655 net.cpp:408] relu3 <- ip3
I0428 20:25:44.102416  2655 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:25:44.103202  2655 net.cpp:124] Setting up relu3
I0428 20:25:44.103214  2655 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:25:44.103216  2655 net.cpp:139] Memory required for data: 29214800
I0428 20:25:44.103219  2655 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:25:44.103225  2655 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:25:44.103229  2655 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:25:44.103235  2655 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:25:44.103256  2655 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:25:44.103302  2655 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:25:44.103307  2655 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:25:44.103310  2655 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:25:44.103313  2655 net.cpp:139] Memory required for data: 29222800
I0428 20:25:44.103317  2655 layer_factory.hpp:77] Creating layer accuracy
I0428 20:25:44.103322  2655 net.cpp:86] Creating Layer accuracy
I0428 20:25:44.103325  2655 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:25:44.103329  2655 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:25:44.103333  2655 net.cpp:382] accuracy -> accuracy
I0428 20:25:44.103340  2655 net.cpp:124] Setting up accuracy
I0428 20:25:44.103344  2655 net.cpp:131] Top shape: (1)
I0428 20:25:44.103346  2655 net.cpp:139] Memory required for data: 29222804
I0428 20:25:44.103349  2655 layer_factory.hpp:77] Creating layer loss
I0428 20:25:44.103355  2655 net.cpp:86] Creating Layer loss
I0428 20:25:44.103358  2655 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:25:44.103361  2655 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:25:44.103365  2655 net.cpp:382] loss -> loss
I0428 20:25:44.103371  2655 layer_factory.hpp:77] Creating layer loss
I0428 20:25:44.103618  2655 net.cpp:124] Setting up loss
I0428 20:25:44.103627  2655 net.cpp:131] Top shape: (1)
I0428 20:25:44.103631  2655 net.cpp:134]     with loss weight 1
I0428 20:25:44.103644  2655 net.cpp:139] Memory required for data: 29222808
I0428 20:25:44.103648  2655 net.cpp:200] loss needs backward computation.
I0428 20:25:44.103652  2655 net.cpp:202] accuracy does not need backward computation.
I0428 20:25:44.103655  2655 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:25:44.103659  2655 net.cpp:200] relu3 needs backward computation.
I0428 20:25:44.103662  2655 net.cpp:200] ip3 needs backward computation.
I0428 20:25:44.103664  2655 net.cpp:200] relu2 needs backward computation.
I0428 20:25:44.103674  2655 net.cpp:200] ip2 needs backward computation.
I0428 20:25:44.103677  2655 net.cpp:200] relu1 needs backward computation.
I0428 20:25:44.103679  2655 net.cpp:200] ip1 needs backward computation.
I0428 20:25:44.103698  2655 net.cpp:200] pool1 needs backward computation.
I0428 20:25:44.103701  2655 net.cpp:200] conv1 needs backward computation.
I0428 20:25:44.103704  2655 net.cpp:200] pool0 needs backward computation.
I0428 20:25:44.103708  2655 net.cpp:200] conv0 needs backward computation.
I0428 20:25:44.103711  2655 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:25:44.103714  2655 net.cpp:202] mnist does not need backward computation.
I0428 20:25:44.103718  2655 net.cpp:244] This network produces output accuracy
I0428 20:25:44.103720  2655 net.cpp:244] This network produces output loss
I0428 20:25:44.103732  2655 net.cpp:257] Network initialization done.
I0428 20:25:44.103793  2655 solver.cpp:56] Solver scaffolding done.
I0428 20:25:44.104147  2655 caffe.cpp:248] Starting Optimization
I0428 20:25:44.104153  2655 solver.cpp:273] Solving LeNet
I0428 20:25:44.104156  2655 solver.cpp:274] Learning Rate Policy: inv
I0428 20:25:44.104992  2655 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:25:44.199568  2667 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:25:44.201941  2655 solver.cpp:398]     Test net output #0: accuracy = 0.1681
I0428 20:25:44.201974  2655 solver.cpp:398]     Test net output #1: loss = 2.38426 (* 1 = 2.38426 loss)
I0428 20:25:44.206320  2655 solver.cpp:219] Iteration 0 (0 iter/s, 0.102113s/100 iters), loss = 2.41491
I0428 20:25:44.206393  2655 solver.cpp:238]     Train net output #0: loss = 2.41491 (* 1 = 2.41491 loss)
I0428 20:25:44.206403  2655 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:25:44.407409  2655 solver.cpp:219] Iteration 100 (497.499 iter/s, 0.201005s/100 iters), loss = 0.72878
I0428 20:25:44.407460  2655 solver.cpp:238]     Train net output #0: loss = 0.72878 (* 1 = 0.72878 loss)
I0428 20:25:44.407474  2655 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:25:44.617050  2655 solver.cpp:219] Iteration 200 (477.154 iter/s, 0.209576s/100 iters), loss = 0.626721
I0428 20:25:44.617086  2655 solver.cpp:238]     Train net output #0: loss = 0.626721 (* 1 = 0.626721 loss)
I0428 20:25:44.617094  2655 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:25:44.816450  2655 solver.cpp:219] Iteration 300 (501.627 iter/s, 0.199351s/100 iters), loss = 0.404595
I0428 20:25:44.816493  2655 solver.cpp:238]     Train net output #0: loss = 0.404595 (* 1 = 0.404595 loss)
I0428 20:25:44.816499  2655 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:25:45.020898  2655 solver.cpp:219] Iteration 400 (489.262 iter/s, 0.204389s/100 iters), loss = 0.309336
I0428 20:25:45.020931  2655 solver.cpp:238]     Train net output #0: loss = 0.309336 (* 1 = 0.309336 loss)
I0428 20:25:45.020938  2655 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:25:45.221966  2655 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:25:45.318068  2667 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:25:45.320431  2655 solver.cpp:398]     Test net output #0: accuracy = 0.905
I0428 20:25:45.320453  2655 solver.cpp:398]     Test net output #1: loss = 0.313605 (* 1 = 0.313605 loss)
I0428 20:25:45.322337  2655 solver.cpp:219] Iteration 500 (331.797 iter/s, 0.301389s/100 iters), loss = 0.32646
I0428 20:25:45.322360  2655 solver.cpp:238]     Train net output #0: loss = 0.32646 (* 1 = 0.32646 loss)
I0428 20:25:45.322381  2655 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:25:45.532374  2655 solver.cpp:219] Iteration 600 (476.207 iter/s, 0.209993s/100 iters), loss = 0.127078
I0428 20:25:45.532419  2655 solver.cpp:238]     Train net output #0: loss = 0.127078 (* 1 = 0.127078 loss)
I0428 20:25:45.532430  2655 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:25:45.744879  2655 solver.cpp:219] Iteration 700 (470.726 iter/s, 0.212438s/100 iters), loss = 0.317107
I0428 20:25:45.744932  2655 solver.cpp:238]     Train net output #0: loss = 0.317107 (* 1 = 0.317107 loss)
I0428 20:25:45.744946  2655 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:25:45.960544  2655 solver.cpp:219] Iteration 800 (463.828 iter/s, 0.215597s/100 iters), loss = 0.295174
I0428 20:25:45.960588  2655 solver.cpp:238]     Train net output #0: loss = 0.295174 (* 1 = 0.295174 loss)
I0428 20:25:45.960599  2655 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:25:46.174440  2655 solver.cpp:219] Iteration 900 (467.648 iter/s, 0.213836s/100 iters), loss = 0.387538
I0428 20:25:46.174480  2655 solver.cpp:238]     Train net output #0: loss = 0.387538 (* 1 = 0.387538 loss)
I0428 20:25:46.174489  2655 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:25:46.245399  2666 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:25:46.391494  2655 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:25:46.393173  2655 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:25:46.394258  2655 solver.cpp:311] Iteration 1000, loss = 0.136297
I0428 20:25:46.394279  2655 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:25:46.494280  2667 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:25:46.496754  2655 solver.cpp:398]     Test net output #0: accuracy = 0.9448
I0428 20:25:46.496779  2655 solver.cpp:398]     Test net output #1: loss = 0.184068 (* 1 = 0.184068 loss)
I0428 20:25:46.496788  2655 solver.cpp:316] Optimization Done.
I0428 20:25:46.496791  2655 caffe.cpp:259] Optimization Done.
