I0428 19:28:18.449944 21591 caffe.cpp:218] Using GPUs 0
I0428 19:28:18.488828 21591 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:28:19.007678 21591 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test21.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:28:19.007845 21591 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test21.prototxt
I0428 19:28:19.008160 21591 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:28:19.008179 21591 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:28:19.008263 21591 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:28:19.008361 21591 layer_factory.hpp:77] Creating layer mnist
I0428 19:28:19.008493 21591 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:28:19.008527 21591 net.cpp:86] Creating Layer mnist
I0428 19:28:19.008541 21591 net.cpp:382] mnist -> data
I0428 19:28:19.008572 21591 net.cpp:382] mnist -> label
I0428 19:28:19.009771 21591 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:28:19.012240 21591 net.cpp:124] Setting up mnist
I0428 19:28:19.012262 21591 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:28:19.012272 21591 net.cpp:131] Top shape: 64 (64)
I0428 19:28:19.012279 21591 net.cpp:139] Memory required for data: 200960
I0428 19:28:19.012290 21591 layer_factory.hpp:77] Creating layer ip1
I0428 19:28:19.012308 21591 net.cpp:86] Creating Layer ip1
I0428 19:28:19.012322 21591 net.cpp:408] ip1 <- data
I0428 19:28:19.012339 21591 net.cpp:382] ip1 -> ip1
I0428 19:28:19.012688 21591 net.cpp:124] Setting up ip1
I0428 19:28:19.012701 21591 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:28:19.012708 21591 net.cpp:139] Memory required for data: 207360
I0428 19:28:19.012728 21591 layer_factory.hpp:77] Creating layer relu1
I0428 19:28:19.012742 21591 net.cpp:86] Creating Layer relu1
I0428 19:28:19.012754 21591 net.cpp:408] relu1 <- ip1
I0428 19:28:19.012766 21591 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:28:19.289250 21591 net.cpp:124] Setting up relu1
I0428 19:28:19.289279 21591 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:28:19.289284 21591 net.cpp:139] Memory required for data: 213760
I0428 19:28:19.289291 21591 layer_factory.hpp:77] Creating layer ip2
I0428 19:28:19.289306 21591 net.cpp:86] Creating Layer ip2
I0428 19:28:19.289330 21591 net.cpp:408] ip2 <- ip1
I0428 19:28:19.289342 21591 net.cpp:382] ip2 -> ip2
I0428 19:28:19.289471 21591 net.cpp:124] Setting up ip2
I0428 19:28:19.289480 21591 net.cpp:131] Top shape: 64 10 (640)
I0428 19:28:19.289485 21591 net.cpp:139] Memory required for data: 216320
I0428 19:28:19.289500 21591 layer_factory.hpp:77] Creating layer relu2
I0428 19:28:19.289512 21591 net.cpp:86] Creating Layer relu2
I0428 19:28:19.289520 21591 net.cpp:408] relu2 <- ip2
I0428 19:28:19.289526 21591 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:28:19.290290 21591 net.cpp:124] Setting up relu2
I0428 19:28:19.290304 21591 net.cpp:131] Top shape: 64 10 (640)
I0428 19:28:19.290309 21591 net.cpp:139] Memory required for data: 218880
I0428 19:28:19.290315 21591 layer_factory.hpp:77] Creating layer ip3
I0428 19:28:19.290325 21591 net.cpp:86] Creating Layer ip3
I0428 19:28:19.290331 21591 net.cpp:408] ip3 <- ip2
I0428 19:28:19.290339 21591 net.cpp:382] ip3 -> ip3
I0428 19:28:19.290455 21591 net.cpp:124] Setting up ip3
I0428 19:28:19.290465 21591 net.cpp:131] Top shape: 64 10 (640)
I0428 19:28:19.290470 21591 net.cpp:139] Memory required for data: 221440
I0428 19:28:19.290483 21591 layer_factory.hpp:77] Creating layer relu3
I0428 19:28:19.290496 21591 net.cpp:86] Creating Layer relu3
I0428 19:28:19.290503 21591 net.cpp:408] relu3 <- ip3
I0428 19:28:19.290510 21591 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:28:19.290663 21591 net.cpp:124] Setting up relu3
I0428 19:28:19.290673 21591 net.cpp:131] Top shape: 64 10 (640)
I0428 19:28:19.290678 21591 net.cpp:139] Memory required for data: 224000
I0428 19:28:19.290683 21591 layer_factory.hpp:77] Creating layer loss
I0428 19:28:19.290712 21591 net.cpp:86] Creating Layer loss
I0428 19:28:19.290719 21591 net.cpp:408] loss <- ip3
I0428 19:28:19.290724 21591 net.cpp:408] loss <- label
I0428 19:28:19.290733 21591 net.cpp:382] loss -> loss
I0428 19:28:19.290751 21591 layer_factory.hpp:77] Creating layer loss
I0428 19:28:19.291868 21591 net.cpp:124] Setting up loss
I0428 19:28:19.291883 21591 net.cpp:131] Top shape: (1)
I0428 19:28:19.291888 21591 net.cpp:134]     with loss weight 1
I0428 19:28:19.291909 21591 net.cpp:139] Memory required for data: 224004
I0428 19:28:19.291916 21591 net.cpp:200] loss needs backward computation.
I0428 19:28:19.291923 21591 net.cpp:200] relu3 needs backward computation.
I0428 19:28:19.291927 21591 net.cpp:200] ip3 needs backward computation.
I0428 19:28:19.291931 21591 net.cpp:200] relu2 needs backward computation.
I0428 19:28:19.291936 21591 net.cpp:200] ip2 needs backward computation.
I0428 19:28:19.291941 21591 net.cpp:200] relu1 needs backward computation.
I0428 19:28:19.291946 21591 net.cpp:200] ip1 needs backward computation.
I0428 19:28:19.291951 21591 net.cpp:202] mnist does not need backward computation.
I0428 19:28:19.291956 21591 net.cpp:244] This network produces output loss
I0428 19:28:19.291968 21591 net.cpp:257] Network initialization done.
I0428 19:28:19.292199 21591 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test21.prototxt
I0428 19:28:19.292227 21591 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:28:19.292299 21591 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:28:19.292389 21591 layer_factory.hpp:77] Creating layer mnist
I0428 19:28:19.292448 21591 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:28:19.292465 21591 net.cpp:86] Creating Layer mnist
I0428 19:28:19.292474 21591 net.cpp:382] mnist -> data
I0428 19:28:19.292500 21591 net.cpp:382] mnist -> label
I0428 19:28:19.292623 21591 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:28:19.294776 21591 net.cpp:124] Setting up mnist
I0428 19:28:19.294809 21591 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:28:19.294818 21591 net.cpp:131] Top shape: 100 (100)
I0428 19:28:19.294839 21591 net.cpp:139] Memory required for data: 314000
I0428 19:28:19.294845 21591 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:28:19.294860 21591 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:28:19.294868 21591 net.cpp:408] label_mnist_1_split <- label
I0428 19:28:19.294878 21591 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:28:19.294888 21591 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:28:19.294980 21591 net.cpp:124] Setting up label_mnist_1_split
I0428 19:28:19.294988 21591 net.cpp:131] Top shape: 100 (100)
I0428 19:28:19.294996 21591 net.cpp:131] Top shape: 100 (100)
I0428 19:28:19.295001 21591 net.cpp:139] Memory required for data: 314800
I0428 19:28:19.295007 21591 layer_factory.hpp:77] Creating layer ip1
I0428 19:28:19.295017 21591 net.cpp:86] Creating Layer ip1
I0428 19:28:19.295023 21591 net.cpp:408] ip1 <- data
I0428 19:28:19.295032 21591 net.cpp:382] ip1 -> ip1
I0428 19:28:19.295248 21591 net.cpp:124] Setting up ip1
I0428 19:28:19.295258 21591 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:28:19.295264 21591 net.cpp:139] Memory required for data: 324800
I0428 19:28:19.295276 21591 layer_factory.hpp:77] Creating layer relu1
I0428 19:28:19.295284 21591 net.cpp:86] Creating Layer relu1
I0428 19:28:19.295291 21591 net.cpp:408] relu1 <- ip1
I0428 19:28:19.295298 21591 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:28:19.296262 21591 net.cpp:124] Setting up relu1
I0428 19:28:19.296277 21591 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:28:19.296281 21591 net.cpp:139] Memory required for data: 334800
I0428 19:28:19.296288 21591 layer_factory.hpp:77] Creating layer ip2
I0428 19:28:19.296298 21591 net.cpp:86] Creating Layer ip2
I0428 19:28:19.296304 21591 net.cpp:408] ip2 <- ip1
I0428 19:28:19.296314 21591 net.cpp:382] ip2 -> ip2
I0428 19:28:19.296409 21591 net.cpp:124] Setting up ip2
I0428 19:28:19.296418 21591 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:28:19.296423 21591 net.cpp:139] Memory required for data: 338800
I0428 19:28:19.296434 21591 layer_factory.hpp:77] Creating layer relu2
I0428 19:28:19.296443 21591 net.cpp:86] Creating Layer relu2
I0428 19:28:19.296448 21591 net.cpp:408] relu2 <- ip2
I0428 19:28:19.296455 21591 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:28:19.296607 21591 net.cpp:124] Setting up relu2
I0428 19:28:19.296617 21591 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:28:19.296622 21591 net.cpp:139] Memory required for data: 342800
I0428 19:28:19.296627 21591 layer_factory.hpp:77] Creating layer ip3
I0428 19:28:19.296636 21591 net.cpp:86] Creating Layer ip3
I0428 19:28:19.296641 21591 net.cpp:408] ip3 <- ip2
I0428 19:28:19.296649 21591 net.cpp:382] ip3 -> ip3
I0428 19:28:19.296751 21591 net.cpp:124] Setting up ip3
I0428 19:28:19.296761 21591 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:28:19.296766 21591 net.cpp:139] Memory required for data: 346800
I0428 19:28:19.296777 21591 layer_factory.hpp:77] Creating layer relu3
I0428 19:28:19.296784 21591 net.cpp:86] Creating Layer relu3
I0428 19:28:19.296790 21591 net.cpp:408] relu3 <- ip3
I0428 19:28:19.296797 21591 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:28:19.297019 21591 net.cpp:124] Setting up relu3
I0428 19:28:19.297030 21591 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:28:19.297037 21591 net.cpp:139] Memory required for data: 350800
I0428 19:28:19.297044 21591 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:28:19.297053 21591 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:28:19.297060 21591 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:28:19.297067 21591 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:28:19.297077 21591 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:28:19.297163 21591 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:28:19.297186 21591 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:28:19.297194 21591 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:28:19.297199 21591 net.cpp:139] Memory required for data: 358800
I0428 19:28:19.297204 21591 layer_factory.hpp:77] Creating layer accuracy
I0428 19:28:19.297225 21591 net.cpp:86] Creating Layer accuracy
I0428 19:28:19.297230 21591 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:28:19.297236 21591 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:28:19.297243 21591 net.cpp:382] accuracy -> accuracy
I0428 19:28:19.297255 21591 net.cpp:124] Setting up accuracy
I0428 19:28:19.297262 21591 net.cpp:131] Top shape: (1)
I0428 19:28:19.297267 21591 net.cpp:139] Memory required for data: 358804
I0428 19:28:19.297272 21591 layer_factory.hpp:77] Creating layer loss
I0428 19:28:19.297297 21591 net.cpp:86] Creating Layer loss
I0428 19:28:19.297302 21591 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:28:19.297323 21591 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:28:19.297329 21591 net.cpp:382] loss -> loss
I0428 19:28:19.297339 21591 layer_factory.hpp:77] Creating layer loss
I0428 19:28:19.297623 21591 net.cpp:124] Setting up loss
I0428 19:28:19.297634 21591 net.cpp:131] Top shape: (1)
I0428 19:28:19.297641 21591 net.cpp:134]     with loss weight 1
I0428 19:28:19.297660 21591 net.cpp:139] Memory required for data: 358808
I0428 19:28:19.297667 21591 net.cpp:200] loss needs backward computation.
I0428 19:28:19.297673 21591 net.cpp:202] accuracy does not need backward computation.
I0428 19:28:19.297679 21591 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:28:19.297684 21591 net.cpp:200] relu3 needs backward computation.
I0428 19:28:19.297689 21591 net.cpp:200] ip3 needs backward computation.
I0428 19:28:19.297694 21591 net.cpp:200] relu2 needs backward computation.
I0428 19:28:19.297699 21591 net.cpp:200] ip2 needs backward computation.
I0428 19:28:19.297704 21591 net.cpp:200] relu1 needs backward computation.
I0428 19:28:19.297709 21591 net.cpp:200] ip1 needs backward computation.
I0428 19:28:19.297715 21591 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:28:19.297736 21591 net.cpp:202] mnist does not need backward computation.
I0428 19:28:19.297742 21591 net.cpp:244] This network produces output accuracy
I0428 19:28:19.297749 21591 net.cpp:244] This network produces output loss
I0428 19:28:19.297768 21591 net.cpp:257] Network initialization done.
I0428 19:28:19.297804 21591 solver.cpp:56] Solver scaffolding done.
I0428 19:28:19.298008 21591 caffe.cpp:248] Starting Optimization
I0428 19:28:19.298015 21591 solver.cpp:273] Solving LeNet
I0428 19:28:19.298019 21591 solver.cpp:274] Learning Rate Policy: inv
I0428 19:28:19.298137 21591 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:28:19.298311 21591 blocking_queue.cpp:49] Waiting for data
I0428 19:28:19.377812 21598 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:28:19.378314 21591 solver.cpp:398]     Test net output #0: accuracy = 0.0905
I0428 19:28:19.378353 21591 solver.cpp:398]     Test net output #1: loss = 2.31434 (* 1 = 2.31434 loss)
I0428 19:28:19.379179 21591 solver.cpp:219] Iteration 0 (-1.09436e-30 iter/s, 0.0811284s/100 iters), loss = 2.32017
I0428 19:28:19.379222 21591 solver.cpp:238]     Train net output #0: loss = 2.32017 (* 1 = 2.32017 loss)
I0428 19:28:19.379246 21591 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:28:19.433290 21591 solver.cpp:219] Iteration 100 (1850.18 iter/s, 0.0540488s/100 iters), loss = 1.25717
I0428 19:28:19.433326 21591 solver.cpp:238]     Train net output #0: loss = 1.25717 (* 1 = 1.25717 loss)
I0428 19:28:19.433339 21591 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:28:19.481626 21591 solver.cpp:219] Iteration 200 (2070.53 iter/s, 0.0482969s/100 iters), loss = 1.02508
I0428 19:28:19.481655 21591 solver.cpp:238]     Train net output #0: loss = 1.02508 (* 1 = 1.02508 loss)
I0428 19:28:19.481667 21591 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:28:19.524006 21591 solver.cpp:219] Iteration 300 (2361.47 iter/s, 0.0423465s/100 iters), loss = 0.766043
I0428 19:28:19.524032 21591 solver.cpp:238]     Train net output #0: loss = 0.766043 (* 1 = 0.766043 loss)
I0428 19:28:19.524042 21591 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:28:19.570123 21591 solver.cpp:219] Iteration 400 (2169.85 iter/s, 0.0460861s/100 iters), loss = 0.590543
I0428 19:28:19.570150 21591 solver.cpp:238]     Train net output #0: loss = 0.590543 (* 1 = 0.590543 loss)
I0428 19:28:19.570175 21591 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:28:19.626085 21591 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:28:19.704109 21598 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:28:19.704496 21591 solver.cpp:398]     Test net output #0: accuracy = 0.7515
I0428 19:28:19.704516 21591 solver.cpp:398]     Test net output #1: loss = 0.731307 (* 1 = 0.731307 loss)
I0428 19:28:19.705065 21591 solver.cpp:219] Iteration 500 (741.263 iter/s, 0.134905s/100 iters), loss = 0.916237
I0428 19:28:19.705091 21591 solver.cpp:238]     Train net output #0: loss = 0.916237 (* 1 = 0.916237 loss)
I0428 19:28:19.705109 21591 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:28:19.762236 21591 solver.cpp:219] Iteration 600 (1750.11 iter/s, 0.0571392s/100 iters), loss = 0.709602
I0428 19:28:19.762264 21591 solver.cpp:238]     Train net output #0: loss = 0.709602 (* 1 = 0.709602 loss)
I0428 19:28:19.762292 21591 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:28:19.809008 21591 solver.cpp:219] Iteration 700 (2139.52 iter/s, 0.0467395s/100 iters), loss = 0.957909
I0428 19:28:19.809034 21591 solver.cpp:238]     Train net output #0: loss = 0.957909 (* 1 = 0.957909 loss)
I0428 19:28:19.809059 21591 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:28:19.851181 21591 solver.cpp:219] Iteration 800 (2373.7 iter/s, 0.0421284s/100 iters), loss = 0.819103
I0428 19:28:19.851205 21591 solver.cpp:238]     Train net output #0: loss = 0.819103 (* 1 = 0.819103 loss)
I0428 19:28:19.851229 21591 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:28:19.892956 21591 solver.cpp:219] Iteration 900 (2395.42 iter/s, 0.0417464s/100 iters), loss = 0.760475
I0428 19:28:19.892983 21591 solver.cpp:238]     Train net output #0: loss = 0.760475 (* 1 = 0.760475 loss)
I0428 19:28:19.893008 21591 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:28:19.907914 21597 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:28:19.934690 21591 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:28:19.935456 21591 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:28:19.935976 21591 solver.cpp:311] Iteration 1000, loss = 0.790106
I0428 19:28:19.935995 21591 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:28:19.956604 21591 blocking_queue.cpp:49] Waiting for data
I0428 19:28:20.013130 21598 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:28:20.013619 21591 solver.cpp:398]     Test net output #0: accuracy = 0.7627
I0428 19:28:20.013658 21591 solver.cpp:398]     Test net output #1: loss = 0.673593 (* 1 = 0.673593 loss)
I0428 19:28:20.013671 21591 solver.cpp:316] Optimization Done.
I0428 19:28:20.013679 21591 caffe.cpp:259] Optimization Done.
