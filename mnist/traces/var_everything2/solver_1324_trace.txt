I0428 20:18:40.085865  1187 caffe.cpp:218] Using GPUs 0
I0428 20:18:40.115669  1187 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:18:40.554656  1187 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1324.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:18:40.554811  1187 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1324.prototxt
I0428 20:18:40.555160  1187 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:18:40.555191  1187 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:18:40.555292  1187 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:18:40.555358  1187 layer_factory.hpp:77] Creating layer mnist
I0428 20:18:40.555444  1187 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:18:40.555464  1187 net.cpp:86] Creating Layer mnist
I0428 20:18:40.555471  1187 net.cpp:382] mnist -> data
I0428 20:18:40.555490  1187 net.cpp:382] mnist -> label
I0428 20:18:40.556432  1187 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:18:40.558895  1187 net.cpp:124] Setting up mnist
I0428 20:18:40.558926  1187 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:18:40.558933  1187 net.cpp:131] Top shape: 64 (64)
I0428 20:18:40.558935  1187 net.cpp:139] Memory required for data: 200960
I0428 20:18:40.558941  1187 layer_factory.hpp:77] Creating layer conv0
I0428 20:18:40.558954  1187 net.cpp:86] Creating Layer conv0
I0428 20:18:40.558970  1187 net.cpp:408] conv0 <- data
I0428 20:18:40.558981  1187 net.cpp:382] conv0 -> conv0
I0428 20:18:40.793190  1187 net.cpp:124] Setting up conv0
I0428 20:18:40.793215  1187 net.cpp:131] Top shape: 64 50 24 24 (1843200)
I0428 20:18:40.793220  1187 net.cpp:139] Memory required for data: 7573760
I0428 20:18:40.793232  1187 layer_factory.hpp:77] Creating layer pool0
I0428 20:18:40.793248  1187 net.cpp:86] Creating Layer pool0
I0428 20:18:40.793252  1187 net.cpp:408] pool0 <- conv0
I0428 20:18:40.793273  1187 net.cpp:382] pool0 -> pool0
I0428 20:18:40.793323  1187 net.cpp:124] Setting up pool0
I0428 20:18:40.793329  1187 net.cpp:131] Top shape: 64 50 12 12 (460800)
I0428 20:18:40.793332  1187 net.cpp:139] Memory required for data: 9416960
I0428 20:18:40.793334  1187 layer_factory.hpp:77] Creating layer conv1
I0428 20:18:40.793345  1187 net.cpp:86] Creating Layer conv1
I0428 20:18:40.793349  1187 net.cpp:408] conv1 <- pool0
I0428 20:18:40.793355  1187 net.cpp:382] conv1 -> conv1
I0428 20:18:40.796308  1187 net.cpp:124] Setting up conv1
I0428 20:18:40.796322  1187 net.cpp:131] Top shape: 64 10 8 8 (40960)
I0428 20:18:40.796342  1187 net.cpp:139] Memory required for data: 9580800
I0428 20:18:40.796365  1187 layer_factory.hpp:77] Creating layer pool1
I0428 20:18:40.796372  1187 net.cpp:86] Creating Layer pool1
I0428 20:18:40.796391  1187 net.cpp:408] pool1 <- conv1
I0428 20:18:40.796396  1187 net.cpp:382] pool1 -> pool1
I0428 20:18:40.796434  1187 net.cpp:124] Setting up pool1
I0428 20:18:40.796440  1187 net.cpp:131] Top shape: 64 10 4 4 (10240)
I0428 20:18:40.796443  1187 net.cpp:139] Memory required for data: 9621760
I0428 20:18:40.796447  1187 layer_factory.hpp:77] Creating layer ip1
I0428 20:18:40.796453  1187 net.cpp:86] Creating Layer ip1
I0428 20:18:40.796456  1187 net.cpp:408] ip1 <- pool1
I0428 20:18:40.796461  1187 net.cpp:382] ip1 -> ip1
I0428 20:18:40.796607  1187 net.cpp:124] Setting up ip1
I0428 20:18:40.796613  1187 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:18:40.796615  1187 net.cpp:139] Memory required for data: 9634560
I0428 20:18:40.796622  1187 layer_factory.hpp:77] Creating layer relu1
I0428 20:18:40.796628  1187 net.cpp:86] Creating Layer relu1
I0428 20:18:40.796632  1187 net.cpp:408] relu1 <- ip1
I0428 20:18:40.796653  1187 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:18:40.796818  1187 net.cpp:124] Setting up relu1
I0428 20:18:40.796844  1187 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:18:40.796847  1187 net.cpp:139] Memory required for data: 9647360
I0428 20:18:40.796850  1187 layer_factory.hpp:77] Creating layer ip2
I0428 20:18:40.796859  1187 net.cpp:86] Creating Layer ip2
I0428 20:18:40.796862  1187 net.cpp:408] ip2 <- ip1
I0428 20:18:40.796886  1187 net.cpp:382] ip2 -> ip2
I0428 20:18:40.797022  1187 net.cpp:124] Setting up ip2
I0428 20:18:40.797029  1187 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:18:40.797032  1187 net.cpp:139] Memory required for data: 9660160
I0428 20:18:40.797039  1187 layer_factory.hpp:77] Creating layer relu2
I0428 20:18:40.797044  1187 net.cpp:86] Creating Layer relu2
I0428 20:18:40.797047  1187 net.cpp:408] relu2 <- ip2
I0428 20:18:40.797052  1187 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:18:40.797884  1187 net.cpp:124] Setting up relu2
I0428 20:18:40.797896  1187 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:18:40.797916  1187 net.cpp:139] Memory required for data: 9672960
I0428 20:18:40.797919  1187 layer_factory.hpp:77] Creating layer ip3
I0428 20:18:40.797929  1187 net.cpp:86] Creating Layer ip3
I0428 20:18:40.797931  1187 net.cpp:408] ip3 <- ip2
I0428 20:18:40.797936  1187 net.cpp:382] ip3 -> ip3
I0428 20:18:40.798058  1187 net.cpp:124] Setting up ip3
I0428 20:18:40.798064  1187 net.cpp:131] Top shape: 64 10 (640)
I0428 20:18:40.798068  1187 net.cpp:139] Memory required for data: 9675520
I0428 20:18:40.798075  1187 layer_factory.hpp:77] Creating layer relu3
I0428 20:18:40.798081  1187 net.cpp:86] Creating Layer relu3
I0428 20:18:40.798084  1187 net.cpp:408] relu3 <- ip3
I0428 20:18:40.798089  1187 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:18:40.798277  1187 net.cpp:124] Setting up relu3
I0428 20:18:40.798286  1187 net.cpp:131] Top shape: 64 10 (640)
I0428 20:18:40.798290  1187 net.cpp:139] Memory required for data: 9678080
I0428 20:18:40.798293  1187 layer_factory.hpp:77] Creating layer loss
I0428 20:18:40.798300  1187 net.cpp:86] Creating Layer loss
I0428 20:18:40.798303  1187 net.cpp:408] loss <- ip3
I0428 20:18:40.798307  1187 net.cpp:408] loss <- label
I0428 20:18:40.798313  1187 net.cpp:382] loss -> loss
I0428 20:18:40.798346  1187 layer_factory.hpp:77] Creating layer loss
I0428 20:18:40.798606  1187 net.cpp:124] Setting up loss
I0428 20:18:40.798630  1187 net.cpp:131] Top shape: (1)
I0428 20:18:40.798633  1187 net.cpp:134]     with loss weight 1
I0428 20:18:40.798662  1187 net.cpp:139] Memory required for data: 9678084
I0428 20:18:40.798666  1187 net.cpp:200] loss needs backward computation.
I0428 20:18:40.798669  1187 net.cpp:200] relu3 needs backward computation.
I0428 20:18:40.798672  1187 net.cpp:200] ip3 needs backward computation.
I0428 20:18:40.798676  1187 net.cpp:200] relu2 needs backward computation.
I0428 20:18:40.798677  1187 net.cpp:200] ip2 needs backward computation.
I0428 20:18:40.798681  1187 net.cpp:200] relu1 needs backward computation.
I0428 20:18:40.798683  1187 net.cpp:200] ip1 needs backward computation.
I0428 20:18:40.798686  1187 net.cpp:200] pool1 needs backward computation.
I0428 20:18:40.798689  1187 net.cpp:200] conv1 needs backward computation.
I0428 20:18:40.798692  1187 net.cpp:200] pool0 needs backward computation.
I0428 20:18:40.798696  1187 net.cpp:200] conv0 needs backward computation.
I0428 20:18:40.798698  1187 net.cpp:202] mnist does not need backward computation.
I0428 20:18:40.798701  1187 net.cpp:244] This network produces output loss
I0428 20:18:40.798712  1187 net.cpp:257] Network initialization done.
I0428 20:18:40.799082  1187 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1324.prototxt
I0428 20:18:40.799135  1187 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:18:40.799226  1187 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:18:40.799306  1187 layer_factory.hpp:77] Creating layer mnist
I0428 20:18:40.799351  1187 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:18:40.799363  1187 net.cpp:86] Creating Layer mnist
I0428 20:18:40.799370  1187 net.cpp:382] mnist -> data
I0428 20:18:40.799376  1187 net.cpp:382] mnist -> label
I0428 20:18:40.799476  1187 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:18:40.801846  1187 net.cpp:124] Setting up mnist
I0428 20:18:40.801874  1187 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:18:40.801879  1187 net.cpp:131] Top shape: 100 (100)
I0428 20:18:40.801882  1187 net.cpp:139] Memory required for data: 314000
I0428 20:18:40.801884  1187 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:18:40.801894  1187 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:18:40.801898  1187 net.cpp:408] label_mnist_1_split <- label
I0428 20:18:40.801903  1187 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:18:40.801909  1187 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:18:40.802032  1187 net.cpp:124] Setting up label_mnist_1_split
I0428 20:18:40.802042  1187 net.cpp:131] Top shape: 100 (100)
I0428 20:18:40.802044  1187 net.cpp:131] Top shape: 100 (100)
I0428 20:18:40.802047  1187 net.cpp:139] Memory required for data: 314800
I0428 20:18:40.802050  1187 layer_factory.hpp:77] Creating layer conv0
I0428 20:18:40.802074  1187 net.cpp:86] Creating Layer conv0
I0428 20:18:40.802078  1187 net.cpp:408] conv0 <- data
I0428 20:18:40.802083  1187 net.cpp:382] conv0 -> conv0
I0428 20:18:40.803728  1187 net.cpp:124] Setting up conv0
I0428 20:18:40.803741  1187 net.cpp:131] Top shape: 100 50 24 24 (2880000)
I0428 20:18:40.803745  1187 net.cpp:139] Memory required for data: 11834800
I0428 20:18:40.803753  1187 layer_factory.hpp:77] Creating layer pool0
I0428 20:18:40.803761  1187 net.cpp:86] Creating Layer pool0
I0428 20:18:40.803763  1187 net.cpp:408] pool0 <- conv0
I0428 20:18:40.803767  1187 net.cpp:382] pool0 -> pool0
I0428 20:18:40.803833  1187 net.cpp:124] Setting up pool0
I0428 20:18:40.803839  1187 net.cpp:131] Top shape: 100 50 12 12 (720000)
I0428 20:18:40.803843  1187 net.cpp:139] Memory required for data: 14714800
I0428 20:18:40.803845  1187 layer_factory.hpp:77] Creating layer conv1
I0428 20:18:40.803854  1187 net.cpp:86] Creating Layer conv1
I0428 20:18:40.803858  1187 net.cpp:408] conv1 <- pool0
I0428 20:18:40.803867  1187 net.cpp:382] conv1 -> conv1
I0428 20:18:40.805685  1187 net.cpp:124] Setting up conv1
I0428 20:18:40.805713  1187 net.cpp:131] Top shape: 100 10 8 8 (64000)
I0428 20:18:40.805717  1187 net.cpp:139] Memory required for data: 14970800
I0428 20:18:40.805743  1187 layer_factory.hpp:77] Creating layer pool1
I0428 20:18:40.805749  1187 net.cpp:86] Creating Layer pool1
I0428 20:18:40.805753  1187 net.cpp:408] pool1 <- conv1
I0428 20:18:40.805759  1187 net.cpp:382] pool1 -> pool1
I0428 20:18:40.805797  1187 net.cpp:124] Setting up pool1
I0428 20:18:40.805804  1187 net.cpp:131] Top shape: 100 10 4 4 (16000)
I0428 20:18:40.805807  1187 net.cpp:139] Memory required for data: 15034800
I0428 20:18:40.805810  1187 layer_factory.hpp:77] Creating layer ip1
I0428 20:18:40.805817  1187 net.cpp:86] Creating Layer ip1
I0428 20:18:40.805820  1187 net.cpp:408] ip1 <- pool1
I0428 20:18:40.805824  1187 net.cpp:382] ip1 -> ip1
I0428 20:18:40.805984  1187 net.cpp:124] Setting up ip1
I0428 20:18:40.805990  1187 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:18:40.806005  1187 net.cpp:139] Memory required for data: 15054800
I0428 20:18:40.806012  1187 layer_factory.hpp:77] Creating layer relu1
I0428 20:18:40.806018  1187 net.cpp:86] Creating Layer relu1
I0428 20:18:40.806022  1187 net.cpp:408] relu1 <- ip1
I0428 20:18:40.806041  1187 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:18:40.806254  1187 net.cpp:124] Setting up relu1
I0428 20:18:40.806262  1187 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:18:40.806265  1187 net.cpp:139] Memory required for data: 15074800
I0428 20:18:40.806268  1187 layer_factory.hpp:77] Creating layer ip2
I0428 20:18:40.806277  1187 net.cpp:86] Creating Layer ip2
I0428 20:18:40.806279  1187 net.cpp:408] ip2 <- ip1
I0428 20:18:40.806285  1187 net.cpp:382] ip2 -> ip2
I0428 20:18:40.806411  1187 net.cpp:124] Setting up ip2
I0428 20:18:40.806418  1187 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:18:40.806421  1187 net.cpp:139] Memory required for data: 15094800
I0428 20:18:40.806427  1187 layer_factory.hpp:77] Creating layer relu2
I0428 20:18:40.806430  1187 net.cpp:86] Creating Layer relu2
I0428 20:18:40.806433  1187 net.cpp:408] relu2 <- ip2
I0428 20:18:40.806437  1187 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:18:40.806589  1187 net.cpp:124] Setting up relu2
I0428 20:18:40.806597  1187 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:18:40.806601  1187 net.cpp:139] Memory required for data: 15114800
I0428 20:18:40.806603  1187 layer_factory.hpp:77] Creating layer ip3
I0428 20:18:40.806608  1187 net.cpp:86] Creating Layer ip3
I0428 20:18:40.806612  1187 net.cpp:408] ip3 <- ip2
I0428 20:18:40.806617  1187 net.cpp:382] ip3 -> ip3
I0428 20:18:40.806728  1187 net.cpp:124] Setting up ip3
I0428 20:18:40.806735  1187 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:18:40.806736  1187 net.cpp:139] Memory required for data: 15118800
I0428 20:18:40.806743  1187 layer_factory.hpp:77] Creating layer relu3
I0428 20:18:40.806751  1187 net.cpp:86] Creating Layer relu3
I0428 20:18:40.806752  1187 net.cpp:408] relu3 <- ip3
I0428 20:18:40.806756  1187 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:18:40.807648  1187 net.cpp:124] Setting up relu3
I0428 20:18:40.807659  1187 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:18:40.807663  1187 net.cpp:139] Memory required for data: 15122800
I0428 20:18:40.807667  1187 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:18:40.807672  1187 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:18:40.807674  1187 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:18:40.807680  1187 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:18:40.807687  1187 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:18:40.807735  1187 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:18:40.807742  1187 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:18:40.807746  1187 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:18:40.807749  1187 net.cpp:139] Memory required for data: 15130800
I0428 20:18:40.807751  1187 layer_factory.hpp:77] Creating layer accuracy
I0428 20:18:40.807756  1187 net.cpp:86] Creating Layer accuracy
I0428 20:18:40.807760  1187 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:18:40.807763  1187 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:18:40.807770  1187 net.cpp:382] accuracy -> accuracy
I0428 20:18:40.807777  1187 net.cpp:124] Setting up accuracy
I0428 20:18:40.807780  1187 net.cpp:131] Top shape: (1)
I0428 20:18:40.807783  1187 net.cpp:139] Memory required for data: 15130804
I0428 20:18:40.807786  1187 layer_factory.hpp:77] Creating layer loss
I0428 20:18:40.807791  1187 net.cpp:86] Creating Layer loss
I0428 20:18:40.807799  1187 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:18:40.807803  1187 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:18:40.807807  1187 net.cpp:382] loss -> loss
I0428 20:18:40.807813  1187 layer_factory.hpp:77] Creating layer loss
I0428 20:18:40.808078  1187 net.cpp:124] Setting up loss
I0428 20:18:40.808086  1187 net.cpp:131] Top shape: (1)
I0428 20:18:40.808089  1187 net.cpp:134]     with loss weight 1
I0428 20:18:40.808104  1187 net.cpp:139] Memory required for data: 15130808
I0428 20:18:40.808107  1187 net.cpp:200] loss needs backward computation.
I0428 20:18:40.808112  1187 net.cpp:202] accuracy does not need backward computation.
I0428 20:18:40.808115  1187 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:18:40.808118  1187 net.cpp:200] relu3 needs backward computation.
I0428 20:18:40.808121  1187 net.cpp:200] ip3 needs backward computation.
I0428 20:18:40.808125  1187 net.cpp:200] relu2 needs backward computation.
I0428 20:18:40.808127  1187 net.cpp:200] ip2 needs backward computation.
I0428 20:18:40.808130  1187 net.cpp:200] relu1 needs backward computation.
I0428 20:18:40.808132  1187 net.cpp:200] ip1 needs backward computation.
I0428 20:18:40.808151  1187 net.cpp:200] pool1 needs backward computation.
I0428 20:18:40.808153  1187 net.cpp:200] conv1 needs backward computation.
I0428 20:18:40.808156  1187 net.cpp:200] pool0 needs backward computation.
I0428 20:18:40.808159  1187 net.cpp:200] conv0 needs backward computation.
I0428 20:18:40.808162  1187 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:18:40.808166  1187 net.cpp:202] mnist does not need backward computation.
I0428 20:18:40.808174  1187 net.cpp:244] This network produces output accuracy
I0428 20:18:40.808177  1187 net.cpp:244] This network produces output loss
I0428 20:18:40.808188  1187 net.cpp:257] Network initialization done.
I0428 20:18:40.808228  1187 solver.cpp:56] Solver scaffolding done.
I0428 20:18:40.808640  1187 caffe.cpp:248] Starting Optimization
I0428 20:18:40.808645  1187 solver.cpp:273] Solving LeNet
I0428 20:18:40.808647  1187 solver.cpp:274] Learning Rate Policy: inv
I0428 20:18:40.809480  1187 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:18:40.875368  1196 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:18:40.877638  1187 solver.cpp:398]     Test net output #0: accuracy = 0.0607
I0428 20:18:40.877656  1187 solver.cpp:398]     Test net output #1: loss = 2.3747 (* 1 = 2.3747 loss)
I0428 20:18:40.881873  1187 solver.cpp:219] Iteration 0 (0 iter/s, 0.0731965s/100 iters), loss = 2.32976
I0428 20:18:40.881896  1187 solver.cpp:238]     Train net output #0: loss = 2.32976 (* 1 = 2.32976 loss)
I0428 20:18:40.881923  1187 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:18:41.012848  1187 solver.cpp:219] Iteration 100 (763.793 iter/s, 0.130926s/100 iters), loss = 0.364798
I0428 20:18:41.012890  1187 solver.cpp:238]     Train net output #0: loss = 0.364798 (* 1 = 0.364798 loss)
I0428 20:18:41.012898  1187 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:18:41.135097  1187 solver.cpp:219] Iteration 200 (818.371 iter/s, 0.122194s/100 iters), loss = 0.229625
I0428 20:18:41.135138  1187 solver.cpp:238]     Train net output #0: loss = 0.229625 (* 1 = 0.229625 loss)
I0428 20:18:41.135143  1187 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:18:41.260120  1187 solver.cpp:219] Iteration 300 (800.154 iter/s, 0.124976s/100 iters), loss = 0.270338
I0428 20:18:41.260161  1187 solver.cpp:238]     Train net output #0: loss = 0.270338 (* 1 = 0.270338 loss)
I0428 20:18:41.260169  1187 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:18:41.383057  1187 solver.cpp:219] Iteration 400 (813.66 iter/s, 0.122902s/100 iters), loss = 0.108332
I0428 20:18:41.383100  1187 solver.cpp:238]     Train net output #0: loss = 0.108332 (* 1 = 0.108332 loss)
I0428 20:18:41.383106  1187 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:18:41.501660  1187 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:18:41.567209  1196 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:18:41.568917  1187 solver.cpp:398]     Test net output #0: accuracy = 0.9626
I0428 20:18:41.568939  1187 solver.cpp:398]     Test net output #1: loss = 0.118349 (* 1 = 0.118349 loss)
I0428 20:18:41.570099  1187 solver.cpp:219] Iteration 500 (534.801 iter/s, 0.186985s/100 iters), loss = 0.173022
I0428 20:18:41.570123  1187 solver.cpp:238]     Train net output #0: loss = 0.173022 (* 1 = 0.173022 loss)
I0428 20:18:41.570147  1187 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:18:41.696606  1187 solver.cpp:219] Iteration 600 (790.694 iter/s, 0.126471s/100 iters), loss = 0.0921164
I0428 20:18:41.696648  1187 solver.cpp:238]     Train net output #0: loss = 0.0921164 (* 1 = 0.0921164 loss)
I0428 20:18:41.696655  1187 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:18:41.815491  1187 solver.cpp:219] Iteration 700 (841.401 iter/s, 0.118849s/100 iters), loss = 0.187201
I0428 20:18:41.815533  1187 solver.cpp:238]     Train net output #0: loss = 0.187201 (* 1 = 0.187201 loss)
I0428 20:18:41.815541  1187 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:18:41.939543  1187 solver.cpp:219] Iteration 800 (806.497 iter/s, 0.123993s/100 iters), loss = 0.276799
I0428 20:18:41.939607  1187 solver.cpp:238]     Train net output #0: loss = 0.2768 (* 1 = 0.2768 loss)
I0428 20:18:41.939630  1187 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:18:42.073819  1187 solver.cpp:219] Iteration 900 (745.103 iter/s, 0.13421s/100 iters), loss = 0.0984053
I0428 20:18:42.073860  1187 solver.cpp:238]     Train net output #0: loss = 0.0984053 (* 1 = 0.0984053 loss)
I0428 20:18:42.073873  1187 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:18:42.120806  1195 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:18:42.210697  1187 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:18:42.212116  1187 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:18:42.213027  1187 solver.cpp:311] Iteration 1000, loss = 0.122854
I0428 20:18:42.213053  1187 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:18:42.281896  1196 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:18:42.284178  1187 solver.cpp:398]     Test net output #0: accuracy = 0.9758
I0428 20:18:42.284200  1187 solver.cpp:398]     Test net output #1: loss = 0.0790628 (* 1 = 0.0790628 loss)
I0428 20:18:42.284206  1187 solver.cpp:316] Optimization Done.
I0428 20:18:42.284210  1187 caffe.cpp:259] Optimization Done.
