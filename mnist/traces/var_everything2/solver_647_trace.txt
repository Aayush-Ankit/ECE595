I0428 19:51:54.751803 27391 caffe.cpp:218] Using GPUs 0
I0428 19:51:54.784695 27391 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:51:55.238693 27391 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test647.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:51:55.238826 27391 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test647.prototxt
I0428 19:51:55.239151 27391 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:51:55.239166 27391 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:51:55.239246 27391 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:51:55.239310 27391 layer_factory.hpp:77] Creating layer mnist
I0428 19:51:55.239390 27391 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:51:55.239409 27391 net.cpp:86] Creating Layer mnist
I0428 19:51:55.239415 27391 net.cpp:382] mnist -> data
I0428 19:51:55.239435 27391 net.cpp:382] mnist -> label
I0428 19:51:55.240345 27391 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:51:55.242501 27391 net.cpp:124] Setting up mnist
I0428 19:51:55.242548 27391 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:51:55.242558 27391 net.cpp:131] Top shape: 64 (64)
I0428 19:51:55.242561 27391 net.cpp:139] Memory required for data: 200960
I0428 19:51:55.242568 27391 layer_factory.hpp:77] Creating layer conv0
I0428 19:51:55.242581 27391 net.cpp:86] Creating Layer conv0
I0428 19:51:55.242599 27391 net.cpp:408] conv0 <- data
I0428 19:51:55.242609 27391 net.cpp:382] conv0 -> conv0
I0428 19:51:55.466013 27391 net.cpp:124] Setting up conv0
I0428 19:51:55.466039 27391 net.cpp:131] Top shape: 64 5 24 24 (184320)
I0428 19:51:55.466043 27391 net.cpp:139] Memory required for data: 938240
I0428 19:51:55.466058 27391 layer_factory.hpp:77] Creating layer pool0
I0428 19:51:55.466068 27391 net.cpp:86] Creating Layer pool0
I0428 19:51:55.466073 27391 net.cpp:408] pool0 <- conv0
I0428 19:51:55.466078 27391 net.cpp:382] pool0 -> pool0
I0428 19:51:55.466120 27391 net.cpp:124] Setting up pool0
I0428 19:51:55.466142 27391 net.cpp:131] Top shape: 64 5 12 12 (46080)
I0428 19:51:55.466145 27391 net.cpp:139] Memory required for data: 1122560
I0428 19:51:55.466163 27391 layer_factory.hpp:77] Creating layer conv1
I0428 19:51:55.466188 27391 net.cpp:86] Creating Layer conv1
I0428 19:51:55.466192 27391 net.cpp:408] conv1 <- pool0
I0428 19:51:55.466197 27391 net.cpp:382] conv1 -> conv1
I0428 19:51:55.468881 27391 net.cpp:124] Setting up conv1
I0428 19:51:55.468911 27391 net.cpp:131] Top shape: 64 25 8 8 (102400)
I0428 19:51:55.468916 27391 net.cpp:139] Memory required for data: 1532160
I0428 19:51:55.468924 27391 layer_factory.hpp:77] Creating layer pool1
I0428 19:51:55.468932 27391 net.cpp:86] Creating Layer pool1
I0428 19:51:55.468935 27391 net.cpp:408] pool1 <- conv1
I0428 19:51:55.468940 27391 net.cpp:382] pool1 -> pool1
I0428 19:51:55.468979 27391 net.cpp:124] Setting up pool1
I0428 19:51:55.468986 27391 net.cpp:131] Top shape: 64 25 4 4 (25600)
I0428 19:51:55.468988 27391 net.cpp:139] Memory required for data: 1634560
I0428 19:51:55.468991 27391 layer_factory.hpp:77] Creating layer ip1
I0428 19:51:55.468999 27391 net.cpp:86] Creating Layer ip1
I0428 19:51:55.469002 27391 net.cpp:408] ip1 <- pool1
I0428 19:51:55.469007 27391 net.cpp:382] ip1 -> ip1
I0428 19:51:55.470003 27391 net.cpp:124] Setting up ip1
I0428 19:51:55.470015 27391 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:51:55.470033 27391 net.cpp:139] Memory required for data: 1640960
I0428 19:51:55.470041 27391 layer_factory.hpp:77] Creating layer relu1
I0428 19:51:55.470047 27391 net.cpp:86] Creating Layer relu1
I0428 19:51:55.470052 27391 net.cpp:408] relu1 <- ip1
I0428 19:51:55.470055 27391 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:51:55.470222 27391 net.cpp:124] Setting up relu1
I0428 19:51:55.470232 27391 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:51:55.470233 27391 net.cpp:139] Memory required for data: 1647360
I0428 19:51:55.470237 27391 layer_factory.hpp:77] Creating layer ip2
I0428 19:51:55.470243 27391 net.cpp:86] Creating Layer ip2
I0428 19:51:55.470247 27391 net.cpp:408] ip2 <- ip1
I0428 19:51:55.470250 27391 net.cpp:382] ip2 -> ip2
I0428 19:51:55.470343 27391 net.cpp:124] Setting up ip2
I0428 19:51:55.470350 27391 net.cpp:131] Top shape: 64 10 (640)
I0428 19:51:55.470353 27391 net.cpp:139] Memory required for data: 1649920
I0428 19:51:55.470358 27391 layer_factory.hpp:77] Creating layer relu2
I0428 19:51:55.470366 27391 net.cpp:86] Creating Layer relu2
I0428 19:51:55.470368 27391 net.cpp:408] relu2 <- ip2
I0428 19:51:55.470372 27391 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:51:55.471086 27391 net.cpp:124] Setting up relu2
I0428 19:51:55.471113 27391 net.cpp:131] Top shape: 64 10 (640)
I0428 19:51:55.471117 27391 net.cpp:139] Memory required for data: 1652480
I0428 19:51:55.471120 27391 layer_factory.hpp:77] Creating layer ip3
I0428 19:51:55.471127 27391 net.cpp:86] Creating Layer ip3
I0428 19:51:55.471129 27391 net.cpp:408] ip3 <- ip2
I0428 19:51:55.471135 27391 net.cpp:382] ip3 -> ip3
I0428 19:51:55.471232 27391 net.cpp:124] Setting up ip3
I0428 19:51:55.471240 27391 net.cpp:131] Top shape: 64 10 (640)
I0428 19:51:55.471242 27391 net.cpp:139] Memory required for data: 1655040
I0428 19:51:55.471249 27391 layer_factory.hpp:77] Creating layer relu3
I0428 19:51:55.471254 27391 net.cpp:86] Creating Layer relu3
I0428 19:51:55.471256 27391 net.cpp:408] relu3 <- ip3
I0428 19:51:55.471261 27391 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:51:55.471424 27391 net.cpp:124] Setting up relu3
I0428 19:51:55.471433 27391 net.cpp:131] Top shape: 64 10 (640)
I0428 19:51:55.471436 27391 net.cpp:139] Memory required for data: 1657600
I0428 19:51:55.471439 27391 layer_factory.hpp:77] Creating layer loss
I0428 19:51:55.471444 27391 net.cpp:86] Creating Layer loss
I0428 19:51:55.471448 27391 net.cpp:408] loss <- ip3
I0428 19:51:55.471452 27391 net.cpp:408] loss <- label
I0428 19:51:55.471457 27391 net.cpp:382] loss -> loss
I0428 19:51:55.471470 27391 layer_factory.hpp:77] Creating layer loss
I0428 19:51:55.471704 27391 net.cpp:124] Setting up loss
I0428 19:51:55.471712 27391 net.cpp:131] Top shape: (1)
I0428 19:51:55.471715 27391 net.cpp:134]     with loss weight 1
I0428 19:51:55.471729 27391 net.cpp:139] Memory required for data: 1657604
I0428 19:51:55.471732 27391 net.cpp:200] loss needs backward computation.
I0428 19:51:55.471735 27391 net.cpp:200] relu3 needs backward computation.
I0428 19:51:55.471738 27391 net.cpp:200] ip3 needs backward computation.
I0428 19:51:55.471740 27391 net.cpp:200] relu2 needs backward computation.
I0428 19:51:55.471743 27391 net.cpp:200] ip2 needs backward computation.
I0428 19:51:55.471745 27391 net.cpp:200] relu1 needs backward computation.
I0428 19:51:55.471748 27391 net.cpp:200] ip1 needs backward computation.
I0428 19:51:55.471750 27391 net.cpp:200] pool1 needs backward computation.
I0428 19:51:55.471753 27391 net.cpp:200] conv1 needs backward computation.
I0428 19:51:55.471756 27391 net.cpp:200] pool0 needs backward computation.
I0428 19:51:55.471758 27391 net.cpp:200] conv0 needs backward computation.
I0428 19:51:55.471761 27391 net.cpp:202] mnist does not need backward computation.
I0428 19:51:55.471765 27391 net.cpp:244] This network produces output loss
I0428 19:51:55.471776 27391 net.cpp:257] Network initialization done.
I0428 19:51:55.472095 27391 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test647.prototxt
I0428 19:51:55.472151 27391 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:51:55.472237 27391 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:51:55.472316 27391 layer_factory.hpp:77] Creating layer mnist
I0428 19:51:55.472357 27391 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:51:55.472369 27391 net.cpp:86] Creating Layer mnist
I0428 19:51:55.472374 27391 net.cpp:382] mnist -> data
I0428 19:51:55.472380 27391 net.cpp:382] mnist -> label
I0428 19:51:55.472460 27391 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:51:55.474524 27391 net.cpp:124] Setting up mnist
I0428 19:51:55.474551 27391 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:51:55.474571 27391 net.cpp:131] Top shape: 100 (100)
I0428 19:51:55.474575 27391 net.cpp:139] Memory required for data: 314000
I0428 19:51:55.474577 27391 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:51:55.474608 27391 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:51:55.474613 27391 net.cpp:408] label_mnist_1_split <- label
I0428 19:51:55.474618 27391 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:51:55.474624 27391 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:51:55.474668 27391 net.cpp:124] Setting up label_mnist_1_split
I0428 19:51:55.474676 27391 net.cpp:131] Top shape: 100 (100)
I0428 19:51:55.474680 27391 net.cpp:131] Top shape: 100 (100)
I0428 19:51:55.474684 27391 net.cpp:139] Memory required for data: 314800
I0428 19:51:55.474685 27391 layer_factory.hpp:77] Creating layer conv0
I0428 19:51:55.474695 27391 net.cpp:86] Creating Layer conv0
I0428 19:51:55.474699 27391 net.cpp:408] conv0 <- data
I0428 19:51:55.474704 27391 net.cpp:382] conv0 -> conv0
I0428 19:51:55.476256 27391 net.cpp:124] Setting up conv0
I0428 19:51:55.476285 27391 net.cpp:131] Top shape: 100 5 24 24 (288000)
I0428 19:51:55.476289 27391 net.cpp:139] Memory required for data: 1466800
I0428 19:51:55.476300 27391 layer_factory.hpp:77] Creating layer pool0
I0428 19:51:55.476307 27391 net.cpp:86] Creating Layer pool0
I0428 19:51:55.476310 27391 net.cpp:408] pool0 <- conv0
I0428 19:51:55.476315 27391 net.cpp:382] pool0 -> pool0
I0428 19:51:55.476366 27391 net.cpp:124] Setting up pool0
I0428 19:51:55.476372 27391 net.cpp:131] Top shape: 100 5 12 12 (72000)
I0428 19:51:55.476375 27391 net.cpp:139] Memory required for data: 1754800
I0428 19:51:55.476377 27391 layer_factory.hpp:77] Creating layer conv1
I0428 19:51:55.476387 27391 net.cpp:86] Creating Layer conv1
I0428 19:51:55.476389 27391 net.cpp:408] conv1 <- pool0
I0428 19:51:55.476395 27391 net.cpp:382] conv1 -> conv1
I0428 19:51:55.478636 27391 net.cpp:124] Setting up conv1
I0428 19:51:55.478665 27391 net.cpp:131] Top shape: 100 25 8 8 (160000)
I0428 19:51:55.478669 27391 net.cpp:139] Memory required for data: 2394800
I0428 19:51:55.478678 27391 layer_factory.hpp:77] Creating layer pool1
I0428 19:51:55.478683 27391 net.cpp:86] Creating Layer pool1
I0428 19:51:55.478688 27391 net.cpp:408] pool1 <- conv1
I0428 19:51:55.478694 27391 net.cpp:382] pool1 -> pool1
I0428 19:51:55.478731 27391 net.cpp:124] Setting up pool1
I0428 19:51:55.478737 27391 net.cpp:131] Top shape: 100 25 4 4 (40000)
I0428 19:51:55.478740 27391 net.cpp:139] Memory required for data: 2554800
I0428 19:51:55.478744 27391 layer_factory.hpp:77] Creating layer ip1
I0428 19:51:55.478750 27391 net.cpp:86] Creating Layer ip1
I0428 19:51:55.478754 27391 net.cpp:408] ip1 <- pool1
I0428 19:51:55.478759 27391 net.cpp:382] ip1 -> ip1
I0428 19:51:55.478922 27391 net.cpp:124] Setting up ip1
I0428 19:51:55.478931 27391 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:51:55.478943 27391 net.cpp:139] Memory required for data: 2564800
I0428 19:51:55.478951 27391 layer_factory.hpp:77] Creating layer relu1
I0428 19:51:55.478956 27391 net.cpp:86] Creating Layer relu1
I0428 19:51:55.478960 27391 net.cpp:408] relu1 <- ip1
I0428 19:51:55.478963 27391 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:51:55.479173 27391 net.cpp:124] Setting up relu1
I0428 19:51:55.479183 27391 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:51:55.479187 27391 net.cpp:139] Memory required for data: 2574800
I0428 19:51:55.479189 27391 layer_factory.hpp:77] Creating layer ip2
I0428 19:51:55.479197 27391 net.cpp:86] Creating Layer ip2
I0428 19:51:55.479199 27391 net.cpp:408] ip2 <- ip1
I0428 19:51:55.479205 27391 net.cpp:382] ip2 -> ip2
I0428 19:51:55.479321 27391 net.cpp:124] Setting up ip2
I0428 19:51:55.479327 27391 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:51:55.479331 27391 net.cpp:139] Memory required for data: 2578800
I0428 19:51:55.479336 27391 layer_factory.hpp:77] Creating layer relu2
I0428 19:51:55.479339 27391 net.cpp:86] Creating Layer relu2
I0428 19:51:55.479342 27391 net.cpp:408] relu2 <- ip2
I0428 19:51:55.479348 27391 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:51:55.479502 27391 net.cpp:124] Setting up relu2
I0428 19:51:55.479511 27391 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:51:55.479514 27391 net.cpp:139] Memory required for data: 2582800
I0428 19:51:55.479517 27391 layer_factory.hpp:77] Creating layer ip3
I0428 19:51:55.479524 27391 net.cpp:86] Creating Layer ip3
I0428 19:51:55.479527 27391 net.cpp:408] ip3 <- ip2
I0428 19:51:55.479532 27391 net.cpp:382] ip3 -> ip3
I0428 19:51:55.479626 27391 net.cpp:124] Setting up ip3
I0428 19:51:55.479634 27391 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:51:55.479636 27391 net.cpp:139] Memory required for data: 2586800
I0428 19:51:55.479643 27391 layer_factory.hpp:77] Creating layer relu3
I0428 19:51:55.479647 27391 net.cpp:86] Creating Layer relu3
I0428 19:51:55.479650 27391 net.cpp:408] relu3 <- ip3
I0428 19:51:55.479656 27391 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:51:55.480448 27391 net.cpp:124] Setting up relu3
I0428 19:51:55.480474 27391 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:51:55.480479 27391 net.cpp:139] Memory required for data: 2590800
I0428 19:51:55.480481 27391 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:51:55.480487 27391 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:51:55.480491 27391 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:51:55.480496 27391 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:51:55.480502 27391 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:51:55.480545 27391 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:51:55.480550 27391 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:51:55.480553 27391 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:51:55.480556 27391 net.cpp:139] Memory required for data: 2598800
I0428 19:51:55.480558 27391 layer_factory.hpp:77] Creating layer accuracy
I0428 19:51:55.480563 27391 net.cpp:86] Creating Layer accuracy
I0428 19:51:55.480566 27391 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:51:55.480571 27391 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:51:55.480576 27391 net.cpp:382] accuracy -> accuracy
I0428 19:51:55.480582 27391 net.cpp:124] Setting up accuracy
I0428 19:51:55.480587 27391 net.cpp:131] Top shape: (1)
I0428 19:51:55.480590 27391 net.cpp:139] Memory required for data: 2598804
I0428 19:51:55.480598 27391 layer_factory.hpp:77] Creating layer loss
I0428 19:51:55.480602 27391 net.cpp:86] Creating Layer loss
I0428 19:51:55.480605 27391 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:51:55.480609 27391 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:51:55.480618 27391 net.cpp:382] loss -> loss
I0428 19:51:55.480625 27391 layer_factory.hpp:77] Creating layer loss
I0428 19:51:55.480897 27391 net.cpp:124] Setting up loss
I0428 19:51:55.480907 27391 net.cpp:131] Top shape: (1)
I0428 19:51:55.480911 27391 net.cpp:134]     with loss weight 1
I0428 19:51:55.480916 27391 net.cpp:139] Memory required for data: 2598808
I0428 19:51:55.480931 27391 net.cpp:200] loss needs backward computation.
I0428 19:51:55.480936 27391 net.cpp:202] accuracy does not need backward computation.
I0428 19:51:55.480939 27391 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:51:55.480942 27391 net.cpp:200] relu3 needs backward computation.
I0428 19:51:55.480945 27391 net.cpp:200] ip3 needs backward computation.
I0428 19:51:55.480948 27391 net.cpp:200] relu2 needs backward computation.
I0428 19:51:55.480952 27391 net.cpp:200] ip2 needs backward computation.
I0428 19:51:55.480954 27391 net.cpp:200] relu1 needs backward computation.
I0428 19:51:55.480957 27391 net.cpp:200] ip1 needs backward computation.
I0428 19:51:55.480959 27391 net.cpp:200] pool1 needs backward computation.
I0428 19:51:55.480962 27391 net.cpp:200] conv1 needs backward computation.
I0428 19:51:55.480965 27391 net.cpp:200] pool0 needs backward computation.
I0428 19:51:55.480968 27391 net.cpp:200] conv0 needs backward computation.
I0428 19:51:55.480972 27391 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:51:55.480975 27391 net.cpp:202] mnist does not need backward computation.
I0428 19:51:55.480978 27391 net.cpp:244] This network produces output accuracy
I0428 19:51:55.480980 27391 net.cpp:244] This network produces output loss
I0428 19:51:55.480993 27391 net.cpp:257] Network initialization done.
I0428 19:51:55.481032 27391 solver.cpp:56] Solver scaffolding done.
I0428 19:51:55.481402 27391 caffe.cpp:248] Starting Optimization
I0428 19:51:55.481408 27391 solver.cpp:273] Solving LeNet
I0428 19:51:55.481410 27391 solver.cpp:274] Learning Rate Policy: inv
I0428 19:51:55.481689 27391 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:51:55.485949 27391 blocking_queue.cpp:49] Waiting for data
I0428 19:51:55.557682 27398 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:51:55.558297 27391 solver.cpp:398]     Test net output #0: accuracy = 0.1456
I0428 19:51:55.558326 27391 solver.cpp:398]     Test net output #1: loss = 2.29645 (* 1 = 2.29645 loss)
I0428 19:51:55.560698 27391 solver.cpp:219] Iteration 0 (-4.64529e-31 iter/s, 0.0792392s/100 iters), loss = 2.28383
I0428 19:51:55.560734 27391 solver.cpp:238]     Train net output #0: loss = 2.28383 (* 1 = 2.28383 loss)
I0428 19:51:55.560750 27391 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:51:55.652657 27391 solver.cpp:219] Iteration 100 (1088.02 iter/s, 0.0919101s/100 iters), loss = 1.17723
I0428 19:51:55.652685 27391 solver.cpp:238]     Train net output #0: loss = 1.17723 (* 1 = 1.17723 loss)
I0428 19:51:55.652693 27391 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:51:55.737514 27391 solver.cpp:219] Iteration 200 (1179 iter/s, 0.0848179s/100 iters), loss = 0.783245
I0428 19:51:55.737538 27391 solver.cpp:238]     Train net output #0: loss = 0.783245 (* 1 = 0.783245 loss)
I0428 19:51:55.737543 27391 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:51:55.822839 27391 solver.cpp:219] Iteration 300 (1172.46 iter/s, 0.085291s/100 iters), loss = 0.693817
I0428 19:51:55.822877 27391 solver.cpp:238]     Train net output #0: loss = 0.693817 (* 1 = 0.693817 loss)
I0428 19:51:55.822882 27391 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:51:55.910703 27391 solver.cpp:219] Iteration 400 (1138.53 iter/s, 0.0878325s/100 iters), loss = 0.547168
I0428 19:51:55.910742 27391 solver.cpp:238]     Train net output #0: loss = 0.547168 (* 1 = 0.547168 loss)
I0428 19:51:55.910748 27391 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:51:55.995018 27391 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:51:56.049765 27398 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:51:56.050317 27391 solver.cpp:398]     Test net output #0: accuracy = 0.759
I0428 19:51:56.050351 27391 solver.cpp:398]     Test net output #1: loss = 0.624783 (* 1 = 0.624783 loss)
I0428 19:51:56.051223 27391 solver.cpp:219] Iteration 500 (711.827 iter/s, 0.140484s/100 iters), loss = 0.542782
I0428 19:51:56.051291 27391 solver.cpp:238]     Train net output #0: loss = 0.542782 (* 1 = 0.542782 loss)
I0428 19:51:56.051300 27391 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:51:56.146351 27391 solver.cpp:219] Iteration 600 (1052.06 iter/s, 0.0950519s/100 iters), loss = 0.518792
I0428 19:51:56.146392 27391 solver.cpp:238]     Train net output #0: loss = 0.518792 (* 1 = 0.518792 loss)
I0428 19:51:56.146399 27391 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:51:56.241132 27391 solver.cpp:219] Iteration 700 (1055.64 iter/s, 0.0947294s/100 iters), loss = 0.823489
I0428 19:51:56.241155 27391 solver.cpp:238]     Train net output #0: loss = 0.823489 (* 1 = 0.823489 loss)
I0428 19:51:56.241161 27391 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:51:56.325420 27391 solver.cpp:219] Iteration 800 (1186.86 iter/s, 0.0842557s/100 iters), loss = 0.593534
I0428 19:51:56.325444 27391 solver.cpp:238]     Train net output #0: loss = 0.593534 (* 1 = 0.593534 loss)
I0428 19:51:56.325465 27391 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:51:56.408412 27391 solver.cpp:219] Iteration 900 (1205.44 iter/s, 0.0829574s/100 iters), loss = 0.499813
I0428 19:51:56.408437 27391 solver.cpp:238]     Train net output #0: loss = 0.499813 (* 1 = 0.499813 loss)
I0428 19:51:56.408459 27391 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:51:56.436832 27397 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:51:56.491922 27391 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:51:56.492889 27391 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:51:56.493396 27391 solver.cpp:311] Iteration 1000, loss = 0.573708
I0428 19:51:56.493412 27391 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:51:56.546414 27398 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:51:56.546947 27391 solver.cpp:398]     Test net output #0: accuracy = 0.7763
I0428 19:51:56.546982 27391 solver.cpp:398]     Test net output #1: loss = 0.564837 (* 1 = 0.564837 loss)
I0428 19:51:56.546996 27391 solver.cpp:316] Optimization Done.
I0428 19:51:56.547001 27391 caffe.cpp:259] Optimization Done.
