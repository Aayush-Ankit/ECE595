I0428 20:19:27.775090  1367 caffe.cpp:218] Using GPUs 0
I0428 20:19:27.812572  1367 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:19:28.327106  1367 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1342.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:19:28.327249  1367 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1342.prototxt
I0428 20:19:28.327684  1367 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:19:28.327705  1367 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:19:28.327806  1367 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:19:28.327888  1367 layer_factory.hpp:77] Creating layer mnist
I0428 20:19:28.327987  1367 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:19:28.328011  1367 net.cpp:86] Creating Layer mnist
I0428 20:19:28.328019  1367 net.cpp:382] mnist -> data
I0428 20:19:28.328042  1367 net.cpp:382] mnist -> label
I0428 20:19:28.329149  1367 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:19:28.331612  1367 net.cpp:124] Setting up mnist
I0428 20:19:28.331632  1367 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:19:28.331639  1367 net.cpp:131] Top shape: 64 (64)
I0428 20:19:28.331641  1367 net.cpp:139] Memory required for data: 200960
I0428 20:19:28.331648  1367 layer_factory.hpp:77] Creating layer conv0
I0428 20:19:28.331665  1367 net.cpp:86] Creating Layer conv0
I0428 20:19:28.331686  1367 net.cpp:408] conv0 <- data
I0428 20:19:28.331697  1367 net.cpp:382] conv0 -> conv0
I0428 20:19:28.610586  1367 net.cpp:124] Setting up conv0
I0428 20:19:28.610612  1367 net.cpp:131] Top shape: 64 50 24 24 (1843200)
I0428 20:19:28.610616  1367 net.cpp:139] Memory required for data: 7573760
I0428 20:19:28.610631  1367 layer_factory.hpp:77] Creating layer pool0
I0428 20:19:28.610659  1367 net.cpp:86] Creating Layer pool0
I0428 20:19:28.610663  1367 net.cpp:408] pool0 <- conv0
I0428 20:19:28.610668  1367 net.cpp:382] pool0 -> pool0
I0428 20:19:28.610728  1367 net.cpp:124] Setting up pool0
I0428 20:19:28.610735  1367 net.cpp:131] Top shape: 64 50 12 12 (460800)
I0428 20:19:28.610740  1367 net.cpp:139] Memory required for data: 9416960
I0428 20:19:28.610744  1367 layer_factory.hpp:77] Creating layer conv1
I0428 20:19:28.610759  1367 net.cpp:86] Creating Layer conv1
I0428 20:19:28.610765  1367 net.cpp:408] conv1 <- pool0
I0428 20:19:28.610772  1367 net.cpp:382] conv1 -> conv1
I0428 20:19:28.612853  1367 net.cpp:124] Setting up conv1
I0428 20:19:28.612866  1367 net.cpp:131] Top shape: 64 25 8 8 (102400)
I0428 20:19:28.612870  1367 net.cpp:139] Memory required for data: 9826560
I0428 20:19:28.612879  1367 layer_factory.hpp:77] Creating layer pool1
I0428 20:19:28.612886  1367 net.cpp:86] Creating Layer pool1
I0428 20:19:28.612890  1367 net.cpp:408] pool1 <- conv1
I0428 20:19:28.612895  1367 net.cpp:382] pool1 -> pool1
I0428 20:19:28.612937  1367 net.cpp:124] Setting up pool1
I0428 20:19:28.612942  1367 net.cpp:131] Top shape: 64 25 4 4 (25600)
I0428 20:19:28.612946  1367 net.cpp:139] Memory required for data: 9928960
I0428 20:19:28.612948  1367 layer_factory.hpp:77] Creating layer ip1
I0428 20:19:28.612956  1367 net.cpp:86] Creating Layer ip1
I0428 20:19:28.612958  1367 net.cpp:408] ip1 <- pool1
I0428 20:19:28.612963  1367 net.cpp:382] ip1 -> ip1
I0428 20:19:28.613077  1367 net.cpp:124] Setting up ip1
I0428 20:19:28.613085  1367 net.cpp:131] Top shape: 64 10 (640)
I0428 20:19:28.613088  1367 net.cpp:139] Memory required for data: 9931520
I0428 20:19:28.613095  1367 layer_factory.hpp:77] Creating layer relu1
I0428 20:19:28.613101  1367 net.cpp:86] Creating Layer relu1
I0428 20:19:28.613104  1367 net.cpp:408] relu1 <- ip1
I0428 20:19:28.613108  1367 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:19:28.613268  1367 net.cpp:124] Setting up relu1
I0428 20:19:28.613277  1367 net.cpp:131] Top shape: 64 10 (640)
I0428 20:19:28.613281  1367 net.cpp:139] Memory required for data: 9934080
I0428 20:19:28.613284  1367 layer_factory.hpp:77] Creating layer ip2
I0428 20:19:28.613291  1367 net.cpp:86] Creating Layer ip2
I0428 20:19:28.613293  1367 net.cpp:408] ip2 <- ip1
I0428 20:19:28.613297  1367 net.cpp:382] ip2 -> ip2
I0428 20:19:28.613402  1367 net.cpp:124] Setting up ip2
I0428 20:19:28.613409  1367 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:19:28.613412  1367 net.cpp:139] Memory required for data: 9940480
I0428 20:19:28.613417  1367 layer_factory.hpp:77] Creating layer relu2
I0428 20:19:28.613423  1367 net.cpp:86] Creating Layer relu2
I0428 20:19:28.613426  1367 net.cpp:408] relu2 <- ip2
I0428 20:19:28.613430  1367 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:19:28.614140  1367 net.cpp:124] Setting up relu2
I0428 20:19:28.614152  1367 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:19:28.614171  1367 net.cpp:139] Memory required for data: 9946880
I0428 20:19:28.614173  1367 layer_factory.hpp:77] Creating layer ip3
I0428 20:19:28.614179  1367 net.cpp:86] Creating Layer ip3
I0428 20:19:28.614183  1367 net.cpp:408] ip3 <- ip2
I0428 20:19:28.614189  1367 net.cpp:382] ip3 -> ip3
I0428 20:19:28.614281  1367 net.cpp:124] Setting up ip3
I0428 20:19:28.614289  1367 net.cpp:131] Top shape: 64 10 (640)
I0428 20:19:28.614291  1367 net.cpp:139] Memory required for data: 9949440
I0428 20:19:28.614298  1367 layer_factory.hpp:77] Creating layer relu3
I0428 20:19:28.614305  1367 net.cpp:86] Creating Layer relu3
I0428 20:19:28.614306  1367 net.cpp:408] relu3 <- ip3
I0428 20:19:28.614311  1367 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:19:28.614487  1367 net.cpp:124] Setting up relu3
I0428 20:19:28.614496  1367 net.cpp:131] Top shape: 64 10 (640)
I0428 20:19:28.614500  1367 net.cpp:139] Memory required for data: 9952000
I0428 20:19:28.614502  1367 layer_factory.hpp:77] Creating layer loss
I0428 20:19:28.614508  1367 net.cpp:86] Creating Layer loss
I0428 20:19:28.614511  1367 net.cpp:408] loss <- ip3
I0428 20:19:28.614516  1367 net.cpp:408] loss <- label
I0428 20:19:28.614521  1367 net.cpp:382] loss -> loss
I0428 20:19:28.614537  1367 layer_factory.hpp:77] Creating layer loss
I0428 20:19:28.614783  1367 net.cpp:124] Setting up loss
I0428 20:19:28.614791  1367 net.cpp:131] Top shape: (1)
I0428 20:19:28.614795  1367 net.cpp:134]     with loss weight 1
I0428 20:19:28.614809  1367 net.cpp:139] Memory required for data: 9952004
I0428 20:19:28.614812  1367 net.cpp:200] loss needs backward computation.
I0428 20:19:28.614816  1367 net.cpp:200] relu3 needs backward computation.
I0428 20:19:28.614819  1367 net.cpp:200] ip3 needs backward computation.
I0428 20:19:28.614821  1367 net.cpp:200] relu2 needs backward computation.
I0428 20:19:28.614825  1367 net.cpp:200] ip2 needs backward computation.
I0428 20:19:28.614826  1367 net.cpp:200] relu1 needs backward computation.
I0428 20:19:28.614830  1367 net.cpp:200] ip1 needs backward computation.
I0428 20:19:28.614832  1367 net.cpp:200] pool1 needs backward computation.
I0428 20:19:28.614835  1367 net.cpp:200] conv1 needs backward computation.
I0428 20:19:28.614837  1367 net.cpp:200] pool0 needs backward computation.
I0428 20:19:28.614840  1367 net.cpp:200] conv0 needs backward computation.
I0428 20:19:28.614843  1367 net.cpp:202] mnist does not need backward computation.
I0428 20:19:28.614846  1367 net.cpp:244] This network produces output loss
I0428 20:19:28.614856  1367 net.cpp:257] Network initialization done.
I0428 20:19:28.615207  1367 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1342.prototxt
I0428 20:19:28.615247  1367 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:19:28.615332  1367 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:19:28.615409  1367 layer_factory.hpp:77] Creating layer mnist
I0428 20:19:28.615450  1367 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:19:28.615463  1367 net.cpp:86] Creating Layer mnist
I0428 20:19:28.615466  1367 net.cpp:382] mnist -> data
I0428 20:19:28.615474  1367 net.cpp:382] mnist -> label
I0428 20:19:28.615556  1367 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:19:28.617497  1367 net.cpp:124] Setting up mnist
I0428 20:19:28.617528  1367 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:19:28.617533  1367 net.cpp:131] Top shape: 100 (100)
I0428 20:19:28.617537  1367 net.cpp:139] Memory required for data: 314000
I0428 20:19:28.617540  1367 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:19:28.617547  1367 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:19:28.617550  1367 net.cpp:408] label_mnist_1_split <- label
I0428 20:19:28.617566  1367 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:19:28.617574  1367 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:19:28.617629  1367 net.cpp:124] Setting up label_mnist_1_split
I0428 20:19:28.617635  1367 net.cpp:131] Top shape: 100 (100)
I0428 20:19:28.617637  1367 net.cpp:131] Top shape: 100 (100)
I0428 20:19:28.617640  1367 net.cpp:139] Memory required for data: 314800
I0428 20:19:28.617643  1367 layer_factory.hpp:77] Creating layer conv0
I0428 20:19:28.617655  1367 net.cpp:86] Creating Layer conv0
I0428 20:19:28.617658  1367 net.cpp:408] conv0 <- data
I0428 20:19:28.617663  1367 net.cpp:382] conv0 -> conv0
I0428 20:19:28.619421  1367 net.cpp:124] Setting up conv0
I0428 20:19:28.619433  1367 net.cpp:131] Top shape: 100 50 24 24 (2880000)
I0428 20:19:28.619437  1367 net.cpp:139] Memory required for data: 11834800
I0428 20:19:28.619447  1367 layer_factory.hpp:77] Creating layer pool0
I0428 20:19:28.619453  1367 net.cpp:86] Creating Layer pool0
I0428 20:19:28.619457  1367 net.cpp:408] pool0 <- conv0
I0428 20:19:28.619462  1367 net.cpp:382] pool0 -> pool0
I0428 20:19:28.619496  1367 net.cpp:124] Setting up pool0
I0428 20:19:28.619503  1367 net.cpp:131] Top shape: 100 50 12 12 (720000)
I0428 20:19:28.619505  1367 net.cpp:139] Memory required for data: 14714800
I0428 20:19:28.619508  1367 layer_factory.hpp:77] Creating layer conv1
I0428 20:19:28.619518  1367 net.cpp:86] Creating Layer conv1
I0428 20:19:28.619520  1367 net.cpp:408] conv1 <- pool0
I0428 20:19:28.619524  1367 net.cpp:382] conv1 -> conv1
I0428 20:19:28.621359  1367 net.cpp:124] Setting up conv1
I0428 20:19:28.621372  1367 net.cpp:131] Top shape: 100 25 8 8 (160000)
I0428 20:19:28.621376  1367 net.cpp:139] Memory required for data: 15354800
I0428 20:19:28.621384  1367 layer_factory.hpp:77] Creating layer pool1
I0428 20:19:28.621407  1367 net.cpp:86] Creating Layer pool1
I0428 20:19:28.621412  1367 net.cpp:408] pool1 <- conv1
I0428 20:19:28.621418  1367 net.cpp:382] pool1 -> pool1
I0428 20:19:28.621454  1367 net.cpp:124] Setting up pool1
I0428 20:19:28.621469  1367 net.cpp:131] Top shape: 100 25 4 4 (40000)
I0428 20:19:28.621471  1367 net.cpp:139] Memory required for data: 15514800
I0428 20:19:28.621474  1367 layer_factory.hpp:77] Creating layer ip1
I0428 20:19:28.621486  1367 net.cpp:86] Creating Layer ip1
I0428 20:19:28.621490  1367 net.cpp:408] ip1 <- pool1
I0428 20:19:28.621495  1367 net.cpp:382] ip1 -> ip1
I0428 20:19:28.621635  1367 net.cpp:124] Setting up ip1
I0428 20:19:28.621644  1367 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:19:28.621655  1367 net.cpp:139] Memory required for data: 15518800
I0428 20:19:28.621664  1367 layer_factory.hpp:77] Creating layer relu1
I0428 20:19:28.621668  1367 net.cpp:86] Creating Layer relu1
I0428 20:19:28.621671  1367 net.cpp:408] relu1 <- ip1
I0428 20:19:28.621677  1367 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:19:28.621839  1367 net.cpp:124] Setting up relu1
I0428 20:19:28.621866  1367 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:19:28.621870  1367 net.cpp:139] Memory required for data: 15522800
I0428 20:19:28.621872  1367 layer_factory.hpp:77] Creating layer ip2
I0428 20:19:28.621881  1367 net.cpp:86] Creating Layer ip2
I0428 20:19:28.621884  1367 net.cpp:408] ip2 <- ip1
I0428 20:19:28.621889  1367 net.cpp:382] ip2 -> ip2
I0428 20:19:28.622002  1367 net.cpp:124] Setting up ip2
I0428 20:19:28.622010  1367 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:19:28.622018  1367 net.cpp:139] Memory required for data: 15532800
I0428 20:19:28.622025  1367 layer_factory.hpp:77] Creating layer relu2
I0428 20:19:28.622030  1367 net.cpp:86] Creating Layer relu2
I0428 20:19:28.622033  1367 net.cpp:408] relu2 <- ip2
I0428 20:19:28.622037  1367 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:19:28.622208  1367 net.cpp:124] Setting up relu2
I0428 20:19:28.622216  1367 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:19:28.622220  1367 net.cpp:139] Memory required for data: 15542800
I0428 20:19:28.622222  1367 layer_factory.hpp:77] Creating layer ip3
I0428 20:19:28.622228  1367 net.cpp:86] Creating Layer ip3
I0428 20:19:28.622231  1367 net.cpp:408] ip3 <- ip2
I0428 20:19:28.622236  1367 net.cpp:382] ip3 -> ip3
I0428 20:19:28.622347  1367 net.cpp:124] Setting up ip3
I0428 20:19:28.622354  1367 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:19:28.622359  1367 net.cpp:139] Memory required for data: 15546800
I0428 20:19:28.622365  1367 layer_factory.hpp:77] Creating layer relu3
I0428 20:19:28.622371  1367 net.cpp:86] Creating Layer relu3
I0428 20:19:28.622375  1367 net.cpp:408] relu3 <- ip3
I0428 20:19:28.622377  1367 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:19:28.623307  1367 net.cpp:124] Setting up relu3
I0428 20:19:28.623320  1367 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:19:28.623323  1367 net.cpp:139] Memory required for data: 15550800
I0428 20:19:28.623327  1367 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:19:28.623332  1367 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:19:28.623335  1367 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:19:28.623340  1367 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:19:28.623347  1367 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:19:28.623386  1367 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:19:28.623407  1367 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:19:28.623412  1367 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:19:28.623430  1367 net.cpp:139] Memory required for data: 15558800
I0428 20:19:28.623433  1367 layer_factory.hpp:77] Creating layer accuracy
I0428 20:19:28.623438  1367 net.cpp:86] Creating Layer accuracy
I0428 20:19:28.623441  1367 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:19:28.623446  1367 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:19:28.623450  1367 net.cpp:382] accuracy -> accuracy
I0428 20:19:28.623457  1367 net.cpp:124] Setting up accuracy
I0428 20:19:28.623461  1367 net.cpp:131] Top shape: (1)
I0428 20:19:28.623469  1367 net.cpp:139] Memory required for data: 15558804
I0428 20:19:28.623472  1367 layer_factory.hpp:77] Creating layer loss
I0428 20:19:28.623481  1367 net.cpp:86] Creating Layer loss
I0428 20:19:28.623483  1367 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:19:28.623487  1367 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:19:28.623492  1367 net.cpp:382] loss -> loss
I0428 20:19:28.623497  1367 layer_factory.hpp:77] Creating layer loss
I0428 20:19:28.623780  1367 net.cpp:124] Setting up loss
I0428 20:19:28.623790  1367 net.cpp:131] Top shape: (1)
I0428 20:19:28.623793  1367 net.cpp:134]     with loss weight 1
I0428 20:19:28.623808  1367 net.cpp:139] Memory required for data: 15558808
I0428 20:19:28.623811  1367 net.cpp:200] loss needs backward computation.
I0428 20:19:28.623828  1367 net.cpp:202] accuracy does not need backward computation.
I0428 20:19:28.623832  1367 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:19:28.623836  1367 net.cpp:200] relu3 needs backward computation.
I0428 20:19:28.623837  1367 net.cpp:200] ip3 needs backward computation.
I0428 20:19:28.623847  1367 net.cpp:200] relu2 needs backward computation.
I0428 20:19:28.623848  1367 net.cpp:200] ip2 needs backward computation.
I0428 20:19:28.623852  1367 net.cpp:200] relu1 needs backward computation.
I0428 20:19:28.623854  1367 net.cpp:200] ip1 needs backward computation.
I0428 20:19:28.623857  1367 net.cpp:200] pool1 needs backward computation.
I0428 20:19:28.623860  1367 net.cpp:200] conv1 needs backward computation.
I0428 20:19:28.623863  1367 net.cpp:200] pool0 needs backward computation.
I0428 20:19:28.623865  1367 net.cpp:200] conv0 needs backward computation.
I0428 20:19:28.623869  1367 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:19:28.623873  1367 net.cpp:202] mnist does not need backward computation.
I0428 20:19:28.623877  1367 net.cpp:244] This network produces output accuracy
I0428 20:19:28.623879  1367 net.cpp:244] This network produces output loss
I0428 20:19:28.623890  1367 net.cpp:257] Network initialization done.
I0428 20:19:28.623934  1367 solver.cpp:56] Solver scaffolding done.
I0428 20:19:28.624291  1367 caffe.cpp:248] Starting Optimization
I0428 20:19:28.624299  1367 solver.cpp:273] Solving LeNet
I0428 20:19:28.624300  1367 solver.cpp:274] Learning Rate Policy: inv
I0428 20:19:28.624536  1367 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:19:28.630947  1367 blocking_queue.cpp:49] Waiting for data
I0428 20:19:28.706058  1375 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:19:28.706876  1367 solver.cpp:398]     Test net output #0: accuracy = 0.1192
I0428 20:19:28.706894  1367 solver.cpp:398]     Test net output #1: loss = 2.3076 (* 1 = 2.3076 loss)
I0428 20:19:28.711339  1367 solver.cpp:219] Iteration 0 (-1.10282e-30 iter/s, 0.0870124s/100 iters), loss = 2.30363
I0428 20:19:28.711364  1367 solver.cpp:238]     Train net output #0: loss = 2.30363 (* 1 = 2.30363 loss)
I0428 20:19:28.711374  1367 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:19:28.845604  1367 solver.cpp:219] Iteration 100 (745.013 iter/s, 0.134226s/100 iters), loss = 0.802649
I0428 20:19:28.845628  1367 solver.cpp:238]     Train net output #0: loss = 0.802649 (* 1 = 0.802649 loss)
I0428 20:19:28.845634  1367 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:19:28.978766  1367 solver.cpp:219] Iteration 200 (751.162 iter/s, 0.133127s/100 iters), loss = 1.22646
I0428 20:19:28.978806  1367 solver.cpp:238]     Train net output #0: loss = 1.22646 (* 1 = 1.22646 loss)
I0428 20:19:28.978812  1367 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:19:29.116345  1367 solver.cpp:219] Iteration 300 (727.128 iter/s, 0.137527s/100 iters), loss = 0.920787
I0428 20:19:29.116386  1367 solver.cpp:238]     Train net output #0: loss = 0.920787 (* 1 = 0.920787 loss)
I0428 20:19:29.116392  1367 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:19:29.247819  1367 solver.cpp:219] Iteration 400 (760.818 iter/s, 0.131438s/100 iters), loss = 0.989931
I0428 20:19:29.247843  1367 solver.cpp:238]     Train net output #0: loss = 0.989931 (* 1 = 0.989931 loss)
I0428 20:19:29.247848  1367 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:19:29.386577  1367 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:19:29.452997  1375 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:19:29.454735  1367 solver.cpp:398]     Test net output #0: accuracy = 0.6736
I0428 20:19:29.454754  1367 solver.cpp:398]     Test net output #1: loss = 0.822935 (* 1 = 0.822935 loss)
I0428 20:19:29.456050  1367 solver.cpp:219] Iteration 500 (480.355 iter/s, 0.208179s/100 iters), loss = 0.853552
I0428 20:19:29.456087  1367 solver.cpp:238]     Train net output #0: loss = 0.853552 (* 1 = 0.853552 loss)
I0428 20:19:29.456094  1367 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:19:29.595410  1367 solver.cpp:219] Iteration 600 (717.817 iter/s, 0.139311s/100 iters), loss = 0.642498
I0428 20:19:29.595437  1367 solver.cpp:238]     Train net output #0: loss = 0.642498 (* 1 = 0.642498 loss)
I0428 20:19:29.595445  1367 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:19:29.733947  1367 solver.cpp:219] Iteration 700 (722.026 iter/s, 0.138499s/100 iters), loss = 0.882572
I0428 20:19:29.733973  1367 solver.cpp:238]     Train net output #0: loss = 0.882572 (* 1 = 0.882572 loss)
I0428 20:19:29.733996  1367 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:19:29.866753  1367 solver.cpp:219] Iteration 800 (753.189 iter/s, 0.132769s/100 iters), loss = 0.929359
I0428 20:19:29.866777  1367 solver.cpp:238]     Train net output #0: loss = 0.929359 (* 1 = 0.929359 loss)
I0428 20:19:29.866801  1367 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:19:30.004421  1367 solver.cpp:219] Iteration 900 (726.572 iter/s, 0.137633s/100 iters), loss = 0.934126
I0428 20:19:30.004446  1367 solver.cpp:238]     Train net output #0: loss = 0.934126 (* 1 = 0.934126 loss)
I0428 20:19:30.004468  1367 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:19:30.050277  1374 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:19:30.143357  1367 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:19:30.144982  1367 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:19:30.145961  1367 solver.cpp:311] Iteration 1000, loss = 0.765548
I0428 20:19:30.145978  1367 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:19:30.222355  1375 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:19:30.223134  1367 solver.cpp:398]     Test net output #0: accuracy = 0.6801
I0428 20:19:30.223151  1367 solver.cpp:398]     Test net output #1: loss = 0.781348 (* 1 = 0.781348 loss)
I0428 20:19:30.223156  1367 solver.cpp:316] Optimization Done.
I0428 20:19:30.223160  1367 caffe.cpp:259] Optimization Done.
