I0428 20:26:22.513176  2783 caffe.cpp:218] Using GPUs 0
I0428 20:26:22.545151  2783 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:26:23.056103  2783 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1470.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:26:23.056248  2783 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1470.prototxt
I0428 20:26:23.056673  2783 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:26:23.056691  2783 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:26:23.056797  2783 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:26:23.056884  2783 layer_factory.hpp:77] Creating layer mnist
I0428 20:26:23.056984  2783 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:26:23.057010  2783 net.cpp:86] Creating Layer mnist
I0428 20:26:23.057020  2783 net.cpp:382] mnist -> data
I0428 20:26:23.057044  2783 net.cpp:382] mnist -> label
I0428 20:26:23.058135  2783 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:26:23.060555  2783 net.cpp:124] Setting up mnist
I0428 20:26:23.060573  2783 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:26:23.060580  2783 net.cpp:131] Top shape: 64 (64)
I0428 20:26:23.060583  2783 net.cpp:139] Memory required for data: 200960
I0428 20:26:23.060590  2783 layer_factory.hpp:77] Creating layer conv0
I0428 20:26:23.060629  2783 net.cpp:86] Creating Layer conv0
I0428 20:26:23.060652  2783 net.cpp:408] conv0 <- data
I0428 20:26:23.060665  2783 net.cpp:382] conv0 -> conv0
I0428 20:26:23.296281  2783 net.cpp:124] Setting up conv0
I0428 20:26:23.296319  2783 net.cpp:131] Top shape: 64 100 24 24 (3686400)
I0428 20:26:23.296322  2783 net.cpp:139] Memory required for data: 14946560
I0428 20:26:23.296339  2783 layer_factory.hpp:77] Creating layer pool0
I0428 20:26:23.296365  2783 net.cpp:86] Creating Layer pool0
I0428 20:26:23.296370  2783 net.cpp:408] pool0 <- conv0
I0428 20:26:23.296375  2783 net.cpp:382] pool0 -> pool0
I0428 20:26:23.296421  2783 net.cpp:124] Setting up pool0
I0428 20:26:23.296432  2783 net.cpp:131] Top shape: 64 100 12 12 (921600)
I0428 20:26:23.296435  2783 net.cpp:139] Memory required for data: 18632960
I0428 20:26:23.296438  2783 layer_factory.hpp:77] Creating layer conv1
I0428 20:26:23.296449  2783 net.cpp:86] Creating Layer conv1
I0428 20:26:23.296452  2783 net.cpp:408] conv1 <- pool0
I0428 20:26:23.296458  2783 net.cpp:382] conv1 -> conv1
I0428 20:26:23.299464  2783 net.cpp:124] Setting up conv1
I0428 20:26:23.299494  2783 net.cpp:131] Top shape: 64 2 8 8 (8192)
I0428 20:26:23.299497  2783 net.cpp:139] Memory required for data: 18665728
I0428 20:26:23.299506  2783 layer_factory.hpp:77] Creating layer pool1
I0428 20:26:23.299513  2783 net.cpp:86] Creating Layer pool1
I0428 20:26:23.299517  2783 net.cpp:408] pool1 <- conv1
I0428 20:26:23.299522  2783 net.cpp:382] pool1 -> pool1
I0428 20:26:23.299574  2783 net.cpp:124] Setting up pool1
I0428 20:26:23.299579  2783 net.cpp:131] Top shape: 64 2 4 4 (2048)
I0428 20:26:23.299582  2783 net.cpp:139] Memory required for data: 18673920
I0428 20:26:23.299585  2783 layer_factory.hpp:77] Creating layer ip1
I0428 20:26:23.299592  2783 net.cpp:86] Creating Layer ip1
I0428 20:26:23.299597  2783 net.cpp:408] ip1 <- pool1
I0428 20:26:23.299600  2783 net.cpp:382] ip1 -> ip1
I0428 20:26:23.299697  2783 net.cpp:124] Setting up ip1
I0428 20:26:23.299705  2783 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:26:23.299708  2783 net.cpp:139] Memory required for data: 18680320
I0428 20:26:23.299715  2783 layer_factory.hpp:77] Creating layer relu1
I0428 20:26:23.299721  2783 net.cpp:86] Creating Layer relu1
I0428 20:26:23.299724  2783 net.cpp:408] relu1 <- ip1
I0428 20:26:23.299728  2783 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:26:23.299888  2783 net.cpp:124] Setting up relu1
I0428 20:26:23.299896  2783 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:26:23.299901  2783 net.cpp:139] Memory required for data: 18686720
I0428 20:26:23.299903  2783 layer_factory.hpp:77] Creating layer ip2
I0428 20:26:23.299909  2783 net.cpp:86] Creating Layer ip2
I0428 20:26:23.299912  2783 net.cpp:408] ip2 <- ip1
I0428 20:26:23.299917  2783 net.cpp:382] ip2 -> ip2
I0428 20:26:23.300021  2783 net.cpp:124] Setting up ip2
I0428 20:26:23.300029  2783 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:26:23.300031  2783 net.cpp:139] Memory required for data: 18699520
I0428 20:26:23.300037  2783 layer_factory.hpp:77] Creating layer relu2
I0428 20:26:23.300043  2783 net.cpp:86] Creating Layer relu2
I0428 20:26:23.300046  2783 net.cpp:408] relu2 <- ip2
I0428 20:26:23.300065  2783 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:26:23.300890  2783 net.cpp:124] Setting up relu2
I0428 20:26:23.300904  2783 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:26:23.300909  2783 net.cpp:139] Memory required for data: 18712320
I0428 20:26:23.300912  2783 layer_factory.hpp:77] Creating layer ip3
I0428 20:26:23.300920  2783 net.cpp:86] Creating Layer ip3
I0428 20:26:23.300923  2783 net.cpp:408] ip3 <- ip2
I0428 20:26:23.300930  2783 net.cpp:382] ip3 -> ip3
I0428 20:26:23.301035  2783 net.cpp:124] Setting up ip3
I0428 20:26:23.301043  2783 net.cpp:131] Top shape: 64 10 (640)
I0428 20:26:23.301046  2783 net.cpp:139] Memory required for data: 18714880
I0428 20:26:23.301055  2783 layer_factory.hpp:77] Creating layer relu3
I0428 20:26:23.301061  2783 net.cpp:86] Creating Layer relu3
I0428 20:26:23.301064  2783 net.cpp:408] relu3 <- ip3
I0428 20:26:23.301069  2783 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:26:23.301304  2783 net.cpp:124] Setting up relu3
I0428 20:26:23.301313  2783 net.cpp:131] Top shape: 64 10 (640)
I0428 20:26:23.301317  2783 net.cpp:139] Memory required for data: 18717440
I0428 20:26:23.301321  2783 layer_factory.hpp:77] Creating layer loss
I0428 20:26:23.301326  2783 net.cpp:86] Creating Layer loss
I0428 20:26:23.301343  2783 net.cpp:408] loss <- ip3
I0428 20:26:23.301347  2783 net.cpp:408] loss <- label
I0428 20:26:23.301352  2783 net.cpp:382] loss -> loss
I0428 20:26:23.301370  2783 layer_factory.hpp:77] Creating layer loss
I0428 20:26:23.301599  2783 net.cpp:124] Setting up loss
I0428 20:26:23.301606  2783 net.cpp:131] Top shape: (1)
I0428 20:26:23.301610  2783 net.cpp:134]     with loss weight 1
I0428 20:26:23.301623  2783 net.cpp:139] Memory required for data: 18717444
I0428 20:26:23.301626  2783 net.cpp:200] loss needs backward computation.
I0428 20:26:23.301630  2783 net.cpp:200] relu3 needs backward computation.
I0428 20:26:23.301632  2783 net.cpp:200] ip3 needs backward computation.
I0428 20:26:23.301635  2783 net.cpp:200] relu2 needs backward computation.
I0428 20:26:23.301638  2783 net.cpp:200] ip2 needs backward computation.
I0428 20:26:23.301640  2783 net.cpp:200] relu1 needs backward computation.
I0428 20:26:23.301643  2783 net.cpp:200] ip1 needs backward computation.
I0428 20:26:23.301646  2783 net.cpp:200] pool1 needs backward computation.
I0428 20:26:23.301650  2783 net.cpp:200] conv1 needs backward computation.
I0428 20:26:23.301653  2783 net.cpp:200] pool0 needs backward computation.
I0428 20:26:23.301656  2783 net.cpp:200] conv0 needs backward computation.
I0428 20:26:23.301658  2783 net.cpp:202] mnist does not need backward computation.
I0428 20:26:23.301661  2783 net.cpp:244] This network produces output loss
I0428 20:26:23.301671  2783 net.cpp:257] Network initialization done.
I0428 20:26:23.302031  2783 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1470.prototxt
I0428 20:26:23.302059  2783 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:26:23.302157  2783 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:26:23.302260  2783 layer_factory.hpp:77] Creating layer mnist
I0428 20:26:23.302320  2783 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:26:23.302331  2783 net.cpp:86] Creating Layer mnist
I0428 20:26:23.302336  2783 net.cpp:382] mnist -> data
I0428 20:26:23.302345  2783 net.cpp:382] mnist -> label
I0428 20:26:23.302428  2783 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:26:23.304657  2783 net.cpp:124] Setting up mnist
I0428 20:26:23.304672  2783 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:26:23.304700  2783 net.cpp:131] Top shape: 100 (100)
I0428 20:26:23.304704  2783 net.cpp:139] Memory required for data: 314000
I0428 20:26:23.304708  2783 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:26:23.304715  2783 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:26:23.304718  2783 net.cpp:408] label_mnist_1_split <- label
I0428 20:26:23.304724  2783 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:26:23.304731  2783 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:26:23.304828  2783 net.cpp:124] Setting up label_mnist_1_split
I0428 20:26:23.304837  2783 net.cpp:131] Top shape: 100 (100)
I0428 20:26:23.304841  2783 net.cpp:131] Top shape: 100 (100)
I0428 20:26:23.304844  2783 net.cpp:139] Memory required for data: 314800
I0428 20:26:23.304847  2783 layer_factory.hpp:77] Creating layer conv0
I0428 20:26:23.304858  2783 net.cpp:86] Creating Layer conv0
I0428 20:26:23.304862  2783 net.cpp:408] conv0 <- data
I0428 20:26:23.304883  2783 net.cpp:382] conv0 -> conv0
I0428 20:26:23.306619  2783 net.cpp:124] Setting up conv0
I0428 20:26:23.306633  2783 net.cpp:131] Top shape: 100 100 24 24 (5760000)
I0428 20:26:23.306637  2783 net.cpp:139] Memory required for data: 23354800
I0428 20:26:23.306661  2783 layer_factory.hpp:77] Creating layer pool0
I0428 20:26:23.306669  2783 net.cpp:86] Creating Layer pool0
I0428 20:26:23.306673  2783 net.cpp:408] pool0 <- conv0
I0428 20:26:23.306677  2783 net.cpp:382] pool0 -> pool0
I0428 20:26:23.306715  2783 net.cpp:124] Setting up pool0
I0428 20:26:23.306721  2783 net.cpp:131] Top shape: 100 100 12 12 (1440000)
I0428 20:26:23.306725  2783 net.cpp:139] Memory required for data: 29114800
I0428 20:26:23.306727  2783 layer_factory.hpp:77] Creating layer conv1
I0428 20:26:23.306736  2783 net.cpp:86] Creating Layer conv1
I0428 20:26:23.306740  2783 net.cpp:408] conv1 <- pool0
I0428 20:26:23.306746  2783 net.cpp:382] conv1 -> conv1
I0428 20:26:23.308362  2783 net.cpp:124] Setting up conv1
I0428 20:26:23.308392  2783 net.cpp:131] Top shape: 100 2 8 8 (12800)
I0428 20:26:23.308394  2783 net.cpp:139] Memory required for data: 29166000
I0428 20:26:23.308419  2783 layer_factory.hpp:77] Creating layer pool1
I0428 20:26:23.308426  2783 net.cpp:86] Creating Layer pool1
I0428 20:26:23.308431  2783 net.cpp:408] pool1 <- conv1
I0428 20:26:23.308447  2783 net.cpp:382] pool1 -> pool1
I0428 20:26:23.308486  2783 net.cpp:124] Setting up pool1
I0428 20:26:23.308491  2783 net.cpp:131] Top shape: 100 2 4 4 (3200)
I0428 20:26:23.308495  2783 net.cpp:139] Memory required for data: 29178800
I0428 20:26:23.308497  2783 layer_factory.hpp:77] Creating layer ip1
I0428 20:26:23.308503  2783 net.cpp:86] Creating Layer ip1
I0428 20:26:23.308507  2783 net.cpp:408] ip1 <- pool1
I0428 20:26:23.308512  2783 net.cpp:382] ip1 -> ip1
I0428 20:26:23.308614  2783 net.cpp:124] Setting up ip1
I0428 20:26:23.308632  2783 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:26:23.308647  2783 net.cpp:139] Memory required for data: 29188800
I0428 20:26:23.308657  2783 layer_factory.hpp:77] Creating layer relu1
I0428 20:26:23.308665  2783 net.cpp:86] Creating Layer relu1
I0428 20:26:23.308675  2783 net.cpp:408] relu1 <- ip1
I0428 20:26:23.308679  2783 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:26:23.308868  2783 net.cpp:124] Setting up relu1
I0428 20:26:23.308879  2783 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:26:23.308883  2783 net.cpp:139] Memory required for data: 29198800
I0428 20:26:23.308887  2783 layer_factory.hpp:77] Creating layer ip2
I0428 20:26:23.308894  2783 net.cpp:86] Creating Layer ip2
I0428 20:26:23.308897  2783 net.cpp:408] ip2 <- ip1
I0428 20:26:23.308904  2783 net.cpp:382] ip2 -> ip2
I0428 20:26:23.309021  2783 net.cpp:124] Setting up ip2
I0428 20:26:23.309029  2783 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:26:23.309033  2783 net.cpp:139] Memory required for data: 29218800
I0428 20:26:23.309038  2783 layer_factory.hpp:77] Creating layer relu2
I0428 20:26:23.309046  2783 net.cpp:86] Creating Layer relu2
I0428 20:26:23.309048  2783 net.cpp:408] relu2 <- ip2
I0428 20:26:23.309052  2783 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:26:23.309303  2783 net.cpp:124] Setting up relu2
I0428 20:26:23.309312  2783 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:26:23.309315  2783 net.cpp:139] Memory required for data: 29238800
I0428 20:26:23.309319  2783 layer_factory.hpp:77] Creating layer ip3
I0428 20:26:23.309325  2783 net.cpp:86] Creating Layer ip3
I0428 20:26:23.309329  2783 net.cpp:408] ip3 <- ip2
I0428 20:26:23.309334  2783 net.cpp:382] ip3 -> ip3
I0428 20:26:23.309469  2783 net.cpp:124] Setting up ip3
I0428 20:26:23.309476  2783 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:26:23.309479  2783 net.cpp:139] Memory required for data: 29242800
I0428 20:26:23.309487  2783 layer_factory.hpp:77] Creating layer relu3
I0428 20:26:23.309492  2783 net.cpp:86] Creating Layer relu3
I0428 20:26:23.309495  2783 net.cpp:408] relu3 <- ip3
I0428 20:26:23.309501  2783 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:26:23.310405  2783 net.cpp:124] Setting up relu3
I0428 20:26:23.310431  2783 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:26:23.310434  2783 net.cpp:139] Memory required for data: 29246800
I0428 20:26:23.310438  2783 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:26:23.310444  2783 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:26:23.310447  2783 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:26:23.310454  2783 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:26:23.310461  2783 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:26:23.310535  2783 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:26:23.310544  2783 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:26:23.310547  2783 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:26:23.310550  2783 net.cpp:139] Memory required for data: 29254800
I0428 20:26:23.310554  2783 layer_factory.hpp:77] Creating layer accuracy
I0428 20:26:23.310559  2783 net.cpp:86] Creating Layer accuracy
I0428 20:26:23.310561  2783 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:26:23.310566  2783 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:26:23.310570  2783 net.cpp:382] accuracy -> accuracy
I0428 20:26:23.310577  2783 net.cpp:124] Setting up accuracy
I0428 20:26:23.310581  2783 net.cpp:131] Top shape: (1)
I0428 20:26:23.310585  2783 net.cpp:139] Memory required for data: 29254804
I0428 20:26:23.310587  2783 layer_factory.hpp:77] Creating layer loss
I0428 20:26:23.310593  2783 net.cpp:86] Creating Layer loss
I0428 20:26:23.310596  2783 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:26:23.310606  2783 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:26:23.310611  2783 net.cpp:382] loss -> loss
I0428 20:26:23.310616  2783 layer_factory.hpp:77] Creating layer loss
I0428 20:26:23.310883  2783 net.cpp:124] Setting up loss
I0428 20:26:23.310892  2783 net.cpp:131] Top shape: (1)
I0428 20:26:23.310896  2783 net.cpp:134]     with loss weight 1
I0428 20:26:23.310912  2783 net.cpp:139] Memory required for data: 29254808
I0428 20:26:23.310916  2783 net.cpp:200] loss needs backward computation.
I0428 20:26:23.310920  2783 net.cpp:202] accuracy does not need backward computation.
I0428 20:26:23.310925  2783 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:26:23.310935  2783 net.cpp:200] relu3 needs backward computation.
I0428 20:26:23.310937  2783 net.cpp:200] ip3 needs backward computation.
I0428 20:26:23.310941  2783 net.cpp:200] relu2 needs backward computation.
I0428 20:26:23.310943  2783 net.cpp:200] ip2 needs backward computation.
I0428 20:26:23.310946  2783 net.cpp:200] relu1 needs backward computation.
I0428 20:26:23.310950  2783 net.cpp:200] ip1 needs backward computation.
I0428 20:26:23.310953  2783 net.cpp:200] pool1 needs backward computation.
I0428 20:26:23.310958  2783 net.cpp:200] conv1 needs backward computation.
I0428 20:26:23.310961  2783 net.cpp:200] pool0 needs backward computation.
I0428 20:26:23.310964  2783 net.cpp:200] conv0 needs backward computation.
I0428 20:26:23.310968  2783 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:26:23.310972  2783 net.cpp:202] mnist does not need backward computation.
I0428 20:26:23.310976  2783 net.cpp:244] This network produces output accuracy
I0428 20:26:23.310979  2783 net.cpp:244] This network produces output loss
I0428 20:26:23.310992  2783 net.cpp:257] Network initialization done.
I0428 20:26:23.311045  2783 solver.cpp:56] Solver scaffolding done.
I0428 20:26:23.311425  2783 caffe.cpp:248] Starting Optimization
I0428 20:26:23.311430  2783 solver.cpp:273] Solving LeNet
I0428 20:26:23.311434  2783 solver.cpp:274] Learning Rate Policy: inv
I0428 20:26:23.312331  2783 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:26:23.406848  2790 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:26:23.409304  2783 solver.cpp:398]     Test net output #0: accuracy = 0.0685
I0428 20:26:23.409338  2783 solver.cpp:398]     Test net output #1: loss = 2.30918 (* 1 = 2.30918 loss)
I0428 20:26:23.413832  2783 solver.cpp:219] Iteration 0 (0 iter/s, 0.102353s/100 iters), loss = 2.30316
I0428 20:26:23.413856  2783 solver.cpp:238]     Train net output #0: loss = 2.30316 (* 1 = 2.30316 loss)
I0428 20:26:23.413867  2783 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:26:23.613821  2783 solver.cpp:219] Iteration 100 (500.137 iter/s, 0.199945s/100 iters), loss = 1.16826
I0428 20:26:23.613849  2783 solver.cpp:238]     Train net output #0: loss = 1.16826 (* 1 = 1.16826 loss)
I0428 20:26:23.613857  2783 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:26:23.812942  2783 solver.cpp:219] Iteration 200 (502.319 iter/s, 0.199077s/100 iters), loss = 0.643274
I0428 20:26:23.812971  2783 solver.cpp:238]     Train net output #0: loss = 0.643274 (* 1 = 0.643274 loss)
I0428 20:26:23.812978  2783 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:26:24.030472  2783 solver.cpp:219] Iteration 300 (459.81 iter/s, 0.217481s/100 iters), loss = 0.762544
I0428 20:26:24.030517  2783 solver.cpp:238]     Train net output #0: loss = 0.762544 (* 1 = 0.762544 loss)
I0428 20:26:24.030530  2783 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:26:24.253651  2783 solver.cpp:219] Iteration 400 (448.203 iter/s, 0.223113s/100 iters), loss = 0.47586
I0428 20:26:24.253703  2783 solver.cpp:238]     Train net output #0: loss = 0.47586 (* 1 = 0.47586 loss)
I0428 20:26:24.253716  2783 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:26:24.472614  2783 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:26:24.575358  2790 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:26:24.578778  2783 solver.cpp:398]     Test net output #0: accuracy = 0.747
I0428 20:26:24.578806  2783 solver.cpp:398]     Test net output #1: loss = 0.66017 (* 1 = 0.66017 loss)
I0428 20:26:24.580737  2783 solver.cpp:219] Iteration 500 (305.795 iter/s, 0.327016s/100 iters), loss = 0.694875
I0428 20:26:24.580765  2783 solver.cpp:238]     Train net output #0: loss = 0.694875 (* 1 = 0.694875 loss)
I0428 20:26:24.580795  2783 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:26:24.792853  2783 solver.cpp:219] Iteration 600 (471.551 iter/s, 0.212066s/100 iters), loss = 0.650215
I0428 20:26:24.792904  2783 solver.cpp:238]     Train net output #0: loss = 0.650215 (* 1 = 0.650215 loss)
I0428 20:26:24.792917  2783 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:26:25.012178  2783 solver.cpp:219] Iteration 700 (456.084 iter/s, 0.219258s/100 iters), loss = 0.631317
I0428 20:26:25.012225  2783 solver.cpp:238]     Train net output #0: loss = 0.631317 (* 1 = 0.631317 loss)
I0428 20:26:25.012238  2783 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:26:25.227412  2783 solver.cpp:219] Iteration 800 (464.744 iter/s, 0.215172s/100 iters), loss = 0.582885
I0428 20:26:25.227455  2783 solver.cpp:238]     Train net output #0: loss = 0.582885 (* 1 = 0.582885 loss)
I0428 20:26:25.227468  2783 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:26:25.432163  2783 solver.cpp:219] Iteration 900 (488.545 iter/s, 0.204689s/100 iters), loss = 0.399605
I0428 20:26:25.432209  2783 solver.cpp:238]     Train net output #0: loss = 0.399605 (* 1 = 0.399605 loss)
I0428 20:26:25.432220  2783 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:26:25.502257  2789 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:26:25.639787  2783 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:26:25.641518  2783 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:26:25.642570  2783 solver.cpp:311] Iteration 1000, loss = 0.156603
I0428 20:26:25.642588  2783 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:26:25.745470  2790 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:26:25.748872  2783 solver.cpp:398]     Test net output #0: accuracy = 0.9498
I0428 20:26:25.748906  2783 solver.cpp:398]     Test net output #1: loss = 0.167082 (* 1 = 0.167082 loss)
I0428 20:26:25.748916  2783 solver.cpp:316] Optimization Done.
I0428 20:26:25.748922  2783 caffe.cpp:259] Optimization Done.
