I0428 20:02:27.383064 29949 caffe.cpp:218] Using GPUs 0
I0428 20:02:27.412808 29949 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:02:27.926913 29949 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test929.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:02:27.927048 29949 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test929.prototxt
I0428 20:02:27.927465 29949 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:02:27.927489 29949 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:02:27.927590 29949 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:02:27.927670 29949 layer_factory.hpp:77] Creating layer mnist
I0428 20:02:27.927776 29949 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:02:27.927800 29949 net.cpp:86] Creating Layer mnist
I0428 20:02:27.927810 29949 net.cpp:382] mnist -> data
I0428 20:02:27.927831 29949 net.cpp:382] mnist -> label
I0428 20:02:27.928928 29949 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:02:27.931401 29949 net.cpp:124] Setting up mnist
I0428 20:02:27.931418 29949 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:02:27.931424 29949 net.cpp:131] Top shape: 64 (64)
I0428 20:02:27.931427 29949 net.cpp:139] Memory required for data: 200960
I0428 20:02:27.931434 29949 layer_factory.hpp:77] Creating layer conv0
I0428 20:02:27.931478 29949 net.cpp:86] Creating Layer conv0
I0428 20:02:27.931499 29949 net.cpp:408] conv0 <- data
I0428 20:02:27.931514 29949 net.cpp:382] conv0 -> conv0
I0428 20:02:28.224200 29949 net.cpp:124] Setting up conv0
I0428 20:02:28.224230 29949 net.cpp:131] Top shape: 64 10 24 24 (368640)
I0428 20:02:28.224234 29949 net.cpp:139] Memory required for data: 1675520
I0428 20:02:28.224253 29949 layer_factory.hpp:77] Creating layer pool0
I0428 20:02:28.224269 29949 net.cpp:86] Creating Layer pool0
I0428 20:02:28.224274 29949 net.cpp:408] pool0 <- conv0
I0428 20:02:28.224282 29949 net.cpp:382] pool0 -> pool0
I0428 20:02:28.224336 29949 net.cpp:124] Setting up pool0
I0428 20:02:28.224345 29949 net.cpp:131] Top shape: 64 10 12 12 (92160)
I0428 20:02:28.224349 29949 net.cpp:139] Memory required for data: 2044160
I0428 20:02:28.224352 29949 layer_factory.hpp:77] Creating layer conv1
I0428 20:02:28.224365 29949 net.cpp:86] Creating Layer conv1
I0428 20:02:28.224370 29949 net.cpp:408] conv1 <- pool0
I0428 20:02:28.224375 29949 net.cpp:382] conv1 -> conv1
I0428 20:02:28.227612 29949 net.cpp:124] Setting up conv1
I0428 20:02:28.227629 29949 net.cpp:131] Top shape: 64 50 8 8 (204800)
I0428 20:02:28.227634 29949 net.cpp:139] Memory required for data: 2863360
I0428 20:02:28.227645 29949 layer_factory.hpp:77] Creating layer pool1
I0428 20:02:28.227654 29949 net.cpp:86] Creating Layer pool1
I0428 20:02:28.227658 29949 net.cpp:408] pool1 <- conv1
I0428 20:02:28.227664 29949 net.cpp:382] pool1 -> pool1
I0428 20:02:28.227710 29949 net.cpp:124] Setting up pool1
I0428 20:02:28.227720 29949 net.cpp:131] Top shape: 64 50 4 4 (51200)
I0428 20:02:28.227722 29949 net.cpp:139] Memory required for data: 3068160
I0428 20:02:28.227726 29949 layer_factory.hpp:77] Creating layer ip1
I0428 20:02:28.227735 29949 net.cpp:86] Creating Layer ip1
I0428 20:02:28.227740 29949 net.cpp:408] ip1 <- pool1
I0428 20:02:28.227744 29949 net.cpp:382] ip1 -> ip1
I0428 20:02:28.228135 29949 net.cpp:124] Setting up ip1
I0428 20:02:28.228145 29949 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:02:28.228149 29949 net.cpp:139] Memory required for data: 3080960
I0428 20:02:28.228157 29949 layer_factory.hpp:77] Creating layer relu1
I0428 20:02:28.228164 29949 net.cpp:86] Creating Layer relu1
I0428 20:02:28.228168 29949 net.cpp:408] relu1 <- ip1
I0428 20:02:28.228173 29949 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:02:28.228373 29949 net.cpp:124] Setting up relu1
I0428 20:02:28.228384 29949 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:02:28.228387 29949 net.cpp:139] Memory required for data: 3093760
I0428 20:02:28.228390 29949 layer_factory.hpp:77] Creating layer ip2
I0428 20:02:28.228399 29949 net.cpp:86] Creating Layer ip2
I0428 20:02:28.228402 29949 net.cpp:408] ip2 <- ip1
I0428 20:02:28.228407 29949 net.cpp:382] ip2 -> ip2
I0428 20:02:28.228525 29949 net.cpp:124] Setting up ip2
I0428 20:02:28.228533 29949 net.cpp:131] Top shape: 64 10 (640)
I0428 20:02:28.228538 29949 net.cpp:139] Memory required for data: 3096320
I0428 20:02:28.228543 29949 layer_factory.hpp:77] Creating layer relu2
I0428 20:02:28.228550 29949 net.cpp:86] Creating Layer relu2
I0428 20:02:28.228554 29949 net.cpp:408] relu2 <- ip2
I0428 20:02:28.228559 29949 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:02:28.229423 29949 net.cpp:124] Setting up relu2
I0428 20:02:28.229439 29949 net.cpp:131] Top shape: 64 10 (640)
I0428 20:02:28.229444 29949 net.cpp:139] Memory required for data: 3098880
I0428 20:02:28.229447 29949 layer_factory.hpp:77] Creating layer ip3
I0428 20:02:28.229456 29949 net.cpp:86] Creating Layer ip3
I0428 20:02:28.229460 29949 net.cpp:408] ip3 <- ip2
I0428 20:02:28.229466 29949 net.cpp:382] ip3 -> ip3
I0428 20:02:28.229586 29949 net.cpp:124] Setting up ip3
I0428 20:02:28.229595 29949 net.cpp:131] Top shape: 64 10 (640)
I0428 20:02:28.229598 29949 net.cpp:139] Memory required for data: 3101440
I0428 20:02:28.229609 29949 layer_factory.hpp:77] Creating layer relu3
I0428 20:02:28.229614 29949 net.cpp:86] Creating Layer relu3
I0428 20:02:28.229619 29949 net.cpp:408] relu3 <- ip3
I0428 20:02:28.229624 29949 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:02:28.229830 29949 net.cpp:124] Setting up relu3
I0428 20:02:28.229840 29949 net.cpp:131] Top shape: 64 10 (640)
I0428 20:02:28.229843 29949 net.cpp:139] Memory required for data: 3104000
I0428 20:02:28.229847 29949 layer_factory.hpp:77] Creating layer loss
I0428 20:02:28.229854 29949 net.cpp:86] Creating Layer loss
I0428 20:02:28.229857 29949 net.cpp:408] loss <- ip3
I0428 20:02:28.229862 29949 net.cpp:408] loss <- label
I0428 20:02:28.229869 29949 net.cpp:382] loss -> loss
I0428 20:02:28.229885 29949 layer_factory.hpp:77] Creating layer loss
I0428 20:02:28.230170 29949 net.cpp:124] Setting up loss
I0428 20:02:28.230182 29949 net.cpp:131] Top shape: (1)
I0428 20:02:28.230186 29949 net.cpp:134]     with loss weight 1
I0428 20:02:28.230207 29949 net.cpp:139] Memory required for data: 3104004
I0428 20:02:28.230212 29949 net.cpp:200] loss needs backward computation.
I0428 20:02:28.230216 29949 net.cpp:200] relu3 needs backward computation.
I0428 20:02:28.230221 29949 net.cpp:200] ip3 needs backward computation.
I0428 20:02:28.230224 29949 net.cpp:200] relu2 needs backward computation.
I0428 20:02:28.230227 29949 net.cpp:200] ip2 needs backward computation.
I0428 20:02:28.230232 29949 net.cpp:200] relu1 needs backward computation.
I0428 20:02:28.230235 29949 net.cpp:200] ip1 needs backward computation.
I0428 20:02:28.230239 29949 net.cpp:200] pool1 needs backward computation.
I0428 20:02:28.230243 29949 net.cpp:200] conv1 needs backward computation.
I0428 20:02:28.230247 29949 net.cpp:200] pool0 needs backward computation.
I0428 20:02:28.230252 29949 net.cpp:200] conv0 needs backward computation.
I0428 20:02:28.230255 29949 net.cpp:202] mnist does not need backward computation.
I0428 20:02:28.230258 29949 net.cpp:244] This network produces output loss
I0428 20:02:28.230269 29949 net.cpp:257] Network initialization done.
I0428 20:02:28.230655 29949 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test929.prototxt
I0428 20:02:28.230687 29949 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:02:28.230796 29949 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:02:28.230890 29949 layer_factory.hpp:77] Creating layer mnist
I0428 20:02:28.230943 29949 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:02:28.230958 29949 net.cpp:86] Creating Layer mnist
I0428 20:02:28.230964 29949 net.cpp:382] mnist -> data
I0428 20:02:28.230973 29949 net.cpp:382] mnist -> label
I0428 20:02:28.231073 29949 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:02:28.233294 29949 net.cpp:124] Setting up mnist
I0428 20:02:28.233311 29949 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:02:28.233319 29949 net.cpp:131] Top shape: 100 (100)
I0428 20:02:28.233321 29949 net.cpp:139] Memory required for data: 314000
I0428 20:02:28.233326 29949 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:02:28.233335 29949 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:02:28.233340 29949 net.cpp:408] label_mnist_1_split <- label
I0428 20:02:28.233345 29949 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:02:28.233352 29949 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:02:28.233444 29949 net.cpp:124] Setting up label_mnist_1_split
I0428 20:02:28.233453 29949 net.cpp:131] Top shape: 100 (100)
I0428 20:02:28.233458 29949 net.cpp:131] Top shape: 100 (100)
I0428 20:02:28.233461 29949 net.cpp:139] Memory required for data: 314800
I0428 20:02:28.233465 29949 layer_factory.hpp:77] Creating layer conv0
I0428 20:02:28.233475 29949 net.cpp:86] Creating Layer conv0
I0428 20:02:28.233484 29949 net.cpp:408] conv0 <- data
I0428 20:02:28.233489 29949 net.cpp:382] conv0 -> conv0
I0428 20:02:28.235383 29949 net.cpp:124] Setting up conv0
I0428 20:02:28.235400 29949 net.cpp:131] Top shape: 100 10 24 24 (576000)
I0428 20:02:28.235404 29949 net.cpp:139] Memory required for data: 2618800
I0428 20:02:28.235415 29949 layer_factory.hpp:77] Creating layer pool0
I0428 20:02:28.235424 29949 net.cpp:86] Creating Layer pool0
I0428 20:02:28.235427 29949 net.cpp:408] pool0 <- conv0
I0428 20:02:28.235433 29949 net.cpp:382] pool0 -> pool0
I0428 20:02:28.235476 29949 net.cpp:124] Setting up pool0
I0428 20:02:28.235484 29949 net.cpp:131] Top shape: 100 10 12 12 (144000)
I0428 20:02:28.235488 29949 net.cpp:139] Memory required for data: 3194800
I0428 20:02:28.235492 29949 layer_factory.hpp:77] Creating layer conv1
I0428 20:02:28.235502 29949 net.cpp:86] Creating Layer conv1
I0428 20:02:28.235509 29949 net.cpp:408] conv1 <- pool0
I0428 20:02:28.235515 29949 net.cpp:382] conv1 -> conv1
I0428 20:02:28.237323 29949 net.cpp:124] Setting up conv1
I0428 20:02:28.237339 29949 net.cpp:131] Top shape: 100 50 8 8 (320000)
I0428 20:02:28.237344 29949 net.cpp:139] Memory required for data: 4474800
I0428 20:02:28.237354 29949 layer_factory.hpp:77] Creating layer pool1
I0428 20:02:28.237361 29949 net.cpp:86] Creating Layer pool1
I0428 20:02:28.237365 29949 net.cpp:408] pool1 <- conv1
I0428 20:02:28.237375 29949 net.cpp:382] pool1 -> pool1
I0428 20:02:28.237426 29949 net.cpp:124] Setting up pool1
I0428 20:02:28.237432 29949 net.cpp:131] Top shape: 100 50 4 4 (80000)
I0428 20:02:28.237437 29949 net.cpp:139] Memory required for data: 4794800
I0428 20:02:28.237440 29949 layer_factory.hpp:77] Creating layer ip1
I0428 20:02:28.237450 29949 net.cpp:86] Creating Layer ip1
I0428 20:02:28.237454 29949 net.cpp:408] ip1 <- pool1
I0428 20:02:28.237462 29949 net.cpp:382] ip1 -> ip1
I0428 20:02:28.237895 29949 net.cpp:124] Setting up ip1
I0428 20:02:28.237905 29949 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:02:28.237920 29949 net.cpp:139] Memory required for data: 4814800
I0428 20:02:28.237931 29949 layer_factory.hpp:77] Creating layer relu1
I0428 20:02:28.237936 29949 net.cpp:86] Creating Layer relu1
I0428 20:02:28.237941 29949 net.cpp:408] relu1 <- ip1
I0428 20:02:28.237948 29949 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:02:28.238155 29949 net.cpp:124] Setting up relu1
I0428 20:02:28.238165 29949 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:02:28.238169 29949 net.cpp:139] Memory required for data: 4834800
I0428 20:02:28.238173 29949 layer_factory.hpp:77] Creating layer ip2
I0428 20:02:28.238183 29949 net.cpp:86] Creating Layer ip2
I0428 20:02:28.238186 29949 net.cpp:408] ip2 <- ip1
I0428 20:02:28.238191 29949 net.cpp:382] ip2 -> ip2
I0428 20:02:28.238317 29949 net.cpp:124] Setting up ip2
I0428 20:02:28.238325 29949 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:02:28.238329 29949 net.cpp:139] Memory required for data: 4838800
I0428 20:02:28.238335 29949 layer_factory.hpp:77] Creating layer relu2
I0428 20:02:28.238344 29949 net.cpp:86] Creating Layer relu2
I0428 20:02:28.238349 29949 net.cpp:408] relu2 <- ip2
I0428 20:02:28.238354 29949 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:02:28.238548 29949 net.cpp:124] Setting up relu2
I0428 20:02:28.238559 29949 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:02:28.238562 29949 net.cpp:139] Memory required for data: 4842800
I0428 20:02:28.238566 29949 layer_factory.hpp:77] Creating layer ip3
I0428 20:02:28.238574 29949 net.cpp:86] Creating Layer ip3
I0428 20:02:28.238577 29949 net.cpp:408] ip3 <- ip2
I0428 20:02:28.238584 29949 net.cpp:382] ip3 -> ip3
I0428 20:02:28.238703 29949 net.cpp:124] Setting up ip3
I0428 20:02:28.238711 29949 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:02:28.238714 29949 net.cpp:139] Memory required for data: 4846800
I0428 20:02:28.238724 29949 layer_factory.hpp:77] Creating layer relu3
I0428 20:02:28.238731 29949 net.cpp:86] Creating Layer relu3
I0428 20:02:28.238744 29949 net.cpp:408] relu3 <- ip3
I0428 20:02:28.238747 29949 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:02:28.239689 29949 net.cpp:124] Setting up relu3
I0428 20:02:28.239706 29949 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:02:28.239709 29949 net.cpp:139] Memory required for data: 4850800
I0428 20:02:28.239713 29949 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:02:28.239719 29949 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:02:28.239723 29949 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:02:28.239729 29949 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:02:28.239737 29949 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:02:28.239794 29949 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:02:28.239804 29949 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:02:28.239807 29949 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:02:28.239811 29949 net.cpp:139] Memory required for data: 4858800
I0428 20:02:28.239814 29949 layer_factory.hpp:77] Creating layer accuracy
I0428 20:02:28.239821 29949 net.cpp:86] Creating Layer accuracy
I0428 20:02:28.239825 29949 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:02:28.239837 29949 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:02:28.239843 29949 net.cpp:382] accuracy -> accuracy
I0428 20:02:28.239851 29949 net.cpp:124] Setting up accuracy
I0428 20:02:28.239856 29949 net.cpp:131] Top shape: (1)
I0428 20:02:28.239861 29949 net.cpp:139] Memory required for data: 4858804
I0428 20:02:28.239864 29949 layer_factory.hpp:77] Creating layer loss
I0428 20:02:28.239871 29949 net.cpp:86] Creating Layer loss
I0428 20:02:28.239874 29949 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:02:28.239879 29949 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:02:28.239884 29949 net.cpp:382] loss -> loss
I0428 20:02:28.239892 29949 layer_factory.hpp:77] Creating layer loss
I0428 20:02:28.240180 29949 net.cpp:124] Setting up loss
I0428 20:02:28.240195 29949 net.cpp:131] Top shape: (1)
I0428 20:02:28.240198 29949 net.cpp:134]     with loss weight 1
I0428 20:02:28.240218 29949 net.cpp:139] Memory required for data: 4858808
I0428 20:02:28.240222 29949 net.cpp:200] loss needs backward computation.
I0428 20:02:28.240228 29949 net.cpp:202] accuracy does not need backward computation.
I0428 20:02:28.240233 29949 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:02:28.240236 29949 net.cpp:200] relu3 needs backward computation.
I0428 20:02:28.240241 29949 net.cpp:200] ip3 needs backward computation.
I0428 20:02:28.240244 29949 net.cpp:200] relu2 needs backward computation.
I0428 20:02:28.240247 29949 net.cpp:200] ip2 needs backward computation.
I0428 20:02:28.240252 29949 net.cpp:200] relu1 needs backward computation.
I0428 20:02:28.240254 29949 net.cpp:200] ip1 needs backward computation.
I0428 20:02:28.240265 29949 net.cpp:200] pool1 needs backward computation.
I0428 20:02:28.240269 29949 net.cpp:200] conv1 needs backward computation.
I0428 20:02:28.240273 29949 net.cpp:200] pool0 needs backward computation.
I0428 20:02:28.240276 29949 net.cpp:200] conv0 needs backward computation.
I0428 20:02:28.240281 29949 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:02:28.240286 29949 net.cpp:202] mnist does not need backward computation.
I0428 20:02:28.240289 29949 net.cpp:244] This network produces output accuracy
I0428 20:02:28.240293 29949 net.cpp:244] This network produces output loss
I0428 20:02:28.240308 29949 net.cpp:257] Network initialization done.
I0428 20:02:28.240357 29949 solver.cpp:56] Solver scaffolding done.
I0428 20:02:28.240797 29949 caffe.cpp:248] Starting Optimization
I0428 20:02:28.240804 29949 solver.cpp:273] Solving LeNet
I0428 20:02:28.240808 29949 solver.cpp:274] Learning Rate Policy: inv
I0428 20:02:28.241819 29949 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:02:28.246353 29949 blocking_queue.cpp:49] Waiting for data
I0428 20:02:28.319067 29956 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:02:28.319622 29949 solver.cpp:398]     Test net output #0: accuracy = 0.1408
I0428 20:02:28.319646 29949 solver.cpp:398]     Test net output #1: loss = 2.29659 (* 1 = 2.29659 loss)
I0428 20:02:28.323048 29949 solver.cpp:219] Iteration 0 (0 iter/s, 0.0821905s/100 iters), loss = 2.3052
I0428 20:02:28.323078 29949 solver.cpp:238]     Train net output #0: loss = 2.3052 (* 1 = 2.3052 loss)
I0428 20:02:28.323092 29949 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:02:28.432209 29949 solver.cpp:219] Iteration 100 (916.46 iter/s, 0.109116s/100 iters), loss = 1.93346
I0428 20:02:28.432235 29949 solver.cpp:238]     Train net output #0: loss = 1.93346 (* 1 = 1.93346 loss)
I0428 20:02:28.432240 29949 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:02:28.541203 29949 solver.cpp:219] Iteration 200 (917.943 iter/s, 0.108939s/100 iters), loss = 1.67178
I0428 20:02:28.541273 29949 solver.cpp:238]     Train net output #0: loss = 1.67178 (* 1 = 1.67178 loss)
I0428 20:02:28.541280 29949 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:02:28.648901 29949 solver.cpp:219] Iteration 300 (929.22 iter/s, 0.107617s/100 iters), loss = 1.57761
I0428 20:02:28.648941 29949 solver.cpp:238]     Train net output #0: loss = 1.57761 (* 1 = 1.57761 loss)
I0428 20:02:28.648947 29949 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:02:28.759063 29949 solver.cpp:219] Iteration 400 (908.168 iter/s, 0.110112s/100 iters), loss = 1.4506
I0428 20:02:28.759104 29949 solver.cpp:238]     Train net output #0: loss = 1.4506 (* 1 = 1.4506 loss)
I0428 20:02:28.759125 29949 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:02:28.865680 29949 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:02:28.911463 29956 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:02:28.912005 29949 solver.cpp:398]     Test net output #0: accuracy = 0.4543
I0428 20:02:28.912024 29949 solver.cpp:398]     Test net output #1: loss = 1.49973 (* 1 = 1.49973 loss)
I0428 20:02:28.913075 29949 solver.cpp:219] Iteration 500 (649.461 iter/s, 0.153974s/100 iters), loss = 1.40479
I0428 20:02:28.913126 29949 solver.cpp:238]     Train net output #0: loss = 1.40479 (* 1 = 1.40479 loss)
I0428 20:02:28.913148 29949 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:02:29.021814 29949 solver.cpp:219] Iteration 600 (920.14 iter/s, 0.108679s/100 iters), loss = 1.731
I0428 20:02:29.021854 29949 solver.cpp:238]     Train net output #0: loss = 1.731 (* 1 = 1.731 loss)
I0428 20:02:29.021863 29949 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:02:29.129205 29949 solver.cpp:219] Iteration 700 (931.488 iter/s, 0.107355s/100 iters), loss = 1.27505
I0428 20:02:29.129230 29949 solver.cpp:238]     Train net output #0: loss = 1.27505 (* 1 = 1.27505 loss)
I0428 20:02:29.129235 29949 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:02:29.239248 29949 solver.cpp:219] Iteration 800 (909.031 iter/s, 0.110007s/100 iters), loss = 1.68517
I0428 20:02:29.239272 29949 solver.cpp:238]     Train net output #0: loss = 1.68517 (* 1 = 1.68517 loss)
I0428 20:02:29.239284 29949 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:02:29.348894 29949 solver.cpp:219] Iteration 900 (912.317 iter/s, 0.109611s/100 iters), loss = 1.63772
I0428 20:02:29.348933 29949 solver.cpp:238]     Train net output #0: loss = 1.63772 (* 1 = 1.63772 loss)
I0428 20:02:29.348938 29949 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:02:29.384346 29955 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:02:29.455067 29949 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:02:29.456821 29949 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:02:29.457686 29949 solver.cpp:311] Iteration 1000, loss = 1.44665
I0428 20:02:29.457700 29949 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:02:29.527444 29956 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:02:29.527966 29949 solver.cpp:398]     Test net output #0: accuracy = 0.4695
I0428 20:02:29.527987 29949 solver.cpp:398]     Test net output #1: loss = 1.4736 (* 1 = 1.4736 loss)
I0428 20:02:29.527992 29949 solver.cpp:316] Optimization Done.
I0428 20:02:29.527995 29949 caffe.cpp:259] Optimization Done.
