I0428 19:48:13.215306 26428 caffe.cpp:218] Using GPUs 0
I0428 19:48:13.256588 26428 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:48:13.725703 26428 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test538.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:48:13.725836 26428 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test538.prototxt
I0428 19:48:13.726173 26428 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:48:13.726188 26428 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:48:13.726270 26428 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:48:13.726336 26428 layer_factory.hpp:77] Creating layer mnist
I0428 19:48:13.726433 26428 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:48:13.726452 26428 net.cpp:86] Creating Layer mnist
I0428 19:48:13.726459 26428 net.cpp:382] mnist -> data
I0428 19:48:13.726480 26428 net.cpp:382] mnist -> label
I0428 19:48:13.727455 26428 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:48:13.729645 26428 net.cpp:124] Setting up mnist
I0428 19:48:13.729660 26428 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:48:13.729668 26428 net.cpp:131] Top shape: 64 (64)
I0428 19:48:13.729671 26428 net.cpp:139] Memory required for data: 200960
I0428 19:48:13.729676 26428 layer_factory.hpp:77] Creating layer conv0
I0428 19:48:13.729689 26428 net.cpp:86] Creating Layer conv0
I0428 19:48:13.729707 26428 net.cpp:408] conv0 <- data
I0428 19:48:13.729718 26428 net.cpp:382] conv0 -> conv0
I0428 19:48:13.964871 26428 net.cpp:124] Setting up conv0
I0428 19:48:13.964913 26428 net.cpp:131] Top shape: 64 5 24 24 (184320)
I0428 19:48:13.964918 26428 net.cpp:139] Memory required for data: 938240
I0428 19:48:13.964932 26428 layer_factory.hpp:77] Creating layer pool0
I0428 19:48:13.964946 26428 net.cpp:86] Creating Layer pool0
I0428 19:48:13.964949 26428 net.cpp:408] pool0 <- conv0
I0428 19:48:13.964956 26428 net.cpp:382] pool0 -> pool0
I0428 19:48:13.965019 26428 net.cpp:124] Setting up pool0
I0428 19:48:13.965024 26428 net.cpp:131] Top shape: 64 5 12 12 (46080)
I0428 19:48:13.965028 26428 net.cpp:139] Memory required for data: 1122560
I0428 19:48:13.965030 26428 layer_factory.hpp:77] Creating layer conv1
I0428 19:48:13.965041 26428 net.cpp:86] Creating Layer conv1
I0428 19:48:13.965044 26428 net.cpp:408] conv1 <- pool0
I0428 19:48:13.965049 26428 net.cpp:382] conv1 -> conv1
I0428 19:48:13.966991 26428 net.cpp:124] Setting up conv1
I0428 19:48:13.967005 26428 net.cpp:131] Top shape: 64 2 8 8 (8192)
I0428 19:48:13.967010 26428 net.cpp:139] Memory required for data: 1155328
I0428 19:48:13.967018 26428 layer_factory.hpp:77] Creating layer pool1
I0428 19:48:13.967025 26428 net.cpp:86] Creating Layer pool1
I0428 19:48:13.967030 26428 net.cpp:408] pool1 <- conv1
I0428 19:48:13.967034 26428 net.cpp:382] pool1 -> pool1
I0428 19:48:13.967069 26428 net.cpp:124] Setting up pool1
I0428 19:48:13.967075 26428 net.cpp:131] Top shape: 64 2 4 4 (2048)
I0428 19:48:13.967078 26428 net.cpp:139] Memory required for data: 1163520
I0428 19:48:13.967082 26428 layer_factory.hpp:77] Creating layer ip1
I0428 19:48:13.967087 26428 net.cpp:86] Creating Layer ip1
I0428 19:48:13.967092 26428 net.cpp:408] ip1 <- pool1
I0428 19:48:13.967095 26428 net.cpp:382] ip1 -> ip1
I0428 19:48:13.968096 26428 net.cpp:124] Setting up ip1
I0428 19:48:13.968107 26428 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:48:13.968111 26428 net.cpp:139] Memory required for data: 1176320
I0428 19:48:13.968119 26428 layer_factory.hpp:77] Creating layer relu1
I0428 19:48:13.968127 26428 net.cpp:86] Creating Layer relu1
I0428 19:48:13.968129 26428 net.cpp:408] relu1 <- ip1
I0428 19:48:13.968133 26428 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:48:13.968291 26428 net.cpp:124] Setting up relu1
I0428 19:48:13.968300 26428 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:48:13.968303 26428 net.cpp:139] Memory required for data: 1189120
I0428 19:48:13.968307 26428 layer_factory.hpp:77] Creating layer ip2
I0428 19:48:13.968312 26428 net.cpp:86] Creating Layer ip2
I0428 19:48:13.968315 26428 net.cpp:408] ip2 <- ip1
I0428 19:48:13.968320 26428 net.cpp:382] ip2 -> ip2
I0428 19:48:13.968413 26428 net.cpp:124] Setting up ip2
I0428 19:48:13.968420 26428 net.cpp:131] Top shape: 64 10 (640)
I0428 19:48:13.968423 26428 net.cpp:139] Memory required for data: 1191680
I0428 19:48:13.968428 26428 layer_factory.hpp:77] Creating layer relu2
I0428 19:48:13.968435 26428 net.cpp:86] Creating Layer relu2
I0428 19:48:13.968437 26428 net.cpp:408] relu2 <- ip2
I0428 19:48:13.968441 26428 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:48:13.969301 26428 net.cpp:124] Setting up relu2
I0428 19:48:13.969329 26428 net.cpp:131] Top shape: 64 10 (640)
I0428 19:48:13.969332 26428 net.cpp:139] Memory required for data: 1194240
I0428 19:48:13.969336 26428 layer_factory.hpp:77] Creating layer ip3
I0428 19:48:13.969343 26428 net.cpp:86] Creating Layer ip3
I0428 19:48:13.969347 26428 net.cpp:408] ip3 <- ip2
I0428 19:48:13.969353 26428 net.cpp:382] ip3 -> ip3
I0428 19:48:13.969449 26428 net.cpp:124] Setting up ip3
I0428 19:48:13.969456 26428 net.cpp:131] Top shape: 64 10 (640)
I0428 19:48:13.969460 26428 net.cpp:139] Memory required for data: 1196800
I0428 19:48:13.969467 26428 layer_factory.hpp:77] Creating layer relu3
I0428 19:48:13.969472 26428 net.cpp:86] Creating Layer relu3
I0428 19:48:13.969475 26428 net.cpp:408] relu3 <- ip3
I0428 19:48:13.969480 26428 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:48:13.969635 26428 net.cpp:124] Setting up relu3
I0428 19:48:13.969645 26428 net.cpp:131] Top shape: 64 10 (640)
I0428 19:48:13.969647 26428 net.cpp:139] Memory required for data: 1199360
I0428 19:48:13.969650 26428 layer_factory.hpp:77] Creating layer loss
I0428 19:48:13.969655 26428 net.cpp:86] Creating Layer loss
I0428 19:48:13.969658 26428 net.cpp:408] loss <- ip3
I0428 19:48:13.969662 26428 net.cpp:408] loss <- label
I0428 19:48:13.969667 26428 net.cpp:382] loss -> loss
I0428 19:48:13.969686 26428 layer_factory.hpp:77] Creating layer loss
I0428 19:48:13.969914 26428 net.cpp:124] Setting up loss
I0428 19:48:13.969923 26428 net.cpp:131] Top shape: (1)
I0428 19:48:13.969928 26428 net.cpp:134]     with loss weight 1
I0428 19:48:13.969941 26428 net.cpp:139] Memory required for data: 1199364
I0428 19:48:13.969945 26428 net.cpp:200] loss needs backward computation.
I0428 19:48:13.969949 26428 net.cpp:200] relu3 needs backward computation.
I0428 19:48:13.969952 26428 net.cpp:200] ip3 needs backward computation.
I0428 19:48:13.969956 26428 net.cpp:200] relu2 needs backward computation.
I0428 19:48:13.969959 26428 net.cpp:200] ip2 needs backward computation.
I0428 19:48:13.969961 26428 net.cpp:200] relu1 needs backward computation.
I0428 19:48:13.969964 26428 net.cpp:200] ip1 needs backward computation.
I0428 19:48:13.969967 26428 net.cpp:200] pool1 needs backward computation.
I0428 19:48:13.969970 26428 net.cpp:200] conv1 needs backward computation.
I0428 19:48:13.969985 26428 net.cpp:200] pool0 needs backward computation.
I0428 19:48:13.969988 26428 net.cpp:200] conv0 needs backward computation.
I0428 19:48:13.969991 26428 net.cpp:202] mnist does not need backward computation.
I0428 19:48:13.969995 26428 net.cpp:244] This network produces output loss
I0428 19:48:13.970003 26428 net.cpp:257] Network initialization done.
I0428 19:48:13.970362 26428 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test538.prototxt
I0428 19:48:13.970389 26428 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:48:13.970494 26428 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:48:13.970571 26428 layer_factory.hpp:77] Creating layer mnist
I0428 19:48:13.970613 26428 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:48:13.970625 26428 net.cpp:86] Creating Layer mnist
I0428 19:48:13.970629 26428 net.cpp:382] mnist -> data
I0428 19:48:13.970638 26428 net.cpp:382] mnist -> label
I0428 19:48:13.970715 26428 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:48:13.972653 26428 net.cpp:124] Setting up mnist
I0428 19:48:13.972666 26428 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:48:13.972671 26428 net.cpp:131] Top shape: 100 (100)
I0428 19:48:13.972674 26428 net.cpp:139] Memory required for data: 314000
I0428 19:48:13.972677 26428 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:48:13.972687 26428 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:48:13.972690 26428 net.cpp:408] label_mnist_1_split <- label
I0428 19:48:13.972695 26428 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:48:13.972702 26428 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:48:13.972789 26428 net.cpp:124] Setting up label_mnist_1_split
I0428 19:48:13.972807 26428 net.cpp:131] Top shape: 100 (100)
I0428 19:48:13.972834 26428 net.cpp:131] Top shape: 100 (100)
I0428 19:48:13.972837 26428 net.cpp:139] Memory required for data: 314800
I0428 19:48:13.972841 26428 layer_factory.hpp:77] Creating layer conv0
I0428 19:48:13.972848 26428 net.cpp:86] Creating Layer conv0
I0428 19:48:13.972851 26428 net.cpp:408] conv0 <- data
I0428 19:48:13.972856 26428 net.cpp:382] conv0 -> conv0
I0428 19:48:13.974488 26428 net.cpp:124] Setting up conv0
I0428 19:48:13.974516 26428 net.cpp:131] Top shape: 100 5 24 24 (288000)
I0428 19:48:13.974520 26428 net.cpp:139] Memory required for data: 1466800
I0428 19:48:13.974544 26428 layer_factory.hpp:77] Creating layer pool0
I0428 19:48:13.974550 26428 net.cpp:86] Creating Layer pool0
I0428 19:48:13.974553 26428 net.cpp:408] pool0 <- conv0
I0428 19:48:13.974557 26428 net.cpp:382] pool0 -> pool0
I0428 19:48:13.974591 26428 net.cpp:124] Setting up pool0
I0428 19:48:13.974597 26428 net.cpp:131] Top shape: 100 5 12 12 (72000)
I0428 19:48:13.974601 26428 net.cpp:139] Memory required for data: 1754800
I0428 19:48:13.974603 26428 layer_factory.hpp:77] Creating layer conv1
I0428 19:48:13.974611 26428 net.cpp:86] Creating Layer conv1
I0428 19:48:13.974614 26428 net.cpp:408] conv1 <- pool0
I0428 19:48:13.974618 26428 net.cpp:382] conv1 -> conv1
I0428 19:48:13.976713 26428 net.cpp:124] Setting up conv1
I0428 19:48:13.976755 26428 net.cpp:131] Top shape: 100 2 8 8 (12800)
I0428 19:48:13.976759 26428 net.cpp:139] Memory required for data: 1806000
I0428 19:48:13.976768 26428 layer_factory.hpp:77] Creating layer pool1
I0428 19:48:13.976788 26428 net.cpp:86] Creating Layer pool1
I0428 19:48:13.976793 26428 net.cpp:408] pool1 <- conv1
I0428 19:48:13.976796 26428 net.cpp:382] pool1 -> pool1
I0428 19:48:13.976886 26428 net.cpp:124] Setting up pool1
I0428 19:48:13.976893 26428 net.cpp:131] Top shape: 100 2 4 4 (3200)
I0428 19:48:13.976897 26428 net.cpp:139] Memory required for data: 1818800
I0428 19:48:13.976900 26428 layer_factory.hpp:77] Creating layer ip1
I0428 19:48:13.976907 26428 net.cpp:86] Creating Layer ip1
I0428 19:48:13.976910 26428 net.cpp:408] ip1 <- pool1
I0428 19:48:13.976915 26428 net.cpp:382] ip1 -> ip1
I0428 19:48:13.977088 26428 net.cpp:124] Setting up ip1
I0428 19:48:13.977097 26428 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:48:13.977111 26428 net.cpp:139] Memory required for data: 1838800
I0428 19:48:13.977120 26428 layer_factory.hpp:77] Creating layer relu1
I0428 19:48:13.977125 26428 net.cpp:86] Creating Layer relu1
I0428 19:48:13.977128 26428 net.cpp:408] relu1 <- ip1
I0428 19:48:13.977133 26428 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:48:13.977324 26428 net.cpp:124] Setting up relu1
I0428 19:48:13.977332 26428 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:48:13.977335 26428 net.cpp:139] Memory required for data: 1858800
I0428 19:48:13.977339 26428 layer_factory.hpp:77] Creating layer ip2
I0428 19:48:13.977345 26428 net.cpp:86] Creating Layer ip2
I0428 19:48:13.977349 26428 net.cpp:408] ip2 <- ip1
I0428 19:48:13.977354 26428 net.cpp:382] ip2 -> ip2
I0428 19:48:13.977450 26428 net.cpp:124] Setting up ip2
I0428 19:48:13.977458 26428 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:48:13.977468 26428 net.cpp:139] Memory required for data: 1862800
I0428 19:48:13.977473 26428 layer_factory.hpp:77] Creating layer relu2
I0428 19:48:13.977478 26428 net.cpp:86] Creating Layer relu2
I0428 19:48:13.977496 26428 net.cpp:408] relu2 <- ip2
I0428 19:48:13.977517 26428 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:48:13.977658 26428 net.cpp:124] Setting up relu2
I0428 19:48:13.977664 26428 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:48:13.977668 26428 net.cpp:139] Memory required for data: 1866800
I0428 19:48:13.977677 26428 layer_factory.hpp:77] Creating layer ip3
I0428 19:48:13.977682 26428 net.cpp:86] Creating Layer ip3
I0428 19:48:13.977685 26428 net.cpp:408] ip3 <- ip2
I0428 19:48:13.977690 26428 net.cpp:382] ip3 -> ip3
I0428 19:48:13.977777 26428 net.cpp:124] Setting up ip3
I0428 19:48:13.977784 26428 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:48:13.977788 26428 net.cpp:139] Memory required for data: 1870800
I0428 19:48:13.977795 26428 layer_factory.hpp:77] Creating layer relu3
I0428 19:48:13.977807 26428 net.cpp:86] Creating Layer relu3
I0428 19:48:13.977823 26428 net.cpp:408] relu3 <- ip3
I0428 19:48:13.977828 26428 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:48:13.978637 26428 net.cpp:124] Setting up relu3
I0428 19:48:13.978649 26428 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:48:13.978652 26428 net.cpp:139] Memory required for data: 1874800
I0428 19:48:13.978657 26428 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:48:13.978662 26428 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:48:13.978664 26428 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:48:13.978669 26428 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:48:13.978675 26428 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:48:13.978714 26428 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:48:13.978719 26428 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:48:13.978723 26428 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:48:13.978725 26428 net.cpp:139] Memory required for data: 1882800
I0428 19:48:13.978734 26428 layer_factory.hpp:77] Creating layer accuracy
I0428 19:48:13.978744 26428 net.cpp:86] Creating Layer accuracy
I0428 19:48:13.978747 26428 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:48:13.978751 26428 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:48:13.978755 26428 net.cpp:382] accuracy -> accuracy
I0428 19:48:13.978762 26428 net.cpp:124] Setting up accuracy
I0428 19:48:13.978766 26428 net.cpp:131] Top shape: (1)
I0428 19:48:13.978770 26428 net.cpp:139] Memory required for data: 1882804
I0428 19:48:13.978771 26428 layer_factory.hpp:77] Creating layer loss
I0428 19:48:13.978775 26428 net.cpp:86] Creating Layer loss
I0428 19:48:13.978778 26428 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:48:13.978782 26428 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:48:13.978786 26428 net.cpp:382] loss -> loss
I0428 19:48:13.978791 26428 layer_factory.hpp:77] Creating layer loss
I0428 19:48:13.979028 26428 net.cpp:124] Setting up loss
I0428 19:48:13.979038 26428 net.cpp:131] Top shape: (1)
I0428 19:48:13.979041 26428 net.cpp:134]     with loss weight 1
I0428 19:48:13.979046 26428 net.cpp:139] Memory required for data: 1882808
I0428 19:48:13.979064 26428 net.cpp:200] loss needs backward computation.
I0428 19:48:13.979069 26428 net.cpp:202] accuracy does not need backward computation.
I0428 19:48:13.979073 26428 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:48:13.979075 26428 net.cpp:200] relu3 needs backward computation.
I0428 19:48:13.979079 26428 net.cpp:200] ip3 needs backward computation.
I0428 19:48:13.979081 26428 net.cpp:200] relu2 needs backward computation.
I0428 19:48:13.979084 26428 net.cpp:200] ip2 needs backward computation.
I0428 19:48:13.979087 26428 net.cpp:200] relu1 needs backward computation.
I0428 19:48:13.979095 26428 net.cpp:200] ip1 needs backward computation.
I0428 19:48:13.979099 26428 net.cpp:200] pool1 needs backward computation.
I0428 19:48:13.979101 26428 net.cpp:200] conv1 needs backward computation.
I0428 19:48:13.979104 26428 net.cpp:200] pool0 needs backward computation.
I0428 19:48:13.979107 26428 net.cpp:200] conv0 needs backward computation.
I0428 19:48:13.979110 26428 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:48:13.979115 26428 net.cpp:202] mnist does not need backward computation.
I0428 19:48:13.979117 26428 net.cpp:244] This network produces output accuracy
I0428 19:48:13.979120 26428 net.cpp:244] This network produces output loss
I0428 19:48:13.979130 26428 net.cpp:257] Network initialization done.
I0428 19:48:13.979169 26428 solver.cpp:56] Solver scaffolding done.
I0428 19:48:13.979506 26428 caffe.cpp:248] Starting Optimization
I0428 19:48:13.979518 26428 solver.cpp:273] Solving LeNet
I0428 19:48:13.979522 26428 solver.cpp:274] Learning Rate Policy: inv
I0428 19:48:13.980347 26428 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:48:13.983260 26428 blocking_queue.cpp:49] Waiting for data
I0428 19:48:14.085417 26435 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:48:14.085928 26428 solver.cpp:398]     Test net output #0: accuracy = 0.1228
I0428 19:48:14.085952 26428 solver.cpp:398]     Test net output #1: loss = 2.30431 (* 1 = 2.30431 loss)
I0428 19:48:14.087857 26428 solver.cpp:219] Iteration 0 (0 iter/s, 0.108306s/100 iters), loss = 2.29918
I0428 19:48:14.087893 26428 solver.cpp:238]     Train net output #0: loss = 2.29918 (* 1 = 2.29918 loss)
I0428 19:48:14.087905 26428 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:48:14.158931 26428 solver.cpp:219] Iteration 100 (1407.83 iter/s, 0.0710312s/100 iters), loss = 1.71944
I0428 19:48:14.158972 26428 solver.cpp:238]     Train net output #0: loss = 1.71944 (* 1 = 1.71944 loss)
I0428 19:48:14.158977 26428 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:48:14.228876 26428 solver.cpp:219] Iteration 200 (1430.69 iter/s, 0.0698962s/100 iters), loss = 1.36112
I0428 19:48:14.228901 26428 solver.cpp:238]     Train net output #0: loss = 1.36112 (* 1 = 1.36112 loss)
I0428 19:48:14.228907 26428 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:48:14.299576 26428 solver.cpp:219] Iteration 300 (1415.1 iter/s, 0.0706663s/100 iters), loss = 1.08287
I0428 19:48:14.299615 26428 solver.cpp:238]     Train net output #0: loss = 1.08287 (* 1 = 1.08287 loss)
I0428 19:48:14.299621 26428 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:48:14.372524 26428 solver.cpp:219] Iteration 400 (1371.46 iter/s, 0.072915s/100 iters), loss = 0.824798
I0428 19:48:14.372562 26428 solver.cpp:238]     Train net output #0: loss = 0.824798 (* 1 = 0.824798 loss)
I0428 19:48:14.372568 26428 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:48:14.440215 26428 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:48:14.548962 26435 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:48:14.549520 26428 solver.cpp:398]     Test net output #0: accuracy = 0.7298
I0428 19:48:14.549554 26428 solver.cpp:398]     Test net output #1: loss = 0.768362 (* 1 = 0.768362 loss)
I0428 19:48:14.550215 26428 solver.cpp:219] Iteration 500 (562.893 iter/s, 0.177654s/100 iters), loss = 0.680785
I0428 19:48:14.550274 26428 solver.cpp:238]     Train net output #0: loss = 0.680785 (* 1 = 0.680785 loss)
I0428 19:48:14.550297 26428 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:48:14.624615 26428 solver.cpp:219] Iteration 600 (1344.99 iter/s, 0.0743499s/100 iters), loss = 0.734923
I0428 19:48:14.624655 26428 solver.cpp:238]     Train net output #0: loss = 0.734923 (* 1 = 0.734923 loss)
I0428 19:48:14.624660 26428 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:48:14.695302 26428 solver.cpp:219] Iteration 700 (1415.66 iter/s, 0.0706386s/100 iters), loss = 0.826757
I0428 19:48:14.695339 26428 solver.cpp:238]     Train net output #0: loss = 0.826757 (* 1 = 0.826757 loss)
I0428 19:48:14.695345 26428 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:48:14.765597 26428 solver.cpp:219] Iteration 800 (1423.17 iter/s, 0.0702659s/100 iters), loss = 0.75372
I0428 19:48:14.765636 26428 solver.cpp:238]     Train net output #0: loss = 0.75372 (* 1 = 0.75372 loss)
I0428 19:48:14.765641 26428 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:48:14.837612 26428 solver.cpp:219] Iteration 900 (1389.2 iter/s, 0.071984s/100 iters), loss = 0.744368
I0428 19:48:14.837651 26428 solver.cpp:238]     Train net output #0: loss = 0.744368 (* 1 = 0.744368 loss)
I0428 19:48:14.837656 26428 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:48:14.861644 26434 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:48:14.908831 26428 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:48:14.909586 26428 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:48:14.910045 26428 solver.cpp:311] Iteration 1000, loss = 0.418953
I0428 19:48:14.910078 26428 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:48:15.000615 26435 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:48:15.001116 26428 solver.cpp:398]     Test net output #0: accuracy = 0.8622
I0428 19:48:15.001157 26428 solver.cpp:398]     Test net output #1: loss = 0.39565 (* 1 = 0.39565 loss)
I0428 19:48:15.001161 26428 solver.cpp:316] Optimization Done.
I0428 19:48:15.001163 26428 caffe.cpp:259] Optimization Done.
