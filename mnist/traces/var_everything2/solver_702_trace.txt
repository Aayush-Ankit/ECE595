I0428 19:54:00.959915 27919 caffe.cpp:218] Using GPUs 0
I0428 19:54:00.997364 27919 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:54:01.510915 27919 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test702.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:54:01.511059 27919 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test702.prototxt
I0428 19:54:01.511399 27919 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:54:01.511415 27919 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:54:01.511497 27919 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0428 19:54:01.511570 27919 layer_factory.hpp:77] Creating layer mnist
I0428 19:54:01.511670 27919 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:54:01.511693 27919 net.cpp:86] Creating Layer mnist
I0428 19:54:01.511701 27919 net.cpp:382] mnist -> data
I0428 19:54:01.511724 27919 net.cpp:382] mnist -> label
I0428 19:54:01.512809 27919 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:54:01.515496 27919 net.cpp:124] Setting up mnist
I0428 19:54:01.515513 27919 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:54:01.515519 27919 net.cpp:131] Top shape: 64 (64)
I0428 19:54:01.515522 27919 net.cpp:139] Memory required for data: 200960
I0428 19:54:01.515529 27919 layer_factory.hpp:77] Creating layer conv0
I0428 19:54:01.515576 27919 net.cpp:86] Creating Layer conv0
I0428 19:54:01.515583 27919 net.cpp:408] conv0 <- data
I0428 19:54:01.515612 27919 net.cpp:382] conv0 -> conv0
I0428 19:54:01.803230 27919 net.cpp:124] Setting up conv0
I0428 19:54:01.803256 27919 net.cpp:131] Top shape: 64 5 24 24 (184320)
I0428 19:54:01.803261 27919 net.cpp:139] Memory required for data: 938240
I0428 19:54:01.803277 27919 layer_factory.hpp:77] Creating layer pool0
I0428 19:54:01.803292 27919 net.cpp:86] Creating Layer pool0
I0428 19:54:01.803295 27919 net.cpp:408] pool0 <- conv0
I0428 19:54:01.803302 27919 net.cpp:382] pool0 -> pool0
I0428 19:54:01.803350 27919 net.cpp:124] Setting up pool0
I0428 19:54:01.803356 27919 net.cpp:131] Top shape: 64 5 12 12 (46080)
I0428 19:54:01.803375 27919 net.cpp:139] Memory required for data: 1122560
I0428 19:54:01.803380 27919 layer_factory.hpp:77] Creating layer conv1
I0428 19:54:01.803390 27919 net.cpp:86] Creating Layer conv1
I0428 19:54:01.803395 27919 net.cpp:408] conv1 <- pool0
I0428 19:54:01.803400 27919 net.cpp:382] conv1 -> conv1
I0428 19:54:01.806259 27919 net.cpp:124] Setting up conv1
I0428 19:54:01.806274 27919 net.cpp:131] Top shape: 64 100 8 8 (409600)
I0428 19:54:01.806278 27919 net.cpp:139] Memory required for data: 2760960
I0428 19:54:01.806288 27919 layer_factory.hpp:77] Creating layer pool1
I0428 19:54:01.806294 27919 net.cpp:86] Creating Layer pool1
I0428 19:54:01.806298 27919 net.cpp:408] pool1 <- conv1
I0428 19:54:01.806304 27919 net.cpp:382] pool1 -> pool1
I0428 19:54:01.806342 27919 net.cpp:124] Setting up pool1
I0428 19:54:01.806349 27919 net.cpp:131] Top shape: 64 100 4 4 (102400)
I0428 19:54:01.806351 27919 net.cpp:139] Memory required for data: 3170560
I0428 19:54:01.806354 27919 layer_factory.hpp:77] Creating layer ip1
I0428 19:54:01.806361 27919 net.cpp:86] Creating Layer ip1
I0428 19:54:01.806365 27919 net.cpp:408] ip1 <- pool1
I0428 19:54:01.806370 27919 net.cpp:382] ip1 -> ip1
I0428 19:54:01.806556 27919 net.cpp:124] Setting up ip1
I0428 19:54:01.806565 27919 net.cpp:131] Top shape: 64 10 (640)
I0428 19:54:01.806568 27919 net.cpp:139] Memory required for data: 3173120
I0428 19:54:01.806576 27919 layer_factory.hpp:77] Creating layer relu1
I0428 19:54:01.806581 27919 net.cpp:86] Creating Layer relu1
I0428 19:54:01.806584 27919 net.cpp:408] relu1 <- ip1
I0428 19:54:01.806588 27919 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:54:01.806754 27919 net.cpp:124] Setting up relu1
I0428 19:54:01.806764 27919 net.cpp:131] Top shape: 64 10 (640)
I0428 19:54:01.806767 27919 net.cpp:139] Memory required for data: 3175680
I0428 19:54:01.806771 27919 layer_factory.hpp:77] Creating layer loss
I0428 19:54:01.806776 27919 net.cpp:86] Creating Layer loss
I0428 19:54:01.806779 27919 net.cpp:408] loss <- ip1
I0428 19:54:01.806783 27919 net.cpp:408] loss <- label
I0428 19:54:01.806788 27919 net.cpp:382] loss -> loss
I0428 19:54:01.806805 27919 layer_factory.hpp:77] Creating layer loss
I0428 19:54:01.807641 27919 net.cpp:124] Setting up loss
I0428 19:54:01.807653 27919 net.cpp:131] Top shape: (1)
I0428 19:54:01.807668 27919 net.cpp:134]     with loss weight 1
I0428 19:54:01.807698 27919 net.cpp:139] Memory required for data: 3175684
I0428 19:54:01.807700 27919 net.cpp:200] loss needs backward computation.
I0428 19:54:01.807704 27919 net.cpp:200] relu1 needs backward computation.
I0428 19:54:01.807708 27919 net.cpp:200] ip1 needs backward computation.
I0428 19:54:01.807710 27919 net.cpp:200] pool1 needs backward computation.
I0428 19:54:01.807713 27919 net.cpp:200] conv1 needs backward computation.
I0428 19:54:01.807716 27919 net.cpp:200] pool0 needs backward computation.
I0428 19:54:01.807719 27919 net.cpp:200] conv0 needs backward computation.
I0428 19:54:01.807723 27919 net.cpp:202] mnist does not need backward computation.
I0428 19:54:01.807726 27919 net.cpp:244] This network produces output loss
I0428 19:54:01.807734 27919 net.cpp:257] Network initialization done.
I0428 19:54:01.807996 27919 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test702.prototxt
I0428 19:54:01.808019 27919 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:54:01.808094 27919 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0428 19:54:01.808163 27919 layer_factory.hpp:77] Creating layer mnist
I0428 19:54:01.808208 27919 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:54:01.808220 27919 net.cpp:86] Creating Layer mnist
I0428 19:54:01.808225 27919 net.cpp:382] mnist -> data
I0428 19:54:01.808233 27919 net.cpp:382] mnist -> label
I0428 19:54:01.808316 27919 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:54:01.810221 27919 net.cpp:124] Setting up mnist
I0428 19:54:01.810235 27919 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:54:01.810240 27919 net.cpp:131] Top shape: 100 (100)
I0428 19:54:01.810243 27919 net.cpp:139] Memory required for data: 314000
I0428 19:54:01.810247 27919 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:54:01.810257 27919 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:54:01.810261 27919 net.cpp:408] label_mnist_1_split <- label
I0428 19:54:01.810266 27919 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:54:01.810272 27919 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:54:01.810328 27919 net.cpp:124] Setting up label_mnist_1_split
I0428 19:54:01.810333 27919 net.cpp:131] Top shape: 100 (100)
I0428 19:54:01.810338 27919 net.cpp:131] Top shape: 100 (100)
I0428 19:54:01.810340 27919 net.cpp:139] Memory required for data: 314800
I0428 19:54:01.810343 27919 layer_factory.hpp:77] Creating layer conv0
I0428 19:54:01.810351 27919 net.cpp:86] Creating Layer conv0
I0428 19:54:01.810354 27919 net.cpp:408] conv0 <- data
I0428 19:54:01.810359 27919 net.cpp:382] conv0 -> conv0
I0428 19:54:01.811272 27919 net.cpp:124] Setting up conv0
I0428 19:54:01.811285 27919 net.cpp:131] Top shape: 100 5 24 24 (288000)
I0428 19:54:01.811288 27919 net.cpp:139] Memory required for data: 1466800
I0428 19:54:01.811297 27919 layer_factory.hpp:77] Creating layer pool0
I0428 19:54:01.811305 27919 net.cpp:86] Creating Layer pool0
I0428 19:54:01.811307 27919 net.cpp:408] pool0 <- conv0
I0428 19:54:01.811311 27919 net.cpp:382] pool0 -> pool0
I0428 19:54:01.811347 27919 net.cpp:124] Setting up pool0
I0428 19:54:01.811352 27919 net.cpp:131] Top shape: 100 5 12 12 (72000)
I0428 19:54:01.811354 27919 net.cpp:139] Memory required for data: 1754800
I0428 19:54:01.811357 27919 layer_factory.hpp:77] Creating layer conv1
I0428 19:54:01.811365 27919 net.cpp:86] Creating Layer conv1
I0428 19:54:01.811368 27919 net.cpp:408] conv1 <- pool0
I0428 19:54:01.811373 27919 net.cpp:382] conv1 -> conv1
I0428 19:54:01.812952 27919 net.cpp:124] Setting up conv1
I0428 19:54:01.812968 27919 net.cpp:131] Top shape: 100 100 8 8 (640000)
I0428 19:54:01.812971 27919 net.cpp:139] Memory required for data: 4314800
I0428 19:54:01.812980 27919 layer_factory.hpp:77] Creating layer pool1
I0428 19:54:01.812994 27919 net.cpp:86] Creating Layer pool1
I0428 19:54:01.813007 27919 net.cpp:408] pool1 <- conv1
I0428 19:54:01.813012 27919 net.cpp:382] pool1 -> pool1
I0428 19:54:01.813050 27919 net.cpp:124] Setting up pool1
I0428 19:54:01.813074 27919 net.cpp:131] Top shape: 100 100 4 4 (160000)
I0428 19:54:01.813078 27919 net.cpp:139] Memory required for data: 4954800
I0428 19:54:01.813081 27919 layer_factory.hpp:77] Creating layer ip1
I0428 19:54:01.813087 27919 net.cpp:86] Creating Layer ip1
I0428 19:54:01.813091 27919 net.cpp:408] ip1 <- pool1
I0428 19:54:01.813096 27919 net.cpp:382] ip1 -> ip1
I0428 19:54:01.813294 27919 net.cpp:124] Setting up ip1
I0428 19:54:01.813302 27919 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:54:01.813305 27919 net.cpp:139] Memory required for data: 4958800
I0428 19:54:01.813313 27919 layer_factory.hpp:77] Creating layer relu1
I0428 19:54:01.813318 27919 net.cpp:86] Creating Layer relu1
I0428 19:54:01.813329 27919 net.cpp:408] relu1 <- ip1
I0428 19:54:01.813334 27919 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:54:01.814139 27919 net.cpp:124] Setting up relu1
I0428 19:54:01.814152 27919 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:54:01.814154 27919 net.cpp:139] Memory required for data: 4962800
I0428 19:54:01.814158 27919 layer_factory.hpp:77] Creating layer ip1_relu1_0_split
I0428 19:54:01.814165 27919 net.cpp:86] Creating Layer ip1_relu1_0_split
I0428 19:54:01.814168 27919 net.cpp:408] ip1_relu1_0_split <- ip1
I0428 19:54:01.814173 27919 net.cpp:382] ip1_relu1_0_split -> ip1_relu1_0_split_0
I0428 19:54:01.814180 27919 net.cpp:382] ip1_relu1_0_split -> ip1_relu1_0_split_1
I0428 19:54:01.814221 27919 net.cpp:124] Setting up ip1_relu1_0_split
I0428 19:54:01.814226 27919 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:54:01.814229 27919 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:54:01.814232 27919 net.cpp:139] Memory required for data: 4970800
I0428 19:54:01.814235 27919 layer_factory.hpp:77] Creating layer accuracy
I0428 19:54:01.814240 27919 net.cpp:86] Creating Layer accuracy
I0428 19:54:01.814250 27919 net.cpp:408] accuracy <- ip1_relu1_0_split_0
I0428 19:54:01.814255 27919 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:54:01.814260 27919 net.cpp:382] accuracy -> accuracy
I0428 19:54:01.814266 27919 net.cpp:124] Setting up accuracy
I0428 19:54:01.814270 27919 net.cpp:131] Top shape: (1)
I0428 19:54:01.814272 27919 net.cpp:139] Memory required for data: 4970804
I0428 19:54:01.814275 27919 layer_factory.hpp:77] Creating layer loss
I0428 19:54:01.814285 27919 net.cpp:86] Creating Layer loss
I0428 19:54:01.814288 27919 net.cpp:408] loss <- ip1_relu1_0_split_1
I0428 19:54:01.814292 27919 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:54:01.814296 27919 net.cpp:382] loss -> loss
I0428 19:54:01.814302 27919 layer_factory.hpp:77] Creating layer loss
I0428 19:54:01.814529 27919 net.cpp:124] Setting up loss
I0428 19:54:01.814538 27919 net.cpp:131] Top shape: (1)
I0428 19:54:01.814543 27919 net.cpp:134]     with loss weight 1
I0428 19:54:01.814548 27919 net.cpp:139] Memory required for data: 4970808
I0428 19:54:01.814551 27919 net.cpp:200] loss needs backward computation.
I0428 19:54:01.814555 27919 net.cpp:202] accuracy does not need backward computation.
I0428 19:54:01.814559 27919 net.cpp:200] ip1_relu1_0_split needs backward computation.
I0428 19:54:01.814563 27919 net.cpp:200] relu1 needs backward computation.
I0428 19:54:01.814566 27919 net.cpp:200] ip1 needs backward computation.
I0428 19:54:01.814569 27919 net.cpp:200] pool1 needs backward computation.
I0428 19:54:01.814573 27919 net.cpp:200] conv1 needs backward computation.
I0428 19:54:01.814575 27919 net.cpp:200] pool0 needs backward computation.
I0428 19:54:01.814579 27919 net.cpp:200] conv0 needs backward computation.
I0428 19:54:01.814589 27919 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:54:01.814594 27919 net.cpp:202] mnist does not need backward computation.
I0428 19:54:01.814601 27919 net.cpp:244] This network produces output accuracy
I0428 19:54:01.814604 27919 net.cpp:244] This network produces output loss
I0428 19:54:01.814623 27919 net.cpp:257] Network initialization done.
I0428 19:54:01.814656 27919 solver.cpp:56] Solver scaffolding done.
I0428 19:54:01.814867 27919 caffe.cpp:248] Starting Optimization
I0428 19:54:01.814874 27919 solver.cpp:273] Solving LeNet
I0428 19:54:01.814877 27919 solver.cpp:274] Learning Rate Policy: inv
I0428 19:54:01.814983 27919 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:54:01.819303 27919 blocking_queue.cpp:49] Waiting for data
I0428 19:54:01.891168 27926 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:54:01.891715 27919 solver.cpp:398]     Test net output #0: accuracy = 0.0705
I0428 19:54:01.891748 27919 solver.cpp:398]     Test net output #1: loss = 2.32384 (* 1 = 2.32384 loss)
I0428 19:54:01.895205 27919 solver.cpp:219] Iteration 0 (-2.06103e-31 iter/s, 0.0803041s/100 iters), loss = 2.32817
I0428 19:54:01.895229 27919 solver.cpp:238]     Train net output #0: loss = 2.32817 (* 1 = 2.32817 loss)
I0428 19:54:01.895261 27919 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:54:02.027750 27919 solver.cpp:219] Iteration 100 (754.662 iter/s, 0.13251s/100 iters), loss = 1.5977
I0428 19:54:02.027791 27919 solver.cpp:238]     Train net output #0: loss = 1.5977 (* 1 = 1.5977 loss)
I0428 19:54:02.027798 27919 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:54:02.168828 27919 solver.cpp:219] Iteration 200 (709.056 iter/s, 0.141032s/100 iters), loss = 1.60947
I0428 19:54:02.168871 27919 solver.cpp:238]     Train net output #0: loss = 1.60947 (* 1 = 1.60947 loss)
I0428 19:54:02.168882 27919 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:54:02.319479 27919 solver.cpp:219] Iteration 300 (664.04 iter/s, 0.150593s/100 iters), loss = 1.43957
I0428 19:54:02.319525 27919 solver.cpp:238]     Train net output #0: loss = 1.43957 (* 1 = 1.43957 loss)
I0428 19:54:02.319536 27919 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:54:02.471504 27919 solver.cpp:219] Iteration 400 (658.03 iter/s, 0.151969s/100 iters), loss = 1.35855
I0428 19:54:02.471544 27919 solver.cpp:238]     Train net output #0: loss = 1.35855 (* 1 = 1.35855 loss)
I0428 19:54:02.471552 27919 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:54:02.613296 27919 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:54:02.690191 27926 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:54:02.690773 27919 solver.cpp:398]     Test net output #0: accuracy = 0.5754
I0428 19:54:02.690800 27919 solver.cpp:398]     Test net output #1: loss = 1.09388 (* 1 = 1.09388 loss)
I0428 19:54:02.692214 27919 solver.cpp:219] Iteration 500 (453.197 iter/s, 0.220655s/100 iters), loss = 1.36497
I0428 19:54:02.692257 27919 solver.cpp:238]     Train net output #0: loss = 1.36497 (* 1 = 1.36497 loss)
I0428 19:54:02.692267 27919 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:54:02.836119 27919 solver.cpp:219] Iteration 600 (695.165 iter/s, 0.143851s/100 iters), loss = 1.00657
I0428 19:54:02.836154 27919 solver.cpp:238]     Train net output #0: loss = 1.00657 (* 1 = 1.00657 loss)
I0428 19:54:02.836163 27919 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:54:02.977226 27919 solver.cpp:219] Iteration 700 (708.943 iter/s, 0.141055s/100 iters), loss = 0.529029
I0428 19:54:02.977269 27919 solver.cpp:238]     Train net output #0: loss = 0.529029 (* 1 = 0.529029 loss)
I0428 19:54:02.977282 27919 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:54:03.117794 27919 solver.cpp:219] Iteration 800 (711.671 iter/s, 0.140514s/100 iters), loss = 0.725058
I0428 19:54:03.117827 27919 solver.cpp:238]     Train net output #0: loss = 0.725058 (* 1 = 0.725058 loss)
I0428 19:54:03.117836 27919 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:54:03.259011 27919 solver.cpp:219] Iteration 900 (708.367 iter/s, 0.14117s/100 iters), loss = 0.682921
I0428 19:54:03.259047 27919 solver.cpp:238]     Train net output #0: loss = 0.682921 (* 1 = 0.682921 loss)
I0428 19:54:03.259054 27919 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:54:03.306339 27925 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:54:03.403894 27919 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:54:03.405632 27919 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:54:03.406659 27919 solver.cpp:311] Iteration 1000, loss = 0.637424
I0428 19:54:03.406683 27919 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:54:03.469739 27926 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:54:03.470304 27919 solver.cpp:398]     Test net output #0: accuracy = 0.7614
I0428 19:54:03.470329 27919 solver.cpp:398]     Test net output #1: loss = 0.594794 (* 1 = 0.594794 loss)
I0428 19:54:03.470335 27919 solver.cpp:316] Optimization Done.
I0428 19:54:03.470340 27919 caffe.cpp:259] Optimization Done.
