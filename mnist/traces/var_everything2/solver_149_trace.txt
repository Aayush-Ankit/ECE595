I0428 19:32:18.733336 22808 caffe.cpp:218] Using GPUs 0
I0428 19:32:18.770784 22808 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:32:19.289718 22808 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test149.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:32:19.289861 22808 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test149.prototxt
I0428 19:32:19.290233 22808 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:32:19.290251 22808 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:32:19.290338 22808 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool0"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:32:19.290412 22808 layer_factory.hpp:77] Creating layer mnist
I0428 19:32:19.290513 22808 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:32:19.290537 22808 net.cpp:86] Creating Layer mnist
I0428 19:32:19.290545 22808 net.cpp:382] mnist -> data
I0428 19:32:19.290572 22808 net.cpp:382] mnist -> label
I0428 19:32:19.291664 22808 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:32:19.294133 22808 net.cpp:124] Setting up mnist
I0428 19:32:19.294152 22808 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:32:19.294162 22808 net.cpp:131] Top shape: 64 (64)
I0428 19:32:19.294167 22808 net.cpp:139] Memory required for data: 200960
I0428 19:32:19.294173 22808 layer_factory.hpp:77] Creating layer conv0
I0428 19:32:19.294191 22808 net.cpp:86] Creating Layer conv0
I0428 19:32:19.294198 22808 net.cpp:408] conv0 <- data
I0428 19:32:19.294209 22808 net.cpp:382] conv0 -> conv0
I0428 19:32:19.583652 22808 net.cpp:124] Setting up conv0
I0428 19:32:19.583683 22808 net.cpp:131] Top shape: 64 10 24 24 (368640)
I0428 19:32:19.583686 22808 net.cpp:139] Memory required for data: 1675520
I0428 19:32:19.583701 22808 layer_factory.hpp:77] Creating layer pool0
I0428 19:32:19.583714 22808 net.cpp:86] Creating Layer pool0
I0428 19:32:19.583739 22808 net.cpp:408] pool0 <- conv0
I0428 19:32:19.583745 22808 net.cpp:382] pool0 -> pool0
I0428 19:32:19.583794 22808 net.cpp:124] Setting up pool0
I0428 19:32:19.583801 22808 net.cpp:131] Top shape: 64 10 12 12 (92160)
I0428 19:32:19.583804 22808 net.cpp:139] Memory required for data: 2044160
I0428 19:32:19.583807 22808 layer_factory.hpp:77] Creating layer ip1
I0428 19:32:19.583816 22808 net.cpp:86] Creating Layer ip1
I0428 19:32:19.583818 22808 net.cpp:408] ip1 <- pool0
I0428 19:32:19.583823 22808 net.cpp:382] ip1 -> ip1
I0428 19:32:19.585222 22808 net.cpp:124] Setting up ip1
I0428 19:32:19.585235 22808 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:32:19.585239 22808 net.cpp:139] Memory required for data: 2056960
I0428 19:32:19.585247 22808 layer_factory.hpp:77] Creating layer relu1
I0428 19:32:19.585254 22808 net.cpp:86] Creating Layer relu1
I0428 19:32:19.585258 22808 net.cpp:408] relu1 <- ip1
I0428 19:32:19.585263 22808 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:32:19.585441 22808 net.cpp:124] Setting up relu1
I0428 19:32:19.585451 22808 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:32:19.585454 22808 net.cpp:139] Memory required for data: 2069760
I0428 19:32:19.585458 22808 layer_factory.hpp:77] Creating layer ip2
I0428 19:32:19.585464 22808 net.cpp:86] Creating Layer ip2
I0428 19:32:19.585467 22808 net.cpp:408] ip2 <- ip1
I0428 19:32:19.585474 22808 net.cpp:382] ip2 -> ip2
I0428 19:32:19.585574 22808 net.cpp:124] Setting up ip2
I0428 19:32:19.585582 22808 net.cpp:131] Top shape: 64 10 (640)
I0428 19:32:19.585584 22808 net.cpp:139] Memory required for data: 2072320
I0428 19:32:19.585592 22808 layer_factory.hpp:77] Creating layer relu2
I0428 19:32:19.585597 22808 net.cpp:86] Creating Layer relu2
I0428 19:32:19.585600 22808 net.cpp:408] relu2 <- ip2
I0428 19:32:19.585604 22808 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:32:19.586366 22808 net.cpp:124] Setting up relu2
I0428 19:32:19.586379 22808 net.cpp:131] Top shape: 64 10 (640)
I0428 19:32:19.586382 22808 net.cpp:139] Memory required for data: 2074880
I0428 19:32:19.586386 22808 layer_factory.hpp:77] Creating layer ip3
I0428 19:32:19.586393 22808 net.cpp:86] Creating Layer ip3
I0428 19:32:19.586396 22808 net.cpp:408] ip3 <- ip2
I0428 19:32:19.586402 22808 net.cpp:382] ip3 -> ip3
I0428 19:32:19.586505 22808 net.cpp:124] Setting up ip3
I0428 19:32:19.586513 22808 net.cpp:131] Top shape: 64 10 (640)
I0428 19:32:19.586516 22808 net.cpp:139] Memory required for data: 2077440
I0428 19:32:19.586522 22808 layer_factory.hpp:77] Creating layer relu3
I0428 19:32:19.586529 22808 net.cpp:86] Creating Layer relu3
I0428 19:32:19.586531 22808 net.cpp:408] relu3 <- ip3
I0428 19:32:19.586536 22808 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:32:19.586693 22808 net.cpp:124] Setting up relu3
I0428 19:32:19.586701 22808 net.cpp:131] Top shape: 64 10 (640)
I0428 19:32:19.586704 22808 net.cpp:139] Memory required for data: 2080000
I0428 19:32:19.586709 22808 layer_factory.hpp:77] Creating layer loss
I0428 19:32:19.586714 22808 net.cpp:86] Creating Layer loss
I0428 19:32:19.586717 22808 net.cpp:408] loss <- ip3
I0428 19:32:19.586722 22808 net.cpp:408] loss <- label
I0428 19:32:19.586727 22808 net.cpp:382] loss -> loss
I0428 19:32:19.586746 22808 layer_factory.hpp:77] Creating layer loss
I0428 19:32:19.586980 22808 net.cpp:124] Setting up loss
I0428 19:32:19.586989 22808 net.cpp:131] Top shape: (1)
I0428 19:32:19.586993 22808 net.cpp:134]     with loss weight 1
I0428 19:32:19.587008 22808 net.cpp:139] Memory required for data: 2080004
I0428 19:32:19.587011 22808 net.cpp:200] loss needs backward computation.
I0428 19:32:19.587015 22808 net.cpp:200] relu3 needs backward computation.
I0428 19:32:19.587018 22808 net.cpp:200] ip3 needs backward computation.
I0428 19:32:19.587021 22808 net.cpp:200] relu2 needs backward computation.
I0428 19:32:19.587024 22808 net.cpp:200] ip2 needs backward computation.
I0428 19:32:19.587028 22808 net.cpp:200] relu1 needs backward computation.
I0428 19:32:19.587030 22808 net.cpp:200] ip1 needs backward computation.
I0428 19:32:19.587044 22808 net.cpp:200] pool0 needs backward computation.
I0428 19:32:19.587049 22808 net.cpp:200] conv0 needs backward computation.
I0428 19:32:19.587052 22808 net.cpp:202] mnist does not need backward computation.
I0428 19:32:19.587055 22808 net.cpp:244] This network produces output loss
I0428 19:32:19.587064 22808 net.cpp:257] Network initialization done.
I0428 19:32:19.587355 22808 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test149.prototxt
I0428 19:32:19.587383 22808 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:32:19.587468 22808 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool0"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:32:19.587533 22808 layer_factory.hpp:77] Creating layer mnist
I0428 19:32:19.587579 22808 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:32:19.587594 22808 net.cpp:86] Creating Layer mnist
I0428 19:32:19.587597 22808 net.cpp:382] mnist -> data
I0428 19:32:19.587605 22808 net.cpp:382] mnist -> label
I0428 19:32:19.587693 22808 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:32:19.588816 22808 net.cpp:124] Setting up mnist
I0428 19:32:19.588856 22808 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:32:19.588861 22808 net.cpp:131] Top shape: 100 (100)
I0428 19:32:19.588865 22808 net.cpp:139] Memory required for data: 314000
I0428 19:32:19.588868 22808 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:32:19.588899 22808 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:32:19.588904 22808 net.cpp:408] label_mnist_1_split <- label
I0428 19:32:19.588909 22808 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:32:19.588917 22808 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:32:19.588963 22808 net.cpp:124] Setting up label_mnist_1_split
I0428 19:32:19.588968 22808 net.cpp:131] Top shape: 100 (100)
I0428 19:32:19.588973 22808 net.cpp:131] Top shape: 100 (100)
I0428 19:32:19.588975 22808 net.cpp:139] Memory required for data: 314800
I0428 19:32:19.588989 22808 layer_factory.hpp:77] Creating layer conv0
I0428 19:32:19.588999 22808 net.cpp:86] Creating Layer conv0
I0428 19:32:19.589001 22808 net.cpp:408] conv0 <- data
I0428 19:32:19.589007 22808 net.cpp:382] conv0 -> conv0
I0428 19:32:19.590878 22808 net.cpp:124] Setting up conv0
I0428 19:32:19.590891 22808 net.cpp:131] Top shape: 100 10 24 24 (576000)
I0428 19:32:19.590894 22808 net.cpp:139] Memory required for data: 2618800
I0428 19:32:19.590903 22808 layer_factory.hpp:77] Creating layer pool0
I0428 19:32:19.590910 22808 net.cpp:86] Creating Layer pool0
I0428 19:32:19.590914 22808 net.cpp:408] pool0 <- conv0
I0428 19:32:19.590919 22808 net.cpp:382] pool0 -> pool0
I0428 19:32:19.590955 22808 net.cpp:124] Setting up pool0
I0428 19:32:19.590961 22808 net.cpp:131] Top shape: 100 10 12 12 (144000)
I0428 19:32:19.590965 22808 net.cpp:139] Memory required for data: 3194800
I0428 19:32:19.590967 22808 layer_factory.hpp:77] Creating layer ip1
I0428 19:32:19.590973 22808 net.cpp:86] Creating Layer ip1
I0428 19:32:19.590976 22808 net.cpp:408] ip1 <- pool0
I0428 19:32:19.590982 22808 net.cpp:382] ip1 -> ip1
I0428 19:32:19.591500 22808 net.cpp:124] Setting up ip1
I0428 19:32:19.591506 22808 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:32:19.591509 22808 net.cpp:139] Memory required for data: 3214800
I0428 19:32:19.591517 22808 layer_factory.hpp:77] Creating layer relu1
I0428 19:32:19.591523 22808 net.cpp:86] Creating Layer relu1
I0428 19:32:19.591526 22808 net.cpp:408] relu1 <- ip1
I0428 19:32:19.591531 22808 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:32:19.591732 22808 net.cpp:124] Setting up relu1
I0428 19:32:19.591740 22808 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:32:19.591744 22808 net.cpp:139] Memory required for data: 3234800
I0428 19:32:19.591747 22808 layer_factory.hpp:77] Creating layer ip2
I0428 19:32:19.591753 22808 net.cpp:86] Creating Layer ip2
I0428 19:32:19.591756 22808 net.cpp:408] ip2 <- ip1
I0428 19:32:19.591761 22808 net.cpp:382] ip2 -> ip2
I0428 19:32:19.591910 22808 net.cpp:124] Setting up ip2
I0428 19:32:19.591918 22808 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:32:19.591922 22808 net.cpp:139] Memory required for data: 3238800
I0428 19:32:19.591928 22808 layer_factory.hpp:77] Creating layer relu2
I0428 19:32:19.591935 22808 net.cpp:86] Creating Layer relu2
I0428 19:32:19.591938 22808 net.cpp:408] relu2 <- ip2
I0428 19:32:19.591943 22808 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:32:19.592814 22808 net.cpp:124] Setting up relu2
I0428 19:32:19.592859 22808 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:32:19.592862 22808 net.cpp:139] Memory required for data: 3242800
I0428 19:32:19.592866 22808 layer_factory.hpp:77] Creating layer ip3
I0428 19:32:19.592876 22808 net.cpp:86] Creating Layer ip3
I0428 19:32:19.592880 22808 net.cpp:408] ip3 <- ip2
I0428 19:32:19.592886 22808 net.cpp:382] ip3 -> ip3
I0428 19:32:19.593000 22808 net.cpp:124] Setting up ip3
I0428 19:32:19.593009 22808 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:32:19.593013 22808 net.cpp:139] Memory required for data: 3246800
I0428 19:32:19.593019 22808 layer_factory.hpp:77] Creating layer relu3
I0428 19:32:19.593025 22808 net.cpp:86] Creating Layer relu3
I0428 19:32:19.593029 22808 net.cpp:408] relu3 <- ip3
I0428 19:32:19.593034 22808 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:32:19.593240 22808 net.cpp:124] Setting up relu3
I0428 19:32:19.593248 22808 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:32:19.593251 22808 net.cpp:139] Memory required for data: 3250800
I0428 19:32:19.593255 22808 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:32:19.593261 22808 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:32:19.593263 22808 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:32:19.593268 22808 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:32:19.593276 22808 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:32:19.593324 22808 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:32:19.593333 22808 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:32:19.593336 22808 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:32:19.593350 22808 net.cpp:139] Memory required for data: 3258800
I0428 19:32:19.593353 22808 layer_factory.hpp:77] Creating layer accuracy
I0428 19:32:19.593359 22808 net.cpp:86] Creating Layer accuracy
I0428 19:32:19.593364 22808 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:32:19.593367 22808 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:32:19.593372 22808 net.cpp:382] accuracy -> accuracy
I0428 19:32:19.593379 22808 net.cpp:124] Setting up accuracy
I0428 19:32:19.593384 22808 net.cpp:131] Top shape: (1)
I0428 19:32:19.593386 22808 net.cpp:139] Memory required for data: 3258804
I0428 19:32:19.593389 22808 layer_factory.hpp:77] Creating layer loss
I0428 19:32:19.593394 22808 net.cpp:86] Creating Layer loss
I0428 19:32:19.593397 22808 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:32:19.593401 22808 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:32:19.593406 22808 net.cpp:382] loss -> loss
I0428 19:32:19.593412 22808 layer_factory.hpp:77] Creating layer loss
I0428 19:32:19.593703 22808 net.cpp:124] Setting up loss
I0428 19:32:19.593713 22808 net.cpp:131] Top shape: (1)
I0428 19:32:19.593716 22808 net.cpp:134]     with loss weight 1
I0428 19:32:19.593724 22808 net.cpp:139] Memory required for data: 3258808
I0428 19:32:19.593727 22808 net.cpp:200] loss needs backward computation.
I0428 19:32:19.593730 22808 net.cpp:202] accuracy does not need backward computation.
I0428 19:32:19.593734 22808 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:32:19.593737 22808 net.cpp:200] relu3 needs backward computation.
I0428 19:32:19.593740 22808 net.cpp:200] ip3 needs backward computation.
I0428 19:32:19.593744 22808 net.cpp:200] relu2 needs backward computation.
I0428 19:32:19.593746 22808 net.cpp:200] ip2 needs backward computation.
I0428 19:32:19.593750 22808 net.cpp:200] relu1 needs backward computation.
I0428 19:32:19.593751 22808 net.cpp:200] ip1 needs backward computation.
I0428 19:32:19.593755 22808 net.cpp:200] pool0 needs backward computation.
I0428 19:32:19.593758 22808 net.cpp:200] conv0 needs backward computation.
I0428 19:32:19.593761 22808 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:32:19.593770 22808 net.cpp:202] mnist does not need backward computation.
I0428 19:32:19.593773 22808 net.cpp:244] This network produces output accuracy
I0428 19:32:19.593777 22808 net.cpp:244] This network produces output loss
I0428 19:32:19.593788 22808 net.cpp:257] Network initialization done.
I0428 19:32:19.593825 22808 solver.cpp:56] Solver scaffolding done.
I0428 19:32:19.594130 22808 caffe.cpp:248] Starting Optimization
I0428 19:32:19.594135 22808 solver.cpp:273] Solving LeNet
I0428 19:32:19.594137 22808 solver.cpp:274] Learning Rate Policy: inv
I0428 19:32:19.594913 22808 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:32:19.599840 22808 blocking_queue.cpp:49] Waiting for data
I0428 19:32:19.649309 22815 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:32:19.649821 22808 solver.cpp:398]     Test net output #0: accuracy = 0.1441
I0428 19:32:19.649838 22808 solver.cpp:398]     Test net output #1: loss = 2.30792 (* 1 = 2.30792 loss)
I0428 19:32:19.652737 22808 solver.cpp:219] Iteration 0 (0 iter/s, 0.0585708s/100 iters), loss = 2.29623
I0428 19:32:19.652761 22808 solver.cpp:238]     Train net output #0: loss = 2.29623 (* 1 = 2.29623 loss)
I0428 19:32:19.652772 22808 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:32:19.726212 22808 solver.cpp:219] Iteration 100 (1361.71 iter/s, 0.0734372s/100 iters), loss = 1.0253
I0428 19:32:19.726253 22808 solver.cpp:238]     Train net output #0: loss = 1.0253 (* 1 = 1.0253 loss)
I0428 19:32:19.726258 22808 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:32:19.800635 22808 solver.cpp:219] Iteration 200 (1344.55 iter/s, 0.0743741s/100 iters), loss = 0.964922
I0428 19:32:19.800674 22808 solver.cpp:238]     Train net output #0: loss = 0.964922 (* 1 = 0.964922 loss)
I0428 19:32:19.800680 22808 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:32:19.874629 22808 solver.cpp:219] Iteration 300 (1352.04 iter/s, 0.0739625s/100 iters), loss = 0.633811
I0428 19:32:19.874668 22808 solver.cpp:238]     Train net output #0: loss = 0.633811 (* 1 = 0.633811 loss)
I0428 19:32:19.874673 22808 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:32:19.950103 22808 solver.cpp:219] Iteration 400 (1325.52 iter/s, 0.0754422s/100 iters), loss = 0.907124
I0428 19:32:19.950140 22808 solver.cpp:238]     Train net output #0: loss = 0.907124 (* 1 = 0.907124 loss)
I0428 19:32:19.950146 22808 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:32:20.023393 22808 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:32:20.071693 22815 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:32:20.072268 22808 solver.cpp:398]     Test net output #0: accuracy = 0.7559
I0428 19:32:20.072286 22808 solver.cpp:398]     Test net output #1: loss = 0.65974 (* 1 = 0.65974 loss)
I0428 19:32:20.073089 22808 solver.cpp:219] Iteration 500 (813.321 iter/s, 0.122953s/100 iters), loss = 0.920882
I0428 19:32:20.073135 22808 solver.cpp:238]     Train net output #0: loss = 0.920882 (* 1 = 0.920882 loss)
I0428 19:32:20.073141 22808 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:32:20.154207 22808 solver.cpp:219] Iteration 600 (1233.55 iter/s, 0.081067s/100 iters), loss = 0.57463
I0428 19:32:20.154247 22808 solver.cpp:238]     Train net output #0: loss = 0.57463 (* 1 = 0.57463 loss)
I0428 19:32:20.154253 22808 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:32:20.230057 22808 solver.cpp:219] Iteration 700 (1318.97 iter/s, 0.075817s/100 iters), loss = 0.638459
I0428 19:32:20.230111 22808 solver.cpp:238]     Train net output #0: loss = 0.638459 (* 1 = 0.638459 loss)
I0428 19:32:20.230116 22808 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:32:20.303397 22808 solver.cpp:219] Iteration 800 (1364.39 iter/s, 0.0732929s/100 iters), loss = 0.685106
I0428 19:32:20.303436 22808 solver.cpp:238]     Train net output #0: loss = 0.685107 (* 1 = 0.685107 loss)
I0428 19:32:20.303442 22808 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:32:20.382400 22808 solver.cpp:219] Iteration 900 (1266.29 iter/s, 0.0789707s/100 iters), loss = 0.747394
I0428 19:32:20.382439 22808 solver.cpp:238]     Train net output #0: loss = 0.747395 (* 1 = 0.747395 loss)
I0428 19:32:20.382447 22808 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:32:20.407454 22814 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:32:20.456493 22808 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:32:20.458165 22808 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:32:20.459236 22808 solver.cpp:311] Iteration 1000, loss = 0.501653
I0428 19:32:20.459251 22808 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:32:20.534407 22815 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:32:20.534930 22808 solver.cpp:398]     Test net output #0: accuracy = 0.774
I0428 19:32:20.534965 22808 solver.cpp:398]     Test net output #1: loss = 0.571933 (* 1 = 0.571933 loss)
I0428 19:32:20.534970 22808 solver.cpp:316] Optimization Done.
I0428 19:32:20.534972 22808 caffe.cpp:259] Optimization Done.
