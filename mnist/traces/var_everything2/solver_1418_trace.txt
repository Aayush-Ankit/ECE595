I0428 20:23:15.036388  2209 caffe.cpp:218] Using GPUs 0
I0428 20:23:15.073907  2209 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:23:15.583647  2209 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1418.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:23:15.583786  2209 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1418.prototxt
I0428 20:23:15.584205  2209 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:23:15.584224  2209 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:23:15.584327  2209 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:23:15.584408  2209 layer_factory.hpp:77] Creating layer mnist
I0428 20:23:15.584512  2209 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:23:15.584535  2209 net.cpp:86] Creating Layer mnist
I0428 20:23:15.584545  2209 net.cpp:382] mnist -> data
I0428 20:23:15.584568  2209 net.cpp:382] mnist -> label
I0428 20:23:15.585675  2209 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:23:15.588145  2209 net.cpp:124] Setting up mnist
I0428 20:23:15.588162  2209 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:23:15.588168  2209 net.cpp:131] Top shape: 64 (64)
I0428 20:23:15.588171  2209 net.cpp:139] Memory required for data: 200960
I0428 20:23:15.588179  2209 layer_factory.hpp:77] Creating layer conv0
I0428 20:23:15.588228  2209 net.cpp:86] Creating Layer conv0
I0428 20:23:15.588249  2209 net.cpp:408] conv0 <- data
I0428 20:23:15.588264  2209 net.cpp:382] conv0 -> conv0
I0428 20:23:15.854156  2209 net.cpp:124] Setting up conv0
I0428 20:23:15.854198  2209 net.cpp:131] Top shape: 64 50 24 24 (1843200)
I0428 20:23:15.854202  2209 net.cpp:139] Memory required for data: 7573760
I0428 20:23:15.854218  2209 layer_factory.hpp:77] Creating layer pool0
I0428 20:23:15.854229  2209 net.cpp:86] Creating Layer pool0
I0428 20:23:15.854233  2209 net.cpp:408] pool0 <- conv0
I0428 20:23:15.854238  2209 net.cpp:382] pool0 -> pool0
I0428 20:23:15.854300  2209 net.cpp:124] Setting up pool0
I0428 20:23:15.854305  2209 net.cpp:131] Top shape: 64 50 12 12 (460800)
I0428 20:23:15.854307  2209 net.cpp:139] Memory required for data: 9416960
I0428 20:23:15.854310  2209 layer_factory.hpp:77] Creating layer conv1
I0428 20:23:15.854321  2209 net.cpp:86] Creating Layer conv1
I0428 20:23:15.854323  2209 net.cpp:408] conv1 <- pool0
I0428 20:23:15.854328  2209 net.cpp:382] conv1 -> conv1
I0428 20:23:15.857832  2209 net.cpp:124] Setting up conv1
I0428 20:23:15.857846  2209 net.cpp:131] Top shape: 64 100 8 8 (409600)
I0428 20:23:15.857851  2209 net.cpp:139] Memory required for data: 11055360
I0428 20:23:15.857859  2209 layer_factory.hpp:77] Creating layer pool1
I0428 20:23:15.857867  2209 net.cpp:86] Creating Layer pool1
I0428 20:23:15.857885  2209 net.cpp:408] pool1 <- conv1
I0428 20:23:15.857890  2209 net.cpp:382] pool1 -> pool1
I0428 20:23:15.857925  2209 net.cpp:124] Setting up pool1
I0428 20:23:15.857939  2209 net.cpp:131] Top shape: 64 100 4 4 (102400)
I0428 20:23:15.857942  2209 net.cpp:139] Memory required for data: 11464960
I0428 20:23:15.857945  2209 layer_factory.hpp:77] Creating layer ip1
I0428 20:23:15.857952  2209 net.cpp:86] Creating Layer ip1
I0428 20:23:15.857955  2209 net.cpp:408] ip1 <- pool1
I0428 20:23:15.857960  2209 net.cpp:382] ip1 -> ip1
I0428 20:23:15.859028  2209 net.cpp:124] Setting up ip1
I0428 20:23:15.859056  2209 net.cpp:131] Top shape: 64 10 (640)
I0428 20:23:15.859061  2209 net.cpp:139] Memory required for data: 11467520
I0428 20:23:15.859069  2209 layer_factory.hpp:77] Creating layer relu1
I0428 20:23:15.859077  2209 net.cpp:86] Creating Layer relu1
I0428 20:23:15.859081  2209 net.cpp:408] relu1 <- ip1
I0428 20:23:15.859086  2209 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:23:15.859289  2209 net.cpp:124] Setting up relu1
I0428 20:23:15.859297  2209 net.cpp:131] Top shape: 64 10 (640)
I0428 20:23:15.859302  2209 net.cpp:139] Memory required for data: 11470080
I0428 20:23:15.859304  2209 layer_factory.hpp:77] Creating layer ip2
I0428 20:23:15.859310  2209 net.cpp:86] Creating Layer ip2
I0428 20:23:15.859314  2209 net.cpp:408] ip2 <- ip1
I0428 20:23:15.859319  2209 net.cpp:382] ip2 -> ip2
I0428 20:23:15.859411  2209 net.cpp:124] Setting up ip2
I0428 20:23:15.859418  2209 net.cpp:131] Top shape: 64 10 (640)
I0428 20:23:15.859421  2209 net.cpp:139] Memory required for data: 11472640
I0428 20:23:15.859426  2209 layer_factory.hpp:77] Creating layer relu2
I0428 20:23:15.859432  2209 net.cpp:86] Creating Layer relu2
I0428 20:23:15.859436  2209 net.cpp:408] relu2 <- ip2
I0428 20:23:15.859439  2209 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:23:15.860234  2209 net.cpp:124] Setting up relu2
I0428 20:23:15.860245  2209 net.cpp:131] Top shape: 64 10 (640)
I0428 20:23:15.860265  2209 net.cpp:139] Memory required for data: 11475200
I0428 20:23:15.860268  2209 layer_factory.hpp:77] Creating layer ip3
I0428 20:23:15.860275  2209 net.cpp:86] Creating Layer ip3
I0428 20:23:15.860280  2209 net.cpp:408] ip3 <- ip2
I0428 20:23:15.860285  2209 net.cpp:382] ip3 -> ip3
I0428 20:23:15.860395  2209 net.cpp:124] Setting up ip3
I0428 20:23:15.860402  2209 net.cpp:131] Top shape: 64 10 (640)
I0428 20:23:15.860405  2209 net.cpp:139] Memory required for data: 11477760
I0428 20:23:15.860414  2209 layer_factory.hpp:77] Creating layer relu3
I0428 20:23:15.860419  2209 net.cpp:86] Creating Layer relu3
I0428 20:23:15.860421  2209 net.cpp:408] relu3 <- ip3
I0428 20:23:15.860426  2209 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:23:15.860587  2209 net.cpp:124] Setting up relu3
I0428 20:23:15.860596  2209 net.cpp:131] Top shape: 64 10 (640)
I0428 20:23:15.860599  2209 net.cpp:139] Memory required for data: 11480320
I0428 20:23:15.860604  2209 layer_factory.hpp:77] Creating layer loss
I0428 20:23:15.860608  2209 net.cpp:86] Creating Layer loss
I0428 20:23:15.860611  2209 net.cpp:408] loss <- ip3
I0428 20:23:15.860615  2209 net.cpp:408] loss <- label
I0428 20:23:15.860621  2209 net.cpp:382] loss -> loss
I0428 20:23:15.860638  2209 layer_factory.hpp:77] Creating layer loss
I0428 20:23:15.860949  2209 net.cpp:124] Setting up loss
I0428 20:23:15.860960  2209 net.cpp:131] Top shape: (1)
I0428 20:23:15.860963  2209 net.cpp:134]     with loss weight 1
I0428 20:23:15.860978  2209 net.cpp:139] Memory required for data: 11480324
I0428 20:23:15.860982  2209 net.cpp:200] loss needs backward computation.
I0428 20:23:15.860986  2209 net.cpp:200] relu3 needs backward computation.
I0428 20:23:15.860991  2209 net.cpp:200] ip3 needs backward computation.
I0428 20:23:15.860993  2209 net.cpp:200] relu2 needs backward computation.
I0428 20:23:15.860996  2209 net.cpp:200] ip2 needs backward computation.
I0428 20:23:15.860999  2209 net.cpp:200] relu1 needs backward computation.
I0428 20:23:15.861002  2209 net.cpp:200] ip1 needs backward computation.
I0428 20:23:15.861006  2209 net.cpp:200] pool1 needs backward computation.
I0428 20:23:15.861009  2209 net.cpp:200] conv1 needs backward computation.
I0428 20:23:15.861012  2209 net.cpp:200] pool0 needs backward computation.
I0428 20:23:15.861016  2209 net.cpp:200] conv0 needs backward computation.
I0428 20:23:15.861021  2209 net.cpp:202] mnist does not need backward computation.
I0428 20:23:15.861023  2209 net.cpp:244] This network produces output loss
I0428 20:23:15.861033  2209 net.cpp:257] Network initialization done.
I0428 20:23:15.861431  2209 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1418.prototxt
I0428 20:23:15.861472  2209 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:23:15.861562  2209 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:23:15.861637  2209 layer_factory.hpp:77] Creating layer mnist
I0428 20:23:15.861682  2209 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:23:15.861695  2209 net.cpp:86] Creating Layer mnist
I0428 20:23:15.861698  2209 net.cpp:382] mnist -> data
I0428 20:23:15.861706  2209 net.cpp:382] mnist -> label
I0428 20:23:15.861789  2209 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:23:15.862982  2209 net.cpp:124] Setting up mnist
I0428 20:23:15.863009  2209 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:23:15.863030  2209 net.cpp:131] Top shape: 100 (100)
I0428 20:23:15.863034  2209 net.cpp:139] Memory required for data: 314000
I0428 20:23:15.863037  2209 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:23:15.863044  2209 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:23:15.863047  2209 net.cpp:408] label_mnist_1_split <- label
I0428 20:23:15.863052  2209 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:23:15.863059  2209 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:23:15.863111  2209 net.cpp:124] Setting up label_mnist_1_split
I0428 20:23:15.863117  2209 net.cpp:131] Top shape: 100 (100)
I0428 20:23:15.863121  2209 net.cpp:131] Top shape: 100 (100)
I0428 20:23:15.863123  2209 net.cpp:139] Memory required for data: 314800
I0428 20:23:15.863127  2209 layer_factory.hpp:77] Creating layer conv0
I0428 20:23:15.863135  2209 net.cpp:86] Creating Layer conv0
I0428 20:23:15.863138  2209 net.cpp:408] conv0 <- data
I0428 20:23:15.863143  2209 net.cpp:382] conv0 -> conv0
I0428 20:23:15.864894  2209 net.cpp:124] Setting up conv0
I0428 20:23:15.864908  2209 net.cpp:131] Top shape: 100 50 24 24 (2880000)
I0428 20:23:15.864912  2209 net.cpp:139] Memory required for data: 11834800
I0428 20:23:15.864922  2209 layer_factory.hpp:77] Creating layer pool0
I0428 20:23:15.864943  2209 net.cpp:86] Creating Layer pool0
I0428 20:23:15.864946  2209 net.cpp:408] pool0 <- conv0
I0428 20:23:15.864951  2209 net.cpp:382] pool0 -> pool0
I0428 20:23:15.864987  2209 net.cpp:124] Setting up pool0
I0428 20:23:15.864992  2209 net.cpp:131] Top shape: 100 50 12 12 (720000)
I0428 20:23:15.864995  2209 net.cpp:139] Memory required for data: 14714800
I0428 20:23:15.864998  2209 layer_factory.hpp:77] Creating layer conv1
I0428 20:23:15.865007  2209 net.cpp:86] Creating Layer conv1
I0428 20:23:15.865010  2209 net.cpp:408] conv1 <- pool0
I0428 20:23:15.865015  2209 net.cpp:382] conv1 -> conv1
I0428 20:23:15.868253  2209 net.cpp:124] Setting up conv1
I0428 20:23:15.868268  2209 net.cpp:131] Top shape: 100 100 8 8 (640000)
I0428 20:23:15.868271  2209 net.cpp:139] Memory required for data: 17274800
I0428 20:23:15.868301  2209 layer_factory.hpp:77] Creating layer pool1
I0428 20:23:15.868314  2209 net.cpp:86] Creating Layer pool1
I0428 20:23:15.868322  2209 net.cpp:408] pool1 <- conv1
I0428 20:23:15.868329  2209 net.cpp:382] pool1 -> pool1
I0428 20:23:15.868366  2209 net.cpp:124] Setting up pool1
I0428 20:23:15.868379  2209 net.cpp:131] Top shape: 100 100 4 4 (160000)
I0428 20:23:15.868382  2209 net.cpp:139] Memory required for data: 17914800
I0428 20:23:15.868386  2209 layer_factory.hpp:77] Creating layer ip1
I0428 20:23:15.868391  2209 net.cpp:86] Creating Layer ip1
I0428 20:23:15.868394  2209 net.cpp:408] ip1 <- pool1
I0428 20:23:15.868399  2209 net.cpp:382] ip1 -> ip1
I0428 20:23:15.868603  2209 net.cpp:124] Setting up ip1
I0428 20:23:15.868623  2209 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:23:15.868626  2209 net.cpp:139] Memory required for data: 17918800
I0428 20:23:15.868634  2209 layer_factory.hpp:77] Creating layer relu1
I0428 20:23:15.868654  2209 net.cpp:86] Creating Layer relu1
I0428 20:23:15.868664  2209 net.cpp:408] relu1 <- ip1
I0428 20:23:15.868669  2209 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:23:15.868854  2209 net.cpp:124] Setting up relu1
I0428 20:23:15.868865  2209 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:23:15.868875  2209 net.cpp:139] Memory required for data: 17922800
I0428 20:23:15.868878  2209 layer_factory.hpp:77] Creating layer ip2
I0428 20:23:15.868885  2209 net.cpp:86] Creating Layer ip2
I0428 20:23:15.868888  2209 net.cpp:408] ip2 <- ip1
I0428 20:23:15.868896  2209 net.cpp:382] ip2 -> ip2
I0428 20:23:15.869020  2209 net.cpp:124] Setting up ip2
I0428 20:23:15.869029  2209 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:23:15.869032  2209 net.cpp:139] Memory required for data: 17926800
I0428 20:23:15.869037  2209 layer_factory.hpp:77] Creating layer relu2
I0428 20:23:15.869042  2209 net.cpp:86] Creating Layer relu2
I0428 20:23:15.869045  2209 net.cpp:408] relu2 <- ip2
I0428 20:23:15.869051  2209 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:23:15.869269  2209 net.cpp:124] Setting up relu2
I0428 20:23:15.869277  2209 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:23:15.869280  2209 net.cpp:139] Memory required for data: 17930800
I0428 20:23:15.869283  2209 layer_factory.hpp:77] Creating layer ip3
I0428 20:23:15.869289  2209 net.cpp:86] Creating Layer ip3
I0428 20:23:15.869292  2209 net.cpp:408] ip3 <- ip2
I0428 20:23:15.869297  2209 net.cpp:382] ip3 -> ip3
I0428 20:23:15.869420  2209 net.cpp:124] Setting up ip3
I0428 20:23:15.869426  2209 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:23:15.869429  2209 net.cpp:139] Memory required for data: 17934800
I0428 20:23:15.869437  2209 layer_factory.hpp:77] Creating layer relu3
I0428 20:23:15.869447  2209 net.cpp:86] Creating Layer relu3
I0428 20:23:15.869451  2209 net.cpp:408] relu3 <- ip3
I0428 20:23:15.869455  2209 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:23:15.870263  2209 net.cpp:124] Setting up relu3
I0428 20:23:15.870276  2209 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:23:15.870280  2209 net.cpp:139] Memory required for data: 17938800
I0428 20:23:15.870283  2209 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:23:15.870288  2209 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:23:15.870292  2209 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:23:15.870296  2209 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:23:15.870302  2209 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:23:15.870352  2209 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:23:15.870357  2209 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:23:15.870360  2209 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:23:15.870363  2209 net.cpp:139] Memory required for data: 17946800
I0428 20:23:15.870365  2209 layer_factory.hpp:77] Creating layer accuracy
I0428 20:23:15.870376  2209 net.cpp:86] Creating Layer accuracy
I0428 20:23:15.870379  2209 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:23:15.870383  2209 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:23:15.870388  2209 net.cpp:382] accuracy -> accuracy
I0428 20:23:15.870400  2209 net.cpp:124] Setting up accuracy
I0428 20:23:15.870404  2209 net.cpp:131] Top shape: (1)
I0428 20:23:15.870412  2209 net.cpp:139] Memory required for data: 17946804
I0428 20:23:15.870414  2209 layer_factory.hpp:77] Creating layer loss
I0428 20:23:15.870419  2209 net.cpp:86] Creating Layer loss
I0428 20:23:15.870422  2209 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:23:15.870425  2209 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:23:15.870430  2209 net.cpp:382] loss -> loss
I0428 20:23:15.870446  2209 layer_factory.hpp:77] Creating layer loss
I0428 20:23:15.870705  2209 net.cpp:124] Setting up loss
I0428 20:23:15.870714  2209 net.cpp:131] Top shape: (1)
I0428 20:23:15.870718  2209 net.cpp:134]     with loss weight 1
I0428 20:23:15.870733  2209 net.cpp:139] Memory required for data: 17946808
I0428 20:23:15.870738  2209 net.cpp:200] loss needs backward computation.
I0428 20:23:15.870743  2209 net.cpp:202] accuracy does not need backward computation.
I0428 20:23:15.870745  2209 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:23:15.870748  2209 net.cpp:200] relu3 needs backward computation.
I0428 20:23:15.870751  2209 net.cpp:200] ip3 needs backward computation.
I0428 20:23:15.870754  2209 net.cpp:200] relu2 needs backward computation.
I0428 20:23:15.870757  2209 net.cpp:200] ip2 needs backward computation.
I0428 20:23:15.870760  2209 net.cpp:200] relu1 needs backward computation.
I0428 20:23:15.870762  2209 net.cpp:200] ip1 needs backward computation.
I0428 20:23:15.870765  2209 net.cpp:200] pool1 needs backward computation.
I0428 20:23:15.870769  2209 net.cpp:200] conv1 needs backward computation.
I0428 20:23:15.870771  2209 net.cpp:200] pool0 needs backward computation.
I0428 20:23:15.870774  2209 net.cpp:200] conv0 needs backward computation.
I0428 20:23:15.870785  2209 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:23:15.870789  2209 net.cpp:202] mnist does not need backward computation.
I0428 20:23:15.870791  2209 net.cpp:244] This network produces output accuracy
I0428 20:23:15.870800  2209 net.cpp:244] This network produces output loss
I0428 20:23:15.870812  2209 net.cpp:257] Network initialization done.
I0428 20:23:15.870853  2209 solver.cpp:56] Solver scaffolding done.
I0428 20:23:15.871189  2209 caffe.cpp:248] Starting Optimization
I0428 20:23:15.871196  2209 solver.cpp:273] Solving LeNet
I0428 20:23:15.871198  2209 solver.cpp:274] Learning Rate Policy: inv
I0428 20:23:15.871428  2209 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:23:15.955358  2209 blocking_queue.cpp:49] Waiting for data
I0428 20:23:15.961800  2216 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:23:15.962993  2209 solver.cpp:398]     Test net output #0: accuracy = 0.1
I0428 20:23:15.963021  2209 solver.cpp:398]     Test net output #1: loss = 2.31152 (* 1 = 2.31152 loss)
I0428 20:23:15.969656  2209 solver.cpp:219] Iteration 0 (0 iter/s, 0.0984298s/100 iters), loss = 2.34546
I0428 20:23:15.969691  2209 solver.cpp:238]     Train net output #0: loss = 2.34546 (* 1 = 2.34546 loss)
I0428 20:23:15.969723  2209 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:23:16.189960  2209 solver.cpp:219] Iteration 100 (454.025 iter/s, 0.220252s/100 iters), loss = 0.57887
I0428 20:23:16.189997  2209 solver.cpp:238]     Train net output #0: loss = 0.57887 (* 1 = 0.57887 loss)
I0428 20:23:16.190011  2209 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:23:16.420387  2209 solver.cpp:219] Iteration 200 (434.082 iter/s, 0.230371s/100 iters), loss = 0.46967
I0428 20:23:16.420442  2209 solver.cpp:238]     Train net output #0: loss = 0.46967 (* 1 = 0.46967 loss)
I0428 20:23:16.420462  2209 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:23:16.659533  2209 solver.cpp:219] Iteration 300 (418.282 iter/s, 0.239073s/100 iters), loss = 0.692977
I0428 20:23:16.659585  2209 solver.cpp:238]     Train net output #0: loss = 0.692977 (* 1 = 0.692977 loss)
I0428 20:23:16.659600  2209 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:23:16.896929  2209 solver.cpp:219] Iteration 400 (421.361 iter/s, 0.237326s/100 iters), loss = 0.993182
I0428 20:23:16.896975  2209 solver.cpp:238]     Train net output #0: loss = 0.993182 (* 1 = 0.993182 loss)
I0428 20:23:16.896987  2209 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:23:17.132169  2209 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:23:17.225162  2216 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:23:17.227562  2209 solver.cpp:398]     Test net output #0: accuracy = 0.8269
I0428 20:23:17.227593  2209 solver.cpp:398]     Test net output #1: loss = 0.598321 (* 1 = 0.598321 loss)
I0428 20:23:17.229717  2209 solver.cpp:219] Iteration 500 (300.551 iter/s, 0.332722s/100 iters), loss = 0.680973
I0428 20:23:17.229768  2209 solver.cpp:238]     Train net output #0: loss = 0.680973 (* 1 = 0.680973 loss)
I0428 20:23:17.229778  2209 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:23:17.454334  2209 solver.cpp:219] Iteration 600 (445.336 iter/s, 0.22455s/100 iters), loss = 0.368939
I0428 20:23:17.454370  2209 solver.cpp:238]     Train net output #0: loss = 0.368939 (* 1 = 0.368939 loss)
I0428 20:23:17.454378  2209 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:23:17.686868  2209 solver.cpp:219] Iteration 700 (430.145 iter/s, 0.23248s/100 iters), loss = 0.567625
I0428 20:23:17.686908  2209 solver.cpp:238]     Train net output #0: loss = 0.567625 (* 1 = 0.567625 loss)
I0428 20:23:17.686918  2209 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:23:17.924252  2209 solver.cpp:219] Iteration 800 (421.359 iter/s, 0.237328s/100 iters), loss = 0.531832
I0428 20:23:17.924294  2209 solver.cpp:238]     Train net output #0: loss = 0.531832 (* 1 = 0.531832 loss)
I0428 20:23:17.924304  2209 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:23:18.165611  2209 solver.cpp:219] Iteration 900 (414.425 iter/s, 0.241298s/100 iters), loss = 0.457033
I0428 20:23:18.165657  2209 solver.cpp:238]     Train net output #0: loss = 0.457033 (* 1 = 0.457033 loss)
I0428 20:23:18.165669  2209 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:23:18.245208  2215 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:23:18.403574  2209 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:23:18.409942  2209 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:23:18.412544  2209 solver.cpp:311] Iteration 1000, loss = 0.426538
I0428 20:23:18.412575  2209 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:23:18.509496  2216 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:23:18.511888  2209 solver.cpp:398]     Test net output #0: accuracy = 0.9506
I0428 20:23:18.511915  2209 solver.cpp:398]     Test net output #1: loss = 0.358685 (* 1 = 0.358685 loss)
I0428 20:23:18.511921  2209 solver.cpp:316] Optimization Done.
I0428 20:23:18.511925  2209 caffe.cpp:259] Optimization Done.
