I0428 20:27:53.427461  3260 caffe.cpp:218] Using GPUs 0
I0428 20:27:53.471681  3260 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:27:53.983898  3260 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1496.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:27:53.984072  3260 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1496.prototxt
I0428 20:27:53.984500  3260 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:27:53.984526  3260 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:27:53.984642  3260 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:27:53.984758  3260 layer_factory.hpp:77] Creating layer mnist
I0428 20:27:53.984897  3260 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:27:53.984935  3260 net.cpp:86] Creating Layer mnist
I0428 20:27:53.984946  3260 net.cpp:382] mnist -> data
I0428 20:27:53.984977  3260 net.cpp:382] mnist -> label
I0428 20:27:53.986248  3260 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:27:53.988716  3260 net.cpp:124] Setting up mnist
I0428 20:27:53.988737  3260 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:27:53.988749  3260 net.cpp:131] Top shape: 64 (64)
I0428 20:27:53.988756  3260 net.cpp:139] Memory required for data: 200960
I0428 20:27:53.988767  3260 layer_factory.hpp:77] Creating layer conv0
I0428 20:27:53.988790  3260 net.cpp:86] Creating Layer conv0
I0428 20:27:53.988822  3260 net.cpp:408] conv0 <- data
I0428 20:27:53.988847  3260 net.cpp:382] conv0 -> conv0
I0428 20:27:54.238487  3260 net.cpp:124] Setting up conv0
I0428 20:27:54.238515  3260 net.cpp:131] Top shape: 64 100 24 24 (3686400)
I0428 20:27:54.238520  3260 net.cpp:139] Memory required for data: 14946560
I0428 20:27:54.238555  3260 layer_factory.hpp:77] Creating layer pool0
I0428 20:27:54.238572  3260 net.cpp:86] Creating Layer pool0
I0428 20:27:54.238586  3260 net.cpp:408] pool0 <- conv0
I0428 20:27:54.238595  3260 net.cpp:382] pool0 -> pool0
I0428 20:27:54.238649  3260 net.cpp:124] Setting up pool0
I0428 20:27:54.238658  3260 net.cpp:131] Top shape: 64 100 12 12 (921600)
I0428 20:27:54.238663  3260 net.cpp:139] Memory required for data: 18632960
I0428 20:27:54.238667  3260 layer_factory.hpp:77] Creating layer conv1
I0428 20:27:54.238682  3260 net.cpp:86] Creating Layer conv1
I0428 20:27:54.238688  3260 net.cpp:408] conv1 <- pool0
I0428 20:27:54.238696  3260 net.cpp:382] conv1 -> conv1
I0428 20:27:54.241684  3260 net.cpp:124] Setting up conv1
I0428 20:27:54.241700  3260 net.cpp:131] Top shape: 64 5 8 8 (20480)
I0428 20:27:54.241706  3260 net.cpp:139] Memory required for data: 18714880
I0428 20:27:54.241734  3260 layer_factory.hpp:77] Creating layer pool1
I0428 20:27:54.241760  3260 net.cpp:86] Creating Layer pool1
I0428 20:27:54.241765  3260 net.cpp:408] pool1 <- conv1
I0428 20:27:54.241777  3260 net.cpp:382] pool1 -> pool1
I0428 20:27:54.241838  3260 net.cpp:124] Setting up pool1
I0428 20:27:54.241847  3260 net.cpp:131] Top shape: 64 5 4 4 (5120)
I0428 20:27:54.241852  3260 net.cpp:139] Memory required for data: 18735360
I0428 20:27:54.241858  3260 layer_factory.hpp:77] Creating layer ip1
I0428 20:27:54.241868  3260 net.cpp:86] Creating Layer ip1
I0428 20:27:54.241873  3260 net.cpp:408] ip1 <- pool1
I0428 20:27:54.241881  3260 net.cpp:382] ip1 -> ip1
I0428 20:27:54.241982  3260 net.cpp:124] Setting up ip1
I0428 20:27:54.241991  3260 net.cpp:131] Top shape: 64 10 (640)
I0428 20:27:54.241997  3260 net.cpp:139] Memory required for data: 18737920
I0428 20:27:54.242022  3260 layer_factory.hpp:77] Creating layer relu1
I0428 20:27:54.242035  3260 net.cpp:86] Creating Layer relu1
I0428 20:27:54.242043  3260 net.cpp:408] relu1 <- ip1
I0428 20:27:54.242051  3260 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:27:54.242241  3260 net.cpp:124] Setting up relu1
I0428 20:27:54.242251  3260 net.cpp:131] Top shape: 64 10 (640)
I0428 20:27:54.242256  3260 net.cpp:139] Memory required for data: 18740480
I0428 20:27:54.242261  3260 layer_factory.hpp:77] Creating layer ip2
I0428 20:27:54.242270  3260 net.cpp:86] Creating Layer ip2
I0428 20:27:54.242275  3260 net.cpp:408] ip2 <- ip1
I0428 20:27:54.242283  3260 net.cpp:382] ip2 -> ip2
I0428 20:27:54.242398  3260 net.cpp:124] Setting up ip2
I0428 20:27:54.242406  3260 net.cpp:131] Top shape: 64 10 (640)
I0428 20:27:54.242410  3260 net.cpp:139] Memory required for data: 18743040
I0428 20:27:54.242419  3260 layer_factory.hpp:77] Creating layer relu2
I0428 20:27:54.242429  3260 net.cpp:86] Creating Layer relu2
I0428 20:27:54.242434  3260 net.cpp:408] relu2 <- ip2
I0428 20:27:54.242442  3260 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:27:54.243208  3260 net.cpp:124] Setting up relu2
I0428 20:27:54.243223  3260 net.cpp:131] Top shape: 64 10 (640)
I0428 20:27:54.243228  3260 net.cpp:139] Memory required for data: 18745600
I0428 20:27:54.243249  3260 layer_factory.hpp:77] Creating layer ip3
I0428 20:27:54.243263  3260 net.cpp:86] Creating Layer ip3
I0428 20:27:54.243268  3260 net.cpp:408] ip3 <- ip2
I0428 20:27:54.243278  3260 net.cpp:382] ip3 -> ip3
I0428 20:27:54.243396  3260 net.cpp:124] Setting up ip3
I0428 20:27:54.243405  3260 net.cpp:131] Top shape: 64 10 (640)
I0428 20:27:54.243410  3260 net.cpp:139] Memory required for data: 18748160
I0428 20:27:54.243438  3260 layer_factory.hpp:77] Creating layer relu3
I0428 20:27:54.243449  3260 net.cpp:86] Creating Layer relu3
I0428 20:27:54.243456  3260 net.cpp:408] relu3 <- ip3
I0428 20:27:54.243463  3260 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:27:54.243669  3260 net.cpp:124] Setting up relu3
I0428 20:27:54.243680  3260 net.cpp:131] Top shape: 64 10 (640)
I0428 20:27:54.243683  3260 net.cpp:139] Memory required for data: 18750720
I0428 20:27:54.243688  3260 layer_factory.hpp:77] Creating layer loss
I0428 20:27:54.243701  3260 net.cpp:86] Creating Layer loss
I0428 20:27:54.243706  3260 net.cpp:408] loss <- ip3
I0428 20:27:54.243712  3260 net.cpp:408] loss <- label
I0428 20:27:54.243721  3260 net.cpp:382] loss -> loss
I0428 20:27:54.243744  3260 layer_factory.hpp:77] Creating layer loss
I0428 20:27:54.244014  3260 net.cpp:124] Setting up loss
I0428 20:27:54.244025  3260 net.cpp:131] Top shape: (1)
I0428 20:27:54.244030  3260 net.cpp:134]     with loss weight 1
I0428 20:27:54.244065  3260 net.cpp:139] Memory required for data: 18750724
I0428 20:27:54.244072  3260 net.cpp:200] loss needs backward computation.
I0428 20:27:54.244079  3260 net.cpp:200] relu3 needs backward computation.
I0428 20:27:54.244084  3260 net.cpp:200] ip3 needs backward computation.
I0428 20:27:54.244089  3260 net.cpp:200] relu2 needs backward computation.
I0428 20:27:54.244094  3260 net.cpp:200] ip2 needs backward computation.
I0428 20:27:54.244102  3260 net.cpp:200] relu1 needs backward computation.
I0428 20:27:54.244107  3260 net.cpp:200] ip1 needs backward computation.
I0428 20:27:54.244112  3260 net.cpp:200] pool1 needs backward computation.
I0428 20:27:54.244117  3260 net.cpp:200] conv1 needs backward computation.
I0428 20:27:54.244122  3260 net.cpp:200] pool0 needs backward computation.
I0428 20:27:54.244127  3260 net.cpp:200] conv0 needs backward computation.
I0428 20:27:54.244133  3260 net.cpp:202] mnist does not need backward computation.
I0428 20:27:54.244138  3260 net.cpp:244] This network produces output loss
I0428 20:27:54.244154  3260 net.cpp:257] Network initialization done.
I0428 20:27:54.244520  3260 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1496.prototxt
I0428 20:27:54.244554  3260 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:27:54.244649  3260 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:27:54.244766  3260 layer_factory.hpp:77] Creating layer mnist
I0428 20:27:54.244854  3260 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:27:54.244873  3260 net.cpp:86] Creating Layer mnist
I0428 20:27:54.244882  3260 net.cpp:382] mnist -> data
I0428 20:27:54.244894  3260 net.cpp:382] mnist -> label
I0428 20:27:54.245020  3260 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:27:54.247408  3260 net.cpp:124] Setting up mnist
I0428 20:27:54.247439  3260 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:27:54.247447  3260 net.cpp:131] Top shape: 100 (100)
I0428 20:27:54.247452  3260 net.cpp:139] Memory required for data: 314000
I0428 20:27:54.247457  3260 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:27:54.247472  3260 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:27:54.247476  3260 net.cpp:408] label_mnist_1_split <- label
I0428 20:27:54.247486  3260 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:27:54.247498  3260 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:27:54.247601  3260 net.cpp:124] Setting up label_mnist_1_split
I0428 20:27:54.247609  3260 net.cpp:131] Top shape: 100 (100)
I0428 20:27:54.247615  3260 net.cpp:131] Top shape: 100 (100)
I0428 20:27:54.247620  3260 net.cpp:139] Memory required for data: 314800
I0428 20:27:54.247625  3260 layer_factory.hpp:77] Creating layer conv0
I0428 20:27:54.247640  3260 net.cpp:86] Creating Layer conv0
I0428 20:27:54.247648  3260 net.cpp:408] conv0 <- data
I0428 20:27:54.247656  3260 net.cpp:382] conv0 -> conv0
I0428 20:27:54.249313  3260 net.cpp:124] Setting up conv0
I0428 20:27:54.249328  3260 net.cpp:131] Top shape: 100 100 24 24 (5760000)
I0428 20:27:54.249336  3260 net.cpp:139] Memory required for data: 23354800
I0428 20:27:54.249364  3260 layer_factory.hpp:77] Creating layer pool0
I0428 20:27:54.249377  3260 net.cpp:86] Creating Layer pool0
I0428 20:27:54.249383  3260 net.cpp:408] pool0 <- conv0
I0428 20:27:54.249392  3260 net.cpp:382] pool0 -> pool0
I0428 20:27:54.249435  3260 net.cpp:124] Setting up pool0
I0428 20:27:54.249444  3260 net.cpp:131] Top shape: 100 100 12 12 (1440000)
I0428 20:27:54.249449  3260 net.cpp:139] Memory required for data: 29114800
I0428 20:27:54.249454  3260 layer_factory.hpp:77] Creating layer conv1
I0428 20:27:54.249467  3260 net.cpp:86] Creating Layer conv1
I0428 20:27:54.249475  3260 net.cpp:408] conv1 <- pool0
I0428 20:27:54.249485  3260 net.cpp:382] conv1 -> conv1
I0428 20:27:54.251243  3260 net.cpp:124] Setting up conv1
I0428 20:27:54.251257  3260 net.cpp:131] Top shape: 100 5 8 8 (32000)
I0428 20:27:54.251263  3260 net.cpp:139] Memory required for data: 29242800
I0428 20:27:54.251278  3260 layer_factory.hpp:77] Creating layer pool1
I0428 20:27:54.251287  3260 net.cpp:86] Creating Layer pool1
I0428 20:27:54.251292  3260 net.cpp:408] pool1 <- conv1
I0428 20:27:54.251305  3260 net.cpp:382] pool1 -> pool1
I0428 20:27:54.251364  3260 net.cpp:124] Setting up pool1
I0428 20:27:54.251372  3260 net.cpp:131] Top shape: 100 5 4 4 (8000)
I0428 20:27:54.251377  3260 net.cpp:139] Memory required for data: 29274800
I0428 20:27:54.251382  3260 layer_factory.hpp:77] Creating layer ip1
I0428 20:27:54.251394  3260 net.cpp:86] Creating Layer ip1
I0428 20:27:54.251401  3260 net.cpp:408] ip1 <- pool1
I0428 20:27:54.251410  3260 net.cpp:382] ip1 -> ip1
I0428 20:27:54.251518  3260 net.cpp:124] Setting up ip1
I0428 20:27:54.251528  3260 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:27:54.251544  3260 net.cpp:139] Memory required for data: 29278800
I0428 20:27:54.251557  3260 layer_factory.hpp:77] Creating layer relu1
I0428 20:27:54.251569  3260 net.cpp:86] Creating Layer relu1
I0428 20:27:54.251579  3260 net.cpp:408] relu1 <- ip1
I0428 20:27:54.251586  3260 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:27:54.251751  3260 net.cpp:124] Setting up relu1
I0428 20:27:54.251761  3260 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:27:54.251767  3260 net.cpp:139] Memory required for data: 29282800
I0428 20:27:54.251772  3260 layer_factory.hpp:77] Creating layer ip2
I0428 20:27:54.251785  3260 net.cpp:86] Creating Layer ip2
I0428 20:27:54.251791  3260 net.cpp:408] ip2 <- ip1
I0428 20:27:54.251801  3260 net.cpp:382] ip2 -> ip2
I0428 20:27:54.251907  3260 net.cpp:124] Setting up ip2
I0428 20:27:54.251916  3260 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:27:54.251921  3260 net.cpp:139] Memory required for data: 29286800
I0428 20:27:54.251930  3260 layer_factory.hpp:77] Creating layer relu2
I0428 20:27:54.251938  3260 net.cpp:86] Creating Layer relu2
I0428 20:27:54.251947  3260 net.cpp:408] relu2 <- ip2
I0428 20:27:54.251955  3260 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:27:54.252149  3260 net.cpp:124] Setting up relu2
I0428 20:27:54.252158  3260 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:27:54.252163  3260 net.cpp:139] Memory required for data: 29290800
I0428 20:27:54.252168  3260 layer_factory.hpp:77] Creating layer ip3
I0428 20:27:54.252177  3260 net.cpp:86] Creating Layer ip3
I0428 20:27:54.252182  3260 net.cpp:408] ip3 <- ip2
I0428 20:27:54.252192  3260 net.cpp:382] ip3 -> ip3
I0428 20:27:54.252341  3260 net.cpp:124] Setting up ip3
I0428 20:27:54.252351  3260 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:27:54.252357  3260 net.cpp:139] Memory required for data: 29294800
I0428 20:27:54.252369  3260 layer_factory.hpp:77] Creating layer relu3
I0428 20:27:54.252378  3260 net.cpp:86] Creating Layer relu3
I0428 20:27:54.252385  3260 net.cpp:408] relu3 <- ip3
I0428 20:27:54.252394  3260 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:27:54.253226  3260 net.cpp:124] Setting up relu3
I0428 20:27:54.253239  3260 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:27:54.253245  3260 net.cpp:139] Memory required for data: 29298800
I0428 20:27:54.253250  3260 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:27:54.253257  3260 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:27:54.253262  3260 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:27:54.253273  3260 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:27:54.253281  3260 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:27:54.253326  3260 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:27:54.253335  3260 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:27:54.253341  3260 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:27:54.253346  3260 net.cpp:139] Memory required for data: 29306800
I0428 20:27:54.253351  3260 layer_factory.hpp:77] Creating layer accuracy
I0428 20:27:54.253361  3260 net.cpp:86] Creating Layer accuracy
I0428 20:27:54.253367  3260 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:27:54.253373  3260 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:27:54.253382  3260 net.cpp:382] accuracy -> accuracy
I0428 20:27:54.253392  3260 net.cpp:124] Setting up accuracy
I0428 20:27:54.253401  3260 net.cpp:131] Top shape: (1)
I0428 20:27:54.253406  3260 net.cpp:139] Memory required for data: 29306804
I0428 20:27:54.253410  3260 layer_factory.hpp:77] Creating layer loss
I0428 20:27:54.253420  3260 net.cpp:86] Creating Layer loss
I0428 20:27:54.253427  3260 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:27:54.253432  3260 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:27:54.253439  3260 net.cpp:382] loss -> loss
I0428 20:27:54.253448  3260 layer_factory.hpp:77] Creating layer loss
I0428 20:27:54.253698  3260 net.cpp:124] Setting up loss
I0428 20:27:54.253711  3260 net.cpp:131] Top shape: (1)
I0428 20:27:54.253716  3260 net.cpp:134]     with loss weight 1
I0428 20:27:54.253737  3260 net.cpp:139] Memory required for data: 29306808
I0428 20:27:54.253746  3260 net.cpp:200] loss needs backward computation.
I0428 20:27:54.253751  3260 net.cpp:202] accuracy does not need backward computation.
I0428 20:27:54.253757  3260 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:27:54.253763  3260 net.cpp:200] relu3 needs backward computation.
I0428 20:27:54.253768  3260 net.cpp:200] ip3 needs backward computation.
I0428 20:27:54.253773  3260 net.cpp:200] relu2 needs backward computation.
I0428 20:27:54.253778  3260 net.cpp:200] ip2 needs backward computation.
I0428 20:27:54.253783  3260 net.cpp:200] relu1 needs backward computation.
I0428 20:27:54.253788  3260 net.cpp:200] ip1 needs backward computation.
I0428 20:27:54.253793  3260 net.cpp:200] pool1 needs backward computation.
I0428 20:27:54.253798  3260 net.cpp:200] conv1 needs backward computation.
I0428 20:27:54.253804  3260 net.cpp:200] pool0 needs backward computation.
I0428 20:27:54.253813  3260 net.cpp:200] conv0 needs backward computation.
I0428 20:27:54.253818  3260 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:27:54.253824  3260 net.cpp:202] mnist does not need backward computation.
I0428 20:27:54.253829  3260 net.cpp:244] This network produces output accuracy
I0428 20:27:54.253835  3260 net.cpp:244] This network produces output loss
I0428 20:27:54.253852  3260 net.cpp:257] Network initialization done.
I0428 20:27:54.253901  3260 solver.cpp:56] Solver scaffolding done.
I0428 20:27:54.254264  3260 caffe.cpp:248] Starting Optimization
I0428 20:27:54.254271  3260 solver.cpp:273] Solving LeNet
I0428 20:27:54.254276  3260 solver.cpp:274] Learning Rate Policy: inv
I0428 20:27:54.255034  3260 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:27:54.350500  3268 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:27:54.353003  3260 solver.cpp:398]     Test net output #0: accuracy = 0.1139
I0428 20:27:54.353025  3260 solver.cpp:398]     Test net output #1: loss = 2.30511 (* 1 = 2.30511 loss)
I0428 20:27:54.357455  3260 solver.cpp:219] Iteration 0 (-7.8753e-43 iter/s, 0.103143s/100 iters), loss = 2.29539
I0428 20:27:54.357482  3260 solver.cpp:238]     Train net output #0: loss = 2.29539 (* 1 = 2.29539 loss)
I0428 20:27:54.357514  3260 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:27:54.572773  3260 solver.cpp:219] Iteration 100 (464.543 iter/s, 0.215265s/100 iters), loss = 0.862624
I0428 20:27:54.572844  3260 solver.cpp:238]     Train net output #0: loss = 0.862624 (* 1 = 0.862624 loss)
I0428 20:27:54.572857  3260 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:27:54.799412  3260 solver.cpp:219] Iteration 200 (441.394 iter/s, 0.226555s/100 iters), loss = 0.505854
I0428 20:27:54.799465  3260 solver.cpp:238]     Train net output #0: loss = 0.505854 (* 1 = 0.505854 loss)
I0428 20:27:54.799479  3260 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:27:55.019613  3260 solver.cpp:219] Iteration 300 (454.274 iter/s, 0.220131s/100 iters), loss = 0.57807
I0428 20:27:55.019656  3260 solver.cpp:238]     Train net output #0: loss = 0.57807 (* 1 = 0.57807 loss)
I0428 20:27:55.019667  3260 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:27:55.234333  3260 solver.cpp:219] Iteration 400 (465.855 iter/s, 0.214659s/100 iters), loss = 0.457251
I0428 20:27:55.234375  3260 solver.cpp:238]     Train net output #0: loss = 0.457251 (* 1 = 0.457251 loss)
I0428 20:27:55.234387  3260 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:27:55.450546  3260 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:27:55.553411  3268 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:27:55.556205  3260 solver.cpp:398]     Test net output #0: accuracy = 0.9244
I0428 20:27:55.556233  3260 solver.cpp:398]     Test net output #1: loss = 0.280638 (* 1 = 0.280638 loss)
I0428 20:27:55.558188  3260 solver.cpp:219] Iteration 500 (308.837 iter/s, 0.323795s/100 iters), loss = 0.189303
I0428 20:27:55.558217  3260 solver.cpp:238]     Train net output #0: loss = 0.189303 (* 1 = 0.189303 loss)
I0428 20:27:55.558245  3260 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:27:55.777815  3260 solver.cpp:219] Iteration 600 (455.418 iter/s, 0.219578s/100 iters), loss = 0.262132
I0428 20:27:55.777861  3260 solver.cpp:238]     Train net output #0: loss = 0.262132 (* 1 = 0.262132 loss)
I0428 20:27:55.777873  3260 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:27:55.989478  3260 solver.cpp:219] Iteration 700 (472.587 iter/s, 0.211601s/100 iters), loss = 0.267442
I0428 20:27:55.989521  3260 solver.cpp:238]     Train net output #0: loss = 0.267442 (* 1 = 0.267442 loss)
I0428 20:27:55.989531  3260 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:27:56.205425  3260 solver.cpp:219] Iteration 800 (463.204 iter/s, 0.215888s/100 iters), loss = 0.307307
I0428 20:27:56.205474  3260 solver.cpp:238]     Train net output #0: loss = 0.307307 (* 1 = 0.307307 loss)
I0428 20:27:56.205488  3260 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:27:56.424229  3260 solver.cpp:219] Iteration 900 (457.169 iter/s, 0.218738s/100 iters), loss = 0.184875
I0428 20:27:56.424278  3260 solver.cpp:238]     Train net output #0: loss = 0.184875 (* 1 = 0.184875 loss)
I0428 20:27:56.424290  3260 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:27:56.494781  3267 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:27:56.639760  3260 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:27:56.641769  3260 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:27:56.643051  3260 solver.cpp:311] Iteration 1000, loss = 0.177731
I0428 20:27:56.643081  3260 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:27:56.749491  3268 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:27:56.753127  3260 solver.cpp:398]     Test net output #0: accuracy = 0.9581
I0428 20:27:56.753155  3260 solver.cpp:398]     Test net output #1: loss = 0.154799 (* 1 = 0.154799 loss)
I0428 20:27:56.753162  3260 solver.cpp:316] Optimization Done.
I0428 20:27:56.753166  3260 caffe.cpp:259] Optimization Done.
