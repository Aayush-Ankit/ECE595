I0428 19:46:47.466163 26061 caffe.cpp:218] Using GPUs 0
I0428 19:46:47.507237 26061 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:46:48.023963 26061 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test499.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:46:48.024132 26061 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test499.prototxt
I0428 19:46:48.024559 26061 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:46:48.024586 26061 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:46:48.024701 26061 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:46:48.024822 26061 layer_factory.hpp:77] Creating layer mnist
I0428 19:46:48.024965 26061 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:46:48.024998 26061 net.cpp:86] Creating Layer mnist
I0428 19:46:48.025012 26061 net.cpp:382] mnist -> data
I0428 19:46:48.025043 26061 net.cpp:382] mnist -> label
I0428 19:46:48.026312 26061 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:46:48.028770 26061 net.cpp:124] Setting up mnist
I0428 19:46:48.028790 26061 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:46:48.028800 26061 net.cpp:131] Top shape: 64 (64)
I0428 19:46:48.028806 26061 net.cpp:139] Memory required for data: 200960
I0428 19:46:48.028823 26061 layer_factory.hpp:77] Creating layer conv0
I0428 19:46:48.028861 26061 net.cpp:86] Creating Layer conv0
I0428 19:46:48.028883 26061 net.cpp:408] conv0 <- data
I0428 19:46:48.028904 26061 net.cpp:382] conv0 -> conv0
I0428 19:46:48.319572 26061 net.cpp:124] Setting up conv0
I0428 19:46:48.319607 26061 net.cpp:131] Top shape: 64 2 24 24 (73728)
I0428 19:46:48.319613 26061 net.cpp:139] Memory required for data: 495872
I0428 19:46:48.319636 26061 layer_factory.hpp:77] Creating layer pool0
I0428 19:46:48.319655 26061 net.cpp:86] Creating Layer pool0
I0428 19:46:48.319664 26061 net.cpp:408] pool0 <- conv0
I0428 19:46:48.319674 26061 net.cpp:382] pool0 -> pool0
I0428 19:46:48.319746 26061 net.cpp:124] Setting up pool0
I0428 19:46:48.319757 26061 net.cpp:131] Top shape: 64 2 12 12 (18432)
I0428 19:46:48.319763 26061 net.cpp:139] Memory required for data: 569600
I0428 19:46:48.319769 26061 layer_factory.hpp:77] Creating layer conv1
I0428 19:46:48.319787 26061 net.cpp:86] Creating Layer conv1
I0428 19:46:48.319795 26061 net.cpp:408] conv1 <- pool0
I0428 19:46:48.319805 26061 net.cpp:382] conv1 -> conv1
I0428 19:46:48.322804 26061 net.cpp:124] Setting up conv1
I0428 19:46:48.322824 26061 net.cpp:131] Top shape: 64 100 8 8 (409600)
I0428 19:46:48.322830 26061 net.cpp:139] Memory required for data: 2208000
I0428 19:46:48.322845 26061 layer_factory.hpp:77] Creating layer pool1
I0428 19:46:48.322859 26061 net.cpp:86] Creating Layer pool1
I0428 19:46:48.322865 26061 net.cpp:408] pool1 <- conv1
I0428 19:46:48.322875 26061 net.cpp:382] pool1 -> pool1
I0428 19:46:48.322923 26061 net.cpp:124] Setting up pool1
I0428 19:46:48.322933 26061 net.cpp:131] Top shape: 64 100 4 4 (102400)
I0428 19:46:48.322939 26061 net.cpp:139] Memory required for data: 2617600
I0428 19:46:48.322945 26061 layer_factory.hpp:77] Creating layer ip1
I0428 19:46:48.322957 26061 net.cpp:86] Creating Layer ip1
I0428 19:46:48.322968 26061 net.cpp:408] ip1 <- pool1
I0428 19:46:48.322978 26061 net.cpp:382] ip1 -> ip1
I0428 19:46:48.324506 26061 net.cpp:124] Setting up ip1
I0428 19:46:48.324522 26061 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:46:48.324527 26061 net.cpp:139] Memory required for data: 2630400
I0428 19:46:48.324542 26061 layer_factory.hpp:77] Creating layer relu1
I0428 19:46:48.324553 26061 net.cpp:86] Creating Layer relu1
I0428 19:46:48.324559 26061 net.cpp:408] relu1 <- ip1
I0428 19:46:48.324568 26061 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:46:48.324771 26061 net.cpp:124] Setting up relu1
I0428 19:46:48.324793 26061 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:46:48.324798 26061 net.cpp:139] Memory required for data: 2643200
I0428 19:46:48.324820 26061 layer_factory.hpp:77] Creating layer ip2
I0428 19:46:48.324832 26061 net.cpp:86] Creating Layer ip2
I0428 19:46:48.324851 26061 net.cpp:408] ip2 <- ip1
I0428 19:46:48.324862 26061 net.cpp:382] ip2 -> ip2
I0428 19:46:48.324997 26061 net.cpp:124] Setting up ip2
I0428 19:46:48.325007 26061 net.cpp:131] Top shape: 64 10 (640)
I0428 19:46:48.325013 26061 net.cpp:139] Memory required for data: 2645760
I0428 19:46:48.325023 26061 layer_factory.hpp:77] Creating layer relu2
I0428 19:46:48.325034 26061 net.cpp:86] Creating Layer relu2
I0428 19:46:48.325044 26061 net.cpp:408] relu2 <- ip2
I0428 19:46:48.325052 26061 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:46:48.325865 26061 net.cpp:124] Setting up relu2
I0428 19:46:48.325883 26061 net.cpp:131] Top shape: 64 10 (640)
I0428 19:46:48.325889 26061 net.cpp:139] Memory required for data: 2648320
I0428 19:46:48.325896 26061 layer_factory.hpp:77] Creating layer ip3
I0428 19:46:48.325908 26061 net.cpp:86] Creating Layer ip3
I0428 19:46:48.325914 26061 net.cpp:408] ip3 <- ip2
I0428 19:46:48.325924 26061 net.cpp:382] ip3 -> ip3
I0428 19:46:48.326038 26061 net.cpp:124] Setting up ip3
I0428 19:46:48.326050 26061 net.cpp:131] Top shape: 64 10 (640)
I0428 19:46:48.326055 26061 net.cpp:139] Memory required for data: 2650880
I0428 19:46:48.326069 26061 layer_factory.hpp:77] Creating layer relu3
I0428 19:46:48.326078 26061 net.cpp:86] Creating Layer relu3
I0428 19:46:48.326088 26061 net.cpp:408] relu3 <- ip3
I0428 19:46:48.326097 26061 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:46:48.326287 26061 net.cpp:124] Setting up relu3
I0428 19:46:48.326298 26061 net.cpp:131] Top shape: 64 10 (640)
I0428 19:46:48.326304 26061 net.cpp:139] Memory required for data: 2653440
I0428 19:46:48.326309 26061 layer_factory.hpp:77] Creating layer loss
I0428 19:46:48.326319 26061 net.cpp:86] Creating Layer loss
I0428 19:46:48.326325 26061 net.cpp:408] loss <- ip3
I0428 19:46:48.326333 26061 net.cpp:408] loss <- label
I0428 19:46:48.326342 26061 net.cpp:382] loss -> loss
I0428 19:46:48.326364 26061 layer_factory.hpp:77] Creating layer loss
I0428 19:46:48.326622 26061 net.cpp:124] Setting up loss
I0428 19:46:48.326633 26061 net.cpp:131] Top shape: (1)
I0428 19:46:48.326639 26061 net.cpp:134]     with loss weight 1
I0428 19:46:48.326660 26061 net.cpp:139] Memory required for data: 2653444
I0428 19:46:48.326668 26061 net.cpp:200] loss needs backward computation.
I0428 19:46:48.326674 26061 net.cpp:200] relu3 needs backward computation.
I0428 19:46:48.326680 26061 net.cpp:200] ip3 needs backward computation.
I0428 19:46:48.326685 26061 net.cpp:200] relu2 needs backward computation.
I0428 19:46:48.326690 26061 net.cpp:200] ip2 needs backward computation.
I0428 19:46:48.326696 26061 net.cpp:200] relu1 needs backward computation.
I0428 19:46:48.326701 26061 net.cpp:200] ip1 needs backward computation.
I0428 19:46:48.326707 26061 net.cpp:200] pool1 needs backward computation.
I0428 19:46:48.326714 26061 net.cpp:200] conv1 needs backward computation.
I0428 19:46:48.326719 26061 net.cpp:200] pool0 needs backward computation.
I0428 19:46:48.326725 26061 net.cpp:200] conv0 needs backward computation.
I0428 19:46:48.326731 26061 net.cpp:202] mnist does not need backward computation.
I0428 19:46:48.326737 26061 net.cpp:244] This network produces output loss
I0428 19:46:48.326752 26061 net.cpp:257] Network initialization done.
I0428 19:46:48.327118 26061 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test499.prototxt
I0428 19:46:48.327157 26061 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:46:48.327265 26061 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:46:48.327389 26061 layer_factory.hpp:77] Creating layer mnist
I0428 19:46:48.327455 26061 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:46:48.327476 26061 net.cpp:86] Creating Layer mnist
I0428 19:46:48.327483 26061 net.cpp:382] mnist -> data
I0428 19:46:48.327497 26061 net.cpp:382] mnist -> label
I0428 19:46:48.327627 26061 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:46:48.328969 26061 net.cpp:124] Setting up mnist
I0428 19:46:48.328986 26061 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:46:48.328995 26061 net.cpp:131] Top shape: 100 (100)
I0428 19:46:48.329001 26061 net.cpp:139] Memory required for data: 314000
I0428 19:46:48.329012 26061 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:46:48.329023 26061 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:46:48.329030 26061 net.cpp:408] label_mnist_1_split <- label
I0428 19:46:48.329040 26061 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:46:48.329051 26061 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:46:48.329111 26061 net.cpp:124] Setting up label_mnist_1_split
I0428 19:46:48.329121 26061 net.cpp:131] Top shape: 100 (100)
I0428 19:46:48.329128 26061 net.cpp:131] Top shape: 100 (100)
I0428 19:46:48.329133 26061 net.cpp:139] Memory required for data: 314800
I0428 19:46:48.329139 26061 layer_factory.hpp:77] Creating layer conv0
I0428 19:46:48.329154 26061 net.cpp:86] Creating Layer conv0
I0428 19:46:48.329162 26061 net.cpp:408] conv0 <- data
I0428 19:46:48.329171 26061 net.cpp:382] conv0 -> conv0
I0428 19:46:48.330690 26061 net.cpp:124] Setting up conv0
I0428 19:46:48.330710 26061 net.cpp:131] Top shape: 100 2 24 24 (115200)
I0428 19:46:48.330718 26061 net.cpp:139] Memory required for data: 775600
I0428 19:46:48.330734 26061 layer_factory.hpp:77] Creating layer pool0
I0428 19:46:48.330744 26061 net.cpp:86] Creating Layer pool0
I0428 19:46:48.330752 26061 net.cpp:408] pool0 <- conv0
I0428 19:46:48.330761 26061 net.cpp:382] pool0 -> pool0
I0428 19:46:48.330866 26061 net.cpp:124] Setting up pool0
I0428 19:46:48.330875 26061 net.cpp:131] Top shape: 100 2 12 12 (28800)
I0428 19:46:48.330883 26061 net.cpp:139] Memory required for data: 890800
I0428 19:46:48.330896 26061 layer_factory.hpp:77] Creating layer conv1
I0428 19:46:48.330911 26061 net.cpp:86] Creating Layer conv1
I0428 19:46:48.330919 26061 net.cpp:408] conv1 <- pool0
I0428 19:46:48.330929 26061 net.cpp:382] conv1 -> conv1
I0428 19:46:48.332550 26061 net.cpp:124] Setting up conv1
I0428 19:46:48.332568 26061 net.cpp:131] Top shape: 100 100 8 8 (640000)
I0428 19:46:48.332574 26061 net.cpp:139] Memory required for data: 3450800
I0428 19:46:48.332588 26061 layer_factory.hpp:77] Creating layer pool1
I0428 19:46:48.332600 26061 net.cpp:86] Creating Layer pool1
I0428 19:46:48.332607 26061 net.cpp:408] pool1 <- conv1
I0428 19:46:48.332626 26061 net.cpp:382] pool1 -> pool1
I0428 19:46:48.332672 26061 net.cpp:124] Setting up pool1
I0428 19:46:48.332684 26061 net.cpp:131] Top shape: 100 100 4 4 (160000)
I0428 19:46:48.332690 26061 net.cpp:139] Memory required for data: 4090800
I0428 19:46:48.332698 26061 layer_factory.hpp:77] Creating layer ip1
I0428 19:46:48.332708 26061 net.cpp:86] Creating Layer ip1
I0428 19:46:48.332715 26061 net.cpp:408] ip1 <- pool1
I0428 19:46:48.332725 26061 net.cpp:382] ip1 -> ip1
I0428 19:46:48.333354 26061 net.cpp:124] Setting up ip1
I0428 19:46:48.333366 26061 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:46:48.333386 26061 net.cpp:139] Memory required for data: 4110800
I0428 19:46:48.333401 26061 layer_factory.hpp:77] Creating layer relu1
I0428 19:46:48.333411 26061 net.cpp:86] Creating Layer relu1
I0428 19:46:48.333418 26061 net.cpp:408] relu1 <- ip1
I0428 19:46:48.333426 26061 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:46:48.333607 26061 net.cpp:124] Setting up relu1
I0428 19:46:48.333618 26061 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:46:48.333624 26061 net.cpp:139] Memory required for data: 4130800
I0428 19:46:48.333631 26061 layer_factory.hpp:77] Creating layer ip2
I0428 19:46:48.333643 26061 net.cpp:86] Creating Layer ip2
I0428 19:46:48.333650 26061 net.cpp:408] ip2 <- ip1
I0428 19:46:48.333659 26061 net.cpp:382] ip2 -> ip2
I0428 19:46:48.333778 26061 net.cpp:124] Setting up ip2
I0428 19:46:48.333788 26061 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:46:48.333793 26061 net.cpp:139] Memory required for data: 4134800
I0428 19:46:48.333804 26061 layer_factory.hpp:77] Creating layer relu2
I0428 19:46:48.333817 26061 net.cpp:86] Creating Layer relu2
I0428 19:46:48.333823 26061 net.cpp:408] relu2 <- ip2
I0428 19:46:48.333832 26061 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:46:48.334087 26061 net.cpp:124] Setting up relu2
I0428 19:46:48.334098 26061 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:46:48.334105 26061 net.cpp:139] Memory required for data: 4138800
I0428 19:46:48.334110 26061 layer_factory.hpp:77] Creating layer ip3
I0428 19:46:48.334121 26061 net.cpp:86] Creating Layer ip3
I0428 19:46:48.334128 26061 net.cpp:408] ip3 <- ip2
I0428 19:46:48.334139 26061 net.cpp:382] ip3 -> ip3
I0428 19:46:48.334259 26061 net.cpp:124] Setting up ip3
I0428 19:46:48.334269 26061 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:46:48.334273 26061 net.cpp:139] Memory required for data: 4142800
I0428 19:46:48.334290 26061 layer_factory.hpp:77] Creating layer relu3
I0428 19:46:48.334300 26061 net.cpp:86] Creating Layer relu3
I0428 19:46:48.334305 26061 net.cpp:408] relu3 <- ip3
I0428 19:46:48.334312 26061 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:46:48.335214 26061 net.cpp:124] Setting up relu3
I0428 19:46:48.335229 26061 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:46:48.335235 26061 net.cpp:139] Memory required for data: 4146800
I0428 19:46:48.335242 26061 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:46:48.335253 26061 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:46:48.335258 26061 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:46:48.335268 26061 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:46:48.335280 26061 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:46:48.335330 26061 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:46:48.335338 26061 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:46:48.335346 26061 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:46:48.335351 26061 net.cpp:139] Memory required for data: 4154800
I0428 19:46:48.335357 26061 layer_factory.hpp:77] Creating layer accuracy
I0428 19:46:48.335367 26061 net.cpp:86] Creating Layer accuracy
I0428 19:46:48.335374 26061 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:46:48.335382 26061 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:46:48.335391 26061 net.cpp:382] accuracy -> accuracy
I0428 19:46:48.335402 26061 net.cpp:124] Setting up accuracy
I0428 19:46:48.335412 26061 net.cpp:131] Top shape: (1)
I0428 19:46:48.335417 26061 net.cpp:139] Memory required for data: 4154804
I0428 19:46:48.335422 26061 layer_factory.hpp:77] Creating layer loss
I0428 19:46:48.335430 26061 net.cpp:86] Creating Layer loss
I0428 19:46:48.335436 26061 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:46:48.335443 26061 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:46:48.335453 26061 net.cpp:382] loss -> loss
I0428 19:46:48.335464 26061 layer_factory.hpp:77] Creating layer loss
I0428 19:46:48.335765 26061 net.cpp:124] Setting up loss
I0428 19:46:48.335777 26061 net.cpp:131] Top shape: (1)
I0428 19:46:48.335782 26061 net.cpp:134]     with loss weight 1
I0428 19:46:48.335793 26061 net.cpp:139] Memory required for data: 4154808
I0428 19:46:48.335810 26061 net.cpp:200] loss needs backward computation.
I0428 19:46:48.335817 26061 net.cpp:202] accuracy does not need backward computation.
I0428 19:46:48.335824 26061 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:46:48.335829 26061 net.cpp:200] relu3 needs backward computation.
I0428 19:46:48.335834 26061 net.cpp:200] ip3 needs backward computation.
I0428 19:46:48.335840 26061 net.cpp:200] relu2 needs backward computation.
I0428 19:46:48.335845 26061 net.cpp:200] ip2 needs backward computation.
I0428 19:46:48.335850 26061 net.cpp:200] relu1 needs backward computation.
I0428 19:46:48.335855 26061 net.cpp:200] ip1 needs backward computation.
I0428 19:46:48.335861 26061 net.cpp:200] pool1 needs backward computation.
I0428 19:46:48.335866 26061 net.cpp:200] conv1 needs backward computation.
I0428 19:46:48.335872 26061 net.cpp:200] pool0 needs backward computation.
I0428 19:46:48.335877 26061 net.cpp:200] conv0 needs backward computation.
I0428 19:46:48.335885 26061 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:46:48.335891 26061 net.cpp:202] mnist does not need backward computation.
I0428 19:46:48.335896 26061 net.cpp:244] This network produces output accuracy
I0428 19:46:48.335901 26061 net.cpp:244] This network produces output loss
I0428 19:46:48.335922 26061 net.cpp:257] Network initialization done.
I0428 19:46:48.335978 26061 solver.cpp:56] Solver scaffolding done.
I0428 19:46:48.336343 26061 caffe.cpp:248] Starting Optimization
I0428 19:46:48.336350 26061 solver.cpp:273] Solving LeNet
I0428 19:46:48.336355 26061 solver.cpp:274] Learning Rate Policy: inv
I0428 19:46:48.337211 26061 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:46:48.393677 26068 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:46:48.394593 26061 solver.cpp:398]     Test net output #0: accuracy = 0.1061
I0428 19:46:48.394615 26061 solver.cpp:398]     Test net output #1: loss = 2.30193 (* 1 = 2.30193 loss)
I0428 19:46:48.398351 26061 solver.cpp:219] Iteration 0 (0 iter/s, 0.0619663s/100 iters), loss = 2.31098
I0428 19:46:48.398380 26061 solver.cpp:238]     Train net output #0: loss = 2.31098 (* 1 = 2.31098 loss)
I0428 19:46:48.398412 26061 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:46:48.549860 26061 solver.cpp:219] Iteration 100 (660.205 iter/s, 0.151468s/100 iters), loss = 0.416507
I0428 19:46:48.549890 26061 solver.cpp:238]     Train net output #0: loss = 0.416507 (* 1 = 0.416507 loss)
I0428 19:46:48.549899 26061 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:46:48.701257 26061 solver.cpp:219] Iteration 200 (660.697 iter/s, 0.151355s/100 iters), loss = 0.294678
I0428 19:46:48.701292 26061 solver.cpp:238]     Train net output #0: loss = 0.294678 (* 1 = 0.294678 loss)
I0428 19:46:48.701303 26061 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:46:48.868228 26061 solver.cpp:219] Iteration 300 (599.074 iter/s, 0.166924s/100 iters), loss = 0.41575
I0428 19:46:48.868273 26061 solver.cpp:238]     Train net output #0: loss = 0.41575 (* 1 = 0.41575 loss)
I0428 19:46:48.868288 26061 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:46:49.047401 26061 solver.cpp:219] Iteration 400 (558.297 iter/s, 0.179116s/100 iters), loss = 0.223351
I0428 19:46:49.047451 26061 solver.cpp:238]     Train net output #0: loss = 0.223351 (* 1 = 0.223351 loss)
I0428 19:46:49.047467 26061 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:46:49.228494 26061 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:46:49.234084 26061 blocking_queue.cpp:49] Waiting for data
I0428 19:46:49.300611 26068 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:46:49.301254 26061 solver.cpp:398]     Test net output #0: accuracy = 0.9529
I0428 19:46:49.301285 26061 solver.cpp:398]     Test net output #1: loss = 0.15652 (* 1 = 0.15652 loss)
I0428 19:46:49.302881 26061 solver.cpp:219] Iteration 500 (391.518 iter/s, 0.255416s/100 iters), loss = 0.16169
I0428 19:46:49.302928 26061 solver.cpp:238]     Train net output #0: loss = 0.16169 (* 1 = 0.16169 loss)
I0428 19:46:49.302958 26061 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:46:49.456604 26061 solver.cpp:219] Iteration 600 (650.744 iter/s, 0.15367s/100 iters), loss = 0.117773
I0428 19:46:49.456634 26061 solver.cpp:238]     Train net output #0: loss = 0.117772 (* 1 = 0.117772 loss)
I0428 19:46:49.456641 26061 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:46:49.609875 26061 solver.cpp:219] Iteration 700 (652.621 iter/s, 0.153228s/100 iters), loss = 0.273951
I0428 19:46:49.609904 26061 solver.cpp:238]     Train net output #0: loss = 0.273951 (* 1 = 0.273951 loss)
I0428 19:46:49.609912 26061 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:46:49.756933 26061 solver.cpp:219] Iteration 800 (680.19 iter/s, 0.147018s/100 iters), loss = 0.185209
I0428 19:46:49.756976 26061 solver.cpp:238]     Train net output #0: loss = 0.185209 (* 1 = 0.185209 loss)
I0428 19:46:49.756983 26061 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:46:49.901886 26061 solver.cpp:219] Iteration 900 (690.14 iter/s, 0.144898s/100 iters), loss = 0.165772
I0428 19:46:49.901922 26061 solver.cpp:238]     Train net output #0: loss = 0.165772 (* 1 = 0.165772 loss)
I0428 19:46:49.901928 26061 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:46:49.949901 26067 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:46:50.044021 26061 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:46:50.046445 26061 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:46:50.047590 26061 solver.cpp:311] Iteration 1000, loss = 0.0754679
I0428 19:46:50.047605 26061 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:46:50.116026 26068 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:46:50.116611 26061 solver.cpp:398]     Test net output #0: accuracy = 0.9726
I0428 19:46:50.116646 26061 solver.cpp:398]     Test net output #1: loss = 0.0896135 (* 1 = 0.0896135 loss)
I0428 19:46:50.116650 26061 solver.cpp:316] Optimization Done.
I0428 19:46:50.116653 26061 caffe.cpp:259] Optimization Done.
