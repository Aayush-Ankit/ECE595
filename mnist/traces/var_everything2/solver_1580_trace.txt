I0428 20:32:56.883672  4220 caffe.cpp:218] Using GPUs 0
I0428 20:32:56.922657  4220 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:32:57.447212  4220 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1580.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:32:57.447356  4220 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1580.prototxt
I0428 20:32:57.447773  4220 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:32:57.447793  4220 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:32:57.447896  4220 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:32:57.447976  4220 layer_factory.hpp:77] Creating layer mnist
I0428 20:32:57.448076  4220 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:32:57.448101  4220 net.cpp:86] Creating Layer mnist
I0428 20:32:57.448112  4220 net.cpp:382] mnist -> data
I0428 20:32:57.448135  4220 net.cpp:382] mnist -> label
I0428 20:32:57.449254  4220 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:32:57.451702  4220 net.cpp:124] Setting up mnist
I0428 20:32:57.451723  4220 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:32:57.451730  4220 net.cpp:131] Top shape: 64 (64)
I0428 20:32:57.451733  4220 net.cpp:139] Memory required for data: 200960
I0428 20:32:57.451740  4220 layer_factory.hpp:77] Creating layer conv0
I0428 20:32:57.451768  4220 net.cpp:86] Creating Layer conv0
I0428 20:32:57.451795  4220 net.cpp:408] conv0 <- data
I0428 20:32:57.451807  4220 net.cpp:382] conv0 -> conv0
I0428 20:32:57.747176  4220 net.cpp:124] Setting up conv0
I0428 20:32:57.747206  4220 net.cpp:131] Top shape: 64 100 24 24 (3686400)
I0428 20:32:57.747211  4220 net.cpp:139] Memory required for data: 14946560
I0428 20:32:57.747228  4220 layer_factory.hpp:77] Creating layer pool0
I0428 20:32:57.747244  4220 net.cpp:86] Creating Layer pool0
I0428 20:32:57.747249  4220 net.cpp:408] pool0 <- conv0
I0428 20:32:57.747256  4220 net.cpp:382] pool0 -> pool0
I0428 20:32:57.747309  4220 net.cpp:124] Setting up pool0
I0428 20:32:57.747318  4220 net.cpp:131] Top shape: 64 100 12 12 (921600)
I0428 20:32:57.747323  4220 net.cpp:139] Memory required for data: 18632960
I0428 20:32:57.747325  4220 layer_factory.hpp:77] Creating layer conv1
I0428 20:32:57.747339  4220 net.cpp:86] Creating Layer conv1
I0428 20:32:57.747344  4220 net.cpp:408] conv1 <- pool0
I0428 20:32:57.747350  4220 net.cpp:382] conv1 -> conv1
I0428 20:32:57.749835  4220 net.cpp:124] Setting up conv1
I0428 20:32:57.749853  4220 net.cpp:131] Top shape: 64 25 8 8 (102400)
I0428 20:32:57.749858  4220 net.cpp:139] Memory required for data: 19042560
I0428 20:32:57.749868  4220 layer_factory.hpp:77] Creating layer pool1
I0428 20:32:57.749897  4220 net.cpp:86] Creating Layer pool1
I0428 20:32:57.749902  4220 net.cpp:408] pool1 <- conv1
I0428 20:32:57.749909  4220 net.cpp:382] pool1 -> pool1
I0428 20:32:57.749956  4220 net.cpp:124] Setting up pool1
I0428 20:32:57.749963  4220 net.cpp:131] Top shape: 64 25 4 4 (25600)
I0428 20:32:57.749966  4220 net.cpp:139] Memory required for data: 19144960
I0428 20:32:57.749969  4220 layer_factory.hpp:77] Creating layer ip1
I0428 20:32:57.749977  4220 net.cpp:86] Creating Layer ip1
I0428 20:32:57.749981  4220 net.cpp:408] ip1 <- pool1
I0428 20:32:57.749986  4220 net.cpp:382] ip1 -> ip1
I0428 20:32:57.750120  4220 net.cpp:124] Setting up ip1
I0428 20:32:57.750128  4220 net.cpp:131] Top shape: 64 10 (640)
I0428 20:32:57.750133  4220 net.cpp:139] Memory required for data: 19147520
I0428 20:32:57.750140  4220 layer_factory.hpp:77] Creating layer relu1
I0428 20:32:57.750150  4220 net.cpp:86] Creating Layer relu1
I0428 20:32:57.750154  4220 net.cpp:408] relu1 <- ip1
I0428 20:32:57.750159  4220 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:32:57.750349  4220 net.cpp:124] Setting up relu1
I0428 20:32:57.750360  4220 net.cpp:131] Top shape: 64 10 (640)
I0428 20:32:57.750363  4220 net.cpp:139] Memory required for data: 19150080
I0428 20:32:57.750367  4220 layer_factory.hpp:77] Creating layer ip2
I0428 20:32:57.750375  4220 net.cpp:86] Creating Layer ip2
I0428 20:32:57.750380  4220 net.cpp:408] ip2 <- ip1
I0428 20:32:57.750386  4220 net.cpp:382] ip2 -> ip2
I0428 20:32:57.750496  4220 net.cpp:124] Setting up ip2
I0428 20:32:57.750504  4220 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:32:57.750509  4220 net.cpp:139] Memory required for data: 19162880
I0428 20:32:57.750514  4220 layer_factory.hpp:77] Creating layer relu2
I0428 20:32:57.750520  4220 net.cpp:86] Creating Layer relu2
I0428 20:32:57.750527  4220 net.cpp:408] relu2 <- ip2
I0428 20:32:57.750532  4220 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:32:57.751344  4220 net.cpp:124] Setting up relu2
I0428 20:32:57.751358  4220 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:32:57.751363  4220 net.cpp:139] Memory required for data: 19175680
I0428 20:32:57.751366  4220 layer_factory.hpp:77] Creating layer ip3
I0428 20:32:57.751375  4220 net.cpp:86] Creating Layer ip3
I0428 20:32:57.751379  4220 net.cpp:408] ip3 <- ip2
I0428 20:32:57.751385  4220 net.cpp:382] ip3 -> ip3
I0428 20:32:57.751499  4220 net.cpp:124] Setting up ip3
I0428 20:32:57.751508  4220 net.cpp:131] Top shape: 64 10 (640)
I0428 20:32:57.751512  4220 net.cpp:139] Memory required for data: 19178240
I0428 20:32:57.751520  4220 layer_factory.hpp:77] Creating layer relu3
I0428 20:32:57.751528  4220 net.cpp:86] Creating Layer relu3
I0428 20:32:57.751531  4220 net.cpp:408] relu3 <- ip3
I0428 20:32:57.751536  4220 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:32:57.751727  4220 net.cpp:124] Setting up relu3
I0428 20:32:57.751739  4220 net.cpp:131] Top shape: 64 10 (640)
I0428 20:32:57.751742  4220 net.cpp:139] Memory required for data: 19180800
I0428 20:32:57.751746  4220 layer_factory.hpp:77] Creating layer loss
I0428 20:32:57.751752  4220 net.cpp:86] Creating Layer loss
I0428 20:32:57.751758  4220 net.cpp:408] loss <- ip3
I0428 20:32:57.751763  4220 net.cpp:408] loss <- label
I0428 20:32:57.751770  4220 net.cpp:382] loss -> loss
I0428 20:32:57.751785  4220 layer_factory.hpp:77] Creating layer loss
I0428 20:32:57.752043  4220 net.cpp:124] Setting up loss
I0428 20:32:57.752053  4220 net.cpp:131] Top shape: (1)
I0428 20:32:57.752058  4220 net.cpp:134]     with loss weight 1
I0428 20:32:57.752074  4220 net.cpp:139] Memory required for data: 19180804
I0428 20:32:57.752079  4220 net.cpp:200] loss needs backward computation.
I0428 20:32:57.752082  4220 net.cpp:200] relu3 needs backward computation.
I0428 20:32:57.752086  4220 net.cpp:200] ip3 needs backward computation.
I0428 20:32:57.752089  4220 net.cpp:200] relu2 needs backward computation.
I0428 20:32:57.752092  4220 net.cpp:200] ip2 needs backward computation.
I0428 20:32:57.752096  4220 net.cpp:200] relu1 needs backward computation.
I0428 20:32:57.752099  4220 net.cpp:200] ip1 needs backward computation.
I0428 20:32:57.752102  4220 net.cpp:200] pool1 needs backward computation.
I0428 20:32:57.752106  4220 net.cpp:200] conv1 needs backward computation.
I0428 20:32:57.752110  4220 net.cpp:200] pool0 needs backward computation.
I0428 20:32:57.752113  4220 net.cpp:200] conv0 needs backward computation.
I0428 20:32:57.752118  4220 net.cpp:202] mnist does not need backward computation.
I0428 20:32:57.752122  4220 net.cpp:244] This network produces output loss
I0428 20:32:57.752133  4220 net.cpp:257] Network initialization done.
I0428 20:32:57.752502  4220 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1580.prototxt
I0428 20:32:57.752532  4220 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:32:57.752635  4220 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:32:57.752725  4220 layer_factory.hpp:77] Creating layer mnist
I0428 20:32:57.752774  4220 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:32:57.752789  4220 net.cpp:86] Creating Layer mnist
I0428 20:32:57.752796  4220 net.cpp:382] mnist -> data
I0428 20:32:57.752805  4220 net.cpp:382] mnist -> label
I0428 20:32:57.752912  4220 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:32:57.755223  4220 net.cpp:124] Setting up mnist
I0428 20:32:57.755239  4220 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:32:57.755244  4220 net.cpp:131] Top shape: 100 (100)
I0428 20:32:57.755247  4220 net.cpp:139] Memory required for data: 314000
I0428 20:32:57.755252  4220 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:32:57.755259  4220 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:32:57.755264  4220 net.cpp:408] label_mnist_1_split <- label
I0428 20:32:57.755270  4220 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:32:57.755277  4220 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:32:57.755398  4220 net.cpp:124] Setting up label_mnist_1_split
I0428 20:32:57.755409  4220 net.cpp:131] Top shape: 100 (100)
I0428 20:32:57.755412  4220 net.cpp:131] Top shape: 100 (100)
I0428 20:32:57.755416  4220 net.cpp:139] Memory required for data: 314800
I0428 20:32:57.755420  4220 layer_factory.hpp:77] Creating layer conv0
I0428 20:32:57.755430  4220 net.cpp:86] Creating Layer conv0
I0428 20:32:57.755436  4220 net.cpp:408] conv0 <- data
I0428 20:32:57.755442  4220 net.cpp:382] conv0 -> conv0
I0428 20:32:57.757201  4220 net.cpp:124] Setting up conv0
I0428 20:32:57.757218  4220 net.cpp:131] Top shape: 100 100 24 24 (5760000)
I0428 20:32:57.757222  4220 net.cpp:139] Memory required for data: 23354800
I0428 20:32:57.757232  4220 layer_factory.hpp:77] Creating layer pool0
I0428 20:32:57.757241  4220 net.cpp:86] Creating Layer pool0
I0428 20:32:57.757246  4220 net.cpp:408] pool0 <- conv0
I0428 20:32:57.757252  4220 net.cpp:382] pool0 -> pool0
I0428 20:32:57.757294  4220 net.cpp:124] Setting up pool0
I0428 20:32:57.757308  4220 net.cpp:131] Top shape: 100 100 12 12 (1440000)
I0428 20:32:57.757310  4220 net.cpp:139] Memory required for data: 29114800
I0428 20:32:57.757313  4220 layer_factory.hpp:77] Creating layer conv1
I0428 20:32:57.757324  4220 net.cpp:86] Creating Layer conv1
I0428 20:32:57.757328  4220 net.cpp:408] conv1 <- pool0
I0428 20:32:57.757335  4220 net.cpp:382] conv1 -> conv1
I0428 20:32:57.759419  4220 net.cpp:124] Setting up conv1
I0428 20:32:57.759434  4220 net.cpp:131] Top shape: 100 25 8 8 (160000)
I0428 20:32:57.759438  4220 net.cpp:139] Memory required for data: 29754800
I0428 20:32:57.759449  4220 layer_factory.hpp:77] Creating layer pool1
I0428 20:32:57.759459  4220 net.cpp:86] Creating Layer pool1
I0428 20:32:57.759462  4220 net.cpp:408] pool1 <- conv1
I0428 20:32:57.759469  4220 net.cpp:382] pool1 -> pool1
I0428 20:32:57.759511  4220 net.cpp:124] Setting up pool1
I0428 20:32:57.759526  4220 net.cpp:131] Top shape: 100 25 4 4 (40000)
I0428 20:32:57.759531  4220 net.cpp:139] Memory required for data: 29914800
I0428 20:32:57.759533  4220 layer_factory.hpp:77] Creating layer ip1
I0428 20:32:57.759539  4220 net.cpp:86] Creating Layer ip1
I0428 20:32:57.759543  4220 net.cpp:408] ip1 <- pool1
I0428 20:32:57.759551  4220 net.cpp:382] ip1 -> ip1
I0428 20:32:57.759690  4220 net.cpp:124] Setting up ip1
I0428 20:32:57.759711  4220 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:32:57.759714  4220 net.cpp:139] Memory required for data: 29918800
I0428 20:32:57.759724  4220 layer_factory.hpp:77] Creating layer relu1
I0428 20:32:57.759730  4220 net.cpp:86] Creating Layer relu1
I0428 20:32:57.759734  4220 net.cpp:408] relu1 <- ip1
I0428 20:32:57.759742  4220 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:32:57.759934  4220 net.cpp:124] Setting up relu1
I0428 20:32:57.759945  4220 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:32:57.759948  4220 net.cpp:139] Memory required for data: 29922800
I0428 20:32:57.759953  4220 layer_factory.hpp:77] Creating layer ip2
I0428 20:32:57.759961  4220 net.cpp:86] Creating Layer ip2
I0428 20:32:57.759965  4220 net.cpp:408] ip2 <- ip1
I0428 20:32:57.759971  4220 net.cpp:382] ip2 -> ip2
I0428 20:32:57.760092  4220 net.cpp:124] Setting up ip2
I0428 20:32:57.760100  4220 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:32:57.760104  4220 net.cpp:139] Memory required for data: 29942800
I0428 20:32:57.760110  4220 layer_factory.hpp:77] Creating layer relu2
I0428 20:32:57.760116  4220 net.cpp:86] Creating Layer relu2
I0428 20:32:57.760120  4220 net.cpp:408] relu2 <- ip2
I0428 20:32:57.760125  4220 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:32:57.760423  4220 net.cpp:124] Setting up relu2
I0428 20:32:57.760434  4220 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:32:57.760437  4220 net.cpp:139] Memory required for data: 29962800
I0428 20:32:57.760442  4220 layer_factory.hpp:77] Creating layer ip3
I0428 20:32:57.760448  4220 net.cpp:86] Creating Layer ip3
I0428 20:32:57.760452  4220 net.cpp:408] ip3 <- ip2
I0428 20:32:57.760458  4220 net.cpp:382] ip3 -> ip3
I0428 20:32:57.760576  4220 net.cpp:124] Setting up ip3
I0428 20:32:57.760586  4220 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:32:57.760588  4220 net.cpp:139] Memory required for data: 29966800
I0428 20:32:57.760597  4220 layer_factory.hpp:77] Creating layer relu3
I0428 20:32:57.760604  4220 net.cpp:86] Creating Layer relu3
I0428 20:32:57.760607  4220 net.cpp:408] relu3 <- ip3
I0428 20:32:57.760612  4220 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:32:57.761484  4220 net.cpp:124] Setting up relu3
I0428 20:32:57.761497  4220 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:32:57.761502  4220 net.cpp:139] Memory required for data: 29970800
I0428 20:32:57.761505  4220 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:32:57.761512  4220 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:32:57.761514  4220 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:32:57.761520  4220 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:32:57.761526  4220 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:32:57.761574  4220 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:32:57.761581  4220 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:32:57.761585  4220 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:32:57.761589  4220 net.cpp:139] Memory required for data: 29978800
I0428 20:32:57.761591  4220 layer_factory.hpp:77] Creating layer accuracy
I0428 20:32:57.761597  4220 net.cpp:86] Creating Layer accuracy
I0428 20:32:57.761601  4220 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:32:57.761605  4220 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:32:57.761610  4220 net.cpp:382] accuracy -> accuracy
I0428 20:32:57.761617  4220 net.cpp:124] Setting up accuracy
I0428 20:32:57.761621  4220 net.cpp:131] Top shape: (1)
I0428 20:32:57.761625  4220 net.cpp:139] Memory required for data: 29978804
I0428 20:32:57.761627  4220 layer_factory.hpp:77] Creating layer loss
I0428 20:32:57.761633  4220 net.cpp:86] Creating Layer loss
I0428 20:32:57.761637  4220 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:32:57.761641  4220 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:32:57.761646  4220 net.cpp:382] loss -> loss
I0428 20:32:57.761653  4220 layer_factory.hpp:77] Creating layer loss
I0428 20:32:57.761914  4220 net.cpp:124] Setting up loss
I0428 20:32:57.761924  4220 net.cpp:131] Top shape: (1)
I0428 20:32:57.761927  4220 net.cpp:134]     with loss weight 1
I0428 20:32:57.761946  4220 net.cpp:139] Memory required for data: 29978808
I0428 20:32:57.761950  4220 net.cpp:200] loss needs backward computation.
I0428 20:32:57.761955  4220 net.cpp:202] accuracy does not need backward computation.
I0428 20:32:57.761958  4220 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:32:57.761962  4220 net.cpp:200] relu3 needs backward computation.
I0428 20:32:57.761965  4220 net.cpp:200] ip3 needs backward computation.
I0428 20:32:57.761970  4220 net.cpp:200] relu2 needs backward computation.
I0428 20:32:57.761972  4220 net.cpp:200] ip2 needs backward computation.
I0428 20:32:57.761976  4220 net.cpp:200] relu1 needs backward computation.
I0428 20:32:57.761978  4220 net.cpp:200] ip1 needs backward computation.
I0428 20:32:57.761982  4220 net.cpp:200] pool1 needs backward computation.
I0428 20:32:57.761986  4220 net.cpp:200] conv1 needs backward computation.
I0428 20:32:57.761989  4220 net.cpp:200] pool0 needs backward computation.
I0428 20:32:57.761993  4220 net.cpp:200] conv0 needs backward computation.
I0428 20:32:57.761997  4220 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:32:57.762001  4220 net.cpp:202] mnist does not need backward computation.
I0428 20:32:57.762004  4220 net.cpp:244] This network produces output accuracy
I0428 20:32:57.762009  4220 net.cpp:244] This network produces output loss
I0428 20:32:57.762022  4220 net.cpp:257] Network initialization done.
I0428 20:32:57.762068  4220 solver.cpp:56] Solver scaffolding done.
I0428 20:32:57.762435  4220 caffe.cpp:248] Starting Optimization
I0428 20:32:57.762441  4220 solver.cpp:273] Solving LeNet
I0428 20:32:57.762444  4220 solver.cpp:274] Learning Rate Policy: inv
I0428 20:32:57.763270  4220 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:32:57.860275  4227 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:32:57.862854  4220 solver.cpp:398]     Test net output #0: accuracy = 0.1124
I0428 20:32:57.862890  4220 solver.cpp:398]     Test net output #1: loss = 2.30638 (* 1 = 2.30638 loss)
I0428 20:32:57.867344  4220 solver.cpp:219] Iteration 0 (-7.74918e-43 iter/s, 0.104869s/100 iters), loss = 2.30861
I0428 20:32:57.867384  4220 solver.cpp:238]     Train net output #0: loss = 2.30861 (* 1 = 2.30861 loss)
I0428 20:32:57.867395  4220 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:32:58.090031  4220 solver.cpp:219] Iteration 100 (449.159 iter/s, 0.222638s/100 iters), loss = 1.16693
I0428 20:32:58.090077  4220 solver.cpp:238]     Train net output #0: loss = 1.16693 (* 1 = 1.16693 loss)
I0428 20:32:58.090090  4220 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:32:58.315868  4220 solver.cpp:219] Iteration 200 (442.923 iter/s, 0.225773s/100 iters), loss = 0.648405
I0428 20:32:58.315917  4220 solver.cpp:238]     Train net output #0: loss = 0.648405 (* 1 = 0.648405 loss)
I0428 20:32:58.315929  4220 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:32:58.544420  4220 solver.cpp:219] Iteration 300 (437.663 iter/s, 0.228486s/100 iters), loss = 0.43048
I0428 20:32:58.544467  4220 solver.cpp:238]     Train net output #0: loss = 0.43048 (* 1 = 0.43048 loss)
I0428 20:32:58.544481  4220 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:32:58.774369  4220 solver.cpp:219] Iteration 400 (435 iter/s, 0.229885s/100 iters), loss = 0.0686204
I0428 20:32:58.774417  4220 solver.cpp:238]     Train net output #0: loss = 0.0686204 (* 1 = 0.0686204 loss)
I0428 20:32:58.774430  4220 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:32:58.997880  4220 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:32:59.101402  4227 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:32:59.105007  4220 solver.cpp:398]     Test net output #0: accuracy = 0.9587
I0428 20:32:59.105033  4220 solver.cpp:398]     Test net output #1: loss = 0.139351 (* 1 = 0.139351 loss)
I0428 20:32:59.107000  4220 solver.cpp:219] Iteration 500 (300.692 iter/s, 0.332567s/100 iters), loss = 0.144991
I0428 20:32:59.107028  4220 solver.cpp:238]     Train net output #0: loss = 0.144992 (* 1 = 0.144992 loss)
I0428 20:32:59.107053  4220 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:32:59.321873  4220 solver.cpp:219] Iteration 600 (465.495 iter/s, 0.214825s/100 iters), loss = 0.118201
I0428 20:32:59.321916  4220 solver.cpp:238]     Train net output #0: loss = 0.118201 (* 1 = 0.118201 loss)
I0428 20:32:59.321926  4220 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:32:59.549141  4220 solver.cpp:219] Iteration 700 (440.127 iter/s, 0.227207s/100 iters), loss = 0.229464
I0428 20:32:59.549182  4220 solver.cpp:238]     Train net output #0: loss = 0.229464 (* 1 = 0.229464 loss)
I0428 20:32:59.549193  4220 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:32:59.775670  4220 solver.cpp:219] Iteration 800 (441.558 iter/s, 0.226471s/100 iters), loss = 0.279539
I0428 20:32:59.775712  4220 solver.cpp:238]     Train net output #0: loss = 0.279539 (* 1 = 0.279539 loss)
I0428 20:32:59.775722  4220 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:33:00.002651  4220 solver.cpp:219] Iteration 900 (440.677 iter/s, 0.226924s/100 iters), loss = 0.177391
I0428 20:33:00.002702  4220 solver.cpp:238]     Train net output #0: loss = 0.177391 (* 1 = 0.177391 loss)
I0428 20:33:00.002713  4220 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:33:00.078305  4226 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:33:00.228152  4220 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:33:00.231086  4220 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:33:00.232846  4220 solver.cpp:311] Iteration 1000, loss = 0.0929533
I0428 20:33:00.232872  4220 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:33:00.333361  4227 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:33:00.336971  4220 solver.cpp:398]     Test net output #0: accuracy = 0.9717
I0428 20:33:00.336992  4220 solver.cpp:398]     Test net output #1: loss = 0.0907223 (* 1 = 0.0907223 loss)
I0428 20:33:00.336998  4220 solver.cpp:316] Optimization Done.
I0428 20:33:00.337002  4220 caffe.cpp:259] Optimization Done.
