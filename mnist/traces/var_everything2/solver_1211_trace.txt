I0428 20:14:01.137537 32586 caffe.cpp:218] Using GPUs 0
I0428 20:14:01.178966 32586 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:14:01.692391 32586 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1211.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:14:01.692567 32586 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1211.prototxt
I0428 20:14:01.692926 32586 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:14:01.692942 32586 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:14:01.693027 32586 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0428 20:14:01.693095 32586 layer_factory.hpp:77] Creating layer mnist
I0428 20:14:01.693197 32586 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:14:01.693220 32586 net.cpp:86] Creating Layer mnist
I0428 20:14:01.693228 32586 net.cpp:382] mnist -> data
I0428 20:14:01.693251 32586 net.cpp:382] mnist -> label
I0428 20:14:01.694360 32586 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:14:01.696830 32586 net.cpp:124] Setting up mnist
I0428 20:14:01.696847 32586 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:14:01.696853 32586 net.cpp:131] Top shape: 64 (64)
I0428 20:14:01.696856 32586 net.cpp:139] Memory required for data: 200960
I0428 20:14:01.696873 32586 layer_factory.hpp:77] Creating layer conv0
I0428 20:14:01.696892 32586 net.cpp:86] Creating Layer conv0
I0428 20:14:01.696897 32586 net.cpp:408] conv0 <- data
I0428 20:14:01.696910 32586 net.cpp:382] conv0 -> conv0
I0428 20:14:01.989696 32586 net.cpp:124] Setting up conv0
I0428 20:14:01.989729 32586 net.cpp:131] Top shape: 64 50 24 24 (1843200)
I0428 20:14:01.989734 32586 net.cpp:139] Memory required for data: 7573760
I0428 20:14:01.989753 32586 layer_factory.hpp:77] Creating layer pool0
I0428 20:14:01.989768 32586 net.cpp:86] Creating Layer pool0
I0428 20:14:01.989773 32586 net.cpp:408] pool0 <- conv0
I0428 20:14:01.989780 32586 net.cpp:382] pool0 -> pool0
I0428 20:14:01.989838 32586 net.cpp:124] Setting up pool0
I0428 20:14:01.989848 32586 net.cpp:131] Top shape: 64 50 12 12 (460800)
I0428 20:14:01.989869 32586 net.cpp:139] Memory required for data: 9416960
I0428 20:14:01.989873 32586 layer_factory.hpp:77] Creating layer conv1
I0428 20:14:01.989887 32586 net.cpp:86] Creating Layer conv1
I0428 20:14:01.989892 32586 net.cpp:408] conv1 <- pool0
I0428 20:14:01.989899 32586 net.cpp:382] conv1 -> conv1
I0428 20:14:01.992127 32586 net.cpp:124] Setting up conv1
I0428 20:14:01.992147 32586 net.cpp:131] Top shape: 64 2 8 8 (8192)
I0428 20:14:01.992156 32586 net.cpp:139] Memory required for data: 9449728
I0428 20:14:01.992169 32586 layer_factory.hpp:77] Creating layer pool1
I0428 20:14:01.992178 32586 net.cpp:86] Creating Layer pool1
I0428 20:14:01.992182 32586 net.cpp:408] pool1 <- conv1
I0428 20:14:01.992188 32586 net.cpp:382] pool1 -> pool1
I0428 20:14:01.992233 32586 net.cpp:124] Setting up pool1
I0428 20:14:01.992245 32586 net.cpp:131] Top shape: 64 2 4 4 (2048)
I0428 20:14:01.992249 32586 net.cpp:139] Memory required for data: 9457920
I0428 20:14:01.992252 32586 layer_factory.hpp:77] Creating layer ip1
I0428 20:14:01.992262 32586 net.cpp:86] Creating Layer ip1
I0428 20:14:01.992266 32586 net.cpp:408] ip1 <- pool1
I0428 20:14:01.992271 32586 net.cpp:382] ip1 -> ip1
I0428 20:14:01.992393 32586 net.cpp:124] Setting up ip1
I0428 20:14:01.992401 32586 net.cpp:131] Top shape: 64 10 (640)
I0428 20:14:01.992405 32586 net.cpp:139] Memory required for data: 9460480
I0428 20:14:01.992413 32586 layer_factory.hpp:77] Creating layer relu1
I0428 20:14:01.992421 32586 net.cpp:86] Creating Layer relu1
I0428 20:14:01.992425 32586 net.cpp:408] relu1 <- ip1
I0428 20:14:01.992430 32586 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:14:01.992643 32586 net.cpp:124] Setting up relu1
I0428 20:14:01.992666 32586 net.cpp:131] Top shape: 64 10 (640)
I0428 20:14:01.992669 32586 net.cpp:139] Memory required for data: 9463040
I0428 20:14:01.992672 32586 layer_factory.hpp:77] Creating layer loss
I0428 20:14:01.992681 32586 net.cpp:86] Creating Layer loss
I0428 20:14:01.992687 32586 net.cpp:408] loss <- ip1
I0428 20:14:01.992691 32586 net.cpp:408] loss <- label
I0428 20:14:01.992697 32586 net.cpp:382] loss -> loss
I0428 20:14:01.992712 32586 layer_factory.hpp:77] Creating layer loss
I0428 20:14:01.993677 32586 net.cpp:124] Setting up loss
I0428 20:14:01.993693 32586 net.cpp:131] Top shape: (1)
I0428 20:14:01.993696 32586 net.cpp:134]     with loss weight 1
I0428 20:14:01.993713 32586 net.cpp:139] Memory required for data: 9463044
I0428 20:14:01.993717 32586 net.cpp:200] loss needs backward computation.
I0428 20:14:01.993722 32586 net.cpp:200] relu1 needs backward computation.
I0428 20:14:01.993726 32586 net.cpp:200] ip1 needs backward computation.
I0428 20:14:01.993729 32586 net.cpp:200] pool1 needs backward computation.
I0428 20:14:01.993732 32586 net.cpp:200] conv1 needs backward computation.
I0428 20:14:01.993736 32586 net.cpp:200] pool0 needs backward computation.
I0428 20:14:01.993739 32586 net.cpp:200] conv0 needs backward computation.
I0428 20:14:01.993743 32586 net.cpp:202] mnist does not need backward computation.
I0428 20:14:01.993748 32586 net.cpp:244] This network produces output loss
I0428 20:14:01.993757 32586 net.cpp:257] Network initialization done.
I0428 20:14:01.994065 32586 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1211.prototxt
I0428 20:14:01.994093 32586 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:14:01.994182 32586 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0428 20:14:01.994271 32586 layer_factory.hpp:77] Creating layer mnist
I0428 20:14:01.994328 32586 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:14:01.994343 32586 net.cpp:86] Creating Layer mnist
I0428 20:14:01.994350 32586 net.cpp:382] mnist -> data
I0428 20:14:01.994359 32586 net.cpp:382] mnist -> label
I0428 20:14:01.994468 32586 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:14:01.996892 32586 net.cpp:124] Setting up mnist
I0428 20:14:01.996908 32586 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:14:01.996917 32586 net.cpp:131] Top shape: 100 (100)
I0428 20:14:01.996919 32586 net.cpp:139] Memory required for data: 314000
I0428 20:14:01.996924 32586 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:14:01.996932 32586 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:14:01.996937 32586 net.cpp:408] label_mnist_1_split <- label
I0428 20:14:01.996942 32586 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:14:01.996949 32586 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:14:01.997002 32586 net.cpp:124] Setting up label_mnist_1_split
I0428 20:14:01.997009 32586 net.cpp:131] Top shape: 100 (100)
I0428 20:14:01.997014 32586 net.cpp:131] Top shape: 100 (100)
I0428 20:14:01.997017 32586 net.cpp:139] Memory required for data: 314800
I0428 20:14:01.997021 32586 layer_factory.hpp:77] Creating layer conv0
I0428 20:14:01.997032 32586 net.cpp:86] Creating Layer conv0
I0428 20:14:01.997037 32586 net.cpp:408] conv0 <- data
I0428 20:14:01.997045 32586 net.cpp:382] conv0 -> conv0
I0428 20:14:01.998163 32586 net.cpp:124] Setting up conv0
I0428 20:14:01.998178 32586 net.cpp:131] Top shape: 100 50 24 24 (2880000)
I0428 20:14:01.998183 32586 net.cpp:139] Memory required for data: 11834800
I0428 20:14:01.998195 32586 layer_factory.hpp:77] Creating layer pool0
I0428 20:14:01.998203 32586 net.cpp:86] Creating Layer pool0
I0428 20:14:01.998208 32586 net.cpp:408] pool0 <- conv0
I0428 20:14:01.998214 32586 net.cpp:382] pool0 -> pool0
I0428 20:14:01.998258 32586 net.cpp:124] Setting up pool0
I0428 20:14:01.998265 32586 net.cpp:131] Top shape: 100 50 12 12 (720000)
I0428 20:14:01.998268 32586 net.cpp:139] Memory required for data: 14714800
I0428 20:14:01.998272 32586 layer_factory.hpp:77] Creating layer conv1
I0428 20:14:01.998286 32586 net.cpp:86] Creating Layer conv1
I0428 20:14:01.998291 32586 net.cpp:408] conv1 <- pool0
I0428 20:14:01.998298 32586 net.cpp:382] conv1 -> conv1
I0428 20:14:02.000069 32586 net.cpp:124] Setting up conv1
I0428 20:14:02.000085 32586 net.cpp:131] Top shape: 100 2 8 8 (12800)
I0428 20:14:02.000090 32586 net.cpp:139] Memory required for data: 14766000
I0428 20:14:02.000100 32586 layer_factory.hpp:77] Creating layer pool1
I0428 20:14:02.000110 32586 net.cpp:86] Creating Layer pool1
I0428 20:14:02.000126 32586 net.cpp:408] pool1 <- conv1
I0428 20:14:02.000134 32586 net.cpp:382] pool1 -> pool1
I0428 20:14:02.000178 32586 net.cpp:124] Setting up pool1
I0428 20:14:02.000186 32586 net.cpp:131] Top shape: 100 2 4 4 (3200)
I0428 20:14:02.000190 32586 net.cpp:139] Memory required for data: 14778800
I0428 20:14:02.000193 32586 layer_factory.hpp:77] Creating layer ip1
I0428 20:14:02.000202 32586 net.cpp:86] Creating Layer ip1
I0428 20:14:02.000208 32586 net.cpp:408] ip1 <- pool1
I0428 20:14:02.000216 32586 net.cpp:382] ip1 -> ip1
I0428 20:14:02.000334 32586 net.cpp:124] Setting up ip1
I0428 20:14:02.000342 32586 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:14:02.000346 32586 net.cpp:139] Memory required for data: 14782800
I0428 20:14:02.000355 32586 layer_factory.hpp:77] Creating layer relu1
I0428 20:14:02.000363 32586 net.cpp:86] Creating Layer relu1
I0428 20:14:02.000368 32586 net.cpp:408] relu1 <- ip1
I0428 20:14:02.000375 32586 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:14:02.001324 32586 net.cpp:124] Setting up relu1
I0428 20:14:02.001340 32586 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:14:02.001344 32586 net.cpp:139] Memory required for data: 14786800
I0428 20:14:02.001348 32586 layer_factory.hpp:77] Creating layer ip1_relu1_0_split
I0428 20:14:02.001356 32586 net.cpp:86] Creating Layer ip1_relu1_0_split
I0428 20:14:02.001360 32586 net.cpp:408] ip1_relu1_0_split <- ip1
I0428 20:14:02.001368 32586 net.cpp:382] ip1_relu1_0_split -> ip1_relu1_0_split_0
I0428 20:14:02.001374 32586 net.cpp:382] ip1_relu1_0_split -> ip1_relu1_0_split_1
I0428 20:14:02.001430 32586 net.cpp:124] Setting up ip1_relu1_0_split
I0428 20:14:02.001437 32586 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:14:02.001443 32586 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:14:02.001446 32586 net.cpp:139] Memory required for data: 14794800
I0428 20:14:02.001451 32586 layer_factory.hpp:77] Creating layer accuracy
I0428 20:14:02.001456 32586 net.cpp:86] Creating Layer accuracy
I0428 20:14:02.001461 32586 net.cpp:408] accuracy <- ip1_relu1_0_split_0
I0428 20:14:02.001466 32586 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:14:02.001471 32586 net.cpp:382] accuracy -> accuracy
I0428 20:14:02.001479 32586 net.cpp:124] Setting up accuracy
I0428 20:14:02.001485 32586 net.cpp:131] Top shape: (1)
I0428 20:14:02.001489 32586 net.cpp:139] Memory required for data: 14794804
I0428 20:14:02.001492 32586 layer_factory.hpp:77] Creating layer loss
I0428 20:14:02.001507 32586 net.cpp:86] Creating Layer loss
I0428 20:14:02.001513 32586 net.cpp:408] loss <- ip1_relu1_0_split_1
I0428 20:14:02.001518 32586 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:14:02.001523 32586 net.cpp:382] loss -> loss
I0428 20:14:02.001529 32586 layer_factory.hpp:77] Creating layer loss
I0428 20:14:02.001821 32586 net.cpp:124] Setting up loss
I0428 20:14:02.001832 32586 net.cpp:131] Top shape: (1)
I0428 20:14:02.001835 32586 net.cpp:134]     with loss weight 1
I0428 20:14:02.001844 32586 net.cpp:139] Memory required for data: 14794808
I0428 20:14:02.001847 32586 net.cpp:200] loss needs backward computation.
I0428 20:14:02.001852 32586 net.cpp:202] accuracy does not need backward computation.
I0428 20:14:02.001857 32586 net.cpp:200] ip1_relu1_0_split needs backward computation.
I0428 20:14:02.001862 32586 net.cpp:200] relu1 needs backward computation.
I0428 20:14:02.001864 32586 net.cpp:200] ip1 needs backward computation.
I0428 20:14:02.001868 32586 net.cpp:200] pool1 needs backward computation.
I0428 20:14:02.001873 32586 net.cpp:200] conv1 needs backward computation.
I0428 20:14:02.001876 32586 net.cpp:200] pool0 needs backward computation.
I0428 20:14:02.001888 32586 net.cpp:200] conv0 needs backward computation.
I0428 20:14:02.001893 32586 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:14:02.001898 32586 net.cpp:202] mnist does not need backward computation.
I0428 20:14:02.001901 32586 net.cpp:244] This network produces output accuracy
I0428 20:14:02.001905 32586 net.cpp:244] This network produces output loss
I0428 20:14:02.001932 32586 net.cpp:257] Network initialization done.
I0428 20:14:02.001973 32586 solver.cpp:56] Solver scaffolding done.
I0428 20:14:02.002225 32586 caffe.cpp:248] Starting Optimization
I0428 20:14:02.002233 32586 solver.cpp:273] Solving LeNet
I0428 20:14:02.002235 32586 solver.cpp:274] Learning Rate Policy: inv
I0428 20:14:02.003127 32586 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:14:02.009449 32586 blocking_queue.cpp:49] Waiting for data
I0428 20:14:02.080603 32593 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:14:02.081275 32586 solver.cpp:398]     Test net output #0: accuracy = 0.1091
I0428 20:14:02.081297 32586 solver.cpp:398]     Test net output #1: loss = 2.35225 (* 1 = 2.35225 loss)
I0428 20:14:02.085378 32586 solver.cpp:219] Iteration 0 (0 iter/s, 0.0831109s/100 iters), loss = 2.3288
I0428 20:14:02.085403 32586 solver.cpp:238]     Train net output #0: loss = 2.3288 (* 1 = 2.3288 loss)
I0428 20:14:02.085415 32586 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:14:02.189543 32586 solver.cpp:219] Iteration 100 (960.408 iter/s, 0.104122s/100 iters), loss = 0.443429
I0428 20:14:02.189576 32586 solver.cpp:238]     Train net output #0: loss = 0.443429 (* 1 = 0.443429 loss)
I0428 20:14:02.189584 32586 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:14:02.294836 32586 solver.cpp:219] Iteration 200 (950.158 iter/s, 0.105246s/100 iters), loss = 0.265652
I0428 20:14:02.294883 32586 solver.cpp:238]     Train net output #0: loss = 0.265653 (* 1 = 0.265653 loss)
I0428 20:14:02.294894 32586 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:14:02.400553 32586 solver.cpp:219] Iteration 300 (946.401 iter/s, 0.105663s/100 iters), loss = 0.214167
I0428 20:14:02.400584 32586 solver.cpp:238]     Train net output #0: loss = 0.214167 (* 1 = 0.214167 loss)
I0428 20:14:02.400591 32586 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:14:02.509878 32586 solver.cpp:219] Iteration 400 (915.07 iter/s, 0.109281s/100 iters), loss = 0.170283
I0428 20:14:02.509918 32586 solver.cpp:238]     Train net output #0: loss = 0.170283 (* 1 = 0.170283 loss)
I0428 20:14:02.509928 32586 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:14:02.613701 32586 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:14:02.674026 32593 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:14:02.675500 32586 solver.cpp:398]     Test net output #0: accuracy = 0.944
I0428 20:14:02.675525 32586 solver.cpp:398]     Test net output #1: loss = 0.180683 (* 1 = 0.180683 loss)
I0428 20:14:02.676543 32586 solver.cpp:219] Iteration 500 (600.19 iter/s, 0.166614s/100 iters), loss = 0.117086
I0428 20:14:02.676570 32586 solver.cpp:238]     Train net output #0: loss = 0.117086 (* 1 = 0.117086 loss)
I0428 20:14:02.676578 32586 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:14:02.784855 32586 solver.cpp:219] Iteration 600 (923.602 iter/s, 0.108272s/100 iters), loss = 0.189323
I0428 20:14:02.784886 32586 solver.cpp:238]     Train net output #0: loss = 0.189323 (* 1 = 0.189323 loss)
I0428 20:14:02.784894 32586 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:14:02.888284 32586 solver.cpp:219] Iteration 700 (967.238 iter/s, 0.103387s/100 iters), loss = 0.208147
I0428 20:14:02.888315 32586 solver.cpp:238]     Train net output #0: loss = 0.208147 (* 1 = 0.208147 loss)
I0428 20:14:02.888324 32586 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:14:02.996106 32586 solver.cpp:219] Iteration 800 (927.816 iter/s, 0.10778s/100 iters), loss = 0.208363
I0428 20:14:02.996139 32586 solver.cpp:238]     Train net output #0: loss = 0.208363 (* 1 = 0.208363 loss)
I0428 20:14:02.996146 32586 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:14:03.099200 32586 solver.cpp:219] Iteration 900 (970.4 iter/s, 0.10305s/100 iters), loss = 0.389039
I0428 20:14:03.099234 32586 solver.cpp:238]     Train net output #0: loss = 0.389039 (* 1 = 0.389039 loss)
I0428 20:14:03.099242 32586 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:14:03.133831 32592 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:14:03.202584 32586 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:14:03.203397 32586 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:14:03.204087 32586 solver.cpp:311] Iteration 1000, loss = 0.265986
I0428 20:14:03.204109 32586 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:14:03.273633 32593 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:14:03.275827 32586 solver.cpp:398]     Test net output #0: accuracy = 0.9592
I0428 20:14:03.275858 32586 solver.cpp:398]     Test net output #1: loss = 0.124158 (* 1 = 0.124158 loss)
I0428 20:14:03.275866 32586 solver.cpp:316] Optimization Done.
I0428 20:14:03.275871 32586 caffe.cpp:259] Optimization Done.
