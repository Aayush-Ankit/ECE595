I0428 20:08:26.155653 31381 caffe.cpp:218] Using GPUs 0
I0428 20:08:26.184329 31381 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:08:26.626463 31381 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1083.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:08:26.626596 31381 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1083.prototxt
I0428 20:08:26.626920 31381 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:08:26.626950 31381 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:08:26.627045 31381 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:08:26.627117 31381 layer_factory.hpp:77] Creating layer mnist
I0428 20:08:26.627197 31381 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:08:26.627216 31381 net.cpp:86] Creating Layer mnist
I0428 20:08:26.627224 31381 net.cpp:382] mnist -> data
I0428 20:08:26.627241 31381 net.cpp:382] mnist -> label
I0428 20:08:26.628255 31381 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:08:26.630709 31381 net.cpp:124] Setting up mnist
I0428 20:08:26.630739 31381 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:08:26.630744 31381 net.cpp:131] Top shape: 64 (64)
I0428 20:08:26.630748 31381 net.cpp:139] Memory required for data: 200960
I0428 20:08:26.630753 31381 layer_factory.hpp:77] Creating layer conv0
I0428 20:08:26.630787 31381 net.cpp:86] Creating Layer conv0
I0428 20:08:26.630805 31381 net.cpp:408] conv0 <- data
I0428 20:08:26.630815 31381 net.cpp:382] conv0 -> conv0
I0428 20:08:26.910140 31381 net.cpp:124] Setting up conv0
I0428 20:08:26.910171 31381 net.cpp:131] Top shape: 64 25 24 24 (921600)
I0428 20:08:26.910176 31381 net.cpp:139] Memory required for data: 3887360
I0428 20:08:26.910193 31381 layer_factory.hpp:77] Creating layer pool0
I0428 20:08:26.910208 31381 net.cpp:86] Creating Layer pool0
I0428 20:08:26.910213 31381 net.cpp:408] pool0 <- conv0
I0428 20:08:26.910220 31381 net.cpp:382] pool0 -> pool0
I0428 20:08:26.910275 31381 net.cpp:124] Setting up pool0
I0428 20:08:26.910282 31381 net.cpp:131] Top shape: 64 25 12 12 (230400)
I0428 20:08:26.910285 31381 net.cpp:139] Memory required for data: 4808960
I0428 20:08:26.910289 31381 layer_factory.hpp:77] Creating layer conv1
I0428 20:08:26.910302 31381 net.cpp:86] Creating Layer conv1
I0428 20:08:26.910306 31381 net.cpp:408] conv1 <- pool0
I0428 20:08:26.910313 31381 net.cpp:382] conv1 -> conv1
I0428 20:08:26.913532 31381 net.cpp:124] Setting up conv1
I0428 20:08:26.913548 31381 net.cpp:131] Top shape: 64 10 8 8 (40960)
I0428 20:08:26.913553 31381 net.cpp:139] Memory required for data: 4972800
I0428 20:08:26.913565 31381 layer_factory.hpp:77] Creating layer pool1
I0428 20:08:26.913574 31381 net.cpp:86] Creating Layer pool1
I0428 20:08:26.913580 31381 net.cpp:408] pool1 <- conv1
I0428 20:08:26.913588 31381 net.cpp:382] pool1 -> pool1
I0428 20:08:26.913632 31381 net.cpp:124] Setting up pool1
I0428 20:08:26.913640 31381 net.cpp:131] Top shape: 64 10 4 4 (10240)
I0428 20:08:26.913643 31381 net.cpp:139] Memory required for data: 5013760
I0428 20:08:26.913647 31381 layer_factory.hpp:77] Creating layer ip1
I0428 20:08:26.913655 31381 net.cpp:86] Creating Layer ip1
I0428 20:08:26.913658 31381 net.cpp:408] ip1 <- pool1
I0428 20:08:26.913666 31381 net.cpp:382] ip1 -> ip1
I0428 20:08:26.913843 31381 net.cpp:124] Setting up ip1
I0428 20:08:26.913852 31381 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:08:26.913856 31381 net.cpp:139] Memory required for data: 5026560
I0428 20:08:26.913864 31381 layer_factory.hpp:77] Creating layer relu1
I0428 20:08:26.913872 31381 net.cpp:86] Creating Layer relu1
I0428 20:08:26.913877 31381 net.cpp:408] relu1 <- ip1
I0428 20:08:26.913882 31381 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:08:26.914090 31381 net.cpp:124] Setting up relu1
I0428 20:08:26.914100 31381 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:08:26.914104 31381 net.cpp:139] Memory required for data: 5039360
I0428 20:08:26.914108 31381 layer_factory.hpp:77] Creating layer ip2
I0428 20:08:26.914115 31381 net.cpp:86] Creating Layer ip2
I0428 20:08:26.914119 31381 net.cpp:408] ip2 <- ip1
I0428 20:08:26.914125 31381 net.cpp:382] ip2 -> ip2
I0428 20:08:26.914248 31381 net.cpp:124] Setting up ip2
I0428 20:08:26.914258 31381 net.cpp:131] Top shape: 64 10 (640)
I0428 20:08:26.914260 31381 net.cpp:139] Memory required for data: 5041920
I0428 20:08:26.914266 31381 layer_factory.hpp:77] Creating layer relu2
I0428 20:08:26.914273 31381 net.cpp:86] Creating Layer relu2
I0428 20:08:26.914278 31381 net.cpp:408] relu2 <- ip2
I0428 20:08:26.914283 31381 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:08:26.915132 31381 net.cpp:124] Setting up relu2
I0428 20:08:26.915145 31381 net.cpp:131] Top shape: 64 10 (640)
I0428 20:08:26.915149 31381 net.cpp:139] Memory required for data: 5044480
I0428 20:08:26.915153 31381 layer_factory.hpp:77] Creating layer ip3
I0428 20:08:26.915161 31381 net.cpp:86] Creating Layer ip3
I0428 20:08:26.915165 31381 net.cpp:408] ip3 <- ip2
I0428 20:08:26.915174 31381 net.cpp:382] ip3 -> ip3
I0428 20:08:26.915298 31381 net.cpp:124] Setting up ip3
I0428 20:08:26.915307 31381 net.cpp:131] Top shape: 64 10 (640)
I0428 20:08:26.915311 31381 net.cpp:139] Memory required for data: 5047040
I0428 20:08:26.915320 31381 layer_factory.hpp:77] Creating layer relu3
I0428 20:08:26.915328 31381 net.cpp:86] Creating Layer relu3
I0428 20:08:26.915331 31381 net.cpp:408] relu3 <- ip3
I0428 20:08:26.915336 31381 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:08:26.915549 31381 net.cpp:124] Setting up relu3
I0428 20:08:26.915560 31381 net.cpp:131] Top shape: 64 10 (640)
I0428 20:08:26.915565 31381 net.cpp:139] Memory required for data: 5049600
I0428 20:08:26.915567 31381 layer_factory.hpp:77] Creating layer loss
I0428 20:08:26.915577 31381 net.cpp:86] Creating Layer loss
I0428 20:08:26.915581 31381 net.cpp:408] loss <- ip3
I0428 20:08:26.915586 31381 net.cpp:408] loss <- label
I0428 20:08:26.915592 31381 net.cpp:382] loss -> loss
I0428 20:08:26.915608 31381 layer_factory.hpp:77] Creating layer loss
I0428 20:08:26.915899 31381 net.cpp:124] Setting up loss
I0428 20:08:26.915916 31381 net.cpp:131] Top shape: (1)
I0428 20:08:26.915920 31381 net.cpp:134]     with loss weight 1
I0428 20:08:26.915942 31381 net.cpp:139] Memory required for data: 5049604
I0428 20:08:26.915946 31381 net.cpp:200] loss needs backward computation.
I0428 20:08:26.915961 31381 net.cpp:200] relu3 needs backward computation.
I0428 20:08:26.915963 31381 net.cpp:200] ip3 needs backward computation.
I0428 20:08:26.915967 31381 net.cpp:200] relu2 needs backward computation.
I0428 20:08:26.915971 31381 net.cpp:200] ip2 needs backward computation.
I0428 20:08:26.915973 31381 net.cpp:200] relu1 needs backward computation.
I0428 20:08:26.915977 31381 net.cpp:200] ip1 needs backward computation.
I0428 20:08:26.915980 31381 net.cpp:200] pool1 needs backward computation.
I0428 20:08:26.915983 31381 net.cpp:200] conv1 needs backward computation.
I0428 20:08:26.915987 31381 net.cpp:200] pool0 needs backward computation.
I0428 20:08:26.915990 31381 net.cpp:200] conv0 needs backward computation.
I0428 20:08:26.915997 31381 net.cpp:202] mnist does not need backward computation.
I0428 20:08:26.916000 31381 net.cpp:244] This network produces output loss
I0428 20:08:26.916010 31381 net.cpp:257] Network initialization done.
I0428 20:08:26.916398 31381 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1083.prototxt
I0428 20:08:26.916437 31381 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:08:26.916548 31381 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:08:26.916653 31381 layer_factory.hpp:77] Creating layer mnist
I0428 20:08:26.916705 31381 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:08:26.916719 31381 net.cpp:86] Creating Layer mnist
I0428 20:08:26.916726 31381 net.cpp:382] mnist -> data
I0428 20:08:26.916735 31381 net.cpp:382] mnist -> label
I0428 20:08:26.916851 31381 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:08:26.919035 31381 net.cpp:124] Setting up mnist
I0428 20:08:26.919056 31381 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:08:26.919062 31381 net.cpp:131] Top shape: 100 (100)
I0428 20:08:26.919066 31381 net.cpp:139] Memory required for data: 314000
I0428 20:08:26.919070 31381 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:08:26.919078 31381 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:08:26.919081 31381 net.cpp:408] label_mnist_1_split <- label
I0428 20:08:26.919116 31381 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:08:26.919126 31381 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:08:26.919193 31381 net.cpp:124] Setting up label_mnist_1_split
I0428 20:08:26.919200 31381 net.cpp:131] Top shape: 100 (100)
I0428 20:08:26.919206 31381 net.cpp:131] Top shape: 100 (100)
I0428 20:08:26.919210 31381 net.cpp:139] Memory required for data: 314800
I0428 20:08:26.919214 31381 layer_factory.hpp:77] Creating layer conv0
I0428 20:08:26.919224 31381 net.cpp:86] Creating Layer conv0
I0428 20:08:26.919226 31381 net.cpp:408] conv0 <- data
I0428 20:08:26.919234 31381 net.cpp:382] conv0 -> conv0
I0428 20:08:26.921257 31381 net.cpp:124] Setting up conv0
I0428 20:08:26.921274 31381 net.cpp:131] Top shape: 100 25 24 24 (1440000)
I0428 20:08:26.921278 31381 net.cpp:139] Memory required for data: 6074800
I0428 20:08:26.921289 31381 layer_factory.hpp:77] Creating layer pool0
I0428 20:08:26.921296 31381 net.cpp:86] Creating Layer pool0
I0428 20:08:26.921309 31381 net.cpp:408] pool0 <- conv0
I0428 20:08:26.921316 31381 net.cpp:382] pool0 -> pool0
I0428 20:08:26.921361 31381 net.cpp:124] Setting up pool0
I0428 20:08:26.921368 31381 net.cpp:131] Top shape: 100 25 12 12 (360000)
I0428 20:08:26.921371 31381 net.cpp:139] Memory required for data: 7514800
I0428 20:08:26.921375 31381 layer_factory.hpp:77] Creating layer conv1
I0428 20:08:26.921387 31381 net.cpp:86] Creating Layer conv1
I0428 20:08:26.921391 31381 net.cpp:408] conv1 <- pool0
I0428 20:08:26.921396 31381 net.cpp:382] conv1 -> conv1
I0428 20:08:26.923794 31381 net.cpp:124] Setting up conv1
I0428 20:08:26.923810 31381 net.cpp:131] Top shape: 100 10 8 8 (64000)
I0428 20:08:26.923815 31381 net.cpp:139] Memory required for data: 7770800
I0428 20:08:26.923825 31381 layer_factory.hpp:77] Creating layer pool1
I0428 20:08:26.923831 31381 net.cpp:86] Creating Layer pool1
I0428 20:08:26.923835 31381 net.cpp:408] pool1 <- conv1
I0428 20:08:26.923842 31381 net.cpp:382] pool1 -> pool1
I0428 20:08:26.923894 31381 net.cpp:124] Setting up pool1
I0428 20:08:26.923902 31381 net.cpp:131] Top shape: 100 10 4 4 (16000)
I0428 20:08:26.923904 31381 net.cpp:139] Memory required for data: 7834800
I0428 20:08:26.923916 31381 layer_factory.hpp:77] Creating layer ip1
I0428 20:08:26.923925 31381 net.cpp:86] Creating Layer ip1
I0428 20:08:26.923928 31381 net.cpp:408] ip1 <- pool1
I0428 20:08:26.923935 31381 net.cpp:382] ip1 -> ip1
I0428 20:08:26.924154 31381 net.cpp:124] Setting up ip1
I0428 20:08:26.924162 31381 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:08:26.924177 31381 net.cpp:139] Memory required for data: 7854800
I0428 20:08:26.924193 31381 layer_factory.hpp:77] Creating layer relu1
I0428 20:08:26.924201 31381 net.cpp:86] Creating Layer relu1
I0428 20:08:26.924209 31381 net.cpp:408] relu1 <- ip1
I0428 20:08:26.924216 31381 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:08:26.924428 31381 net.cpp:124] Setting up relu1
I0428 20:08:26.924439 31381 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:08:26.924443 31381 net.cpp:139] Memory required for data: 7874800
I0428 20:08:26.924446 31381 layer_factory.hpp:77] Creating layer ip2
I0428 20:08:26.924456 31381 net.cpp:86] Creating Layer ip2
I0428 20:08:26.924460 31381 net.cpp:408] ip2 <- ip1
I0428 20:08:26.924466 31381 net.cpp:382] ip2 -> ip2
I0428 20:08:26.924595 31381 net.cpp:124] Setting up ip2
I0428 20:08:26.924604 31381 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:08:26.924607 31381 net.cpp:139] Memory required for data: 7878800
I0428 20:08:26.924613 31381 layer_factory.hpp:77] Creating layer relu2
I0428 20:08:26.924620 31381 net.cpp:86] Creating Layer relu2
I0428 20:08:26.924624 31381 net.cpp:408] relu2 <- ip2
I0428 20:08:26.924629 31381 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:08:26.924839 31381 net.cpp:124] Setting up relu2
I0428 20:08:26.924849 31381 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:08:26.924852 31381 net.cpp:139] Memory required for data: 7882800
I0428 20:08:26.924856 31381 layer_factory.hpp:77] Creating layer ip3
I0428 20:08:26.924865 31381 net.cpp:86] Creating Layer ip3
I0428 20:08:26.924870 31381 net.cpp:408] ip3 <- ip2
I0428 20:08:26.924876 31381 net.cpp:382] ip3 -> ip3
I0428 20:08:26.924995 31381 net.cpp:124] Setting up ip3
I0428 20:08:26.925004 31381 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:08:26.925007 31381 net.cpp:139] Memory required for data: 7886800
I0428 20:08:26.925016 31381 layer_factory.hpp:77] Creating layer relu3
I0428 20:08:26.925024 31381 net.cpp:86] Creating Layer relu3
I0428 20:08:26.925027 31381 net.cpp:408] relu3 <- ip3
I0428 20:08:26.925031 31381 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:08:26.925957 31381 net.cpp:124] Setting up relu3
I0428 20:08:26.925973 31381 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:08:26.925977 31381 net.cpp:139] Memory required for data: 7890800
I0428 20:08:26.925981 31381 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:08:26.925988 31381 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:08:26.925992 31381 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:08:26.925997 31381 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:08:26.926004 31381 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:08:26.926056 31381 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:08:26.926069 31381 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:08:26.926074 31381 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:08:26.926077 31381 net.cpp:139] Memory required for data: 7898800
I0428 20:08:26.926081 31381 layer_factory.hpp:77] Creating layer accuracy
I0428 20:08:26.926086 31381 net.cpp:86] Creating Layer accuracy
I0428 20:08:26.926090 31381 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:08:26.926102 31381 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:08:26.926108 31381 net.cpp:382] accuracy -> accuracy
I0428 20:08:26.926116 31381 net.cpp:124] Setting up accuracy
I0428 20:08:26.926121 31381 net.cpp:131] Top shape: (1)
I0428 20:08:26.926125 31381 net.cpp:139] Memory required for data: 7898804
I0428 20:08:26.926133 31381 layer_factory.hpp:77] Creating layer loss
I0428 20:08:26.926141 31381 net.cpp:86] Creating Layer loss
I0428 20:08:26.926144 31381 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:08:26.926154 31381 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:08:26.926159 31381 net.cpp:382] loss -> loss
I0428 20:08:26.926165 31381 layer_factory.hpp:77] Creating layer loss
I0428 20:08:26.926466 31381 net.cpp:124] Setting up loss
I0428 20:08:26.926477 31381 net.cpp:131] Top shape: (1)
I0428 20:08:26.926481 31381 net.cpp:134]     with loss weight 1
I0428 20:08:26.926501 31381 net.cpp:139] Memory required for data: 7898808
I0428 20:08:26.926506 31381 net.cpp:200] loss needs backward computation.
I0428 20:08:26.926511 31381 net.cpp:202] accuracy does not need backward computation.
I0428 20:08:26.926515 31381 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:08:26.926518 31381 net.cpp:200] relu3 needs backward computation.
I0428 20:08:26.926522 31381 net.cpp:200] ip3 needs backward computation.
I0428 20:08:26.926525 31381 net.cpp:200] relu2 needs backward computation.
I0428 20:08:26.926528 31381 net.cpp:200] ip2 needs backward computation.
I0428 20:08:26.926532 31381 net.cpp:200] relu1 needs backward computation.
I0428 20:08:26.926534 31381 net.cpp:200] ip1 needs backward computation.
I0428 20:08:26.926538 31381 net.cpp:200] pool1 needs backward computation.
I0428 20:08:26.926542 31381 net.cpp:200] conv1 needs backward computation.
I0428 20:08:26.926553 31381 net.cpp:200] pool0 needs backward computation.
I0428 20:08:26.926558 31381 net.cpp:200] conv0 needs backward computation.
I0428 20:08:26.926561 31381 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:08:26.926566 31381 net.cpp:202] mnist does not need backward computation.
I0428 20:08:26.926569 31381 net.cpp:244] This network produces output accuracy
I0428 20:08:26.926573 31381 net.cpp:244] This network produces output loss
I0428 20:08:26.926587 31381 net.cpp:257] Network initialization done.
I0428 20:08:26.926636 31381 solver.cpp:56] Solver scaffolding done.
I0428 20:08:26.927036 31381 caffe.cpp:248] Starting Optimization
I0428 20:08:26.927043 31381 solver.cpp:273] Solving LeNet
I0428 20:08:26.927047 31381 solver.cpp:274] Learning Rate Policy: inv
I0428 20:08:26.927999 31381 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:08:26.933518 31381 blocking_queue.cpp:49] Waiting for data
I0428 20:08:27.004328 31389 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:08:27.004972 31381 solver.cpp:398]     Test net output #0: accuracy = 0.0832
I0428 20:08:27.004995 31381 solver.cpp:398]     Test net output #1: loss = 2.31274 (* 1 = 2.31274 loss)
I0428 20:08:27.009023 31381 solver.cpp:219] Iteration 0 (0 iter/s, 0.0819477s/100 iters), loss = 2.30877
I0428 20:08:27.009052 31381 solver.cpp:238]     Train net output #0: loss = 2.30877 (* 1 = 2.30877 loss)
I0428 20:08:27.009074 31381 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:08:27.106428 31381 solver.cpp:219] Iteration 100 (1027.09 iter/s, 0.0973628s/100 iters), loss = 1.54516
I0428 20:08:27.106457 31381 solver.cpp:238]     Train net output #0: loss = 1.54516 (* 1 = 1.54516 loss)
I0428 20:08:27.106474 31381 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:08:27.200737 31381 solver.cpp:219] Iteration 200 (1060.79 iter/s, 0.0942694s/100 iters), loss = 1.36045
I0428 20:08:27.200767 31381 solver.cpp:238]     Train net output #0: loss = 1.36045 (* 1 = 1.36045 loss)
I0428 20:08:27.200788 31381 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:08:27.296427 31381 solver.cpp:219] Iteration 300 (1045.47 iter/s, 0.0956506s/100 iters), loss = 1.69451
I0428 20:08:27.296455 31381 solver.cpp:238]     Train net output #0: loss = 1.69451 (* 1 = 1.69451 loss)
I0428 20:08:27.296463 31381 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:08:27.392493 31381 solver.cpp:219] Iteration 400 (1041.37 iter/s, 0.0960276s/100 iters), loss = 1.61883
I0428 20:08:27.392521 31381 solver.cpp:238]     Train net output #0: loss = 1.61883 (* 1 = 1.61883 loss)
I0428 20:08:27.392534 31381 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:08:27.485667 31381 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:08:27.562134 31389 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:08:27.562772 31381 solver.cpp:398]     Test net output #0: accuracy = 0.4657
I0428 20:08:27.562798 31381 solver.cpp:398]     Test net output #1: loss = 1.45235 (* 1 = 1.45235 loss)
I0428 20:08:27.563804 31381 solver.cpp:219] Iteration 500 (583.876 iter/s, 0.171269s/100 iters), loss = 1.31552
I0428 20:08:27.563861 31381 solver.cpp:238]     Train net output #0: loss = 1.31552 (* 1 = 1.31552 loss)
I0428 20:08:27.563897 31381 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:08:27.661826 31381 solver.cpp:219] Iteration 600 (1020.85 iter/s, 0.0979576s/100 iters), loss = 1.54489
I0428 20:08:27.661852 31381 solver.cpp:238]     Train net output #0: loss = 1.54489 (* 1 = 1.54489 loss)
I0428 20:08:27.661859 31381 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:08:27.753217 31381 solver.cpp:219] Iteration 700 (1094.62 iter/s, 0.0913561s/100 iters), loss = 1.31
I0428 20:08:27.753240 31381 solver.cpp:238]     Train net output #0: loss = 1.31 (* 1 = 1.31 loss)
I0428 20:08:27.753262 31381 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:08:27.843607 31381 solver.cpp:219] Iteration 800 (1106.71 iter/s, 0.0903576s/100 iters), loss = 1.69745
I0428 20:08:27.843647 31381 solver.cpp:238]     Train net output #0: loss = 1.69745 (* 1 = 1.69745 loss)
I0428 20:08:27.843653 31381 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:08:27.933746 31381 solver.cpp:219] Iteration 900 (1109.82 iter/s, 0.0901048s/100 iters), loss = 1.65793
I0428 20:08:27.933785 31381 solver.cpp:238]     Train net output #0: loss = 1.65793 (* 1 = 1.65793 loss)
I0428 20:08:27.933791 31381 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:08:27.963773 31387 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:08:28.022356 31381 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:08:28.023293 31381 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:08:28.023943 31381 solver.cpp:311] Iteration 1000, loss = 1.43135
I0428 20:08:28.023957 31381 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:08:28.099781 31389 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:08:28.100381 31381 solver.cpp:398]     Test net output #0: accuracy = 0.4899
I0428 20:08:28.100399 31381 solver.cpp:398]     Test net output #1: loss = 1.42953 (* 1 = 1.42953 loss)
I0428 20:08:28.100424 31381 solver.cpp:316] Optimization Done.
I0428 20:08:28.100426 31381 caffe.cpp:259] Optimization Done.
