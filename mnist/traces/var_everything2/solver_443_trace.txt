I0428 19:44:15.676504 25565 caffe.cpp:218] Using GPUs 0
I0428 19:44:15.716452 25565 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:44:16.240221 25565 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test443.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:44:16.240394 25565 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test443.prototxt
I0428 19:44:16.240826 25565 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:44:16.240856 25565 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:44:16.240970 25565 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:44:16.241087 25565 layer_factory.hpp:77] Creating layer mnist
I0428 19:44:16.241219 25565 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:44:16.241252 25565 net.cpp:86] Creating Layer mnist
I0428 19:44:16.241267 25565 net.cpp:382] mnist -> data
I0428 19:44:16.241298 25565 net.cpp:382] mnist -> label
I0428 19:44:16.242578 25565 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:44:16.245069 25565 net.cpp:124] Setting up mnist
I0428 19:44:16.245090 25565 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:44:16.245100 25565 net.cpp:131] Top shape: 64 (64)
I0428 19:44:16.245105 25565 net.cpp:139] Memory required for data: 200960
I0428 19:44:16.245116 25565 layer_factory.hpp:77] Creating layer conv0
I0428 19:44:16.245141 25565 net.cpp:86] Creating Layer conv0
I0428 19:44:16.245165 25565 net.cpp:408] conv0 <- data
I0428 19:44:16.245187 25565 net.cpp:382] conv0 -> conv0
I0428 19:44:16.532868 25565 net.cpp:124] Setting up conv0
I0428 19:44:16.532899 25565 net.cpp:131] Top shape: 64 2 24 24 (73728)
I0428 19:44:16.532907 25565 net.cpp:139] Memory required for data: 495872
I0428 19:44:16.532929 25565 layer_factory.hpp:77] Creating layer pool0
I0428 19:44:16.532958 25565 net.cpp:86] Creating Layer pool0
I0428 19:44:16.532974 25565 net.cpp:408] pool0 <- conv0
I0428 19:44:16.532984 25565 net.cpp:382] pool0 -> pool0
I0428 19:44:16.533046 25565 net.cpp:124] Setting up pool0
I0428 19:44:16.533056 25565 net.cpp:131] Top shape: 64 2 12 12 (18432)
I0428 19:44:16.533061 25565 net.cpp:139] Memory required for data: 569600
I0428 19:44:16.533067 25565 layer_factory.hpp:77] Creating layer conv1
I0428 19:44:16.533084 25565 net.cpp:86] Creating Layer conv1
I0428 19:44:16.533092 25565 net.cpp:408] conv1 <- pool0
I0428 19:44:16.533100 25565 net.cpp:382] conv1 -> conv1
I0428 19:44:16.535873 25565 net.cpp:124] Setting up conv1
I0428 19:44:16.535890 25565 net.cpp:131] Top shape: 64 50 8 8 (204800)
I0428 19:44:16.535897 25565 net.cpp:139] Memory required for data: 1388800
I0428 19:44:16.535912 25565 layer_factory.hpp:77] Creating layer pool1
I0428 19:44:16.535923 25565 net.cpp:86] Creating Layer pool1
I0428 19:44:16.535930 25565 net.cpp:408] pool1 <- conv1
I0428 19:44:16.535939 25565 net.cpp:382] pool1 -> pool1
I0428 19:44:16.535986 25565 net.cpp:124] Setting up pool1
I0428 19:44:16.535996 25565 net.cpp:131] Top shape: 64 50 4 4 (51200)
I0428 19:44:16.536002 25565 net.cpp:139] Memory required for data: 1593600
I0428 19:44:16.536007 25565 layer_factory.hpp:77] Creating layer ip1
I0428 19:44:16.536020 25565 net.cpp:86] Creating Layer ip1
I0428 19:44:16.536031 25565 net.cpp:408] ip1 <- pool1
I0428 19:44:16.536039 25565 net.cpp:382] ip1 -> ip1
I0428 19:44:16.537060 25565 net.cpp:124] Setting up ip1
I0428 19:44:16.537076 25565 net.cpp:131] Top shape: 64 10 (640)
I0428 19:44:16.537082 25565 net.cpp:139] Memory required for data: 1596160
I0428 19:44:16.537096 25565 layer_factory.hpp:77] Creating layer relu1
I0428 19:44:16.537107 25565 net.cpp:86] Creating Layer relu1
I0428 19:44:16.537113 25565 net.cpp:408] relu1 <- ip1
I0428 19:44:16.537122 25565 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:44:16.537307 25565 net.cpp:124] Setting up relu1
I0428 19:44:16.537318 25565 net.cpp:131] Top shape: 64 10 (640)
I0428 19:44:16.537323 25565 net.cpp:139] Memory required for data: 1598720
I0428 19:44:16.537329 25565 layer_factory.hpp:77] Creating layer ip2
I0428 19:44:16.537340 25565 net.cpp:86] Creating Layer ip2
I0428 19:44:16.537346 25565 net.cpp:408] ip2 <- ip1
I0428 19:44:16.537355 25565 net.cpp:382] ip2 -> ip2
I0428 19:44:16.537466 25565 net.cpp:124] Setting up ip2
I0428 19:44:16.537475 25565 net.cpp:131] Top shape: 64 10 (640)
I0428 19:44:16.537480 25565 net.cpp:139] Memory required for data: 1601280
I0428 19:44:16.537490 25565 layer_factory.hpp:77] Creating layer relu2
I0428 19:44:16.537500 25565 net.cpp:86] Creating Layer relu2
I0428 19:44:16.537510 25565 net.cpp:408] relu2 <- ip2
I0428 19:44:16.537518 25565 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:44:16.538331 25565 net.cpp:124] Setting up relu2
I0428 19:44:16.538345 25565 net.cpp:131] Top shape: 64 10 (640)
I0428 19:44:16.538352 25565 net.cpp:139] Memory required for data: 1603840
I0428 19:44:16.538357 25565 layer_factory.hpp:77] Creating layer ip3
I0428 19:44:16.538370 25565 net.cpp:86] Creating Layer ip3
I0428 19:44:16.538377 25565 net.cpp:408] ip3 <- ip2
I0428 19:44:16.538388 25565 net.cpp:382] ip3 -> ip3
I0428 19:44:16.538504 25565 net.cpp:124] Setting up ip3
I0428 19:44:16.538517 25565 net.cpp:131] Top shape: 64 10 (640)
I0428 19:44:16.538522 25565 net.cpp:139] Memory required for data: 1606400
I0428 19:44:16.538537 25565 layer_factory.hpp:77] Creating layer relu3
I0428 19:44:16.538548 25565 net.cpp:86] Creating Layer relu3
I0428 19:44:16.538554 25565 net.cpp:408] relu3 <- ip3
I0428 19:44:16.538563 25565 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:44:16.538749 25565 net.cpp:124] Setting up relu3
I0428 19:44:16.538760 25565 net.cpp:131] Top shape: 64 10 (640)
I0428 19:44:16.538766 25565 net.cpp:139] Memory required for data: 1608960
I0428 19:44:16.538771 25565 layer_factory.hpp:77] Creating layer loss
I0428 19:44:16.538780 25565 net.cpp:86] Creating Layer loss
I0428 19:44:16.538786 25565 net.cpp:408] loss <- ip3
I0428 19:44:16.538794 25565 net.cpp:408] loss <- label
I0428 19:44:16.538802 25565 net.cpp:382] loss -> loss
I0428 19:44:16.538825 25565 layer_factory.hpp:77] Creating layer loss
I0428 19:44:16.539104 25565 net.cpp:124] Setting up loss
I0428 19:44:16.539115 25565 net.cpp:131] Top shape: (1)
I0428 19:44:16.539121 25565 net.cpp:134]     with loss weight 1
I0428 19:44:16.539157 25565 net.cpp:139] Memory required for data: 1608964
I0428 19:44:16.539165 25565 net.cpp:200] loss needs backward computation.
I0428 19:44:16.539171 25565 net.cpp:200] relu3 needs backward computation.
I0428 19:44:16.539176 25565 net.cpp:200] ip3 needs backward computation.
I0428 19:44:16.539182 25565 net.cpp:200] relu2 needs backward computation.
I0428 19:44:16.539187 25565 net.cpp:200] ip2 needs backward computation.
I0428 19:44:16.539192 25565 net.cpp:200] relu1 needs backward computation.
I0428 19:44:16.539197 25565 net.cpp:200] ip1 needs backward computation.
I0428 19:44:16.539202 25565 net.cpp:200] pool1 needs backward computation.
I0428 19:44:16.539208 25565 net.cpp:200] conv1 needs backward computation.
I0428 19:44:16.539213 25565 net.cpp:200] pool0 needs backward computation.
I0428 19:44:16.539219 25565 net.cpp:200] conv0 needs backward computation.
I0428 19:44:16.539239 25565 net.cpp:202] mnist does not need backward computation.
I0428 19:44:16.539244 25565 net.cpp:244] This network produces output loss
I0428 19:44:16.539259 25565 net.cpp:257] Network initialization done.
I0428 19:44:16.539608 25565 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test443.prototxt
I0428 19:44:16.539644 25565 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:44:16.539752 25565 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:44:16.539878 25565 layer_factory.hpp:77] Creating layer mnist
I0428 19:44:16.539939 25565 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:44:16.539963 25565 net.cpp:86] Creating Layer mnist
I0428 19:44:16.539973 25565 net.cpp:382] mnist -> data
I0428 19:44:16.539985 25565 net.cpp:382] mnist -> label
I0428 19:44:16.540118 25565 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:44:16.542287 25565 net.cpp:124] Setting up mnist
I0428 19:44:16.542304 25565 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:44:16.542315 25565 net.cpp:131] Top shape: 100 (100)
I0428 19:44:16.542320 25565 net.cpp:139] Memory required for data: 314000
I0428 19:44:16.542325 25565 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:44:16.542351 25565 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:44:16.542358 25565 net.cpp:408] label_mnist_1_split <- label
I0428 19:44:16.542366 25565 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:44:16.542377 25565 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:44:16.542524 25565 net.cpp:124] Setting up label_mnist_1_split
I0428 19:44:16.542534 25565 net.cpp:131] Top shape: 100 (100)
I0428 19:44:16.542541 25565 net.cpp:131] Top shape: 100 (100)
I0428 19:44:16.542546 25565 net.cpp:139] Memory required for data: 314800
I0428 19:44:16.542552 25565 layer_factory.hpp:77] Creating layer conv0
I0428 19:44:16.542568 25565 net.cpp:86] Creating Layer conv0
I0428 19:44:16.542575 25565 net.cpp:408] conv0 <- data
I0428 19:44:16.542587 25565 net.cpp:382] conv0 -> conv0
I0428 19:44:16.544124 25565 net.cpp:124] Setting up conv0
I0428 19:44:16.544142 25565 net.cpp:131] Top shape: 100 2 24 24 (115200)
I0428 19:44:16.544147 25565 net.cpp:139] Memory required for data: 775600
I0428 19:44:16.544162 25565 layer_factory.hpp:77] Creating layer pool0
I0428 19:44:16.544175 25565 net.cpp:86] Creating Layer pool0
I0428 19:44:16.544184 25565 net.cpp:408] pool0 <- conv0
I0428 19:44:16.544193 25565 net.cpp:382] pool0 -> pool0
I0428 19:44:16.544240 25565 net.cpp:124] Setting up pool0
I0428 19:44:16.544250 25565 net.cpp:131] Top shape: 100 2 12 12 (28800)
I0428 19:44:16.544255 25565 net.cpp:139] Memory required for data: 890800
I0428 19:44:16.544260 25565 layer_factory.hpp:77] Creating layer conv1
I0428 19:44:16.544275 25565 net.cpp:86] Creating Layer conv1
I0428 19:44:16.544282 25565 net.cpp:408] conv1 <- pool0
I0428 19:44:16.544294 25565 net.cpp:382] conv1 -> conv1
I0428 19:44:16.546109 25565 net.cpp:124] Setting up conv1
I0428 19:44:16.546124 25565 net.cpp:131] Top shape: 100 50 8 8 (320000)
I0428 19:44:16.546130 25565 net.cpp:139] Memory required for data: 2170800
I0428 19:44:16.546144 25565 layer_factory.hpp:77] Creating layer pool1
I0428 19:44:16.546154 25565 net.cpp:86] Creating Layer pool1
I0428 19:44:16.546160 25565 net.cpp:408] pool1 <- conv1
I0428 19:44:16.546171 25565 net.cpp:382] pool1 -> pool1
I0428 19:44:16.546216 25565 net.cpp:124] Setting up pool1
I0428 19:44:16.546226 25565 net.cpp:131] Top shape: 100 50 4 4 (80000)
I0428 19:44:16.546231 25565 net.cpp:139] Memory required for data: 2490800
I0428 19:44:16.546236 25565 layer_factory.hpp:77] Creating layer ip1
I0428 19:44:16.546247 25565 net.cpp:86] Creating Layer ip1
I0428 19:44:16.546252 25565 net.cpp:408] ip1 <- pool1
I0428 19:44:16.546264 25565 net.cpp:382] ip1 -> ip1
I0428 19:44:16.546419 25565 net.cpp:124] Setting up ip1
I0428 19:44:16.546429 25565 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:44:16.546452 25565 net.cpp:139] Memory required for data: 2494800
I0428 19:44:16.546465 25565 layer_factory.hpp:77] Creating layer relu1
I0428 19:44:16.546474 25565 net.cpp:86] Creating Layer relu1
I0428 19:44:16.546480 25565 net.cpp:408] relu1 <- ip1
I0428 19:44:16.546489 25565 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:44:16.546664 25565 net.cpp:124] Setting up relu1
I0428 19:44:16.546676 25565 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:44:16.546681 25565 net.cpp:139] Memory required for data: 2498800
I0428 19:44:16.546689 25565 layer_factory.hpp:77] Creating layer ip2
I0428 19:44:16.546700 25565 net.cpp:86] Creating Layer ip2
I0428 19:44:16.546705 25565 net.cpp:408] ip2 <- ip1
I0428 19:44:16.546715 25565 net.cpp:382] ip2 -> ip2
I0428 19:44:16.546825 25565 net.cpp:124] Setting up ip2
I0428 19:44:16.546834 25565 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:44:16.546839 25565 net.cpp:139] Memory required for data: 2502800
I0428 19:44:16.546849 25565 layer_factory.hpp:77] Creating layer relu2
I0428 19:44:16.546856 25565 net.cpp:86] Creating Layer relu2
I0428 19:44:16.546862 25565 net.cpp:408] relu2 <- ip2
I0428 19:44:16.546871 25565 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:44:16.547143 25565 net.cpp:124] Setting up relu2
I0428 19:44:16.547153 25565 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:44:16.547159 25565 net.cpp:139] Memory required for data: 2506800
I0428 19:44:16.547165 25565 layer_factory.hpp:77] Creating layer ip3
I0428 19:44:16.547176 25565 net.cpp:86] Creating Layer ip3
I0428 19:44:16.547183 25565 net.cpp:408] ip3 <- ip2
I0428 19:44:16.547191 25565 net.cpp:382] ip3 -> ip3
I0428 19:44:16.547302 25565 net.cpp:124] Setting up ip3
I0428 19:44:16.547312 25565 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:44:16.547317 25565 net.cpp:139] Memory required for data: 2510800
I0428 19:44:16.547330 25565 layer_factory.hpp:77] Creating layer relu3
I0428 19:44:16.547339 25565 net.cpp:86] Creating Layer relu3
I0428 19:44:16.547345 25565 net.cpp:408] relu3 <- ip3
I0428 19:44:16.547354 25565 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:44:16.548138 25565 net.cpp:124] Setting up relu3
I0428 19:44:16.548151 25565 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:44:16.548157 25565 net.cpp:139] Memory required for data: 2514800
I0428 19:44:16.548162 25565 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:44:16.548173 25565 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:44:16.548179 25565 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:44:16.548187 25565 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:44:16.548197 25565 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:44:16.548271 25565 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:44:16.548280 25565 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:44:16.548287 25565 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:44:16.548292 25565 net.cpp:139] Memory required for data: 2522800
I0428 19:44:16.548298 25565 layer_factory.hpp:77] Creating layer accuracy
I0428 19:44:16.548307 25565 net.cpp:86] Creating Layer accuracy
I0428 19:44:16.548312 25565 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:44:16.548319 25565 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:44:16.548329 25565 net.cpp:382] accuracy -> accuracy
I0428 19:44:16.548341 25565 net.cpp:124] Setting up accuracy
I0428 19:44:16.548349 25565 net.cpp:131] Top shape: (1)
I0428 19:44:16.548354 25565 net.cpp:139] Memory required for data: 2522804
I0428 19:44:16.548359 25565 layer_factory.hpp:77] Creating layer loss
I0428 19:44:16.548370 25565 net.cpp:86] Creating Layer loss
I0428 19:44:16.548377 25565 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:44:16.548383 25565 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:44:16.548393 25565 net.cpp:382] loss -> loss
I0428 19:44:16.548403 25565 layer_factory.hpp:77] Creating layer loss
I0428 19:44:16.548660 25565 net.cpp:124] Setting up loss
I0428 19:44:16.548673 25565 net.cpp:131] Top shape: (1)
I0428 19:44:16.548679 25565 net.cpp:134]     with loss weight 1
I0428 19:44:16.548688 25565 net.cpp:139] Memory required for data: 2522808
I0428 19:44:16.548704 25565 net.cpp:200] loss needs backward computation.
I0428 19:44:16.548712 25565 net.cpp:202] accuracy does not need backward computation.
I0428 19:44:16.548718 25565 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:44:16.548723 25565 net.cpp:200] relu3 needs backward computation.
I0428 19:44:16.548738 25565 net.cpp:200] ip3 needs backward computation.
I0428 19:44:16.548743 25565 net.cpp:200] relu2 needs backward computation.
I0428 19:44:16.548748 25565 net.cpp:200] ip2 needs backward computation.
I0428 19:44:16.548753 25565 net.cpp:200] relu1 needs backward computation.
I0428 19:44:16.548758 25565 net.cpp:200] ip1 needs backward computation.
I0428 19:44:16.548763 25565 net.cpp:200] pool1 needs backward computation.
I0428 19:44:16.548769 25565 net.cpp:200] conv1 needs backward computation.
I0428 19:44:16.548777 25565 net.cpp:200] pool0 needs backward computation.
I0428 19:44:16.548782 25565 net.cpp:200] conv0 needs backward computation.
I0428 19:44:16.548789 25565 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:44:16.548820 25565 net.cpp:202] mnist does not need backward computation.
I0428 19:44:16.548836 25565 net.cpp:244] This network produces output accuracy
I0428 19:44:16.548843 25565 net.cpp:244] This network produces output loss
I0428 19:44:16.548861 25565 net.cpp:257] Network initialization done.
I0428 19:44:16.548912 25565 solver.cpp:56] Solver scaffolding done.
I0428 19:44:16.549270 25565 caffe.cpp:248] Starting Optimization
I0428 19:44:16.549278 25565 solver.cpp:273] Solving LeNet
I0428 19:44:16.549283 25565 solver.cpp:274] Learning Rate Policy: inv
I0428 19:44:16.549451 25565 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:44:16.553458 25565 blocking_queue.cpp:49] Waiting for data
I0428 19:44:16.626430 25573 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:44:16.626927 25565 solver.cpp:398]     Test net output #0: accuracy = 0.0875
I0428 19:44:16.626951 25565 solver.cpp:398]     Test net output #1: loss = 2.31594 (* 1 = 2.31594 loss)
I0428 19:44:16.629003 25565 solver.cpp:219] Iteration 0 (-1.03696e-42 iter/s, 0.0796836s/100 iters), loss = 2.30671
I0428 19:44:16.629034 25565 solver.cpp:238]     Train net output #0: loss = 2.30671 (* 1 = 2.30671 loss)
I0428 19:44:16.629051 25565 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:44:16.732569 25565 solver.cpp:219] Iteration 100 (965.937 iter/s, 0.103526s/100 iters), loss = 1.22274
I0428 19:44:16.732612 25565 solver.cpp:238]     Train net output #0: loss = 1.22274 (* 1 = 1.22274 loss)
I0428 19:44:16.732637 25565 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:44:16.838675 25565 solver.cpp:219] Iteration 200 (942.91 iter/s, 0.106055s/100 iters), loss = 0.858012
I0428 19:44:16.838702 25565 solver.cpp:238]     Train net output #0: loss = 0.858012 (* 1 = 0.858012 loss)
I0428 19:44:16.838727 25565 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:44:16.943167 25565 solver.cpp:219] Iteration 300 (957.33 iter/s, 0.104457s/100 iters), loss = 0.551221
I0428 19:44:16.943194 25565 solver.cpp:238]     Train net output #0: loss = 0.551221 (* 1 = 0.551221 loss)
I0428 19:44:16.943219 25565 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:44:17.045487 25565 solver.cpp:219] Iteration 400 (977.667 iter/s, 0.102284s/100 iters), loss = 0.664082
I0428 19:44:17.045514 25565 solver.cpp:238]     Train net output #0: loss = 0.664082 (* 1 = 0.664082 loss)
I0428 19:44:17.045524 25565 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:44:17.146255 25565 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:44:17.191704 25573 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:44:17.192184 25565 solver.cpp:398]     Test net output #0: accuracy = 0.9172
I0428 19:44:17.192206 25565 solver.cpp:398]     Test net output #1: loss = 0.472403 (* 1 = 0.472403 loss)
I0428 19:44:17.193222 25565 solver.cpp:219] Iteration 500 (677.058 iter/s, 0.147698s/100 iters), loss = 0.401997
I0428 19:44:17.193261 25565 solver.cpp:238]     Train net output #0: loss = 0.401997 (* 1 = 0.401997 loss)
I0428 19:44:17.193284 25565 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:44:17.295395 25565 solver.cpp:219] Iteration 600 (979.159 iter/s, 0.102128s/100 iters), loss = 0.431749
I0428 19:44:17.295426 25565 solver.cpp:238]     Train net output #0: loss = 0.431749 (* 1 = 0.431749 loss)
I0428 19:44:17.295438 25565 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:44:17.396958 25565 solver.cpp:219] Iteration 700 (984.984 iter/s, 0.101524s/100 iters), loss = 0.446673
I0428 19:44:17.396987 25565 solver.cpp:238]     Train net output #0: loss = 0.446673 (* 1 = 0.446673 loss)
I0428 19:44:17.396997 25565 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:44:17.505544 25565 solver.cpp:219] Iteration 800 (921.27 iter/s, 0.108546s/100 iters), loss = 0.58863
I0428 19:44:17.505591 25565 solver.cpp:238]     Train net output #0: loss = 0.58863 (* 1 = 0.58863 loss)
I0428 19:44:17.505602 25565 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:44:17.606921 25565 solver.cpp:219] Iteration 900 (986.967 iter/s, 0.101321s/100 iters), loss = 0.352829
I0428 19:44:17.606951 25565 solver.cpp:238]     Train net output #0: loss = 0.352829 (* 1 = 0.352829 loss)
I0428 19:44:17.606962 25565 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:44:17.641561 25572 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:44:17.710533 25565 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:44:17.711560 25565 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:44:17.712136 25565 solver.cpp:311] Iteration 1000, loss = 0.481932
I0428 19:44:17.712152 25565 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:44:17.763170 25573 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:44:17.763636 25565 solver.cpp:398]     Test net output #0: accuracy = 0.9468
I0428 19:44:17.763658 25565 solver.cpp:398]     Test net output #1: loss = 0.375079 (* 1 = 0.375079 loss)
I0428 19:44:17.763665 25565 solver.cpp:316] Optimization Done.
I0428 19:44:17.763671 25565 caffe.cpp:259] Optimization Done.
