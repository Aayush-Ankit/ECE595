I0428 19:43:01.597493 25244 caffe.cpp:218] Using GPUs 0
I0428 19:43:01.629380 25244 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:43:02.137687 25244 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test410.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:43:02.137830 25244 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test410.prototxt
I0428 19:43:02.138247 25244 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:43:02.138268 25244 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:43:02.138372 25244 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:43:02.138453 25244 layer_factory.hpp:77] Creating layer mnist
I0428 19:43:02.138555 25244 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:43:02.138577 25244 net.cpp:86] Creating Layer mnist
I0428 19:43:02.138586 25244 net.cpp:382] mnist -> data
I0428 19:43:02.138609 25244 net.cpp:382] mnist -> label
I0428 19:43:02.139693 25244 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:43:02.142153 25244 net.cpp:124] Setting up mnist
I0428 19:43:02.142172 25244 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:43:02.142179 25244 net.cpp:131] Top shape: 64 (64)
I0428 19:43:02.142182 25244 net.cpp:139] Memory required for data: 200960
I0428 19:43:02.142189 25244 layer_factory.hpp:77] Creating layer conv0
I0428 19:43:02.142206 25244 net.cpp:86] Creating Layer conv0
I0428 19:43:02.142226 25244 net.cpp:408] conv0 <- data
I0428 19:43:02.142240 25244 net.cpp:382] conv0 -> conv0
I0428 19:43:02.396255 25244 net.cpp:124] Setting up conv0
I0428 19:43:02.396297 25244 net.cpp:131] Top shape: 64 2 24 24 (73728)
I0428 19:43:02.396302 25244 net.cpp:139] Memory required for data: 495872
I0428 19:43:02.396317 25244 layer_factory.hpp:77] Creating layer pool0
I0428 19:43:02.396344 25244 net.cpp:86] Creating Layer pool0
I0428 19:43:02.396363 25244 net.cpp:408] pool0 <- conv0
I0428 19:43:02.396368 25244 net.cpp:382] pool0 -> pool0
I0428 19:43:02.396431 25244 net.cpp:124] Setting up pool0
I0428 19:43:02.396443 25244 net.cpp:131] Top shape: 64 2 12 12 (18432)
I0428 19:43:02.396447 25244 net.cpp:139] Memory required for data: 569600
I0428 19:43:02.396450 25244 layer_factory.hpp:77] Creating layer conv1
I0428 19:43:02.396461 25244 net.cpp:86] Creating Layer conv1
I0428 19:43:02.396463 25244 net.cpp:408] conv1 <- pool0
I0428 19:43:02.396468 25244 net.cpp:382] conv1 -> conv1
I0428 19:43:02.399317 25244 net.cpp:124] Setting up conv1
I0428 19:43:02.399348 25244 net.cpp:131] Top shape: 64 25 8 8 (102400)
I0428 19:43:02.399368 25244 net.cpp:139] Memory required for data: 979200
I0428 19:43:02.399376 25244 layer_factory.hpp:77] Creating layer pool1
I0428 19:43:02.399384 25244 net.cpp:86] Creating Layer pool1
I0428 19:43:02.399404 25244 net.cpp:408] pool1 <- conv1
I0428 19:43:02.399408 25244 net.cpp:382] pool1 -> pool1
I0428 19:43:02.399447 25244 net.cpp:124] Setting up pool1
I0428 19:43:02.399471 25244 net.cpp:131] Top shape: 64 25 4 4 (25600)
I0428 19:43:02.399474 25244 net.cpp:139] Memory required for data: 1081600
I0428 19:43:02.399478 25244 layer_factory.hpp:77] Creating layer ip1
I0428 19:43:02.399490 25244 net.cpp:86] Creating Layer ip1
I0428 19:43:02.399493 25244 net.cpp:408] ip1 <- pool1
I0428 19:43:02.399498 25244 net.cpp:382] ip1 -> ip1
I0428 19:43:02.399653 25244 net.cpp:124] Setting up ip1
I0428 19:43:02.399662 25244 net.cpp:131] Top shape: 64 10 (640)
I0428 19:43:02.399664 25244 net.cpp:139] Memory required for data: 1084160
I0428 19:43:02.399672 25244 layer_factory.hpp:77] Creating layer relu1
I0428 19:43:02.399677 25244 net.cpp:86] Creating Layer relu1
I0428 19:43:02.399682 25244 net.cpp:408] relu1 <- ip1
I0428 19:43:02.399685 25244 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:43:02.399878 25244 net.cpp:124] Setting up relu1
I0428 19:43:02.399886 25244 net.cpp:131] Top shape: 64 10 (640)
I0428 19:43:02.399889 25244 net.cpp:139] Memory required for data: 1086720
I0428 19:43:02.399893 25244 layer_factory.hpp:77] Creating layer ip2
I0428 19:43:02.399897 25244 net.cpp:86] Creating Layer ip2
I0428 19:43:02.399901 25244 net.cpp:408] ip2 <- ip1
I0428 19:43:02.399905 25244 net.cpp:382] ip2 -> ip2
I0428 19:43:02.399997 25244 net.cpp:124] Setting up ip2
I0428 19:43:02.400004 25244 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:43:02.400007 25244 net.cpp:139] Memory required for data: 1099520
I0428 19:43:02.400012 25244 layer_factory.hpp:77] Creating layer relu2
I0428 19:43:02.400018 25244 net.cpp:86] Creating Layer relu2
I0428 19:43:02.400022 25244 net.cpp:408] relu2 <- ip2
I0428 19:43:02.400025 25244 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:43:02.400861 25244 net.cpp:124] Setting up relu2
I0428 19:43:02.400874 25244 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:43:02.400893 25244 net.cpp:139] Memory required for data: 1112320
I0428 19:43:02.400897 25244 layer_factory.hpp:77] Creating layer ip3
I0428 19:43:02.400919 25244 net.cpp:86] Creating Layer ip3
I0428 19:43:02.400923 25244 net.cpp:408] ip3 <- ip2
I0428 19:43:02.400929 25244 net.cpp:382] ip3 -> ip3
I0428 19:43:02.401036 25244 net.cpp:124] Setting up ip3
I0428 19:43:02.401044 25244 net.cpp:131] Top shape: 64 10 (640)
I0428 19:43:02.401047 25244 net.cpp:139] Memory required for data: 1114880
I0428 19:43:02.401056 25244 layer_factory.hpp:77] Creating layer relu3
I0428 19:43:02.401073 25244 net.cpp:86] Creating Layer relu3
I0428 19:43:02.401077 25244 net.cpp:408] relu3 <- ip3
I0428 19:43:02.401082 25244 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:43:02.401298 25244 net.cpp:124] Setting up relu3
I0428 19:43:02.401307 25244 net.cpp:131] Top shape: 64 10 (640)
I0428 19:43:02.401311 25244 net.cpp:139] Memory required for data: 1117440
I0428 19:43:02.401314 25244 layer_factory.hpp:77] Creating layer loss
I0428 19:43:02.401320 25244 net.cpp:86] Creating Layer loss
I0428 19:43:02.401324 25244 net.cpp:408] loss <- ip3
I0428 19:43:02.401327 25244 net.cpp:408] loss <- label
I0428 19:43:02.401332 25244 net.cpp:382] loss -> loss
I0428 19:43:02.401351 25244 layer_factory.hpp:77] Creating layer loss
I0428 19:43:02.401605 25244 net.cpp:124] Setting up loss
I0428 19:43:02.401617 25244 net.cpp:131] Top shape: (1)
I0428 19:43:02.401620 25244 net.cpp:134]     with loss weight 1
I0428 19:43:02.401635 25244 net.cpp:139] Memory required for data: 1117444
I0428 19:43:02.401639 25244 net.cpp:200] loss needs backward computation.
I0428 19:43:02.401643 25244 net.cpp:200] relu3 needs backward computation.
I0428 19:43:02.401646 25244 net.cpp:200] ip3 needs backward computation.
I0428 19:43:02.401649 25244 net.cpp:200] relu2 needs backward computation.
I0428 19:43:02.401653 25244 net.cpp:200] ip2 needs backward computation.
I0428 19:43:02.401655 25244 net.cpp:200] relu1 needs backward computation.
I0428 19:43:02.401657 25244 net.cpp:200] ip1 needs backward computation.
I0428 19:43:02.401661 25244 net.cpp:200] pool1 needs backward computation.
I0428 19:43:02.401664 25244 net.cpp:200] conv1 needs backward computation.
I0428 19:43:02.401667 25244 net.cpp:200] pool0 needs backward computation.
I0428 19:43:02.401670 25244 net.cpp:200] conv0 needs backward computation.
I0428 19:43:02.401674 25244 net.cpp:202] mnist does not need backward computation.
I0428 19:43:02.401676 25244 net.cpp:244] This network produces output loss
I0428 19:43:02.401686 25244 net.cpp:257] Network initialization done.
I0428 19:43:02.402086 25244 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test410.prototxt
I0428 19:43:02.402125 25244 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:43:02.402245 25244 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:43:02.402323 25244 layer_factory.hpp:77] Creating layer mnist
I0428 19:43:02.402367 25244 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:43:02.402379 25244 net.cpp:86] Creating Layer mnist
I0428 19:43:02.402384 25244 net.cpp:382] mnist -> data
I0428 19:43:02.402392 25244 net.cpp:382] mnist -> label
I0428 19:43:02.402473 25244 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:43:02.404700 25244 net.cpp:124] Setting up mnist
I0428 19:43:02.404713 25244 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:43:02.404719 25244 net.cpp:131] Top shape: 100 (100)
I0428 19:43:02.404723 25244 net.cpp:139] Memory required for data: 314000
I0428 19:43:02.404727 25244 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:43:02.404737 25244 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:43:02.404742 25244 net.cpp:408] label_mnist_1_split <- label
I0428 19:43:02.404747 25244 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:43:02.404754 25244 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:43:02.404866 25244 net.cpp:124] Setting up label_mnist_1_split
I0428 19:43:02.404876 25244 net.cpp:131] Top shape: 100 (100)
I0428 19:43:02.404881 25244 net.cpp:131] Top shape: 100 (100)
I0428 19:43:02.404884 25244 net.cpp:139] Memory required for data: 314800
I0428 19:43:02.404887 25244 layer_factory.hpp:77] Creating layer conv0
I0428 19:43:02.404896 25244 net.cpp:86] Creating Layer conv0
I0428 19:43:02.404901 25244 net.cpp:408] conv0 <- data
I0428 19:43:02.404906 25244 net.cpp:382] conv0 -> conv0
I0428 19:43:02.406630 25244 net.cpp:124] Setting up conv0
I0428 19:43:02.406658 25244 net.cpp:131] Top shape: 100 2 24 24 (115200)
I0428 19:43:02.406662 25244 net.cpp:139] Memory required for data: 775600
I0428 19:43:02.406672 25244 layer_factory.hpp:77] Creating layer pool0
I0428 19:43:02.406678 25244 net.cpp:86] Creating Layer pool0
I0428 19:43:02.406682 25244 net.cpp:408] pool0 <- conv0
I0428 19:43:02.406687 25244 net.cpp:382] pool0 -> pool0
I0428 19:43:02.406721 25244 net.cpp:124] Setting up pool0
I0428 19:43:02.406729 25244 net.cpp:131] Top shape: 100 2 12 12 (28800)
I0428 19:43:02.406733 25244 net.cpp:139] Memory required for data: 890800
I0428 19:43:02.406735 25244 layer_factory.hpp:77] Creating layer conv1
I0428 19:43:02.406744 25244 net.cpp:86] Creating Layer conv1
I0428 19:43:02.406747 25244 net.cpp:408] conv1 <- pool0
I0428 19:43:02.406752 25244 net.cpp:382] conv1 -> conv1
I0428 19:43:02.408426 25244 net.cpp:124] Setting up conv1
I0428 19:43:02.408459 25244 net.cpp:131] Top shape: 100 25 8 8 (160000)
I0428 19:43:02.408463 25244 net.cpp:139] Memory required for data: 1530800
I0428 19:43:02.408473 25244 layer_factory.hpp:77] Creating layer pool1
I0428 19:43:02.408480 25244 net.cpp:86] Creating Layer pool1
I0428 19:43:02.408484 25244 net.cpp:408] pool1 <- conv1
I0428 19:43:02.408489 25244 net.cpp:382] pool1 -> pool1
I0428 19:43:02.408526 25244 net.cpp:124] Setting up pool1
I0428 19:43:02.408535 25244 net.cpp:131] Top shape: 100 25 4 4 (40000)
I0428 19:43:02.408538 25244 net.cpp:139] Memory required for data: 1690800
I0428 19:43:02.408542 25244 layer_factory.hpp:77] Creating layer ip1
I0428 19:43:02.408548 25244 net.cpp:86] Creating Layer ip1
I0428 19:43:02.408551 25244 net.cpp:408] ip1 <- pool1
I0428 19:43:02.408557 25244 net.cpp:382] ip1 -> ip1
I0428 19:43:02.408694 25244 net.cpp:124] Setting up ip1
I0428 19:43:02.408702 25244 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:43:02.408718 25244 net.cpp:139] Memory required for data: 1694800
I0428 19:43:02.408726 25244 layer_factory.hpp:77] Creating layer relu1
I0428 19:43:02.408731 25244 net.cpp:86] Creating Layer relu1
I0428 19:43:02.408735 25244 net.cpp:408] relu1 <- ip1
I0428 19:43:02.408740 25244 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:43:02.408956 25244 net.cpp:124] Setting up relu1
I0428 19:43:02.408967 25244 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:43:02.408970 25244 net.cpp:139] Memory required for data: 1698800
I0428 19:43:02.408974 25244 layer_factory.hpp:77] Creating layer ip2
I0428 19:43:02.408982 25244 net.cpp:86] Creating Layer ip2
I0428 19:43:02.408985 25244 net.cpp:408] ip2 <- ip1
I0428 19:43:02.408991 25244 net.cpp:382] ip2 -> ip2
I0428 19:43:02.409096 25244 net.cpp:124] Setting up ip2
I0428 19:43:02.409104 25244 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:43:02.409108 25244 net.cpp:139] Memory required for data: 1718800
I0428 19:43:02.409114 25244 layer_factory.hpp:77] Creating layer relu2
I0428 19:43:02.409119 25244 net.cpp:86] Creating Layer relu2
I0428 19:43:02.409122 25244 net.cpp:408] relu2 <- ip2
I0428 19:43:02.409127 25244 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:43:02.409409 25244 net.cpp:124] Setting up relu2
I0428 19:43:02.409418 25244 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:43:02.409422 25244 net.cpp:139] Memory required for data: 1738800
I0428 19:43:02.409425 25244 layer_factory.hpp:77] Creating layer ip3
I0428 19:43:02.409431 25244 net.cpp:86] Creating Layer ip3
I0428 19:43:02.409435 25244 net.cpp:408] ip3 <- ip2
I0428 19:43:02.409440 25244 net.cpp:382] ip3 -> ip3
I0428 19:43:02.409607 25244 net.cpp:124] Setting up ip3
I0428 19:43:02.409617 25244 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:43:02.409621 25244 net.cpp:139] Memory required for data: 1742800
I0428 19:43:02.409631 25244 layer_factory.hpp:77] Creating layer relu3
I0428 19:43:02.409636 25244 net.cpp:86] Creating Layer relu3
I0428 19:43:02.409641 25244 net.cpp:408] relu3 <- ip3
I0428 19:43:02.409644 25244 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:43:02.410522 25244 net.cpp:124] Setting up relu3
I0428 19:43:02.410534 25244 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:43:02.410553 25244 net.cpp:139] Memory required for data: 1746800
I0428 19:43:02.410557 25244 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:43:02.410562 25244 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:43:02.410565 25244 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:43:02.410571 25244 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:43:02.410578 25244 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:43:02.410637 25244 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:43:02.410645 25244 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:43:02.410650 25244 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:43:02.410653 25244 net.cpp:139] Memory required for data: 1754800
I0428 19:43:02.410656 25244 layer_factory.hpp:77] Creating layer accuracy
I0428 19:43:02.410661 25244 net.cpp:86] Creating Layer accuracy
I0428 19:43:02.410665 25244 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:43:02.410670 25244 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:43:02.410675 25244 net.cpp:382] accuracy -> accuracy
I0428 19:43:02.410682 25244 net.cpp:124] Setting up accuracy
I0428 19:43:02.410686 25244 net.cpp:131] Top shape: (1)
I0428 19:43:02.410689 25244 net.cpp:139] Memory required for data: 1754804
I0428 19:43:02.410692 25244 layer_factory.hpp:77] Creating layer loss
I0428 19:43:02.410697 25244 net.cpp:86] Creating Layer loss
I0428 19:43:02.410701 25244 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:43:02.410704 25244 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:43:02.410708 25244 net.cpp:382] loss -> loss
I0428 19:43:02.410714 25244 layer_factory.hpp:77] Creating layer loss
I0428 19:43:02.410948 25244 net.cpp:124] Setting up loss
I0428 19:43:02.410959 25244 net.cpp:131] Top shape: (1)
I0428 19:43:02.410962 25244 net.cpp:134]     with loss weight 1
I0428 19:43:02.410969 25244 net.cpp:139] Memory required for data: 1754808
I0428 19:43:02.410991 25244 net.cpp:200] loss needs backward computation.
I0428 19:43:02.410996 25244 net.cpp:202] accuracy does not need backward computation.
I0428 19:43:02.411000 25244 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:43:02.411003 25244 net.cpp:200] relu3 needs backward computation.
I0428 19:43:02.411006 25244 net.cpp:200] ip3 needs backward computation.
I0428 19:43:02.411010 25244 net.cpp:200] relu2 needs backward computation.
I0428 19:43:02.411012 25244 net.cpp:200] ip2 needs backward computation.
I0428 19:43:02.411015 25244 net.cpp:200] relu1 needs backward computation.
I0428 19:43:02.411018 25244 net.cpp:200] ip1 needs backward computation.
I0428 19:43:02.411021 25244 net.cpp:200] pool1 needs backward computation.
I0428 19:43:02.411025 25244 net.cpp:200] conv1 needs backward computation.
I0428 19:43:02.411027 25244 net.cpp:200] pool0 needs backward computation.
I0428 19:43:02.411031 25244 net.cpp:200] conv0 needs backward computation.
I0428 19:43:02.411034 25244 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:43:02.411038 25244 net.cpp:202] mnist does not need backward computation.
I0428 19:43:02.411041 25244 net.cpp:244] This network produces output accuracy
I0428 19:43:02.411043 25244 net.cpp:244] This network produces output loss
I0428 19:43:02.411061 25244 net.cpp:257] Network initialization done.
I0428 19:43:02.411108 25244 solver.cpp:56] Solver scaffolding done.
I0428 19:43:02.411489 25244 caffe.cpp:248] Starting Optimization
I0428 19:43:02.411496 25244 solver.cpp:273] Solving LeNet
I0428 19:43:02.411499 25244 solver.cpp:274] Learning Rate Policy: inv
I0428 19:43:02.412439 25244 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:43:02.415973 25244 blocking_queue.cpp:49] Waiting for data
I0428 19:43:02.482894 25251 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:43:02.483357 25244 solver.cpp:398]     Test net output #0: accuracy = 0.1112
I0428 19:43:02.483392 25244 solver.cpp:398]     Test net output #1: loss = 2.30107 (* 1 = 2.30107 loss)
I0428 19:43:02.484771 25244 solver.cpp:219] Iteration 0 (-4.10746e-31 iter/s, 0.0732324s/100 iters), loss = 2.3137
I0428 19:43:02.484838 25244 solver.cpp:238]     Train net output #0: loss = 2.3137 (* 1 = 2.3137 loss)
I0428 19:43:02.484869 25244 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:43:02.578017 25244 solver.cpp:219] Iteration 100 (1072.99 iter/s, 0.0931973s/100 iters), loss = 1.0628
I0428 19:43:02.578050 25244 solver.cpp:238]     Train net output #0: loss = 1.0628 (* 1 = 1.0628 loss)
I0428 19:43:02.578058 25244 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:43:02.664844 25244 solver.cpp:219] Iteration 200 (1152.29 iter/s, 0.086784s/100 iters), loss = 0.773309
I0428 19:43:02.664873 25244 solver.cpp:238]     Train net output #0: loss = 0.773309 (* 1 = 0.773309 loss)
I0428 19:43:02.664881 25244 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:43:02.751530 25244 solver.cpp:219] Iteration 300 (1154.1 iter/s, 0.0866478s/100 iters), loss = 0.898158
I0428 19:43:02.751561 25244 solver.cpp:238]     Train net output #0: loss = 0.898158 (* 1 = 0.898158 loss)
I0428 19:43:02.751569 25244 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:43:02.839063 25244 solver.cpp:219] Iteration 400 (1142.95 iter/s, 0.0874926s/100 iters), loss = 0.664947
I0428 19:43:02.839095 25244 solver.cpp:238]     Train net output #0: loss = 0.664947 (* 1 = 0.664947 loss)
I0428 19:43:02.839103 25244 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:43:02.925573 25244 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:43:02.988878 25251 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:43:02.989401 25244 solver.cpp:398]     Test net output #0: accuracy = 0.7618
I0428 19:43:02.989424 25244 solver.cpp:398]     Test net output #1: loss = 0.634534 (* 1 = 0.634534 loss)
I0428 19:43:02.990628 25244 solver.cpp:219] Iteration 500 (659.978 iter/s, 0.15152s/100 iters), loss = 0.439586
I0428 19:43:02.990672 25244 solver.cpp:238]     Train net output #0: loss = 0.439587 (* 1 = 0.439587 loss)
I0428 19:43:02.990702 25244 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:43:03.083060 25244 solver.cpp:219] Iteration 600 (1082.5 iter/s, 0.0923791s/100 iters), loss = 0.648214
I0428 19:43:03.083096 25244 solver.cpp:238]     Train net output #0: loss = 0.648214 (* 1 = 0.648214 loss)
I0428 19:43:03.083102 25244 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:43:03.171651 25244 solver.cpp:219] Iteration 700 (1129.42 iter/s, 0.0885414s/100 iters), loss = 0.734525
I0428 19:43:03.171676 25244 solver.cpp:238]     Train net output #0: loss = 0.734525 (* 1 = 0.734525 loss)
I0428 19:43:03.171682 25244 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:43:03.259688 25244 solver.cpp:219] Iteration 800 (1136.33 iter/s, 0.0880026s/100 iters), loss = 0.591959
I0428 19:43:03.259727 25244 solver.cpp:238]     Train net output #0: loss = 0.59196 (* 1 = 0.59196 loss)
I0428 19:43:03.259732 25244 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:43:03.345347 25244 solver.cpp:219] Iteration 900 (1167.89 iter/s, 0.0856248s/100 iters), loss = 0.537313
I0428 19:43:03.345376 25244 solver.cpp:238]     Train net output #0: loss = 0.537314 (* 1 = 0.537314 loss)
I0428 19:43:03.345396 25244 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:43:03.373976 25250 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:43:03.428803 25244 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:43:03.429590 25244 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:43:03.430119 25244 solver.cpp:311] Iteration 1000, loss = 0.570794
I0428 19:43:03.430150 25244 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:43:03.492480 25251 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:43:03.493041 25244 solver.cpp:398]     Test net output #0: accuracy = 0.7787
I0428 19:43:03.493078 25244 solver.cpp:398]     Test net output #1: loss = 0.566725 (* 1 = 0.566725 loss)
I0428 19:43:03.493085 25244 solver.cpp:316] Optimization Done.
I0428 19:43:03.493090 25244 caffe.cpp:259] Optimization Done.
