I0428 19:45:57.084174 25918 caffe.cpp:218] Using GPUs 0
I0428 19:45:57.122710 25918 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:45:57.632676 25918 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test482.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:45:57.632828 25918 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test482.prototxt
I0428 19:45:57.633255 25918 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:45:57.633275 25918 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:45:57.633379 25918 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:45:57.633460 25918 layer_factory.hpp:77] Creating layer mnist
I0428 19:45:57.633560 25918 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:45:57.633584 25918 net.cpp:86] Creating Layer mnist
I0428 19:45:57.633594 25918 net.cpp:382] mnist -> data
I0428 19:45:57.633615 25918 net.cpp:382] mnist -> label
I0428 19:45:57.634732 25918 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:45:57.637413 25918 net.cpp:124] Setting up mnist
I0428 19:45:57.637436 25918 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:45:57.637450 25918 net.cpp:131] Top shape: 64 (64)
I0428 19:45:57.637454 25918 net.cpp:139] Memory required for data: 200960
I0428 19:45:57.637460 25918 layer_factory.hpp:77] Creating layer conv0
I0428 19:45:57.637502 25918 net.cpp:86] Creating Layer conv0
I0428 19:45:57.637523 25918 net.cpp:408] conv0 <- data
I0428 19:45:57.637538 25918 net.cpp:382] conv0 -> conv0
I0428 19:45:57.928171 25918 net.cpp:124] Setting up conv0
I0428 19:45:57.928200 25918 net.cpp:131] Top shape: 64 2 24 24 (73728)
I0428 19:45:57.928205 25918 net.cpp:139] Memory required for data: 495872
I0428 19:45:57.928221 25918 layer_factory.hpp:77] Creating layer pool0
I0428 19:45:57.928236 25918 net.cpp:86] Creating Layer pool0
I0428 19:45:57.928241 25918 net.cpp:408] pool0 <- conv0
I0428 19:45:57.928247 25918 net.cpp:382] pool0 -> pool0
I0428 19:45:57.928299 25918 net.cpp:124] Setting up pool0
I0428 19:45:57.928306 25918 net.cpp:131] Top shape: 64 2 12 12 (18432)
I0428 19:45:57.928309 25918 net.cpp:139] Memory required for data: 569600
I0428 19:45:57.928313 25918 layer_factory.hpp:77] Creating layer conv1
I0428 19:45:57.928325 25918 net.cpp:86] Creating Layer conv1
I0428 19:45:57.928328 25918 net.cpp:408] conv1 <- pool0
I0428 19:45:57.928333 25918 net.cpp:382] conv1 -> conv1
I0428 19:45:57.931355 25918 net.cpp:124] Setting up conv1
I0428 19:45:57.931372 25918 net.cpp:131] Top shape: 64 100 8 8 (409600)
I0428 19:45:57.931377 25918 net.cpp:139] Memory required for data: 2208000
I0428 19:45:57.931386 25918 layer_factory.hpp:77] Creating layer pool1
I0428 19:45:57.931396 25918 net.cpp:86] Creating Layer pool1
I0428 19:45:57.931399 25918 net.cpp:408] pool1 <- conv1
I0428 19:45:57.931406 25918 net.cpp:382] pool1 -> pool1
I0428 19:45:57.931449 25918 net.cpp:124] Setting up pool1
I0428 19:45:57.931455 25918 net.cpp:131] Top shape: 64 100 4 4 (102400)
I0428 19:45:57.931458 25918 net.cpp:139] Memory required for data: 2617600
I0428 19:45:57.931463 25918 layer_factory.hpp:77] Creating layer ip1
I0428 19:45:57.931470 25918 net.cpp:86] Creating Layer ip1
I0428 19:45:57.931474 25918 net.cpp:408] ip1 <- pool1
I0428 19:45:57.931479 25918 net.cpp:382] ip1 -> ip1
I0428 19:45:57.931686 25918 net.cpp:124] Setting up ip1
I0428 19:45:57.931695 25918 net.cpp:131] Top shape: 64 10 (640)
I0428 19:45:57.931699 25918 net.cpp:139] Memory required for data: 2620160
I0428 19:45:57.931707 25918 layer_factory.hpp:77] Creating layer relu1
I0428 19:45:57.931713 25918 net.cpp:86] Creating Layer relu1
I0428 19:45:57.931717 25918 net.cpp:408] relu1 <- ip1
I0428 19:45:57.931722 25918 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:45:57.931910 25918 net.cpp:124] Setting up relu1
I0428 19:45:57.931918 25918 net.cpp:131] Top shape: 64 10 (640)
I0428 19:45:57.931922 25918 net.cpp:139] Memory required for data: 2622720
I0428 19:45:57.931926 25918 layer_factory.hpp:77] Creating layer ip2
I0428 19:45:57.931932 25918 net.cpp:86] Creating Layer ip2
I0428 19:45:57.931936 25918 net.cpp:408] ip2 <- ip1
I0428 19:45:57.931941 25918 net.cpp:382] ip2 -> ip2
I0428 19:45:57.932046 25918 net.cpp:124] Setting up ip2
I0428 19:45:57.932054 25918 net.cpp:131] Top shape: 64 10 (640)
I0428 19:45:57.932057 25918 net.cpp:139] Memory required for data: 2625280
I0428 19:45:57.932063 25918 layer_factory.hpp:77] Creating layer relu2
I0428 19:45:57.932070 25918 net.cpp:86] Creating Layer relu2
I0428 19:45:57.932075 25918 net.cpp:408] relu2 <- ip2
I0428 19:45:57.932078 25918 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:45:57.932929 25918 net.cpp:124] Setting up relu2
I0428 19:45:57.932942 25918 net.cpp:131] Top shape: 64 10 (640)
I0428 19:45:57.932946 25918 net.cpp:139] Memory required for data: 2627840
I0428 19:45:57.932950 25918 layer_factory.hpp:77] Creating layer ip3
I0428 19:45:57.932957 25918 net.cpp:86] Creating Layer ip3
I0428 19:45:57.932961 25918 net.cpp:408] ip3 <- ip2
I0428 19:45:57.932967 25918 net.cpp:382] ip3 -> ip3
I0428 19:45:57.933073 25918 net.cpp:124] Setting up ip3
I0428 19:45:57.933082 25918 net.cpp:131] Top shape: 64 10 (640)
I0428 19:45:57.933085 25918 net.cpp:139] Memory required for data: 2630400
I0428 19:45:57.933094 25918 layer_factory.hpp:77] Creating layer relu3
I0428 19:45:57.933100 25918 net.cpp:86] Creating Layer relu3
I0428 19:45:57.933104 25918 net.cpp:408] relu3 <- ip3
I0428 19:45:57.933107 25918 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:45:57.933289 25918 net.cpp:124] Setting up relu3
I0428 19:45:57.933298 25918 net.cpp:131] Top shape: 64 10 (640)
I0428 19:45:57.933301 25918 net.cpp:139] Memory required for data: 2632960
I0428 19:45:57.933305 25918 layer_factory.hpp:77] Creating layer loss
I0428 19:45:57.933311 25918 net.cpp:86] Creating Layer loss
I0428 19:45:57.933315 25918 net.cpp:408] loss <- ip3
I0428 19:45:57.933318 25918 net.cpp:408] loss <- label
I0428 19:45:57.933324 25918 net.cpp:382] loss -> loss
I0428 19:45:57.933342 25918 layer_factory.hpp:77] Creating layer loss
I0428 19:45:57.933585 25918 net.cpp:124] Setting up loss
I0428 19:45:57.933595 25918 net.cpp:131] Top shape: (1)
I0428 19:45:57.933599 25918 net.cpp:134]     with loss weight 1
I0428 19:45:57.933614 25918 net.cpp:139] Memory required for data: 2632964
I0428 19:45:57.933619 25918 net.cpp:200] loss needs backward computation.
I0428 19:45:57.933622 25918 net.cpp:200] relu3 needs backward computation.
I0428 19:45:57.933625 25918 net.cpp:200] ip3 needs backward computation.
I0428 19:45:57.933629 25918 net.cpp:200] relu2 needs backward computation.
I0428 19:45:57.933631 25918 net.cpp:200] ip2 needs backward computation.
I0428 19:45:57.933634 25918 net.cpp:200] relu1 needs backward computation.
I0428 19:45:57.933637 25918 net.cpp:200] ip1 needs backward computation.
I0428 19:45:57.933641 25918 net.cpp:200] pool1 needs backward computation.
I0428 19:45:57.933645 25918 net.cpp:200] conv1 needs backward computation.
I0428 19:45:57.933648 25918 net.cpp:200] pool0 needs backward computation.
I0428 19:45:57.933651 25918 net.cpp:200] conv0 needs backward computation.
I0428 19:45:57.933655 25918 net.cpp:202] mnist does not need backward computation.
I0428 19:45:57.933658 25918 net.cpp:244] This network produces output loss
I0428 19:45:57.933667 25918 net.cpp:257] Network initialization done.
I0428 19:45:57.934031 25918 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test482.prototxt
I0428 19:45:57.934061 25918 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:45:57.934161 25918 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:45:57.934249 25918 layer_factory.hpp:77] Creating layer mnist
I0428 19:45:57.934298 25918 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:45:57.934311 25918 net.cpp:86] Creating Layer mnist
I0428 19:45:57.934316 25918 net.cpp:382] mnist -> data
I0428 19:45:57.934324 25918 net.cpp:382] mnist -> label
I0428 19:45:57.934415 25918 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:45:57.936450 25918 net.cpp:124] Setting up mnist
I0428 19:45:57.936465 25918 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:45:57.936470 25918 net.cpp:131] Top shape: 100 (100)
I0428 19:45:57.936473 25918 net.cpp:139] Memory required for data: 314000
I0428 19:45:57.936477 25918 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:45:57.936488 25918 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:45:57.936492 25918 net.cpp:408] label_mnist_1_split <- label
I0428 19:45:57.936497 25918 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:45:57.936506 25918 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:45:57.936554 25918 net.cpp:124] Setting up label_mnist_1_split
I0428 19:45:57.936560 25918 net.cpp:131] Top shape: 100 (100)
I0428 19:45:57.936564 25918 net.cpp:131] Top shape: 100 (100)
I0428 19:45:57.936568 25918 net.cpp:139] Memory required for data: 314800
I0428 19:45:57.936571 25918 layer_factory.hpp:77] Creating layer conv0
I0428 19:45:57.936580 25918 net.cpp:86] Creating Layer conv0
I0428 19:45:57.936583 25918 net.cpp:408] conv0 <- data
I0428 19:45:57.936589 25918 net.cpp:382] conv0 -> conv0
I0428 19:45:57.938366 25918 net.cpp:124] Setting up conv0
I0428 19:45:57.938382 25918 net.cpp:131] Top shape: 100 2 24 24 (115200)
I0428 19:45:57.938386 25918 net.cpp:139] Memory required for data: 775600
I0428 19:45:57.938396 25918 layer_factory.hpp:77] Creating layer pool0
I0428 19:45:57.938406 25918 net.cpp:86] Creating Layer pool0
I0428 19:45:57.938410 25918 net.cpp:408] pool0 <- conv0
I0428 19:45:57.938416 25918 net.cpp:382] pool0 -> pool0
I0428 19:45:57.938455 25918 net.cpp:124] Setting up pool0
I0428 19:45:57.938460 25918 net.cpp:131] Top shape: 100 2 12 12 (28800)
I0428 19:45:57.938464 25918 net.cpp:139] Memory required for data: 890800
I0428 19:45:57.938467 25918 layer_factory.hpp:77] Creating layer conv1
I0428 19:45:57.938477 25918 net.cpp:86] Creating Layer conv1
I0428 19:45:57.938479 25918 net.cpp:408] conv1 <- pool0
I0428 19:45:57.938485 25918 net.cpp:382] conv1 -> conv1
I0428 19:45:57.940114 25918 net.cpp:124] Setting up conv1
I0428 19:45:57.940129 25918 net.cpp:131] Top shape: 100 100 8 8 (640000)
I0428 19:45:57.940132 25918 net.cpp:139] Memory required for data: 3450800
I0428 19:45:57.940141 25918 layer_factory.hpp:77] Creating layer pool1
I0428 19:45:57.940150 25918 net.cpp:86] Creating Layer pool1
I0428 19:45:57.940160 25918 net.cpp:408] pool1 <- conv1
I0428 19:45:57.940167 25918 net.cpp:382] pool1 -> pool1
I0428 19:45:57.940209 25918 net.cpp:124] Setting up pool1
I0428 19:45:57.940217 25918 net.cpp:131] Top shape: 100 100 4 4 (160000)
I0428 19:45:57.940219 25918 net.cpp:139] Memory required for data: 4090800
I0428 19:45:57.940223 25918 layer_factory.hpp:77] Creating layer ip1
I0428 19:45:57.940230 25918 net.cpp:86] Creating Layer ip1
I0428 19:45:57.940233 25918 net.cpp:408] ip1 <- pool1
I0428 19:45:57.940241 25918 net.cpp:382] ip1 -> ip1
I0428 19:45:57.940445 25918 net.cpp:124] Setting up ip1
I0428 19:45:57.940454 25918 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:45:57.940469 25918 net.cpp:139] Memory required for data: 4094800
I0428 19:45:57.940477 25918 layer_factory.hpp:77] Creating layer relu1
I0428 19:45:57.940490 25918 net.cpp:86] Creating Layer relu1
I0428 19:45:57.940495 25918 net.cpp:408] relu1 <- ip1
I0428 19:45:57.940500 25918 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:45:57.940739 25918 net.cpp:124] Setting up relu1
I0428 19:45:57.940749 25918 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:45:57.940753 25918 net.cpp:139] Memory required for data: 4098800
I0428 19:45:57.940757 25918 layer_factory.hpp:77] Creating layer ip2
I0428 19:45:57.940764 25918 net.cpp:86] Creating Layer ip2
I0428 19:45:57.940768 25918 net.cpp:408] ip2 <- ip1
I0428 19:45:57.940774 25918 net.cpp:382] ip2 -> ip2
I0428 19:45:57.940918 25918 net.cpp:124] Setting up ip2
I0428 19:45:57.940927 25918 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:45:57.940930 25918 net.cpp:139] Memory required for data: 4102800
I0428 19:45:57.940935 25918 layer_factory.hpp:77] Creating layer relu2
I0428 19:45:57.940942 25918 net.cpp:86] Creating Layer relu2
I0428 19:45:57.940944 25918 net.cpp:408] relu2 <- ip2
I0428 19:45:57.940949 25918 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:45:57.941124 25918 net.cpp:124] Setting up relu2
I0428 19:45:57.941135 25918 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:45:57.941138 25918 net.cpp:139] Memory required for data: 4106800
I0428 19:45:57.941143 25918 layer_factory.hpp:77] Creating layer ip3
I0428 19:45:57.941149 25918 net.cpp:86] Creating Layer ip3
I0428 19:45:57.941159 25918 net.cpp:408] ip3 <- ip2
I0428 19:45:57.941170 25918 net.cpp:382] ip3 -> ip3
I0428 19:45:57.941284 25918 net.cpp:124] Setting up ip3
I0428 19:45:57.941293 25918 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:45:57.941295 25918 net.cpp:139] Memory required for data: 4110800
I0428 19:45:57.941304 25918 layer_factory.hpp:77] Creating layer relu3
I0428 19:45:57.941309 25918 net.cpp:86] Creating Layer relu3
I0428 19:45:57.941313 25918 net.cpp:408] relu3 <- ip3
I0428 19:45:57.941318 25918 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:45:57.942191 25918 net.cpp:124] Setting up relu3
I0428 19:45:57.942203 25918 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:45:57.942207 25918 net.cpp:139] Memory required for data: 4114800
I0428 19:45:57.942211 25918 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:45:57.942217 25918 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:45:57.942221 25918 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:45:57.942243 25918 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:45:57.942250 25918 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:45:57.942303 25918 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:45:57.942309 25918 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:45:57.942313 25918 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:45:57.942317 25918 net.cpp:139] Memory required for data: 4122800
I0428 19:45:57.942320 25918 layer_factory.hpp:77] Creating layer accuracy
I0428 19:45:57.942325 25918 net.cpp:86] Creating Layer accuracy
I0428 19:45:57.942328 25918 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:45:57.942334 25918 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:45:57.942340 25918 net.cpp:382] accuracy -> accuracy
I0428 19:45:57.942348 25918 net.cpp:124] Setting up accuracy
I0428 19:45:57.942353 25918 net.cpp:131] Top shape: (1)
I0428 19:45:57.942355 25918 net.cpp:139] Memory required for data: 4122804
I0428 19:45:57.942364 25918 layer_factory.hpp:77] Creating layer loss
I0428 19:45:57.942373 25918 net.cpp:86] Creating Layer loss
I0428 19:45:57.942378 25918 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:45:57.942381 25918 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:45:57.942387 25918 net.cpp:382] loss -> loss
I0428 19:45:57.942394 25918 layer_factory.hpp:77] Creating layer loss
I0428 19:45:57.942659 25918 net.cpp:124] Setting up loss
I0428 19:45:57.942670 25918 net.cpp:131] Top shape: (1)
I0428 19:45:57.942674 25918 net.cpp:134]     with loss weight 1
I0428 19:45:57.942680 25918 net.cpp:139] Memory required for data: 4122808
I0428 19:45:57.942695 25918 net.cpp:200] loss needs backward computation.
I0428 19:45:57.942699 25918 net.cpp:202] accuracy does not need backward computation.
I0428 19:45:57.942703 25918 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:45:57.942713 25918 net.cpp:200] relu3 needs backward computation.
I0428 19:45:57.942716 25918 net.cpp:200] ip3 needs backward computation.
I0428 19:45:57.942720 25918 net.cpp:200] relu2 needs backward computation.
I0428 19:45:57.942723 25918 net.cpp:200] ip2 needs backward computation.
I0428 19:45:57.942726 25918 net.cpp:200] relu1 needs backward computation.
I0428 19:45:57.942729 25918 net.cpp:200] ip1 needs backward computation.
I0428 19:45:57.942733 25918 net.cpp:200] pool1 needs backward computation.
I0428 19:45:57.942735 25918 net.cpp:200] conv1 needs backward computation.
I0428 19:45:57.942744 25918 net.cpp:200] pool0 needs backward computation.
I0428 19:45:57.942747 25918 net.cpp:200] conv0 needs backward computation.
I0428 19:45:57.942751 25918 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:45:57.942755 25918 net.cpp:202] mnist does not need backward computation.
I0428 19:45:57.942759 25918 net.cpp:244] This network produces output accuracy
I0428 19:45:57.942762 25918 net.cpp:244] This network produces output loss
I0428 19:45:57.942775 25918 net.cpp:257] Network initialization done.
I0428 19:45:57.942821 25918 solver.cpp:56] Solver scaffolding done.
I0428 19:45:57.943199 25918 caffe.cpp:248] Starting Optimization
I0428 19:45:57.943205 25918 solver.cpp:273] Solving LeNet
I0428 19:45:57.943208 25918 solver.cpp:274] Learning Rate Policy: inv
I0428 19:45:57.943480 25918 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:45:57.948662 25918 blocking_queue.cpp:49] Waiting for data
I0428 19:45:58.017596 25925 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:45:58.018206 25918 solver.cpp:398]     Test net output #0: accuracy = 0.1102
I0428 19:45:58.018242 25918 solver.cpp:398]     Test net output #1: loss = 2.29984 (* 1 = 2.29984 loss)
I0428 19:45:58.022056 25918 solver.cpp:219] Iteration 0 (0 iter/s, 0.0788111s/100 iters), loss = 2.27964
I0428 19:45:58.022095 25918 solver.cpp:238]     Train net output #0: loss = 2.27964 (* 1 = 2.27964 loss)
I0428 19:45:58.022106 25918 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:45:58.178032 25918 solver.cpp:219] Iteration 100 (641.29 iter/s, 0.155936s/100 iters), loss = 0.584413
I0428 19:45:58.178073 25918 solver.cpp:238]     Train net output #0: loss = 0.584413 (* 1 = 0.584413 loss)
I0428 19:45:58.178081 25918 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:45:58.321280 25918 solver.cpp:219] Iteration 200 (698.283 iter/s, 0.143208s/100 iters), loss = 0.536089
I0428 19:45:58.321306 25918 solver.cpp:238]     Train net output #0: loss = 0.536089 (* 1 = 0.536089 loss)
I0428 19:45:58.321312 25918 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:45:58.463557 25918 solver.cpp:219] Iteration 300 (703.039 iter/s, 0.14224s/100 iters), loss = 0.473139
I0428 19:45:58.463583 25918 solver.cpp:238]     Train net output #0: loss = 0.473139 (* 1 = 0.473139 loss)
I0428 19:45:58.463590 25918 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:45:58.610286 25918 solver.cpp:219] Iteration 400 (681.719 iter/s, 0.146688s/100 iters), loss = 0.415471
I0428 19:45:58.610323 25918 solver.cpp:238]     Train net output #0: loss = 0.415471 (* 1 = 0.415471 loss)
I0428 19:45:58.610332 25918 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:45:58.774591 25918 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:45:58.836108 25925 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:45:58.837944 25918 solver.cpp:398]     Test net output #0: accuracy = 0.8582
I0428 19:45:58.837972 25918 solver.cpp:398]     Test net output #1: loss = 0.400633 (* 1 = 0.400633 loss)
I0428 19:45:58.839423 25918 solver.cpp:219] Iteration 500 (436.521 iter/s, 0.229084s/100 iters), loss = 0.372568
I0428 19:45:58.839454 25918 solver.cpp:238]     Train net output #0: loss = 0.372568 (* 1 = 0.372568 loss)
I0428 19:45:58.839479 25918 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:45:58.996242 25918 solver.cpp:219] Iteration 600 (637.864 iter/s, 0.156773s/100 iters), loss = 0.4343
I0428 19:45:58.996282 25918 solver.cpp:238]     Train net output #0: loss = 0.4343 (* 1 = 0.4343 loss)
I0428 19:45:58.996292 25918 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:45:59.166746 25918 solver.cpp:219] Iteration 700 (586.686 iter/s, 0.170449s/100 iters), loss = 0.506618
I0428 19:45:59.166793 25918 solver.cpp:238]     Train net output #0: loss = 0.506618 (* 1 = 0.506618 loss)
I0428 19:45:59.166805 25918 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:45:59.339831 25918 solver.cpp:219] Iteration 800 (577.954 iter/s, 0.173024s/100 iters), loss = 0.43307
I0428 19:45:59.339876 25918 solver.cpp:238]     Train net output #0: loss = 0.43307 (* 1 = 0.43307 loss)
I0428 19:45:59.339887 25918 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:45:59.510076 25918 solver.cpp:219] Iteration 900 (587.59 iter/s, 0.170187s/100 iters), loss = 0.318891
I0428 19:45:59.510118 25918 solver.cpp:238]     Train net output #0: loss = 0.318891 (* 1 = 0.318891 loss)
I0428 19:45:59.510129 25918 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:45:59.565668 25924 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:45:59.675678 25918 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:45:59.677651 25918 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:45:59.678565 25918 solver.cpp:311] Iteration 1000, loss = 0.324033
I0428 19:45:59.678601 25918 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:45:59.754802 25925 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:45:59.755419 25918 solver.cpp:398]     Test net output #0: accuracy = 0.8708
I0428 19:45:59.755442 25918 solver.cpp:398]     Test net output #1: loss = 0.337346 (* 1 = 0.337346 loss)
I0428 19:45:59.755450 25918 solver.cpp:316] Optimization Done.
I0428 19:45:59.755453 25918 caffe.cpp:259] Optimization Done.
