I0428 19:56:32.416483 28459 caffe.cpp:218] Using GPUs 0
I0428 19:56:32.448729 28459 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:56:32.972920 28459 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test762.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:56:32.973065 28459 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test762.prototxt
I0428 19:56:32.973480 28459 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:56:32.973500 28459 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:56:32.973605 28459 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:56:32.973690 28459 layer_factory.hpp:77] Creating layer mnist
I0428 19:56:32.973788 28459 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:56:32.973810 28459 net.cpp:86] Creating Layer mnist
I0428 19:56:32.973822 28459 net.cpp:382] mnist -> data
I0428 19:56:32.973845 28459 net.cpp:382] mnist -> label
I0428 19:56:32.974925 28459 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:56:32.977373 28459 net.cpp:124] Setting up mnist
I0428 19:56:32.977392 28459 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:56:32.977398 28459 net.cpp:131] Top shape: 64 (64)
I0428 19:56:32.977402 28459 net.cpp:139] Memory required for data: 200960
I0428 19:56:32.977409 28459 layer_factory.hpp:77] Creating layer conv0
I0428 19:56:32.977458 28459 net.cpp:86] Creating Layer conv0
I0428 19:56:32.977479 28459 net.cpp:408] conv0 <- data
I0428 19:56:32.977494 28459 net.cpp:382] conv0 -> conv0
I0428 19:56:33.238126 28459 net.cpp:124] Setting up conv0
I0428 19:56:33.238152 28459 net.cpp:131] Top shape: 64 10 24 24 (368640)
I0428 19:56:33.238157 28459 net.cpp:139] Memory required for data: 1675520
I0428 19:56:33.238170 28459 layer_factory.hpp:77] Creating layer pool0
I0428 19:56:33.238183 28459 net.cpp:86] Creating Layer pool0
I0428 19:56:33.238186 28459 net.cpp:408] pool0 <- conv0
I0428 19:56:33.238191 28459 net.cpp:382] pool0 -> pool0
I0428 19:56:33.238250 28459 net.cpp:124] Setting up pool0
I0428 19:56:33.238255 28459 net.cpp:131] Top shape: 64 10 12 12 (92160)
I0428 19:56:33.238258 28459 net.cpp:139] Memory required for data: 2044160
I0428 19:56:33.238261 28459 layer_factory.hpp:77] Creating layer conv1
I0428 19:56:33.238271 28459 net.cpp:86] Creating Layer conv1
I0428 19:56:33.238275 28459 net.cpp:408] conv1 <- pool0
I0428 19:56:33.238279 28459 net.cpp:382] conv1 -> conv1
I0428 19:56:33.240051 28459 net.cpp:124] Setting up conv1
I0428 19:56:33.240064 28459 net.cpp:131] Top shape: 64 2 8 8 (8192)
I0428 19:56:33.240084 28459 net.cpp:139] Memory required for data: 2076928
I0428 19:56:33.240093 28459 layer_factory.hpp:77] Creating layer pool1
I0428 19:56:33.240116 28459 net.cpp:86] Creating Layer pool1
I0428 19:56:33.240120 28459 net.cpp:408] pool1 <- conv1
I0428 19:56:33.240125 28459 net.cpp:382] pool1 -> pool1
I0428 19:56:33.240192 28459 net.cpp:124] Setting up pool1
I0428 19:56:33.240198 28459 net.cpp:131] Top shape: 64 2 4 4 (2048)
I0428 19:56:33.240200 28459 net.cpp:139] Memory required for data: 2085120
I0428 19:56:33.240203 28459 layer_factory.hpp:77] Creating layer ip1
I0428 19:56:33.240211 28459 net.cpp:86] Creating Layer ip1
I0428 19:56:33.240214 28459 net.cpp:408] ip1 <- pool1
I0428 19:56:33.240218 28459 net.cpp:382] ip1 -> ip1
I0428 19:56:33.240315 28459 net.cpp:124] Setting up ip1
I0428 19:56:33.240324 28459 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:56:33.240326 28459 net.cpp:139] Memory required for data: 2091520
I0428 19:56:33.240334 28459 layer_factory.hpp:77] Creating layer relu1
I0428 19:56:33.240339 28459 net.cpp:86] Creating Layer relu1
I0428 19:56:33.240344 28459 net.cpp:408] relu1 <- ip1
I0428 19:56:33.240347 28459 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:56:33.240520 28459 net.cpp:124] Setting up relu1
I0428 19:56:33.240527 28459 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:56:33.240530 28459 net.cpp:139] Memory required for data: 2097920
I0428 19:56:33.240533 28459 layer_factory.hpp:77] Creating layer ip2
I0428 19:56:33.240540 28459 net.cpp:86] Creating Layer ip2
I0428 19:56:33.240543 28459 net.cpp:408] ip2 <- ip1
I0428 19:56:33.240547 28459 net.cpp:382] ip2 -> ip2
I0428 19:56:33.240634 28459 net.cpp:124] Setting up ip2
I0428 19:56:33.240641 28459 net.cpp:131] Top shape: 64 10 (640)
I0428 19:56:33.240644 28459 net.cpp:139] Memory required for data: 2100480
I0428 19:56:33.240649 28459 layer_factory.hpp:77] Creating layer relu2
I0428 19:56:33.240655 28459 net.cpp:86] Creating Layer relu2
I0428 19:56:33.240658 28459 net.cpp:408] relu2 <- ip2
I0428 19:56:33.240663 28459 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:56:33.241509 28459 net.cpp:124] Setting up relu2
I0428 19:56:33.241521 28459 net.cpp:131] Top shape: 64 10 (640)
I0428 19:56:33.241540 28459 net.cpp:139] Memory required for data: 2103040
I0428 19:56:33.241544 28459 layer_factory.hpp:77] Creating layer ip3
I0428 19:56:33.241551 28459 net.cpp:86] Creating Layer ip3
I0428 19:56:33.241556 28459 net.cpp:408] ip3 <- ip2
I0428 19:56:33.241561 28459 net.cpp:382] ip3 -> ip3
I0428 19:56:33.241669 28459 net.cpp:124] Setting up ip3
I0428 19:56:33.241677 28459 net.cpp:131] Top shape: 64 10 (640)
I0428 19:56:33.241679 28459 net.cpp:139] Memory required for data: 2105600
I0428 19:56:33.241688 28459 layer_factory.hpp:77] Creating layer relu3
I0428 19:56:33.241693 28459 net.cpp:86] Creating Layer relu3
I0428 19:56:33.241695 28459 net.cpp:408] relu3 <- ip3
I0428 19:56:33.241698 28459 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:56:33.241863 28459 net.cpp:124] Setting up relu3
I0428 19:56:33.241870 28459 net.cpp:131] Top shape: 64 10 (640)
I0428 19:56:33.241873 28459 net.cpp:139] Memory required for data: 2108160
I0428 19:56:33.241878 28459 layer_factory.hpp:77] Creating layer loss
I0428 19:56:33.241883 28459 net.cpp:86] Creating Layer loss
I0428 19:56:33.241886 28459 net.cpp:408] loss <- ip3
I0428 19:56:33.241890 28459 net.cpp:408] loss <- label
I0428 19:56:33.241895 28459 net.cpp:382] loss -> loss
I0428 19:56:33.241914 28459 layer_factory.hpp:77] Creating layer loss
I0428 19:56:33.242141 28459 net.cpp:124] Setting up loss
I0428 19:56:33.242149 28459 net.cpp:131] Top shape: (1)
I0428 19:56:33.242153 28459 net.cpp:134]     with loss weight 1
I0428 19:56:33.242167 28459 net.cpp:139] Memory required for data: 2108164
I0428 19:56:33.242171 28459 net.cpp:200] loss needs backward computation.
I0428 19:56:33.242174 28459 net.cpp:200] relu3 needs backward computation.
I0428 19:56:33.242177 28459 net.cpp:200] ip3 needs backward computation.
I0428 19:56:33.242180 28459 net.cpp:200] relu2 needs backward computation.
I0428 19:56:33.242183 28459 net.cpp:200] ip2 needs backward computation.
I0428 19:56:33.242187 28459 net.cpp:200] relu1 needs backward computation.
I0428 19:56:33.242189 28459 net.cpp:200] ip1 needs backward computation.
I0428 19:56:33.242192 28459 net.cpp:200] pool1 needs backward computation.
I0428 19:56:33.242195 28459 net.cpp:200] conv1 needs backward computation.
I0428 19:56:33.242198 28459 net.cpp:200] pool0 needs backward computation.
I0428 19:56:33.242202 28459 net.cpp:200] conv0 needs backward computation.
I0428 19:56:33.242205 28459 net.cpp:202] mnist does not need backward computation.
I0428 19:56:33.242208 28459 net.cpp:244] This network produces output loss
I0428 19:56:33.242218 28459 net.cpp:257] Network initialization done.
I0428 19:56:33.242601 28459 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test762.prototxt
I0428 19:56:33.242630 28459 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:56:33.242722 28459 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:56:33.242800 28459 layer_factory.hpp:77] Creating layer mnist
I0428 19:56:33.242887 28459 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:56:33.242899 28459 net.cpp:86] Creating Layer mnist
I0428 19:56:33.242904 28459 net.cpp:382] mnist -> data
I0428 19:56:33.242911 28459 net.cpp:382] mnist -> label
I0428 19:56:33.242986 28459 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:56:33.245154 28459 net.cpp:124] Setting up mnist
I0428 19:56:33.245178 28459 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:56:33.245184 28459 net.cpp:131] Top shape: 100 (100)
I0428 19:56:33.245187 28459 net.cpp:139] Memory required for data: 314000
I0428 19:56:33.245192 28459 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:56:33.245214 28459 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:56:33.245218 28459 net.cpp:408] label_mnist_1_split <- label
I0428 19:56:33.245223 28459 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:56:33.245229 28459 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:56:33.245285 28459 net.cpp:124] Setting up label_mnist_1_split
I0428 19:56:33.245291 28459 net.cpp:131] Top shape: 100 (100)
I0428 19:56:33.245296 28459 net.cpp:131] Top shape: 100 (100)
I0428 19:56:33.245298 28459 net.cpp:139] Memory required for data: 314800
I0428 19:56:33.245301 28459 layer_factory.hpp:77] Creating layer conv0
I0428 19:56:33.245309 28459 net.cpp:86] Creating Layer conv0
I0428 19:56:33.245312 28459 net.cpp:408] conv0 <- data
I0428 19:56:33.245317 28459 net.cpp:382] conv0 -> conv0
I0428 19:56:33.246942 28459 net.cpp:124] Setting up conv0
I0428 19:56:33.246956 28459 net.cpp:131] Top shape: 100 10 24 24 (576000)
I0428 19:56:33.246960 28459 net.cpp:139] Memory required for data: 2618800
I0428 19:56:33.246970 28459 layer_factory.hpp:77] Creating layer pool0
I0428 19:56:33.246987 28459 net.cpp:86] Creating Layer pool0
I0428 19:56:33.246991 28459 net.cpp:408] pool0 <- conv0
I0428 19:56:33.246997 28459 net.cpp:382] pool0 -> pool0
I0428 19:56:33.247035 28459 net.cpp:124] Setting up pool0
I0428 19:56:33.247054 28459 net.cpp:131] Top shape: 100 10 12 12 (144000)
I0428 19:56:33.247058 28459 net.cpp:139] Memory required for data: 3194800
I0428 19:56:33.247076 28459 layer_factory.hpp:77] Creating layer conv1
I0428 19:56:33.247084 28459 net.cpp:86] Creating Layer conv1
I0428 19:56:33.247087 28459 net.cpp:408] conv1 <- pool0
I0428 19:56:33.247092 28459 net.cpp:382] conv1 -> conv1
I0428 19:56:33.249315 28459 net.cpp:124] Setting up conv1
I0428 19:56:33.249328 28459 net.cpp:131] Top shape: 100 2 8 8 (12800)
I0428 19:56:33.249332 28459 net.cpp:139] Memory required for data: 3246000
I0428 19:56:33.249341 28459 layer_factory.hpp:77] Creating layer pool1
I0428 19:56:33.249348 28459 net.cpp:86] Creating Layer pool1
I0428 19:56:33.249351 28459 net.cpp:408] pool1 <- conv1
I0428 19:56:33.249356 28459 net.cpp:382] pool1 -> pool1
I0428 19:56:33.249394 28459 net.cpp:124] Setting up pool1
I0428 19:56:33.249400 28459 net.cpp:131] Top shape: 100 2 4 4 (3200)
I0428 19:56:33.249403 28459 net.cpp:139] Memory required for data: 3258800
I0428 19:56:33.249406 28459 layer_factory.hpp:77] Creating layer ip1
I0428 19:56:33.249411 28459 net.cpp:86] Creating Layer ip1
I0428 19:56:33.249415 28459 net.cpp:408] ip1 <- pool1
I0428 19:56:33.249423 28459 net.cpp:382] ip1 -> ip1
I0428 19:56:33.249547 28459 net.cpp:124] Setting up ip1
I0428 19:56:33.249553 28459 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:56:33.249581 28459 net.cpp:139] Memory required for data: 3268800
I0428 19:56:33.249588 28459 layer_factory.hpp:77] Creating layer relu1
I0428 19:56:33.249593 28459 net.cpp:86] Creating Layer relu1
I0428 19:56:33.249598 28459 net.cpp:408] relu1 <- ip1
I0428 19:56:33.249603 28459 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:56:33.249858 28459 net.cpp:124] Setting up relu1
I0428 19:56:33.249867 28459 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:56:33.249871 28459 net.cpp:139] Memory required for data: 3278800
I0428 19:56:33.249873 28459 layer_factory.hpp:77] Creating layer ip2
I0428 19:56:33.249882 28459 net.cpp:86] Creating Layer ip2
I0428 19:56:33.249886 28459 net.cpp:408] ip2 <- ip1
I0428 19:56:33.249891 28459 net.cpp:382] ip2 -> ip2
I0428 19:56:33.250010 28459 net.cpp:124] Setting up ip2
I0428 19:56:33.250017 28459 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:56:33.250020 28459 net.cpp:139] Memory required for data: 3282800
I0428 19:56:33.250025 28459 layer_factory.hpp:77] Creating layer relu2
I0428 19:56:33.250036 28459 net.cpp:86] Creating Layer relu2
I0428 19:56:33.250038 28459 net.cpp:408] relu2 <- ip2
I0428 19:56:33.250049 28459 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:56:33.250198 28459 net.cpp:124] Setting up relu2
I0428 19:56:33.250206 28459 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:56:33.250210 28459 net.cpp:139] Memory required for data: 3286800
I0428 19:56:33.250212 28459 layer_factory.hpp:77] Creating layer ip3
I0428 19:56:33.250218 28459 net.cpp:86] Creating Layer ip3
I0428 19:56:33.250221 28459 net.cpp:408] ip3 <- ip2
I0428 19:56:33.250232 28459 net.cpp:382] ip3 -> ip3
I0428 19:56:33.250325 28459 net.cpp:124] Setting up ip3
I0428 19:56:33.250331 28459 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:56:33.250335 28459 net.cpp:139] Memory required for data: 3290800
I0428 19:56:33.250344 28459 layer_factory.hpp:77] Creating layer relu3
I0428 19:56:33.250349 28459 net.cpp:86] Creating Layer relu3
I0428 19:56:33.250351 28459 net.cpp:408] relu3 <- ip3
I0428 19:56:33.250355 28459 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:56:33.251143 28459 net.cpp:124] Setting up relu3
I0428 19:56:33.251155 28459 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:56:33.251158 28459 net.cpp:139] Memory required for data: 3294800
I0428 19:56:33.251161 28459 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:56:33.251168 28459 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:56:33.251171 28459 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:56:33.251176 28459 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:56:33.251183 28459 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:56:33.251224 28459 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:56:33.251229 28459 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:56:33.251240 28459 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:56:33.251242 28459 net.cpp:139] Memory required for data: 3302800
I0428 19:56:33.251245 28459 layer_factory.hpp:77] Creating layer accuracy
I0428 19:56:33.251251 28459 net.cpp:86] Creating Layer accuracy
I0428 19:56:33.251255 28459 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:56:33.251258 28459 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:56:33.251262 28459 net.cpp:382] accuracy -> accuracy
I0428 19:56:33.251269 28459 net.cpp:124] Setting up accuracy
I0428 19:56:33.251273 28459 net.cpp:131] Top shape: (1)
I0428 19:56:33.251276 28459 net.cpp:139] Memory required for data: 3302804
I0428 19:56:33.251279 28459 layer_factory.hpp:77] Creating layer loss
I0428 19:56:33.251283 28459 net.cpp:86] Creating Layer loss
I0428 19:56:33.251286 28459 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:56:33.251289 28459 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:56:33.251294 28459 net.cpp:382] loss -> loss
I0428 19:56:33.251301 28459 layer_factory.hpp:77] Creating layer loss
I0428 19:56:33.251533 28459 net.cpp:124] Setting up loss
I0428 19:56:33.251543 28459 net.cpp:131] Top shape: (1)
I0428 19:56:33.251557 28459 net.cpp:134]     with loss weight 1
I0428 19:56:33.251562 28459 net.cpp:139] Memory required for data: 3302808
I0428 19:56:33.251580 28459 net.cpp:200] loss needs backward computation.
I0428 19:56:33.251585 28459 net.cpp:202] accuracy does not need backward computation.
I0428 19:56:33.251587 28459 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:56:33.251590 28459 net.cpp:200] relu3 needs backward computation.
I0428 19:56:33.251593 28459 net.cpp:200] ip3 needs backward computation.
I0428 19:56:33.251602 28459 net.cpp:200] relu2 needs backward computation.
I0428 19:56:33.251605 28459 net.cpp:200] ip2 needs backward computation.
I0428 19:56:33.251607 28459 net.cpp:200] relu1 needs backward computation.
I0428 19:56:33.251615 28459 net.cpp:200] ip1 needs backward computation.
I0428 19:56:33.251622 28459 net.cpp:200] pool1 needs backward computation.
I0428 19:56:33.251624 28459 net.cpp:200] conv1 needs backward computation.
I0428 19:56:33.251627 28459 net.cpp:200] pool0 needs backward computation.
I0428 19:56:33.251631 28459 net.cpp:200] conv0 needs backward computation.
I0428 19:56:33.251634 28459 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:56:33.251637 28459 net.cpp:202] mnist does not need backward computation.
I0428 19:56:33.251641 28459 net.cpp:244] This network produces output accuracy
I0428 19:56:33.251643 28459 net.cpp:244] This network produces output loss
I0428 19:56:33.251654 28459 net.cpp:257] Network initialization done.
I0428 19:56:33.251694 28459 solver.cpp:56] Solver scaffolding done.
I0428 19:56:33.252084 28459 caffe.cpp:248] Starting Optimization
I0428 19:56:33.252090 28459 solver.cpp:273] Solving LeNet
I0428 19:56:33.252094 28459 solver.cpp:274] Learning Rate Policy: inv
I0428 19:56:33.252918 28459 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:56:33.256434 28459 blocking_queue.cpp:49] Waiting for data
I0428 19:56:33.328697 28466 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:56:33.329243 28459 solver.cpp:398]     Test net output #0: accuracy = 0.0983
I0428 19:56:33.329263 28459 solver.cpp:398]     Test net output #1: loss = 2.30688 (* 1 = 2.30688 loss)
I0428 19:56:33.331601 28459 solver.cpp:219] Iteration 0 (0 iter/s, 0.0794674s/100 iters), loss = 2.31782
I0428 19:56:33.331625 28459 solver.cpp:238]     Train net output #0: loss = 2.31782 (* 1 = 2.31782 loss)
I0428 19:56:33.331636 28459 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:56:33.420491 28459 solver.cpp:219] Iteration 100 (1125.59 iter/s, 0.0888422s/100 iters), loss = 1.13276
I0428 19:56:33.420545 28459 solver.cpp:238]     Train net output #0: loss = 1.13276 (* 1 = 1.13276 loss)
I0428 19:56:33.420557 28459 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:56:33.503145 28459 solver.cpp:219] Iteration 200 (1210.74 iter/s, 0.0825938s/100 iters), loss = 0.869575
I0428 19:56:33.503176 28459 solver.cpp:238]     Train net output #0: loss = 0.869575 (* 1 = 0.869575 loss)
I0428 19:56:33.503185 28459 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:56:33.581120 28459 solver.cpp:219] Iteration 300 (1283.13 iter/s, 0.0779344s/100 iters), loss = 0.522257
I0428 19:56:33.581151 28459 solver.cpp:238]     Train net output #0: loss = 0.522257 (* 1 = 0.522257 loss)
I0428 19:56:33.581159 28459 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:56:33.658082 28459 solver.cpp:219] Iteration 400 (1300 iter/s, 0.0769229s/100 iters), loss = 0.700226
I0428 19:56:33.658113 28459 solver.cpp:238]     Train net output #0: loss = 0.700226 (* 1 = 0.700226 loss)
I0428 19:56:33.658120 28459 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:56:33.750525 28459 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:56:33.808781 28466 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:56:33.809370 28459 solver.cpp:398]     Test net output #0: accuracy = 0.7567
I0428 19:56:33.809398 28459 solver.cpp:398]     Test net output #1: loss = 0.681841 (* 1 = 0.681841 loss)
I0428 19:56:33.810241 28459 solver.cpp:219] Iteration 500 (657.396 iter/s, 0.152115s/100 iters), loss = 0.739377
I0428 19:56:33.810286 28459 solver.cpp:238]     Train net output #0: loss = 0.739377 (* 1 = 0.739377 loss)
I0428 19:56:33.810317 28459 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:56:33.890120 28459 solver.cpp:219] Iteration 600 (1252.7 iter/s, 0.0798277s/100 iters), loss = 0.752995
I0428 19:56:33.890149 28459 solver.cpp:238]     Train net output #0: loss = 0.752995 (* 1 = 0.752995 loss)
I0428 19:56:33.890157 28459 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:56:33.967372 28459 solver.cpp:219] Iteration 700 (1295.12 iter/s, 0.0772127s/100 iters), loss = 0.964334
I0428 19:56:33.967404 28459 solver.cpp:238]     Train net output #0: loss = 0.964334 (* 1 = 0.964334 loss)
I0428 19:56:33.967412 28459 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:56:34.043709 28459 solver.cpp:219] Iteration 800 (1310.7 iter/s, 0.0762952s/100 iters), loss = 0.839611
I0428 19:56:34.043738 28459 solver.cpp:238]     Train net output #0: loss = 0.839611 (* 1 = 0.839611 loss)
I0428 19:56:34.043746 28459 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:56:34.120010 28459 solver.cpp:219] Iteration 900 (1311.26 iter/s, 0.0762624s/100 iters), loss = 0.502998
I0428 19:56:34.120040 28459 solver.cpp:238]     Train net output #0: loss = 0.502998 (* 1 = 0.502998 loss)
I0428 19:56:34.120048 28459 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:56:34.145542 28465 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:56:34.195571 28459 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:56:34.196324 28459 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:56:34.196764 28459 solver.cpp:311] Iteration 1000, loss = 0.60695
I0428 19:56:34.196784 28459 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:56:34.272042 28466 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:56:34.272599 28459 solver.cpp:398]     Test net output #0: accuracy = 0.7748
I0428 19:56:34.272626 28459 solver.cpp:398]     Test net output #1: loss = 0.597551 (* 1 = 0.597551 loss)
I0428 19:56:34.272632 28459 solver.cpp:316] Optimization Done.
I0428 19:56:34.272636 28459 caffe.cpp:259] Optimization Done.
