I0428 20:10:08.440409 31776 caffe.cpp:218] Using GPUs 0
I0428 20:10:08.473045 31776 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:10:08.946772 31776 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1127.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:10:08.946907 31776 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1127.prototxt
I0428 20:10:08.947239 31776 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:10:08.947257 31776 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:10:08.947334 31776 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:10:08.947397 31776 layer_factory.hpp:77] Creating layer mnist
I0428 20:10:08.947477 31776 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:10:08.947512 31776 net.cpp:86] Creating Layer mnist
I0428 20:10:08.947520 31776 net.cpp:382] mnist -> data
I0428 20:10:08.947538 31776 net.cpp:382] mnist -> label
I0428 20:10:08.948499 31776 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:10:08.950835 31776 net.cpp:124] Setting up mnist
I0428 20:10:08.950865 31776 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:10:08.950870 31776 net.cpp:131] Top shape: 64 (64)
I0428 20:10:08.950873 31776 net.cpp:139] Memory required for data: 200960
I0428 20:10:08.950880 31776 layer_factory.hpp:77] Creating layer conv0
I0428 20:10:08.950893 31776 net.cpp:86] Creating Layer conv0
I0428 20:10:08.950911 31776 net.cpp:408] conv0 <- data
I0428 20:10:08.950922 31776 net.cpp:382] conv0 -> conv0
I0428 20:10:09.188161 31776 net.cpp:124] Setting up conv0
I0428 20:10:09.188186 31776 net.cpp:131] Top shape: 64 25 24 24 (921600)
I0428 20:10:09.188190 31776 net.cpp:139] Memory required for data: 3887360
I0428 20:10:09.188205 31776 layer_factory.hpp:77] Creating layer pool0
I0428 20:10:09.188215 31776 net.cpp:86] Creating Layer pool0
I0428 20:10:09.188220 31776 net.cpp:408] pool0 <- conv0
I0428 20:10:09.188225 31776 net.cpp:382] pool0 -> pool0
I0428 20:10:09.188280 31776 net.cpp:124] Setting up pool0
I0428 20:10:09.188287 31776 net.cpp:131] Top shape: 64 25 12 12 (230400)
I0428 20:10:09.188288 31776 net.cpp:139] Memory required for data: 4808960
I0428 20:10:09.188292 31776 layer_factory.hpp:77] Creating layer conv1
I0428 20:10:09.188302 31776 net.cpp:86] Creating Layer conv1
I0428 20:10:09.188304 31776 net.cpp:408] conv1 <- pool0
I0428 20:10:09.188309 31776 net.cpp:382] conv1 -> conv1
I0428 20:10:09.191136 31776 net.cpp:124] Setting up conv1
I0428 20:10:09.191166 31776 net.cpp:131] Top shape: 64 25 8 8 (102400)
I0428 20:10:09.191169 31776 net.cpp:139] Memory required for data: 5218560
I0428 20:10:09.191179 31776 layer_factory.hpp:77] Creating layer pool1
I0428 20:10:09.191187 31776 net.cpp:86] Creating Layer pool1
I0428 20:10:09.191190 31776 net.cpp:408] pool1 <- conv1
I0428 20:10:09.191196 31776 net.cpp:382] pool1 -> pool1
I0428 20:10:09.191231 31776 net.cpp:124] Setting up pool1
I0428 20:10:09.191236 31776 net.cpp:131] Top shape: 64 25 4 4 (25600)
I0428 20:10:09.191239 31776 net.cpp:139] Memory required for data: 5320960
I0428 20:10:09.191242 31776 layer_factory.hpp:77] Creating layer ip1
I0428 20:10:09.191251 31776 net.cpp:86] Creating Layer ip1
I0428 20:10:09.191253 31776 net.cpp:408] ip1 <- pool1
I0428 20:10:09.191258 31776 net.cpp:382] ip1 -> ip1
I0428 20:10:09.191459 31776 net.cpp:124] Setting up ip1
I0428 20:10:09.191468 31776 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:10:09.191470 31776 net.cpp:139] Memory required for data: 5333760
I0428 20:10:09.191476 31776 layer_factory.hpp:77] Creating layer relu1
I0428 20:10:09.191483 31776 net.cpp:86] Creating Layer relu1
I0428 20:10:09.191485 31776 net.cpp:408] relu1 <- ip1
I0428 20:10:09.191489 31776 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:10:09.191673 31776 net.cpp:124] Setting up relu1
I0428 20:10:09.191680 31776 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:10:09.191684 31776 net.cpp:139] Memory required for data: 5346560
I0428 20:10:09.191686 31776 layer_factory.hpp:77] Creating layer ip2
I0428 20:10:09.191692 31776 net.cpp:86] Creating Layer ip2
I0428 20:10:09.191695 31776 net.cpp:408] ip2 <- ip1
I0428 20:10:09.191701 31776 net.cpp:382] ip2 -> ip2
I0428 20:10:09.192631 31776 net.cpp:124] Setting up ip2
I0428 20:10:09.192642 31776 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:10:09.192662 31776 net.cpp:139] Memory required for data: 5352960
I0428 20:10:09.192667 31776 layer_factory.hpp:77] Creating layer relu2
I0428 20:10:09.192675 31776 net.cpp:86] Creating Layer relu2
I0428 20:10:09.192678 31776 net.cpp:408] relu2 <- ip2
I0428 20:10:09.192684 31776 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:10:09.193532 31776 net.cpp:124] Setting up relu2
I0428 20:10:09.193544 31776 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:10:09.193564 31776 net.cpp:139] Memory required for data: 5359360
I0428 20:10:09.193567 31776 layer_factory.hpp:77] Creating layer ip3
I0428 20:10:09.193574 31776 net.cpp:86] Creating Layer ip3
I0428 20:10:09.193578 31776 net.cpp:408] ip3 <- ip2
I0428 20:10:09.193584 31776 net.cpp:382] ip3 -> ip3
I0428 20:10:09.193699 31776 net.cpp:124] Setting up ip3
I0428 20:10:09.193706 31776 net.cpp:131] Top shape: 64 10 (640)
I0428 20:10:09.193709 31776 net.cpp:139] Memory required for data: 5361920
I0428 20:10:09.193717 31776 layer_factory.hpp:77] Creating layer relu3
I0428 20:10:09.193722 31776 net.cpp:86] Creating Layer relu3
I0428 20:10:09.193725 31776 net.cpp:408] relu3 <- ip3
I0428 20:10:09.193728 31776 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:10:09.193898 31776 net.cpp:124] Setting up relu3
I0428 20:10:09.193907 31776 net.cpp:131] Top shape: 64 10 (640)
I0428 20:10:09.193909 31776 net.cpp:139] Memory required for data: 5364480
I0428 20:10:09.193912 31776 layer_factory.hpp:77] Creating layer loss
I0428 20:10:09.193919 31776 net.cpp:86] Creating Layer loss
I0428 20:10:09.193922 31776 net.cpp:408] loss <- ip3
I0428 20:10:09.193927 31776 net.cpp:408] loss <- label
I0428 20:10:09.193931 31776 net.cpp:382] loss -> loss
I0428 20:10:09.193944 31776 layer_factory.hpp:77] Creating layer loss
I0428 20:10:09.194192 31776 net.cpp:124] Setting up loss
I0428 20:10:09.194200 31776 net.cpp:131] Top shape: (1)
I0428 20:10:09.194203 31776 net.cpp:134]     with loss weight 1
I0428 20:10:09.194216 31776 net.cpp:139] Memory required for data: 5364484
I0428 20:10:09.194219 31776 net.cpp:200] loss needs backward computation.
I0428 20:10:09.194222 31776 net.cpp:200] relu3 needs backward computation.
I0428 20:10:09.194226 31776 net.cpp:200] ip3 needs backward computation.
I0428 20:10:09.194227 31776 net.cpp:200] relu2 needs backward computation.
I0428 20:10:09.194231 31776 net.cpp:200] ip2 needs backward computation.
I0428 20:10:09.194233 31776 net.cpp:200] relu1 needs backward computation.
I0428 20:10:09.194236 31776 net.cpp:200] ip1 needs backward computation.
I0428 20:10:09.194238 31776 net.cpp:200] pool1 needs backward computation.
I0428 20:10:09.194242 31776 net.cpp:200] conv1 needs backward computation.
I0428 20:10:09.194244 31776 net.cpp:200] pool0 needs backward computation.
I0428 20:10:09.194247 31776 net.cpp:200] conv0 needs backward computation.
I0428 20:10:09.194249 31776 net.cpp:202] mnist does not need backward computation.
I0428 20:10:09.194254 31776 net.cpp:244] This network produces output loss
I0428 20:10:09.194263 31776 net.cpp:257] Network initialization done.
I0428 20:10:09.194610 31776 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1127.prototxt
I0428 20:10:09.194667 31776 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:10:09.194768 31776 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:10:09.194845 31776 layer_factory.hpp:77] Creating layer mnist
I0428 20:10:09.194888 31776 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:10:09.194901 31776 net.cpp:86] Creating Layer mnist
I0428 20:10:09.194907 31776 net.cpp:382] mnist -> data
I0428 20:10:09.194913 31776 net.cpp:382] mnist -> label
I0428 20:10:09.194993 31776 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:10:09.196969 31776 net.cpp:124] Setting up mnist
I0428 20:10:09.196997 31776 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:10:09.197002 31776 net.cpp:131] Top shape: 100 (100)
I0428 20:10:09.197005 31776 net.cpp:139] Memory required for data: 314000
I0428 20:10:09.197008 31776 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:10:09.197021 31776 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:10:09.197026 31776 net.cpp:408] label_mnist_1_split <- label
I0428 20:10:09.197031 31776 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:10:09.197038 31776 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:10:09.197089 31776 net.cpp:124] Setting up label_mnist_1_split
I0428 20:10:09.197096 31776 net.cpp:131] Top shape: 100 (100)
I0428 20:10:09.197099 31776 net.cpp:131] Top shape: 100 (100)
I0428 20:10:09.197103 31776 net.cpp:139] Memory required for data: 314800
I0428 20:10:09.197105 31776 layer_factory.hpp:77] Creating layer conv0
I0428 20:10:09.197114 31776 net.cpp:86] Creating Layer conv0
I0428 20:10:09.197120 31776 net.cpp:408] conv0 <- data
I0428 20:10:09.197126 31776 net.cpp:382] conv0 -> conv0
I0428 20:10:09.198848 31776 net.cpp:124] Setting up conv0
I0428 20:10:09.198880 31776 net.cpp:131] Top shape: 100 25 24 24 (1440000)
I0428 20:10:09.198884 31776 net.cpp:139] Memory required for data: 6074800
I0428 20:10:09.198892 31776 layer_factory.hpp:77] Creating layer pool0
I0428 20:10:09.198899 31776 net.cpp:86] Creating Layer pool0
I0428 20:10:09.198902 31776 net.cpp:408] pool0 <- conv0
I0428 20:10:09.198907 31776 net.cpp:382] pool0 -> pool0
I0428 20:10:09.198945 31776 net.cpp:124] Setting up pool0
I0428 20:10:09.198951 31776 net.cpp:131] Top shape: 100 25 12 12 (360000)
I0428 20:10:09.198954 31776 net.cpp:139] Memory required for data: 7514800
I0428 20:10:09.198956 31776 layer_factory.hpp:77] Creating layer conv1
I0428 20:10:09.198966 31776 net.cpp:86] Creating Layer conv1
I0428 20:10:09.198972 31776 net.cpp:408] conv1 <- pool0
I0428 20:10:09.198993 31776 net.cpp:382] conv1 -> conv1
I0428 20:10:09.201283 31776 net.cpp:124] Setting up conv1
I0428 20:10:09.201297 31776 net.cpp:131] Top shape: 100 25 8 8 (160000)
I0428 20:10:09.201302 31776 net.cpp:139] Memory required for data: 8154800
I0428 20:10:09.201309 31776 layer_factory.hpp:77] Creating layer pool1
I0428 20:10:09.201315 31776 net.cpp:86] Creating Layer pool1
I0428 20:10:09.201319 31776 net.cpp:408] pool1 <- conv1
I0428 20:10:09.201325 31776 net.cpp:382] pool1 -> pool1
I0428 20:10:09.201367 31776 net.cpp:124] Setting up pool1
I0428 20:10:09.201375 31776 net.cpp:131] Top shape: 100 25 4 4 (40000)
I0428 20:10:09.201376 31776 net.cpp:139] Memory required for data: 8314800
I0428 20:10:09.201380 31776 layer_factory.hpp:77] Creating layer ip1
I0428 20:10:09.201386 31776 net.cpp:86] Creating Layer ip1
I0428 20:10:09.201390 31776 net.cpp:408] ip1 <- pool1
I0428 20:10:09.201395 31776 net.cpp:382] ip1 -> ip1
I0428 20:10:09.201648 31776 net.cpp:124] Setting up ip1
I0428 20:10:09.201656 31776 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:10:09.201670 31776 net.cpp:139] Memory required for data: 8334800
I0428 20:10:09.201678 31776 layer_factory.hpp:77] Creating layer relu1
I0428 20:10:09.201685 31776 net.cpp:86] Creating Layer relu1
I0428 20:10:09.201705 31776 net.cpp:408] relu1 <- ip1
I0428 20:10:09.201711 31776 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:10:09.201889 31776 net.cpp:124] Setting up relu1
I0428 20:10:09.201897 31776 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:10:09.201901 31776 net.cpp:139] Memory required for data: 8354800
I0428 20:10:09.201905 31776 layer_factory.hpp:77] Creating layer ip2
I0428 20:10:09.201912 31776 net.cpp:86] Creating Layer ip2
I0428 20:10:09.201918 31776 net.cpp:408] ip2 <- ip1
I0428 20:10:09.201939 31776 net.cpp:382] ip2 -> ip2
I0428 20:10:09.202067 31776 net.cpp:124] Setting up ip2
I0428 20:10:09.202075 31776 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:10:09.202077 31776 net.cpp:139] Memory required for data: 8364800
I0428 20:10:09.202083 31776 layer_factory.hpp:77] Creating layer relu2
I0428 20:10:09.202090 31776 net.cpp:86] Creating Layer relu2
I0428 20:10:09.202093 31776 net.cpp:408] relu2 <- ip2
I0428 20:10:09.202100 31776 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:10:09.202299 31776 net.cpp:124] Setting up relu2
I0428 20:10:09.202308 31776 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:10:09.202311 31776 net.cpp:139] Memory required for data: 8374800
I0428 20:10:09.202314 31776 layer_factory.hpp:77] Creating layer ip3
I0428 20:10:09.202322 31776 net.cpp:86] Creating Layer ip3
I0428 20:10:09.202327 31776 net.cpp:408] ip3 <- ip2
I0428 20:10:09.202332 31776 net.cpp:382] ip3 -> ip3
I0428 20:10:09.202463 31776 net.cpp:124] Setting up ip3
I0428 20:10:09.202471 31776 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:10:09.202473 31776 net.cpp:139] Memory required for data: 8378800
I0428 20:10:09.202481 31776 layer_factory.hpp:77] Creating layer relu3
I0428 20:10:09.202491 31776 net.cpp:86] Creating Layer relu3
I0428 20:10:09.202493 31776 net.cpp:408] relu3 <- ip3
I0428 20:10:09.202498 31776 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:10:09.203310 31776 net.cpp:124] Setting up relu3
I0428 20:10:09.203337 31776 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:10:09.203341 31776 net.cpp:139] Memory required for data: 8382800
I0428 20:10:09.203343 31776 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:10:09.203348 31776 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:10:09.203351 31776 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:10:09.203357 31776 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:10:09.203362 31776 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:10:09.203413 31776 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:10:09.203419 31776 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:10:09.203423 31776 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:10:09.203425 31776 net.cpp:139] Memory required for data: 8390800
I0428 20:10:09.203429 31776 layer_factory.hpp:77] Creating layer accuracy
I0428 20:10:09.203434 31776 net.cpp:86] Creating Layer accuracy
I0428 20:10:09.203438 31776 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:10:09.203443 31776 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:10:09.203449 31776 net.cpp:382] accuracy -> accuracy
I0428 20:10:09.203455 31776 net.cpp:124] Setting up accuracy
I0428 20:10:09.203459 31776 net.cpp:131] Top shape: (1)
I0428 20:10:09.203462 31776 net.cpp:139] Memory required for data: 8390804
I0428 20:10:09.203472 31776 layer_factory.hpp:77] Creating layer loss
I0428 20:10:09.203493 31776 net.cpp:86] Creating Layer loss
I0428 20:10:09.203496 31776 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:10:09.203500 31776 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:10:09.203505 31776 net.cpp:382] loss -> loss
I0428 20:10:09.203511 31776 layer_factory.hpp:77] Creating layer loss
I0428 20:10:09.203804 31776 net.cpp:124] Setting up loss
I0428 20:10:09.203827 31776 net.cpp:131] Top shape: (1)
I0428 20:10:09.203830 31776 net.cpp:134]     with loss weight 1
I0428 20:10:09.203862 31776 net.cpp:139] Memory required for data: 8390808
I0428 20:10:09.203866 31776 net.cpp:200] loss needs backward computation.
I0428 20:10:09.203871 31776 net.cpp:202] accuracy does not need backward computation.
I0428 20:10:09.203874 31776 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:10:09.203877 31776 net.cpp:200] relu3 needs backward computation.
I0428 20:10:09.203881 31776 net.cpp:200] ip3 needs backward computation.
I0428 20:10:09.203889 31776 net.cpp:200] relu2 needs backward computation.
I0428 20:10:09.203892 31776 net.cpp:200] ip2 needs backward computation.
I0428 20:10:09.203896 31776 net.cpp:200] relu1 needs backward computation.
I0428 20:10:09.203897 31776 net.cpp:200] ip1 needs backward computation.
I0428 20:10:09.203902 31776 net.cpp:200] pool1 needs backward computation.
I0428 20:10:09.203904 31776 net.cpp:200] conv1 needs backward computation.
I0428 20:10:09.203908 31776 net.cpp:200] pool0 needs backward computation.
I0428 20:10:09.203910 31776 net.cpp:200] conv0 needs backward computation.
I0428 20:10:09.203914 31776 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:10:09.203918 31776 net.cpp:202] mnist does not need backward computation.
I0428 20:10:09.203922 31776 net.cpp:244] This network produces output accuracy
I0428 20:10:09.203925 31776 net.cpp:244] This network produces output loss
I0428 20:10:09.203938 31776 net.cpp:257] Network initialization done.
I0428 20:10:09.204013 31776 solver.cpp:56] Solver scaffolding done.
I0428 20:10:09.204423 31776 caffe.cpp:248] Starting Optimization
I0428 20:10:09.204428 31776 solver.cpp:273] Solving LeNet
I0428 20:10:09.204432 31776 solver.cpp:274] Learning Rate Policy: inv
I0428 20:10:09.205194 31776 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:10:09.210777 31776 blocking_queue.cpp:49] Waiting for data
I0428 20:10:09.273927 31783 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:10:09.274547 31776 solver.cpp:398]     Test net output #0: accuracy = 0.0688
I0428 20:10:09.274583 31776 solver.cpp:398]     Test net output #1: loss = 2.33311 (* 1 = 2.33311 loss)
I0428 20:10:09.278219 31776 solver.cpp:219] Iteration 0 (-2.6601e-31 iter/s, 0.0737618s/100 iters), loss = 2.34393
I0428 20:10:09.278257 31776 solver.cpp:238]     Train net output #0: loss = 2.34393 (* 1 = 2.34393 loss)
I0428 20:10:09.278268 31776 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:10:09.381012 31776 solver.cpp:219] Iteration 100 (973.166 iter/s, 0.102757s/100 iters), loss = 0.581292
I0428 20:10:09.381055 31776 solver.cpp:238]     Train net output #0: loss = 0.581292 (* 1 = 0.581292 loss)
I0428 20:10:09.381062 31776 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:10:09.499785 31776 solver.cpp:219] Iteration 200 (842.249 iter/s, 0.11873s/100 iters), loss = 0.463075
I0428 20:10:09.499831 31776 solver.cpp:238]     Train net output #0: loss = 0.463075 (* 1 = 0.463075 loss)
I0428 20:10:09.499841 31776 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:10:09.610286 31776 solver.cpp:219] Iteration 300 (905.423 iter/s, 0.110446s/100 iters), loss = 0.333906
I0428 20:10:09.610321 31776 solver.cpp:238]     Train net output #0: loss = 0.333906 (* 1 = 0.333906 loss)
I0428 20:10:09.610329 31776 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:10:09.721079 31776 solver.cpp:219] Iteration 400 (902.962 iter/s, 0.110747s/100 iters), loss = 0.0822002
I0428 20:10:09.721112 31776 solver.cpp:238]     Train net output #0: loss = 0.0822002 (* 1 = 0.0822002 loss)
I0428 20:10:09.721122 31776 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:10:09.828241 31776 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:10:09.904182 31783 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:10:09.904842 31776 solver.cpp:398]     Test net output #0: accuracy = 0.9655
I0428 20:10:09.904867 31776 solver.cpp:398]     Test net output #1: loss = 0.113548 (* 1 = 0.113548 loss)
I0428 20:10:09.905956 31776 solver.cpp:219] Iteration 500 (541.044 iter/s, 0.184828s/100 iters), loss = 0.0688326
I0428 20:10:09.906003 31776 solver.cpp:238]     Train net output #0: loss = 0.0688326 (* 1 = 0.0688326 loss)
I0428 20:10:09.906013 31776 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:10:10.017051 31776 solver.cpp:219] Iteration 600 (900.605 iter/s, 0.111036s/100 iters), loss = 0.136386
I0428 20:10:10.017083 31776 solver.cpp:238]     Train net output #0: loss = 0.136386 (* 1 = 0.136386 loss)
I0428 20:10:10.017091 31776 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:10:10.124577 31776 solver.cpp:219] Iteration 700 (930.378 iter/s, 0.107483s/100 iters), loss = 0.134217
I0428 20:10:10.124611 31776 solver.cpp:238]     Train net output #0: loss = 0.134217 (* 1 = 0.134217 loss)
I0428 20:10:10.124619 31776 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:10:10.232714 31776 solver.cpp:219] Iteration 800 (925.134 iter/s, 0.108092s/100 iters), loss = 0.220493
I0428 20:10:10.232746 31776 solver.cpp:238]     Train net output #0: loss = 0.220493 (* 1 = 0.220493 loss)
I0428 20:10:10.232754 31776 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:10:10.337054 31776 solver.cpp:219] Iteration 900 (958.787 iter/s, 0.104298s/100 iters), loss = 0.153253
I0428 20:10:10.337080 31776 solver.cpp:238]     Train net output #0: loss = 0.153253 (* 1 = 0.153253 loss)
I0428 20:10:10.337087 31776 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:10:10.372504 31782 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:10:10.447602 31776 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:10:10.448984 31776 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:10:10.449816 31776 solver.cpp:311] Iteration 1000, loss = 0.120119
I0428 20:10:10.449836 31776 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:10:10.525712 31783 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:10:10.526361 31776 solver.cpp:398]     Test net output #0: accuracy = 0.9735
I0428 20:10:10.526399 31776 solver.cpp:398]     Test net output #1: loss = 0.0895089 (* 1 = 0.0895089 loss)
I0428 20:10:10.526404 31776 solver.cpp:316] Optimization Done.
I0428 20:10:10.526422 31776 caffe.cpp:259] Optimization Done.
