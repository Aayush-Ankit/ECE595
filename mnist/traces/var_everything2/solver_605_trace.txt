I0428 19:50:27.398517 27018 caffe.cpp:218] Using GPUs 0
I0428 19:50:27.427279 27018 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:50:27.867949 27018 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test605.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:50:27.868082 27018 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test605.prototxt
I0428 19:50:27.868412 27018 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:50:27.868427 27018 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:50:27.868506 27018 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:50:27.868588 27018 layer_factory.hpp:77] Creating layer mnist
I0428 19:50:27.868670 27018 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:50:27.868690 27018 net.cpp:86] Creating Layer mnist
I0428 19:50:27.868698 27018 net.cpp:382] mnist -> data
I0428 19:50:27.868717 27018 net.cpp:382] mnist -> label
I0428 19:50:27.869736 27018 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:50:27.871969 27018 net.cpp:124] Setting up mnist
I0428 19:50:27.871985 27018 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:50:27.871990 27018 net.cpp:131] Top shape: 64 (64)
I0428 19:50:27.871992 27018 net.cpp:139] Memory required for data: 200960
I0428 19:50:27.871999 27018 layer_factory.hpp:77] Creating layer conv0
I0428 19:50:27.872014 27018 net.cpp:86] Creating Layer conv0
I0428 19:50:27.872030 27018 net.cpp:408] conv0 <- data
I0428 19:50:27.872042 27018 net.cpp:382] conv0 -> conv0
I0428 19:50:28.099472 27018 net.cpp:124] Setting up conv0
I0428 19:50:28.099499 27018 net.cpp:131] Top shape: 64 5 24 24 (184320)
I0428 19:50:28.099503 27018 net.cpp:139] Memory required for data: 938240
I0428 19:50:28.099519 27018 layer_factory.hpp:77] Creating layer pool0
I0428 19:50:28.099546 27018 net.cpp:86] Creating Layer pool0
I0428 19:50:28.099565 27018 net.cpp:408] pool0 <- conv0
I0428 19:50:28.099571 27018 net.cpp:382] pool0 -> pool0
I0428 19:50:28.099632 27018 net.cpp:124] Setting up pool0
I0428 19:50:28.099639 27018 net.cpp:131] Top shape: 64 5 12 12 (46080)
I0428 19:50:28.099643 27018 net.cpp:139] Memory required for data: 1122560
I0428 19:50:28.099645 27018 layer_factory.hpp:77] Creating layer conv1
I0428 19:50:28.099656 27018 net.cpp:86] Creating Layer conv1
I0428 19:50:28.099659 27018 net.cpp:408] conv1 <- pool0
I0428 19:50:28.099664 27018 net.cpp:382] conv1 -> conv1
I0428 19:50:28.102509 27018 net.cpp:124] Setting up conv1
I0428 19:50:28.102522 27018 net.cpp:131] Top shape: 64 10 8 8 (40960)
I0428 19:50:28.102526 27018 net.cpp:139] Memory required for data: 1286400
I0428 19:50:28.102535 27018 layer_factory.hpp:77] Creating layer pool1
I0428 19:50:28.102541 27018 net.cpp:86] Creating Layer pool1
I0428 19:50:28.102545 27018 net.cpp:408] pool1 <- conv1
I0428 19:50:28.102550 27018 net.cpp:382] pool1 -> pool1
I0428 19:50:28.102584 27018 net.cpp:124] Setting up pool1
I0428 19:50:28.102589 27018 net.cpp:131] Top shape: 64 10 4 4 (10240)
I0428 19:50:28.102592 27018 net.cpp:139] Memory required for data: 1327360
I0428 19:50:28.102594 27018 layer_factory.hpp:77] Creating layer ip1
I0428 19:50:28.102602 27018 net.cpp:86] Creating Layer ip1
I0428 19:50:28.102604 27018 net.cpp:408] ip1 <- pool1
I0428 19:50:28.102608 27018 net.cpp:382] ip1 -> ip1
I0428 19:50:28.102705 27018 net.cpp:124] Setting up ip1
I0428 19:50:28.102713 27018 net.cpp:131] Top shape: 64 10 (640)
I0428 19:50:28.102716 27018 net.cpp:139] Memory required for data: 1329920
I0428 19:50:28.102723 27018 layer_factory.hpp:77] Creating layer relu1
I0428 19:50:28.102730 27018 net.cpp:86] Creating Layer relu1
I0428 19:50:28.102732 27018 net.cpp:408] relu1 <- ip1
I0428 19:50:28.102736 27018 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:50:28.102890 27018 net.cpp:124] Setting up relu1
I0428 19:50:28.102898 27018 net.cpp:131] Top shape: 64 10 (640)
I0428 19:50:28.102900 27018 net.cpp:139] Memory required for data: 1332480
I0428 19:50:28.102903 27018 layer_factory.hpp:77] Creating layer ip2
I0428 19:50:28.102910 27018 net.cpp:86] Creating Layer ip2
I0428 19:50:28.102912 27018 net.cpp:408] ip2 <- ip1
I0428 19:50:28.102917 27018 net.cpp:382] ip2 -> ip2
I0428 19:50:28.103029 27018 net.cpp:124] Setting up ip2
I0428 19:50:28.103036 27018 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:50:28.103039 27018 net.cpp:139] Memory required for data: 1345280
I0428 19:50:28.103044 27018 layer_factory.hpp:77] Creating layer relu2
I0428 19:50:28.103049 27018 net.cpp:86] Creating Layer relu2
I0428 19:50:28.103052 27018 net.cpp:408] relu2 <- ip2
I0428 19:50:28.103056 27018 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:50:28.103847 27018 net.cpp:124] Setting up relu2
I0428 19:50:28.103858 27018 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:50:28.103862 27018 net.cpp:139] Memory required for data: 1358080
I0428 19:50:28.103865 27018 layer_factory.hpp:77] Creating layer ip3
I0428 19:50:28.103871 27018 net.cpp:86] Creating Layer ip3
I0428 19:50:28.103874 27018 net.cpp:408] ip3 <- ip2
I0428 19:50:28.103879 27018 net.cpp:382] ip3 -> ip3
I0428 19:50:28.103973 27018 net.cpp:124] Setting up ip3
I0428 19:50:28.103981 27018 net.cpp:131] Top shape: 64 10 (640)
I0428 19:50:28.103984 27018 net.cpp:139] Memory required for data: 1360640
I0428 19:50:28.103991 27018 layer_factory.hpp:77] Creating layer relu3
I0428 19:50:28.103996 27018 net.cpp:86] Creating Layer relu3
I0428 19:50:28.103999 27018 net.cpp:408] relu3 <- ip3
I0428 19:50:28.104002 27018 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:50:28.104177 27018 net.cpp:124] Setting up relu3
I0428 19:50:28.104187 27018 net.cpp:131] Top shape: 64 10 (640)
I0428 19:50:28.104189 27018 net.cpp:139] Memory required for data: 1363200
I0428 19:50:28.104192 27018 layer_factory.hpp:77] Creating layer loss
I0428 19:50:28.104198 27018 net.cpp:86] Creating Layer loss
I0428 19:50:28.104202 27018 net.cpp:408] loss <- ip3
I0428 19:50:28.104205 27018 net.cpp:408] loss <- label
I0428 19:50:28.104210 27018 net.cpp:382] loss -> loss
I0428 19:50:28.104228 27018 layer_factory.hpp:77] Creating layer loss
I0428 19:50:28.104451 27018 net.cpp:124] Setting up loss
I0428 19:50:28.104460 27018 net.cpp:131] Top shape: (1)
I0428 19:50:28.104463 27018 net.cpp:134]     with loss weight 1
I0428 19:50:28.104476 27018 net.cpp:139] Memory required for data: 1363204
I0428 19:50:28.104480 27018 net.cpp:200] loss needs backward computation.
I0428 19:50:28.104483 27018 net.cpp:200] relu3 needs backward computation.
I0428 19:50:28.104486 27018 net.cpp:200] ip3 needs backward computation.
I0428 19:50:28.104490 27018 net.cpp:200] relu2 needs backward computation.
I0428 19:50:28.104491 27018 net.cpp:200] ip2 needs backward computation.
I0428 19:50:28.104495 27018 net.cpp:200] relu1 needs backward computation.
I0428 19:50:28.104497 27018 net.cpp:200] ip1 needs backward computation.
I0428 19:50:28.104499 27018 net.cpp:200] pool1 needs backward computation.
I0428 19:50:28.104502 27018 net.cpp:200] conv1 needs backward computation.
I0428 19:50:28.104506 27018 net.cpp:200] pool0 needs backward computation.
I0428 19:50:28.104508 27018 net.cpp:200] conv0 needs backward computation.
I0428 19:50:28.104511 27018 net.cpp:202] mnist does not need backward computation.
I0428 19:50:28.104513 27018 net.cpp:244] This network produces output loss
I0428 19:50:28.104522 27018 net.cpp:257] Network initialization done.
I0428 19:50:28.104882 27018 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test605.prototxt
I0428 19:50:28.104925 27018 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:50:28.105031 27018 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:50:28.105129 27018 layer_factory.hpp:77] Creating layer mnist
I0428 19:50:28.105188 27018 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:50:28.105202 27018 net.cpp:86] Creating Layer mnist
I0428 19:50:28.105207 27018 net.cpp:382] mnist -> data
I0428 19:50:28.105216 27018 net.cpp:382] mnist -> label
I0428 19:50:28.105315 27018 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:50:28.107252 27018 net.cpp:124] Setting up mnist
I0428 19:50:28.107280 27018 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:50:28.107302 27018 net.cpp:131] Top shape: 100 (100)
I0428 19:50:28.107306 27018 net.cpp:139] Memory required for data: 314000
I0428 19:50:28.107309 27018 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:50:28.107331 27018 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:50:28.107334 27018 net.cpp:408] label_mnist_1_split <- label
I0428 19:50:28.107338 27018 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:50:28.107345 27018 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:50:28.107388 27018 net.cpp:124] Setting up label_mnist_1_split
I0428 19:50:28.107395 27018 net.cpp:131] Top shape: 100 (100)
I0428 19:50:28.107398 27018 net.cpp:131] Top shape: 100 (100)
I0428 19:50:28.107401 27018 net.cpp:139] Memory required for data: 314800
I0428 19:50:28.107404 27018 layer_factory.hpp:77] Creating layer conv0
I0428 19:50:28.107412 27018 net.cpp:86] Creating Layer conv0
I0428 19:50:28.107415 27018 net.cpp:408] conv0 <- data
I0428 19:50:28.107420 27018 net.cpp:382] conv0 -> conv0
I0428 19:50:28.109124 27018 net.cpp:124] Setting up conv0
I0428 19:50:28.109139 27018 net.cpp:131] Top shape: 100 5 24 24 (288000)
I0428 19:50:28.109144 27018 net.cpp:139] Memory required for data: 1466800
I0428 19:50:28.109154 27018 layer_factory.hpp:77] Creating layer pool0
I0428 19:50:28.109161 27018 net.cpp:86] Creating Layer pool0
I0428 19:50:28.109165 27018 net.cpp:408] pool0 <- conv0
I0428 19:50:28.109170 27018 net.cpp:382] pool0 -> pool0
I0428 19:50:28.109225 27018 net.cpp:124] Setting up pool0
I0428 19:50:28.109241 27018 net.cpp:131] Top shape: 100 5 12 12 (72000)
I0428 19:50:28.109244 27018 net.cpp:139] Memory required for data: 1754800
I0428 19:50:28.109247 27018 layer_factory.hpp:77] Creating layer conv1
I0428 19:50:28.109256 27018 net.cpp:86] Creating Layer conv1
I0428 19:50:28.109259 27018 net.cpp:408] conv1 <- pool0
I0428 19:50:28.109264 27018 net.cpp:382] conv1 -> conv1
I0428 19:50:28.111394 27018 net.cpp:124] Setting up conv1
I0428 19:50:28.111409 27018 net.cpp:131] Top shape: 100 10 8 8 (64000)
I0428 19:50:28.111413 27018 net.cpp:139] Memory required for data: 2010800
I0428 19:50:28.111438 27018 layer_factory.hpp:77] Creating layer pool1
I0428 19:50:28.111443 27018 net.cpp:86] Creating Layer pool1
I0428 19:50:28.111448 27018 net.cpp:408] pool1 <- conv1
I0428 19:50:28.111452 27018 net.cpp:382] pool1 -> pool1
I0428 19:50:28.111505 27018 net.cpp:124] Setting up pool1
I0428 19:50:28.111511 27018 net.cpp:131] Top shape: 100 10 4 4 (16000)
I0428 19:50:28.111515 27018 net.cpp:139] Memory required for data: 2074800
I0428 19:50:28.111518 27018 layer_factory.hpp:77] Creating layer ip1
I0428 19:50:28.111523 27018 net.cpp:86] Creating Layer ip1
I0428 19:50:28.111527 27018 net.cpp:408] ip1 <- pool1
I0428 19:50:28.111532 27018 net.cpp:382] ip1 -> ip1
I0428 19:50:28.111675 27018 net.cpp:124] Setting up ip1
I0428 19:50:28.111681 27018 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:50:28.111696 27018 net.cpp:139] Memory required for data: 2078800
I0428 19:50:28.111702 27018 layer_factory.hpp:77] Creating layer relu1
I0428 19:50:28.111707 27018 net.cpp:86] Creating Layer relu1
I0428 19:50:28.111711 27018 net.cpp:408] relu1 <- ip1
I0428 19:50:28.111716 27018 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:50:28.111927 27018 net.cpp:124] Setting up relu1
I0428 19:50:28.111950 27018 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:50:28.111953 27018 net.cpp:139] Memory required for data: 2082800
I0428 19:50:28.111956 27018 layer_factory.hpp:77] Creating layer ip2
I0428 19:50:28.111964 27018 net.cpp:86] Creating Layer ip2
I0428 19:50:28.111968 27018 net.cpp:408] ip2 <- ip1
I0428 19:50:28.111974 27018 net.cpp:382] ip2 -> ip2
I0428 19:50:28.112071 27018 net.cpp:124] Setting up ip2
I0428 19:50:28.112078 27018 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:50:28.112082 27018 net.cpp:139] Memory required for data: 2102800
I0428 19:50:28.112087 27018 layer_factory.hpp:77] Creating layer relu2
I0428 19:50:28.112090 27018 net.cpp:86] Creating Layer relu2
I0428 19:50:28.112094 27018 net.cpp:408] relu2 <- ip2
I0428 19:50:28.112098 27018 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:50:28.112241 27018 net.cpp:124] Setting up relu2
I0428 19:50:28.112249 27018 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:50:28.112252 27018 net.cpp:139] Memory required for data: 2122800
I0428 19:50:28.112256 27018 layer_factory.hpp:77] Creating layer ip3
I0428 19:50:28.112277 27018 net.cpp:86] Creating Layer ip3
I0428 19:50:28.112279 27018 net.cpp:408] ip3 <- ip2
I0428 19:50:28.112284 27018 net.cpp:382] ip3 -> ip3
I0428 19:50:28.112401 27018 net.cpp:124] Setting up ip3
I0428 19:50:28.112408 27018 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:50:28.112411 27018 net.cpp:139] Memory required for data: 2126800
I0428 19:50:28.112419 27018 layer_factory.hpp:77] Creating layer relu3
I0428 19:50:28.112423 27018 net.cpp:86] Creating Layer relu3
I0428 19:50:28.112427 27018 net.cpp:408] relu3 <- ip3
I0428 19:50:28.112432 27018 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:50:28.113286 27018 net.cpp:124] Setting up relu3
I0428 19:50:28.113297 27018 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:50:28.113301 27018 net.cpp:139] Memory required for data: 2130800
I0428 19:50:28.113304 27018 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:50:28.113309 27018 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:50:28.113313 27018 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:50:28.113318 27018 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:50:28.113324 27018 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:50:28.113371 27018 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:50:28.113379 27018 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:50:28.113382 27018 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:50:28.113384 27018 net.cpp:139] Memory required for data: 2138800
I0428 19:50:28.113387 27018 layer_factory.hpp:77] Creating layer accuracy
I0428 19:50:28.113392 27018 net.cpp:86] Creating Layer accuracy
I0428 19:50:28.113395 27018 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:50:28.113399 27018 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:50:28.113404 27018 net.cpp:382] accuracy -> accuracy
I0428 19:50:28.113410 27018 net.cpp:124] Setting up accuracy
I0428 19:50:28.113414 27018 net.cpp:131] Top shape: (1)
I0428 19:50:28.113416 27018 net.cpp:139] Memory required for data: 2138804
I0428 19:50:28.113420 27018 layer_factory.hpp:77] Creating layer loss
I0428 19:50:28.113423 27018 net.cpp:86] Creating Layer loss
I0428 19:50:28.113425 27018 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:50:28.113430 27018 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:50:28.113440 27018 net.cpp:382] loss -> loss
I0428 19:50:28.113445 27018 layer_factory.hpp:77] Creating layer loss
I0428 19:50:28.113682 27018 net.cpp:124] Setting up loss
I0428 19:50:28.113692 27018 net.cpp:131] Top shape: (1)
I0428 19:50:28.113695 27018 net.cpp:134]     with loss weight 1
I0428 19:50:28.113701 27018 net.cpp:139] Memory required for data: 2138808
I0428 19:50:28.113729 27018 net.cpp:200] loss needs backward computation.
I0428 19:50:28.113734 27018 net.cpp:202] accuracy does not need backward computation.
I0428 19:50:28.113739 27018 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:50:28.113742 27018 net.cpp:200] relu3 needs backward computation.
I0428 19:50:28.113745 27018 net.cpp:200] ip3 needs backward computation.
I0428 19:50:28.113749 27018 net.cpp:200] relu2 needs backward computation.
I0428 19:50:28.113751 27018 net.cpp:200] ip2 needs backward computation.
I0428 19:50:28.113755 27018 net.cpp:200] relu1 needs backward computation.
I0428 19:50:28.113759 27018 net.cpp:200] ip1 needs backward computation.
I0428 19:50:28.113761 27018 net.cpp:200] pool1 needs backward computation.
I0428 19:50:28.113765 27018 net.cpp:200] conv1 needs backward computation.
I0428 19:50:28.113768 27018 net.cpp:200] pool0 needs backward computation.
I0428 19:50:28.113771 27018 net.cpp:200] conv0 needs backward computation.
I0428 19:50:28.113775 27018 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:50:28.113780 27018 net.cpp:202] mnist does not need backward computation.
I0428 19:50:28.113782 27018 net.cpp:244] This network produces output accuracy
I0428 19:50:28.113801 27018 net.cpp:244] This network produces output loss
I0428 19:50:28.113829 27018 net.cpp:257] Network initialization done.
I0428 19:50:28.113874 27018 solver.cpp:56] Solver scaffolding done.
I0428 19:50:28.114298 27018 caffe.cpp:248] Starting Optimization
I0428 19:50:28.114305 27018 solver.cpp:273] Solving LeNet
I0428 19:50:28.114307 27018 solver.cpp:274] Learning Rate Policy: inv
I0428 19:50:28.115123 27018 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:50:28.118238 27018 blocking_queue.cpp:49] Waiting for data
I0428 19:50:28.191277 27025 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:50:28.191803 27018 solver.cpp:398]     Test net output #0: accuracy = 0.0619
I0428 19:50:28.191835 27018 solver.cpp:398]     Test net output #1: loss = 2.34195 (* 1 = 2.34195 loss)
I0428 19:50:28.193763 27018 solver.cpp:219] Iteration 0 (0 iter/s, 0.0794143s/100 iters), loss = 2.34515
I0428 19:50:28.193819 27018 solver.cpp:238]     Train net output #0: loss = 2.34515 (* 1 = 2.34515 loss)
I0428 19:50:28.193830 27018 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:50:28.276290 27018 solver.cpp:219] Iteration 100 (1212.68 iter/s, 0.0824617s/100 iters), loss = 1.44324
I0428 19:50:28.276331 27018 solver.cpp:238]     Train net output #0: loss = 1.44324 (* 1 = 1.44324 loss)
I0428 19:50:28.276337 27018 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:50:28.351582 27018 solver.cpp:219] Iteration 200 (1328.76 iter/s, 0.0752579s/100 iters), loss = 1.32129
I0428 19:50:28.351621 27018 solver.cpp:238]     Train net output #0: loss = 1.32129 (* 1 = 1.32129 loss)
I0428 19:50:28.351627 27018 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:50:28.425499 27018 solver.cpp:219] Iteration 300 (1353.74 iter/s, 0.0738693s/100 iters), loss = 1.12734
I0428 19:50:28.425537 27018 solver.cpp:238]     Train net output #0: loss = 1.12734 (* 1 = 1.12734 loss)
I0428 19:50:28.425544 27018 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:50:28.500011 27018 solver.cpp:219] Iteration 400 (1342.63 iter/s, 0.0744808s/100 iters), loss = 1.19532
I0428 19:50:28.500049 27018 solver.cpp:238]     Train net output #0: loss = 1.19532 (* 1 = 1.19532 loss)
I0428 19:50:28.500056 27018 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:50:28.572476 27018 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:50:28.647524 27025 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:50:28.648066 27018 solver.cpp:398]     Test net output #0: accuracy = 0.7546
I0428 19:50:28.648087 27018 solver.cpp:398]     Test net output #1: loss = 0.904505 (* 1 = 0.904505 loss)
I0428 19:50:28.648954 27018 solver.cpp:219] Iteration 500 (671.561 iter/s, 0.148907s/100 iters), loss = 0.969384
I0428 19:50:28.648977 27018 solver.cpp:238]     Train net output #0: loss = 0.969384 (* 1 = 0.969384 loss)
I0428 19:50:28.648998 27018 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:50:28.731355 27018 solver.cpp:219] Iteration 600 (1214.06 iter/s, 0.0823685s/100 iters), loss = 0.903647
I0428 19:50:28.731396 27018 solver.cpp:238]     Train net output #0: loss = 0.903647 (* 1 = 0.903647 loss)
I0428 19:50:28.731402 27018 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:50:28.807456 27018 solver.cpp:219] Iteration 700 (1314.65 iter/s, 0.0760658s/100 iters), loss = 1.03964
I0428 19:50:28.807508 27018 solver.cpp:238]     Train net output #0: loss = 1.03964 (* 1 = 1.03964 loss)
I0428 19:50:28.807514 27018 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:50:28.882496 27018 solver.cpp:219] Iteration 800 (1333.45 iter/s, 0.0749937s/100 iters), loss = 1.00493
I0428 19:50:28.882524 27018 solver.cpp:238]     Train net output #0: loss = 1.00493 (* 1 = 1.00493 loss)
I0428 19:50:28.882530 27018 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:50:28.956555 27018 solver.cpp:219] Iteration 900 (1350.91 iter/s, 0.0740239s/100 iters), loss = 0.949677
I0428 19:50:28.956595 27018 solver.cpp:238]     Train net output #0: loss = 0.949677 (* 1 = 0.949677 loss)
I0428 19:50:28.956601 27018 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:50:28.981463 27024 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:50:29.029670 27018 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:50:29.030443 27018 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:50:29.030879 27018 solver.cpp:311] Iteration 1000, loss = 0.924857
I0428 19:50:29.030895 27018 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:50:29.105810 27025 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:50:29.106431 27018 solver.cpp:398]     Test net output #0: accuracy = 0.7667
I0428 19:50:29.106461 27018 solver.cpp:398]     Test net output #1: loss = 0.811415 (* 1 = 0.811415 loss)
I0428 19:50:29.106468 27018 solver.cpp:316] Optimization Done.
I0428 19:50:29.106472 27018 caffe.cpp:259] Optimization Done.
