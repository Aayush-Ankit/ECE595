I0428 20:04:44.632882 30451 caffe.cpp:218] Using GPUs 0
I0428 20:04:44.662377 30451 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:04:45.111152 30451 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test980.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:04:45.111302 30451 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test980.prototxt
I0428 20:04:45.111627 30451 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:04:45.111655 30451 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:04:45.111723 30451 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0428 20:04:45.111778 30451 layer_factory.hpp:77] Creating layer mnist
I0428 20:04:45.111856 30451 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:04:45.111876 30451 net.cpp:86] Creating Layer mnist
I0428 20:04:45.111881 30451 net.cpp:382] mnist -> data
I0428 20:04:45.111899 30451 net.cpp:382] mnist -> label
I0428 20:04:45.112846 30451 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:04:45.115066 30451 net.cpp:124] Setting up mnist
I0428 20:04:45.115095 30451 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:04:45.115099 30451 net.cpp:131] Top shape: 64 (64)
I0428 20:04:45.115103 30451 net.cpp:139] Memory required for data: 200960
I0428 20:04:45.115108 30451 layer_factory.hpp:77] Creating layer conv0
I0428 20:04:45.115120 30451 net.cpp:86] Creating Layer conv0
I0428 20:04:45.115124 30451 net.cpp:408] conv0 <- data
I0428 20:04:45.115134 30451 net.cpp:382] conv0 -> conv0
I0428 20:04:45.341559 30451 net.cpp:124] Setting up conv0
I0428 20:04:45.341583 30451 net.cpp:131] Top shape: 64 25 24 24 (921600)
I0428 20:04:45.341588 30451 net.cpp:139] Memory required for data: 3887360
I0428 20:04:45.341635 30451 layer_factory.hpp:77] Creating layer pool0
I0428 20:04:45.341648 30451 net.cpp:86] Creating Layer pool0
I0428 20:04:45.341651 30451 net.cpp:408] pool0 <- conv0
I0428 20:04:45.341656 30451 net.cpp:382] pool0 -> pool0
I0428 20:04:45.341699 30451 net.cpp:124] Setting up pool0
I0428 20:04:45.341706 30451 net.cpp:131] Top shape: 64 25 12 12 (230400)
I0428 20:04:45.341708 30451 net.cpp:139] Memory required for data: 4808960
I0428 20:04:45.341711 30451 layer_factory.hpp:77] Creating layer conv1
I0428 20:04:45.341722 30451 net.cpp:86] Creating Layer conv1
I0428 20:04:45.341724 30451 net.cpp:408] conv1 <- pool0
I0428 20:04:45.341729 30451 net.cpp:382] conv1 -> conv1
I0428 20:04:45.344490 30451 net.cpp:124] Setting up conv1
I0428 20:04:45.344519 30451 net.cpp:131] Top shape: 64 2 8 8 (8192)
I0428 20:04:45.344522 30451 net.cpp:139] Memory required for data: 4841728
I0428 20:04:45.344530 30451 layer_factory.hpp:77] Creating layer pool1
I0428 20:04:45.344538 30451 net.cpp:86] Creating Layer pool1
I0428 20:04:45.344542 30451 net.cpp:408] pool1 <- conv1
I0428 20:04:45.344547 30451 net.cpp:382] pool1 -> pool1
I0428 20:04:45.344581 30451 net.cpp:124] Setting up pool1
I0428 20:04:45.344589 30451 net.cpp:131] Top shape: 64 2 4 4 (2048)
I0428 20:04:45.344593 30451 net.cpp:139] Memory required for data: 4849920
I0428 20:04:45.344595 30451 layer_factory.hpp:77] Creating layer ip1
I0428 20:04:45.344602 30451 net.cpp:86] Creating Layer ip1
I0428 20:04:45.344605 30451 net.cpp:408] ip1 <- pool1
I0428 20:04:45.344610 30451 net.cpp:382] ip1 -> ip1
I0428 20:04:45.344696 30451 net.cpp:124] Setting up ip1
I0428 20:04:45.344702 30451 net.cpp:131] Top shape: 64 10 (640)
I0428 20:04:45.344705 30451 net.cpp:139] Memory required for data: 4852480
I0428 20:04:45.344712 30451 layer_factory.hpp:77] Creating layer relu1
I0428 20:04:45.344732 30451 net.cpp:86] Creating Layer relu1
I0428 20:04:45.344736 30451 net.cpp:408] relu1 <- ip1
I0428 20:04:45.344740 30451 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:04:45.344946 30451 net.cpp:124] Setting up relu1
I0428 20:04:45.344956 30451 net.cpp:131] Top shape: 64 10 (640)
I0428 20:04:45.344959 30451 net.cpp:139] Memory required for data: 4855040
I0428 20:04:45.344962 30451 layer_factory.hpp:77] Creating layer ip2
I0428 20:04:45.344969 30451 net.cpp:86] Creating Layer ip2
I0428 20:04:45.344972 30451 net.cpp:408] ip2 <- ip1
I0428 20:04:45.344977 30451 net.cpp:382] ip2 -> ip2
I0428 20:04:45.345088 30451 net.cpp:124] Setting up ip2
I0428 20:04:45.345095 30451 net.cpp:131] Top shape: 64 10 (640)
I0428 20:04:45.345098 30451 net.cpp:139] Memory required for data: 4857600
I0428 20:04:45.345103 30451 layer_factory.hpp:77] Creating layer relu2
I0428 20:04:45.345109 30451 net.cpp:86] Creating Layer relu2
I0428 20:04:45.345113 30451 net.cpp:408] relu2 <- ip2
I0428 20:04:45.345116 30451 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:04:45.345939 30451 net.cpp:124] Setting up relu2
I0428 20:04:45.345952 30451 net.cpp:131] Top shape: 64 10 (640)
I0428 20:04:45.345954 30451 net.cpp:139] Memory required for data: 4860160
I0428 20:04:45.345958 30451 layer_factory.hpp:77] Creating layer loss
I0428 20:04:45.345964 30451 net.cpp:86] Creating Layer loss
I0428 20:04:45.345968 30451 net.cpp:408] loss <- ip2
I0428 20:04:45.345973 30451 net.cpp:408] loss <- label
I0428 20:04:45.345981 30451 net.cpp:382] loss -> loss
I0428 20:04:45.346000 30451 layer_factory.hpp:77] Creating layer loss
I0428 20:04:45.346237 30451 net.cpp:124] Setting up loss
I0428 20:04:45.346247 30451 net.cpp:131] Top shape: (1)
I0428 20:04:45.346251 30451 net.cpp:134]     with loss weight 1
I0428 20:04:45.346266 30451 net.cpp:139] Memory required for data: 4860164
I0428 20:04:45.346269 30451 net.cpp:200] loss needs backward computation.
I0428 20:04:45.346272 30451 net.cpp:200] relu2 needs backward computation.
I0428 20:04:45.346276 30451 net.cpp:200] ip2 needs backward computation.
I0428 20:04:45.346278 30451 net.cpp:200] relu1 needs backward computation.
I0428 20:04:45.346282 30451 net.cpp:200] ip1 needs backward computation.
I0428 20:04:45.346294 30451 net.cpp:200] pool1 needs backward computation.
I0428 20:04:45.346297 30451 net.cpp:200] conv1 needs backward computation.
I0428 20:04:45.346300 30451 net.cpp:200] pool0 needs backward computation.
I0428 20:04:45.346302 30451 net.cpp:200] conv0 needs backward computation.
I0428 20:04:45.346307 30451 net.cpp:202] mnist does not need backward computation.
I0428 20:04:45.346308 30451 net.cpp:244] This network produces output loss
I0428 20:04:45.346331 30451 net.cpp:257] Network initialization done.
I0428 20:04:45.346616 30451 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test980.prototxt
I0428 20:04:45.346671 30451 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:04:45.346747 30451 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0428 20:04:45.346806 30451 layer_factory.hpp:77] Creating layer mnist
I0428 20:04:45.346848 30451 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:04:45.346859 30451 net.cpp:86] Creating Layer mnist
I0428 20:04:45.346863 30451 net.cpp:382] mnist -> data
I0428 20:04:45.346871 30451 net.cpp:382] mnist -> label
I0428 20:04:45.347015 30451 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:04:45.349318 30451 net.cpp:124] Setting up mnist
I0428 20:04:45.349344 30451 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:04:45.349349 30451 net.cpp:131] Top shape: 100 (100)
I0428 20:04:45.349367 30451 net.cpp:139] Memory required for data: 314000
I0428 20:04:45.349370 30451 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:04:45.349395 30451 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:04:45.349398 30451 net.cpp:408] label_mnist_1_split <- label
I0428 20:04:45.349403 30451 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:04:45.349409 30451 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:04:45.349495 30451 net.cpp:124] Setting up label_mnist_1_split
I0428 20:04:45.349512 30451 net.cpp:131] Top shape: 100 (100)
I0428 20:04:45.349516 30451 net.cpp:131] Top shape: 100 (100)
I0428 20:04:45.349519 30451 net.cpp:139] Memory required for data: 314800
I0428 20:04:45.349522 30451 layer_factory.hpp:77] Creating layer conv0
I0428 20:04:45.349530 30451 net.cpp:86] Creating Layer conv0
I0428 20:04:45.349534 30451 net.cpp:408] conv0 <- data
I0428 20:04:45.349539 30451 net.cpp:382] conv0 -> conv0
I0428 20:04:45.351140 30451 net.cpp:124] Setting up conv0
I0428 20:04:45.351153 30451 net.cpp:131] Top shape: 100 25 24 24 (1440000)
I0428 20:04:45.351156 30451 net.cpp:139] Memory required for data: 6074800
I0428 20:04:45.351166 30451 layer_factory.hpp:77] Creating layer pool0
I0428 20:04:45.351171 30451 net.cpp:86] Creating Layer pool0
I0428 20:04:45.351174 30451 net.cpp:408] pool0 <- conv0
I0428 20:04:45.351178 30451 net.cpp:382] pool0 -> pool0
I0428 20:04:45.351227 30451 net.cpp:124] Setting up pool0
I0428 20:04:45.351238 30451 net.cpp:131] Top shape: 100 25 12 12 (360000)
I0428 20:04:45.351240 30451 net.cpp:139] Memory required for data: 7514800
I0428 20:04:45.351243 30451 layer_factory.hpp:77] Creating layer conv1
I0428 20:04:45.351267 30451 net.cpp:86] Creating Layer conv1
I0428 20:04:45.351270 30451 net.cpp:408] conv1 <- pool0
I0428 20:04:45.351275 30451 net.cpp:382] conv1 -> conv1
I0428 20:04:45.353525 30451 net.cpp:124] Setting up conv1
I0428 20:04:45.353538 30451 net.cpp:131] Top shape: 100 2 8 8 (12800)
I0428 20:04:45.353543 30451 net.cpp:139] Memory required for data: 7566000
I0428 20:04:45.353565 30451 layer_factory.hpp:77] Creating layer pool1
I0428 20:04:45.353586 30451 net.cpp:86] Creating Layer pool1
I0428 20:04:45.353590 30451 net.cpp:408] pool1 <- conv1
I0428 20:04:45.353595 30451 net.cpp:382] pool1 -> pool1
I0428 20:04:45.353675 30451 net.cpp:124] Setting up pool1
I0428 20:04:45.353682 30451 net.cpp:131] Top shape: 100 2 4 4 (3200)
I0428 20:04:45.353685 30451 net.cpp:139] Memory required for data: 7578800
I0428 20:04:45.353688 30451 layer_factory.hpp:77] Creating layer ip1
I0428 20:04:45.353694 30451 net.cpp:86] Creating Layer ip1
I0428 20:04:45.353698 30451 net.cpp:408] ip1 <- pool1
I0428 20:04:45.353703 30451 net.cpp:382] ip1 -> ip1
I0428 20:04:45.353797 30451 net.cpp:124] Setting up ip1
I0428 20:04:45.353804 30451 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:04:45.353808 30451 net.cpp:139] Memory required for data: 7582800
I0428 20:04:45.353814 30451 layer_factory.hpp:77] Creating layer relu1
I0428 20:04:45.353819 30451 net.cpp:86] Creating Layer relu1
I0428 20:04:45.353822 30451 net.cpp:408] relu1 <- ip1
I0428 20:04:45.353827 30451 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:04:45.354003 30451 net.cpp:124] Setting up relu1
I0428 20:04:45.354012 30451 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:04:45.354014 30451 net.cpp:139] Memory required for data: 7586800
I0428 20:04:45.354017 30451 layer_factory.hpp:77] Creating layer ip2
I0428 20:04:45.354024 30451 net.cpp:86] Creating Layer ip2
I0428 20:04:45.354027 30451 net.cpp:408] ip2 <- ip1
I0428 20:04:45.354032 30451 net.cpp:382] ip2 -> ip2
I0428 20:04:45.354121 30451 net.cpp:124] Setting up ip2
I0428 20:04:45.354127 30451 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:04:45.354131 30451 net.cpp:139] Memory required for data: 7590800
I0428 20:04:45.354136 30451 layer_factory.hpp:77] Creating layer relu2
I0428 20:04:45.354141 30451 net.cpp:86] Creating Layer relu2
I0428 20:04:45.354145 30451 net.cpp:408] relu2 <- ip2
I0428 20:04:45.354148 30451 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:04:45.354336 30451 net.cpp:124] Setting up relu2
I0428 20:04:45.354346 30451 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:04:45.354349 30451 net.cpp:139] Memory required for data: 7594800
I0428 20:04:45.354352 30451 layer_factory.hpp:77] Creating layer ip2_relu2_0_split
I0428 20:04:45.354357 30451 net.cpp:86] Creating Layer ip2_relu2_0_split
I0428 20:04:45.354359 30451 net.cpp:408] ip2_relu2_0_split <- ip2
I0428 20:04:45.354363 30451 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_0
I0428 20:04:45.354379 30451 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_1
I0428 20:04:45.354439 30451 net.cpp:124] Setting up ip2_relu2_0_split
I0428 20:04:45.354445 30451 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:04:45.354465 30451 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:04:45.354467 30451 net.cpp:139] Memory required for data: 7602800
I0428 20:04:45.354471 30451 layer_factory.hpp:77] Creating layer accuracy
I0428 20:04:45.354476 30451 net.cpp:86] Creating Layer accuracy
I0428 20:04:45.354483 30451 net.cpp:408] accuracy <- ip2_relu2_0_split_0
I0428 20:04:45.354488 30451 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:04:45.354492 30451 net.cpp:382] accuracy -> accuracy
I0428 20:04:45.354499 30451 net.cpp:124] Setting up accuracy
I0428 20:04:45.354504 30451 net.cpp:131] Top shape: (1)
I0428 20:04:45.354506 30451 net.cpp:139] Memory required for data: 7602804
I0428 20:04:45.354509 30451 layer_factory.hpp:77] Creating layer loss
I0428 20:04:45.354514 30451 net.cpp:86] Creating Layer loss
I0428 20:04:45.354517 30451 net.cpp:408] loss <- ip2_relu2_0_split_1
I0428 20:04:45.354521 30451 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:04:45.354526 30451 net.cpp:382] loss -> loss
I0428 20:04:45.354532 30451 layer_factory.hpp:77] Creating layer loss
I0428 20:04:45.354755 30451 net.cpp:124] Setting up loss
I0428 20:04:45.354764 30451 net.cpp:131] Top shape: (1)
I0428 20:04:45.354768 30451 net.cpp:134]     with loss weight 1
I0428 20:04:45.354789 30451 net.cpp:139] Memory required for data: 7602808
I0428 20:04:45.354792 30451 net.cpp:200] loss needs backward computation.
I0428 20:04:45.354796 30451 net.cpp:202] accuracy does not need backward computation.
I0428 20:04:45.354799 30451 net.cpp:200] ip2_relu2_0_split needs backward computation.
I0428 20:04:45.354809 30451 net.cpp:200] relu2 needs backward computation.
I0428 20:04:45.354826 30451 net.cpp:200] ip2 needs backward computation.
I0428 20:04:45.354830 30451 net.cpp:200] relu1 needs backward computation.
I0428 20:04:45.354831 30451 net.cpp:200] ip1 needs backward computation.
I0428 20:04:45.354835 30451 net.cpp:200] pool1 needs backward computation.
I0428 20:04:45.354837 30451 net.cpp:200] conv1 needs backward computation.
I0428 20:04:45.354841 30451 net.cpp:200] pool0 needs backward computation.
I0428 20:04:45.354843 30451 net.cpp:200] conv0 needs backward computation.
I0428 20:04:45.354847 30451 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:04:45.354851 30451 net.cpp:202] mnist does not need backward computation.
I0428 20:04:45.354853 30451 net.cpp:244] This network produces output accuracy
I0428 20:04:45.354856 30451 net.cpp:244] This network produces output loss
I0428 20:04:45.354866 30451 net.cpp:257] Network initialization done.
I0428 20:04:45.354902 30451 solver.cpp:56] Solver scaffolding done.
I0428 20:04:45.355170 30451 caffe.cpp:248] Starting Optimization
I0428 20:04:45.355175 30451 solver.cpp:273] Solving LeNet
I0428 20:04:45.355178 30451 solver.cpp:274] Learning Rate Policy: inv
I0428 20:04:45.355880 30451 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:04:45.361508 30451 blocking_queue.cpp:49] Waiting for data
I0428 20:04:45.455554 30458 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:04:45.456140 30451 solver.cpp:398]     Test net output #0: accuracy = 0.0655
I0428 20:04:45.456181 30451 solver.cpp:398]     Test net output #1: loss = 2.3356 (* 1 = 2.3356 loss)
I0428 20:04:45.459323 30451 solver.cpp:219] Iteration 0 (2.18647 iter/s, 0.104117s/100 iters), loss = 2.31684
I0428 20:04:45.459362 30451 solver.cpp:238]     Train net output #0: loss = 2.31684 (* 1 = 2.31684 loss)
I0428 20:04:45.459372 30451 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:04:45.541273 30451 solver.cpp:219] Iteration 100 (1220.73 iter/s, 0.081918s/100 iters), loss = 1.4929
I0428 20:04:45.541312 30451 solver.cpp:238]     Train net output #0: loss = 1.4929 (* 1 = 1.4929 loss)
I0428 20:04:45.541318 30451 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:04:45.621799 30451 solver.cpp:219] Iteration 200 (1242.37 iter/s, 0.0804916s/100 iters), loss = 1.52197
I0428 20:04:45.621850 30451 solver.cpp:238]     Train net output #0: loss = 1.52197 (* 1 = 1.52197 loss)
I0428 20:04:45.621855 30451 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:04:45.703827 30451 solver.cpp:219] Iteration 300 (1219.74 iter/s, 0.0819845s/100 iters), loss = 0.821084
I0428 20:04:45.703881 30451 solver.cpp:238]     Train net output #0: loss = 0.821084 (* 1 = 0.821084 loss)
I0428 20:04:45.703886 30451 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:04:45.793754 30451 solver.cpp:219] Iteration 400 (1112.6 iter/s, 0.0898796s/100 iters), loss = 0.998186
I0428 20:04:45.793793 30451 solver.cpp:238]     Train net output #0: loss = 0.998186 (* 1 = 0.998186 loss)
I0428 20:04:45.793799 30451 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:04:45.875568 30451 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:04:45.950749 30458 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:04:45.951335 30451 solver.cpp:398]     Test net output #0: accuracy = 0.7336
I0428 20:04:45.951370 30451 solver.cpp:398]     Test net output #1: loss = 0.754278 (* 1 = 0.754278 loss)
I0428 20:04:45.952266 30451 solver.cpp:219] Iteration 500 (631.017 iter/s, 0.158474s/100 iters), loss = 0.853188
I0428 20:04:45.952318 30451 solver.cpp:238]     Train net output #0: loss = 0.853188 (* 1 = 0.853188 loss)
I0428 20:04:45.952325 30451 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:04:46.041869 30451 solver.cpp:219] Iteration 600 (1116.77 iter/s, 0.0895439s/100 iters), loss = 0.560761
I0428 20:04:46.041923 30451 solver.cpp:238]     Train net output #0: loss = 0.560761 (* 1 = 0.560761 loss)
I0428 20:04:46.041929 30451 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:04:46.124303 30451 solver.cpp:219] Iteration 700 (1213.8 iter/s, 0.0823859s/100 iters), loss = 0.614078
I0428 20:04:46.124343 30451 solver.cpp:238]     Train net output #0: loss = 0.614078 (* 1 = 0.614078 loss)
I0428 20:04:46.124349 30451 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:04:46.203577 30451 solver.cpp:219] Iteration 800 (1261.97 iter/s, 0.0792411s/100 iters), loss = 0.688873
I0428 20:04:46.203616 30451 solver.cpp:238]     Train net output #0: loss = 0.688873 (* 1 = 0.688873 loss)
I0428 20:04:46.203624 30451 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:04:46.282776 30451 solver.cpp:219] Iteration 900 (1263.18 iter/s, 0.0791654s/100 iters), loss = 0.343364
I0428 20:04:46.282819 30451 solver.cpp:238]     Train net output #0: loss = 0.343364 (* 1 = 0.343364 loss)
I0428 20:04:46.282825 30451 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:04:46.309584 30457 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:04:46.361903 30451 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:04:46.362568 30451 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:04:46.363080 30451 solver.cpp:311] Iteration 1000, loss = 0.320336
I0428 20:04:46.363095 30451 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:04:46.416270 30458 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:04:46.416831 30451 solver.cpp:398]     Test net output #0: accuracy = 0.8533
I0428 20:04:46.416865 30451 solver.cpp:398]     Test net output #1: loss = 0.423598 (* 1 = 0.423598 loss)
I0428 20:04:46.416870 30451 solver.cpp:316] Optimization Done.
I0428 20:04:46.416873 30451 caffe.cpp:259] Optimization Done.
