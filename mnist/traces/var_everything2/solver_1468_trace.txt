I0428 20:26:15.418547  2762 caffe.cpp:218] Using GPUs 0
I0428 20:26:15.456317  2762 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:26:15.967067  2762 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1468.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:26:15.967207  2762 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1468.prototxt
I0428 20:26:15.967625  2762 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:26:15.967644  2762 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:26:15.967746  2762 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:26:15.967826  2762 layer_factory.hpp:77] Creating layer mnist
I0428 20:26:15.967927  2762 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:26:15.967950  2762 net.cpp:86] Creating Layer mnist
I0428 20:26:15.967960  2762 net.cpp:382] mnist -> data
I0428 20:26:15.967983  2762 net.cpp:382] mnist -> label
I0428 20:26:15.969086  2762 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:26:15.971542  2762 net.cpp:124] Setting up mnist
I0428 20:26:15.971560  2762 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:26:15.971565  2762 net.cpp:131] Top shape: 64 (64)
I0428 20:26:15.971568  2762 net.cpp:139] Memory required for data: 200960
I0428 20:26:15.971576  2762 layer_factory.hpp:77] Creating layer conv0
I0428 20:26:15.971593  2762 net.cpp:86] Creating Layer conv0
I0428 20:26:15.971613  2762 net.cpp:408] conv0 <- data
I0428 20:26:15.971627  2762 net.cpp:382] conv0 -> conv0
I0428 20:26:16.262555  2762 net.cpp:124] Setting up conv0
I0428 20:26:16.262588  2762 net.cpp:131] Top shape: 64 100 24 24 (3686400)
I0428 20:26:16.262593  2762 net.cpp:139] Memory required for data: 14946560
I0428 20:26:16.262611  2762 layer_factory.hpp:77] Creating layer pool0
I0428 20:26:16.262626  2762 net.cpp:86] Creating Layer pool0
I0428 20:26:16.262631  2762 net.cpp:408] pool0 <- conv0
I0428 20:26:16.262639  2762 net.cpp:382] pool0 -> pool0
I0428 20:26:16.262693  2762 net.cpp:124] Setting up pool0
I0428 20:26:16.262702  2762 net.cpp:131] Top shape: 64 100 12 12 (921600)
I0428 20:26:16.262706  2762 net.cpp:139] Memory required for data: 18632960
I0428 20:26:16.262709  2762 layer_factory.hpp:77] Creating layer conv1
I0428 20:26:16.262722  2762 net.cpp:86] Creating Layer conv1
I0428 20:26:16.262728  2762 net.cpp:408] conv1 <- pool0
I0428 20:26:16.262734  2762 net.cpp:382] conv1 -> conv1
I0428 20:26:16.265858  2762 net.cpp:124] Setting up conv1
I0428 20:26:16.265877  2762 net.cpp:131] Top shape: 64 2 8 8 (8192)
I0428 20:26:16.265882  2762 net.cpp:139] Memory required for data: 18665728
I0428 20:26:16.265897  2762 layer_factory.hpp:77] Creating layer pool1
I0428 20:26:16.265905  2762 net.cpp:86] Creating Layer pool1
I0428 20:26:16.265910  2762 net.cpp:408] pool1 <- conv1
I0428 20:26:16.265916  2762 net.cpp:382] pool1 -> pool1
I0428 20:26:16.265960  2762 net.cpp:124] Setting up pool1
I0428 20:26:16.265969  2762 net.cpp:131] Top shape: 64 2 4 4 (2048)
I0428 20:26:16.265974  2762 net.cpp:139] Memory required for data: 18673920
I0428 20:26:16.265977  2762 layer_factory.hpp:77] Creating layer ip1
I0428 20:26:16.265985  2762 net.cpp:86] Creating Layer ip1
I0428 20:26:16.265991  2762 net.cpp:408] ip1 <- pool1
I0428 20:26:16.265997  2762 net.cpp:382] ip1 -> ip1
I0428 20:26:16.266111  2762 net.cpp:124] Setting up ip1
I0428 20:26:16.266119  2762 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:26:16.266124  2762 net.cpp:139] Memory required for data: 18680320
I0428 20:26:16.266131  2762 layer_factory.hpp:77] Creating layer relu1
I0428 20:26:16.266140  2762 net.cpp:86] Creating Layer relu1
I0428 20:26:16.266144  2762 net.cpp:408] relu1 <- ip1
I0428 20:26:16.266149  2762 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:26:16.266340  2762 net.cpp:124] Setting up relu1
I0428 20:26:16.266350  2762 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:26:16.266355  2762 net.cpp:139] Memory required for data: 18686720
I0428 20:26:16.266357  2762 layer_factory.hpp:77] Creating layer ip2
I0428 20:26:16.266366  2762 net.cpp:86] Creating Layer ip2
I0428 20:26:16.266369  2762 net.cpp:408] ip2 <- ip1
I0428 20:26:16.266376  2762 net.cpp:382] ip2 -> ip2
I0428 20:26:16.266489  2762 net.cpp:124] Setting up ip2
I0428 20:26:16.266496  2762 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:26:16.266500  2762 net.cpp:139] Memory required for data: 18693120
I0428 20:26:16.266506  2762 layer_factory.hpp:77] Creating layer relu2
I0428 20:26:16.266513  2762 net.cpp:86] Creating Layer relu2
I0428 20:26:16.266517  2762 net.cpp:408] relu2 <- ip2
I0428 20:26:16.266522  2762 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:26:16.267357  2762 net.cpp:124] Setting up relu2
I0428 20:26:16.267372  2762 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:26:16.267376  2762 net.cpp:139] Memory required for data: 18699520
I0428 20:26:16.267380  2762 layer_factory.hpp:77] Creating layer ip3
I0428 20:26:16.267390  2762 net.cpp:86] Creating Layer ip3
I0428 20:26:16.267393  2762 net.cpp:408] ip3 <- ip2
I0428 20:26:16.267400  2762 net.cpp:382] ip3 -> ip3
I0428 20:26:16.267518  2762 net.cpp:124] Setting up ip3
I0428 20:26:16.267526  2762 net.cpp:131] Top shape: 64 10 (640)
I0428 20:26:16.267529  2762 net.cpp:139] Memory required for data: 18702080
I0428 20:26:16.267539  2762 layer_factory.hpp:77] Creating layer relu3
I0428 20:26:16.267545  2762 net.cpp:86] Creating Layer relu3
I0428 20:26:16.267549  2762 net.cpp:408] relu3 <- ip3
I0428 20:26:16.267554  2762 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:26:16.267746  2762 net.cpp:124] Setting up relu3
I0428 20:26:16.267757  2762 net.cpp:131] Top shape: 64 10 (640)
I0428 20:26:16.267761  2762 net.cpp:139] Memory required for data: 18704640
I0428 20:26:16.267765  2762 layer_factory.hpp:77] Creating layer loss
I0428 20:26:16.267771  2762 net.cpp:86] Creating Layer loss
I0428 20:26:16.267774  2762 net.cpp:408] loss <- ip3
I0428 20:26:16.267779  2762 net.cpp:408] loss <- label
I0428 20:26:16.267786  2762 net.cpp:382] loss -> loss
I0428 20:26:16.267807  2762 layer_factory.hpp:77] Creating layer loss
I0428 20:26:16.268069  2762 net.cpp:124] Setting up loss
I0428 20:26:16.268079  2762 net.cpp:131] Top shape: (1)
I0428 20:26:16.268084  2762 net.cpp:134]     with loss weight 1
I0428 20:26:16.268101  2762 net.cpp:139] Memory required for data: 18704644
I0428 20:26:16.268105  2762 net.cpp:200] loss needs backward computation.
I0428 20:26:16.268110  2762 net.cpp:200] relu3 needs backward computation.
I0428 20:26:16.268113  2762 net.cpp:200] ip3 needs backward computation.
I0428 20:26:16.268116  2762 net.cpp:200] relu2 needs backward computation.
I0428 20:26:16.268121  2762 net.cpp:200] ip2 needs backward computation.
I0428 20:26:16.268123  2762 net.cpp:200] relu1 needs backward computation.
I0428 20:26:16.268126  2762 net.cpp:200] ip1 needs backward computation.
I0428 20:26:16.268131  2762 net.cpp:200] pool1 needs backward computation.
I0428 20:26:16.268134  2762 net.cpp:200] conv1 needs backward computation.
I0428 20:26:16.268137  2762 net.cpp:200] pool0 needs backward computation.
I0428 20:26:16.268141  2762 net.cpp:200] conv0 needs backward computation.
I0428 20:26:16.268146  2762 net.cpp:202] mnist does not need backward computation.
I0428 20:26:16.268151  2762 net.cpp:244] This network produces output loss
I0428 20:26:16.268162  2762 net.cpp:257] Network initialization done.
I0428 20:26:16.268548  2762 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1468.prototxt
I0428 20:26:16.268580  2762 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:26:16.268689  2762 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:26:16.268785  2762 layer_factory.hpp:77] Creating layer mnist
I0428 20:26:16.268849  2762 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:26:16.268865  2762 net.cpp:86] Creating Layer mnist
I0428 20:26:16.268872  2762 net.cpp:382] mnist -> data
I0428 20:26:16.268882  2762 net.cpp:382] mnist -> label
I0428 20:26:16.268980  2762 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:26:16.271163  2762 net.cpp:124] Setting up mnist
I0428 20:26:16.271179  2762 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:26:16.271185  2762 net.cpp:131] Top shape: 100 (100)
I0428 20:26:16.271189  2762 net.cpp:139] Memory required for data: 314000
I0428 20:26:16.271194  2762 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:26:16.271205  2762 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:26:16.271210  2762 net.cpp:408] label_mnist_1_split <- label
I0428 20:26:16.271216  2762 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:26:16.271224  2762 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:26:16.271332  2762 net.cpp:124] Setting up label_mnist_1_split
I0428 20:26:16.271342  2762 net.cpp:131] Top shape: 100 (100)
I0428 20:26:16.271347  2762 net.cpp:131] Top shape: 100 (100)
I0428 20:26:16.271349  2762 net.cpp:139] Memory required for data: 314800
I0428 20:26:16.271354  2762 layer_factory.hpp:77] Creating layer conv0
I0428 20:26:16.271364  2762 net.cpp:86] Creating Layer conv0
I0428 20:26:16.271368  2762 net.cpp:408] conv0 <- data
I0428 20:26:16.271374  2762 net.cpp:382] conv0 -> conv0
I0428 20:26:16.273303  2762 net.cpp:124] Setting up conv0
I0428 20:26:16.273320  2762 net.cpp:131] Top shape: 100 100 24 24 (5760000)
I0428 20:26:16.273324  2762 net.cpp:139] Memory required for data: 23354800
I0428 20:26:16.273335  2762 layer_factory.hpp:77] Creating layer pool0
I0428 20:26:16.273344  2762 net.cpp:86] Creating Layer pool0
I0428 20:26:16.273349  2762 net.cpp:408] pool0 <- conv0
I0428 20:26:16.273355  2762 net.cpp:382] pool0 -> pool0
I0428 20:26:16.273398  2762 net.cpp:124] Setting up pool0
I0428 20:26:16.273406  2762 net.cpp:131] Top shape: 100 100 12 12 (1440000)
I0428 20:26:16.273408  2762 net.cpp:139] Memory required for data: 29114800
I0428 20:26:16.273412  2762 layer_factory.hpp:77] Creating layer conv1
I0428 20:26:16.273423  2762 net.cpp:86] Creating Layer conv1
I0428 20:26:16.273427  2762 net.cpp:408] conv1 <- pool0
I0428 20:26:16.273433  2762 net.cpp:382] conv1 -> conv1
I0428 20:26:16.275203  2762 net.cpp:124] Setting up conv1
I0428 20:26:16.275220  2762 net.cpp:131] Top shape: 100 2 8 8 (12800)
I0428 20:26:16.275225  2762 net.cpp:139] Memory required for data: 29166000
I0428 20:26:16.275235  2762 layer_factory.hpp:77] Creating layer pool1
I0428 20:26:16.275244  2762 net.cpp:86] Creating Layer pool1
I0428 20:26:16.275256  2762 net.cpp:408] pool1 <- conv1
I0428 20:26:16.275262  2762 net.cpp:382] pool1 -> pool1
I0428 20:26:16.275307  2762 net.cpp:124] Setting up pool1
I0428 20:26:16.275315  2762 net.cpp:131] Top shape: 100 2 4 4 (3200)
I0428 20:26:16.275317  2762 net.cpp:139] Memory required for data: 29178800
I0428 20:26:16.275321  2762 layer_factory.hpp:77] Creating layer ip1
I0428 20:26:16.275329  2762 net.cpp:86] Creating Layer ip1
I0428 20:26:16.275333  2762 net.cpp:408] ip1 <- pool1
I0428 20:26:16.275338  2762 net.cpp:382] ip1 -> ip1
I0428 20:26:16.275460  2762 net.cpp:124] Setting up ip1
I0428 20:26:16.275470  2762 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:26:16.275486  2762 net.cpp:139] Memory required for data: 29188800
I0428 20:26:16.275496  2762 layer_factory.hpp:77] Creating layer relu1
I0428 20:26:16.275504  2762 net.cpp:86] Creating Layer relu1
I0428 20:26:16.275508  2762 net.cpp:408] relu1 <- ip1
I0428 20:26:16.275513  2762 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:26:16.275725  2762 net.cpp:124] Setting up relu1
I0428 20:26:16.275737  2762 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:26:16.275739  2762 net.cpp:139] Memory required for data: 29198800
I0428 20:26:16.275743  2762 layer_factory.hpp:77] Creating layer ip2
I0428 20:26:16.275754  2762 net.cpp:86] Creating Layer ip2
I0428 20:26:16.275758  2762 net.cpp:408] ip2 <- ip1
I0428 20:26:16.275765  2762 net.cpp:382] ip2 -> ip2
I0428 20:26:16.275907  2762 net.cpp:124] Setting up ip2
I0428 20:26:16.275914  2762 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:26:16.275918  2762 net.cpp:139] Memory required for data: 29208800
I0428 20:26:16.275924  2762 layer_factory.hpp:77] Creating layer relu2
I0428 20:26:16.275930  2762 net.cpp:86] Creating Layer relu2
I0428 20:26:16.275933  2762 net.cpp:408] relu2 <- ip2
I0428 20:26:16.275938  2762 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:26:16.276160  2762 net.cpp:124] Setting up relu2
I0428 20:26:16.276175  2762 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:26:16.276178  2762 net.cpp:139] Memory required for data: 29218800
I0428 20:26:16.276182  2762 layer_factory.hpp:77] Creating layer ip3
I0428 20:26:16.276188  2762 net.cpp:86] Creating Layer ip3
I0428 20:26:16.276192  2762 net.cpp:408] ip3 <- ip2
I0428 20:26:16.276199  2762 net.cpp:382] ip3 -> ip3
I0428 20:26:16.276317  2762 net.cpp:124] Setting up ip3
I0428 20:26:16.276326  2762 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:26:16.276329  2762 net.cpp:139] Memory required for data: 29222800
I0428 20:26:16.276340  2762 layer_factory.hpp:77] Creating layer relu3
I0428 20:26:16.276346  2762 net.cpp:86] Creating Layer relu3
I0428 20:26:16.276350  2762 net.cpp:408] relu3 <- ip3
I0428 20:26:16.276356  2762 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:26:16.277314  2762 net.cpp:124] Setting up relu3
I0428 20:26:16.277329  2762 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:26:16.277333  2762 net.cpp:139] Memory required for data: 29226800
I0428 20:26:16.277338  2762 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:26:16.277343  2762 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:26:16.277348  2762 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:26:16.277354  2762 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:26:16.277364  2762 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:26:16.277415  2762 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:26:16.277421  2762 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:26:16.277426  2762 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:26:16.277436  2762 net.cpp:139] Memory required for data: 29234800
I0428 20:26:16.277438  2762 layer_factory.hpp:77] Creating layer accuracy
I0428 20:26:16.277446  2762 net.cpp:86] Creating Layer accuracy
I0428 20:26:16.277451  2762 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:26:16.277456  2762 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:26:16.277460  2762 net.cpp:382] accuracy -> accuracy
I0428 20:26:16.277470  2762 net.cpp:124] Setting up accuracy
I0428 20:26:16.277475  2762 net.cpp:131] Top shape: (1)
I0428 20:26:16.277478  2762 net.cpp:139] Memory required for data: 29234804
I0428 20:26:16.277482  2762 layer_factory.hpp:77] Creating layer loss
I0428 20:26:16.277487  2762 net.cpp:86] Creating Layer loss
I0428 20:26:16.277492  2762 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:26:16.277496  2762 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:26:16.277501  2762 net.cpp:382] loss -> loss
I0428 20:26:16.277508  2762 layer_factory.hpp:77] Creating layer loss
I0428 20:26:16.277801  2762 net.cpp:124] Setting up loss
I0428 20:26:16.277812  2762 net.cpp:131] Top shape: (1)
I0428 20:26:16.277815  2762 net.cpp:134]     with loss weight 1
I0428 20:26:16.277833  2762 net.cpp:139] Memory required for data: 29234808
I0428 20:26:16.277839  2762 net.cpp:200] loss needs backward computation.
I0428 20:26:16.277844  2762 net.cpp:202] accuracy does not need backward computation.
I0428 20:26:16.277848  2762 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:26:16.277851  2762 net.cpp:200] relu3 needs backward computation.
I0428 20:26:16.277855  2762 net.cpp:200] ip3 needs backward computation.
I0428 20:26:16.277858  2762 net.cpp:200] relu2 needs backward computation.
I0428 20:26:16.277868  2762 net.cpp:200] ip2 needs backward computation.
I0428 20:26:16.277873  2762 net.cpp:200] relu1 needs backward computation.
I0428 20:26:16.277875  2762 net.cpp:200] ip1 needs backward computation.
I0428 20:26:16.277879  2762 net.cpp:200] pool1 needs backward computation.
I0428 20:26:16.277884  2762 net.cpp:200] conv1 needs backward computation.
I0428 20:26:16.277887  2762 net.cpp:200] pool0 needs backward computation.
I0428 20:26:16.277896  2762 net.cpp:200] conv0 needs backward computation.
I0428 20:26:16.277900  2762 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:26:16.277904  2762 net.cpp:202] mnist does not need backward computation.
I0428 20:26:16.277907  2762 net.cpp:244] This network produces output accuracy
I0428 20:26:16.277911  2762 net.cpp:244] This network produces output loss
I0428 20:26:16.277925  2762 net.cpp:257] Network initialization done.
I0428 20:26:16.277976  2762 solver.cpp:56] Solver scaffolding done.
I0428 20:26:16.278400  2762 caffe.cpp:248] Starting Optimization
I0428 20:26:16.278408  2762 solver.cpp:273] Solving LeNet
I0428 20:26:16.278410  2762 solver.cpp:274] Learning Rate Policy: inv
I0428 20:26:16.279377  2762 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:26:16.377897  2769 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:26:16.380542  2762 solver.cpp:398]     Test net output #0: accuracy = 0.1224
I0428 20:26:16.380563  2762 solver.cpp:398]     Test net output #1: loss = 2.29607 (* 1 = 2.29607 loss)
I0428 20:26:16.385268  2762 solver.cpp:219] Iteration 0 (0 iter/s, 0.106824s/100 iters), loss = 2.29272
I0428 20:26:16.385298  2762 solver.cpp:238]     Train net output #0: loss = 2.29272 (* 1 = 2.29272 loss)
I0428 20:26:16.385311  2762 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:26:16.593135  2762 solver.cpp:219] Iteration 100 (481.196 iter/s, 0.207816s/100 iters), loss = 0.809809
I0428 20:26:16.593183  2762 solver.cpp:238]     Train net output #0: loss = 0.809809 (* 1 = 0.809809 loss)
I0428 20:26:16.593199  2762 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:26:16.806869  2762 solver.cpp:219] Iteration 200 (468.007 iter/s, 0.213672s/100 iters), loss = 0.63136
I0428 20:26:16.806913  2762 solver.cpp:238]     Train net output #0: loss = 0.63136 (* 1 = 0.63136 loss)
I0428 20:26:16.806932  2762 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:26:17.021143  2762 solver.cpp:219] Iteration 300 (466.824 iter/s, 0.214214s/100 iters), loss = 0.551645
I0428 20:26:17.021184  2762 solver.cpp:238]     Train net output #0: loss = 0.551645 (* 1 = 0.551645 loss)
I0428 20:26:17.021195  2762 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:26:17.226894  2762 solver.cpp:219] Iteration 400 (486.159 iter/s, 0.205694s/100 iters), loss = 0.199098
I0428 20:26:17.226929  2762 solver.cpp:238]     Train net output #0: loss = 0.199098 (* 1 = 0.199098 loss)
I0428 20:26:17.226938  2762 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:26:17.435192  2762 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:26:17.541687  2769 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:26:17.545320  2762 solver.cpp:398]     Test net output #0: accuracy = 0.9369
I0428 20:26:17.545347  2762 solver.cpp:398]     Test net output #1: loss = 0.191111 (* 1 = 0.191111 loss)
I0428 20:26:17.547297  2762 solver.cpp:219] Iteration 500 (312.161 iter/s, 0.320347s/100 iters), loss = 0.203103
I0428 20:26:17.547325  2762 solver.cpp:238]     Train net output #0: loss = 0.203103 (* 1 = 0.203103 loss)
I0428 20:26:17.547356  2762 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:26:17.751116  2762 solver.cpp:219] Iteration 600 (490.735 iter/s, 0.203776s/100 iters), loss = 0.171067
I0428 20:26:17.751147  2762 solver.cpp:238]     Train net output #0: loss = 0.171067 (* 1 = 0.171067 loss)
I0428 20:26:17.751154  2762 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:26:17.961544  2762 solver.cpp:219] Iteration 700 (475.333 iter/s, 0.210379s/100 iters), loss = 0.227332
I0428 20:26:17.961583  2762 solver.cpp:238]     Train net output #0: loss = 0.227332 (* 1 = 0.227332 loss)
I0428 20:26:17.961593  2762 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:26:18.168802  2762 solver.cpp:219] Iteration 800 (482.623 iter/s, 0.207201s/100 iters), loss = 0.22574
I0428 20:26:18.168862  2762 solver.cpp:238]     Train net output #0: loss = 0.225739 (* 1 = 0.225739 loss)
I0428 20:26:18.168879  2762 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:26:18.382781  2762 solver.cpp:219] Iteration 900 (467.5 iter/s, 0.213904s/100 iters), loss = 0.184448
I0428 20:26:18.382822  2762 solver.cpp:238]     Train net output #0: loss = 0.184447 (* 1 = 0.184447 loss)
I0428 20:26:18.382833  2762 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:26:18.452342  2768 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:26:18.588295  2762 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:26:18.589938  2762 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:26:18.591097  2762 solver.cpp:311] Iteration 1000, loss = 0.101262
I0428 20:26:18.591135  2762 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:26:18.698902  2769 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:26:18.701800  2762 solver.cpp:398]     Test net output #0: accuracy = 0.9625
I0428 20:26:18.701830  2762 solver.cpp:398]     Test net output #1: loss = 0.115499 (* 1 = 0.115499 loss)
I0428 20:26:18.701840  2762 solver.cpp:316] Optimization Done.
I0428 20:26:18.701845  2762 caffe.cpp:259] Optimization Done.
