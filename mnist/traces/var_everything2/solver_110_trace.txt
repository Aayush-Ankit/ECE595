I0428 19:31:02.908740 22477 caffe.cpp:218] Using GPUs 0
I0428 19:31:02.950351 22477 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:31:03.462718 22477 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test110.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:31:03.462888 22477 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test110.prototxt
I0428 19:31:03.463265 22477 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:31:03.463290 22477 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:31:03.463392 22477 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool0"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:31:03.463500 22477 layer_factory.hpp:77] Creating layer mnist
I0428 19:31:03.463637 22477 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:31:03.463670 22477 net.cpp:86] Creating Layer mnist
I0428 19:31:03.463685 22477 net.cpp:382] mnist -> data
I0428 19:31:03.463716 22477 net.cpp:382] mnist -> label
I0428 19:31:03.464951 22477 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:31:03.467653 22477 net.cpp:124] Setting up mnist
I0428 19:31:03.467674 22477 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:31:03.467685 22477 net.cpp:131] Top shape: 64 (64)
I0428 19:31:03.467692 22477 net.cpp:139] Memory required for data: 200960
I0428 19:31:03.467702 22477 layer_factory.hpp:77] Creating layer conv0
I0428 19:31:03.467728 22477 net.cpp:86] Creating Layer conv0
I0428 19:31:03.467739 22477 net.cpp:408] conv0 <- data
I0428 19:31:03.467759 22477 net.cpp:382] conv0 -> conv0
I0428 19:31:03.708987 22477 net.cpp:124] Setting up conv0
I0428 19:31:03.709017 22477 net.cpp:131] Top shape: 64 5 24 24 (184320)
I0428 19:31:03.709023 22477 net.cpp:139] Memory required for data: 938240
I0428 19:31:03.709059 22477 layer_factory.hpp:77] Creating layer pool0
I0428 19:31:03.709079 22477 net.cpp:86] Creating Layer pool0
I0428 19:31:03.709103 22477 net.cpp:408] pool0 <- conv0
I0428 19:31:03.709113 22477 net.cpp:382] pool0 -> pool0
I0428 19:31:03.709179 22477 net.cpp:124] Setting up pool0
I0428 19:31:03.709187 22477 net.cpp:131] Top shape: 64 5 12 12 (46080)
I0428 19:31:03.709192 22477 net.cpp:139] Memory required for data: 1122560
I0428 19:31:03.709197 22477 layer_factory.hpp:77] Creating layer ip1
I0428 19:31:03.709208 22477 net.cpp:86] Creating Layer ip1
I0428 19:31:03.709216 22477 net.cpp:408] ip1 <- pool0
I0428 19:31:03.709224 22477 net.cpp:382] ip1 -> ip1
I0428 19:31:03.709498 22477 net.cpp:124] Setting up ip1
I0428 19:31:03.709507 22477 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:31:03.709512 22477 net.cpp:139] Memory required for data: 1135360
I0428 19:31:03.709522 22477 layer_factory.hpp:77] Creating layer relu1
I0428 19:31:03.709535 22477 net.cpp:86] Creating Layer relu1
I0428 19:31:03.709542 22477 net.cpp:408] relu1 <- ip1
I0428 19:31:03.709547 22477 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:31:03.709693 22477 net.cpp:124] Setting up relu1
I0428 19:31:03.709702 22477 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:31:03.709707 22477 net.cpp:139] Memory required for data: 1148160
I0428 19:31:03.709712 22477 layer_factory.hpp:77] Creating layer ip2
I0428 19:31:03.709722 22477 net.cpp:86] Creating Layer ip2
I0428 19:31:03.709727 22477 net.cpp:408] ip2 <- ip1
I0428 19:31:03.709733 22477 net.cpp:382] ip2 -> ip2
I0428 19:31:03.709837 22477 net.cpp:124] Setting up ip2
I0428 19:31:03.709846 22477 net.cpp:131] Top shape: 64 10 (640)
I0428 19:31:03.709849 22477 net.cpp:139] Memory required for data: 1150720
I0428 19:31:03.709877 22477 layer_factory.hpp:77] Creating layer relu2
I0428 19:31:03.709885 22477 net.cpp:86] Creating Layer relu2
I0428 19:31:03.709890 22477 net.cpp:408] relu2 <- ip2
I0428 19:31:03.709897 22477 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:31:03.710702 22477 net.cpp:124] Setting up relu2
I0428 19:31:03.710716 22477 net.cpp:131] Top shape: 64 10 (640)
I0428 19:31:03.710721 22477 net.cpp:139] Memory required for data: 1153280
I0428 19:31:03.710726 22477 layer_factory.hpp:77] Creating layer ip3
I0428 19:31:03.710737 22477 net.cpp:86] Creating Layer ip3
I0428 19:31:03.710743 22477 net.cpp:408] ip3 <- ip2
I0428 19:31:03.710752 22477 net.cpp:382] ip3 -> ip3
I0428 19:31:03.710850 22477 net.cpp:124] Setting up ip3
I0428 19:31:03.710860 22477 net.cpp:131] Top shape: 64 10 (640)
I0428 19:31:03.710865 22477 net.cpp:139] Memory required for data: 1155840
I0428 19:31:03.710873 22477 layer_factory.hpp:77] Creating layer relu3
I0428 19:31:03.710886 22477 net.cpp:86] Creating Layer relu3
I0428 19:31:03.710891 22477 net.cpp:408] relu3 <- ip3
I0428 19:31:03.710897 22477 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:31:03.711066 22477 net.cpp:124] Setting up relu3
I0428 19:31:03.711076 22477 net.cpp:131] Top shape: 64 10 (640)
I0428 19:31:03.711081 22477 net.cpp:139] Memory required for data: 1158400
I0428 19:31:03.711086 22477 layer_factory.hpp:77] Creating layer loss
I0428 19:31:03.711100 22477 net.cpp:86] Creating Layer loss
I0428 19:31:03.711107 22477 net.cpp:408] loss <- ip3
I0428 19:31:03.711113 22477 net.cpp:408] loss <- label
I0428 19:31:03.711122 22477 net.cpp:382] loss -> loss
I0428 19:31:03.711140 22477 layer_factory.hpp:77] Creating layer loss
I0428 19:31:03.711400 22477 net.cpp:124] Setting up loss
I0428 19:31:03.711411 22477 net.cpp:131] Top shape: (1)
I0428 19:31:03.711416 22477 net.cpp:134]     with loss weight 1
I0428 19:31:03.711434 22477 net.cpp:139] Memory required for data: 1158404
I0428 19:31:03.711441 22477 net.cpp:200] loss needs backward computation.
I0428 19:31:03.711447 22477 net.cpp:200] relu3 needs backward computation.
I0428 19:31:03.711452 22477 net.cpp:200] ip3 needs backward computation.
I0428 19:31:03.711457 22477 net.cpp:200] relu2 needs backward computation.
I0428 19:31:03.711460 22477 net.cpp:200] ip2 needs backward computation.
I0428 19:31:03.711465 22477 net.cpp:200] relu1 needs backward computation.
I0428 19:31:03.711470 22477 net.cpp:200] ip1 needs backward computation.
I0428 19:31:03.711485 22477 net.cpp:200] pool0 needs backward computation.
I0428 19:31:03.711491 22477 net.cpp:200] conv0 needs backward computation.
I0428 19:31:03.711496 22477 net.cpp:202] mnist does not need backward computation.
I0428 19:31:03.711501 22477 net.cpp:244] This network produces output loss
I0428 19:31:03.711515 22477 net.cpp:257] Network initialization done.
I0428 19:31:03.711810 22477 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test110.prototxt
I0428 19:31:03.711840 22477 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:31:03.711920 22477 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool0"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:31:03.712049 22477 layer_factory.hpp:77] Creating layer mnist
I0428 19:31:03.712108 22477 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:31:03.712124 22477 net.cpp:86] Creating Layer mnist
I0428 19:31:03.712133 22477 net.cpp:382] mnist -> data
I0428 19:31:03.712146 22477 net.cpp:382] mnist -> label
I0428 19:31:03.712261 22477 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:31:03.714429 22477 net.cpp:124] Setting up mnist
I0428 19:31:03.714445 22477 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:31:03.714454 22477 net.cpp:131] Top shape: 100 (100)
I0428 19:31:03.714459 22477 net.cpp:139] Memory required for data: 314000
I0428 19:31:03.714464 22477 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:31:03.714474 22477 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:31:03.714480 22477 net.cpp:408] label_mnist_1_split <- label
I0428 19:31:03.714490 22477 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:31:03.714501 22477 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:31:03.714553 22477 net.cpp:124] Setting up label_mnist_1_split
I0428 19:31:03.714561 22477 net.cpp:131] Top shape: 100 (100)
I0428 19:31:03.714568 22477 net.cpp:131] Top shape: 100 (100)
I0428 19:31:03.714573 22477 net.cpp:139] Memory required for data: 314800
I0428 19:31:03.714589 22477 layer_factory.hpp:77] Creating layer conv0
I0428 19:31:03.714606 22477 net.cpp:86] Creating Layer conv0
I0428 19:31:03.714612 22477 net.cpp:408] conv0 <- data
I0428 19:31:03.714622 22477 net.cpp:382] conv0 -> conv0
I0428 19:31:03.716125 22477 net.cpp:124] Setting up conv0
I0428 19:31:03.716140 22477 net.cpp:131] Top shape: 100 5 24 24 (288000)
I0428 19:31:03.716146 22477 net.cpp:139] Memory required for data: 1466800
I0428 19:31:03.716159 22477 layer_factory.hpp:77] Creating layer pool0
I0428 19:31:03.716169 22477 net.cpp:86] Creating Layer pool0
I0428 19:31:03.716190 22477 net.cpp:408] pool0 <- conv0
I0428 19:31:03.716200 22477 net.cpp:382] pool0 -> pool0
I0428 19:31:03.716243 22477 net.cpp:124] Setting up pool0
I0428 19:31:03.716250 22477 net.cpp:131] Top shape: 100 5 12 12 (72000)
I0428 19:31:03.716255 22477 net.cpp:139] Memory required for data: 1754800
I0428 19:31:03.716259 22477 layer_factory.hpp:77] Creating layer ip1
I0428 19:31:03.716272 22477 net.cpp:86] Creating Layer ip1
I0428 19:31:03.716279 22477 net.cpp:408] ip1 <- pool0
I0428 19:31:03.716287 22477 net.cpp:382] ip1 -> ip1
I0428 19:31:03.716672 22477 net.cpp:124] Setting up ip1
I0428 19:31:03.716683 22477 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:31:03.716688 22477 net.cpp:139] Memory required for data: 1774800
I0428 19:31:03.716701 22477 layer_factory.hpp:77] Creating layer relu1
I0428 19:31:03.716709 22477 net.cpp:86] Creating Layer relu1
I0428 19:31:03.716717 22477 net.cpp:408] relu1 <- ip1
I0428 19:31:03.716728 22477 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:31:03.716928 22477 net.cpp:124] Setting up relu1
I0428 19:31:03.716939 22477 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:31:03.716945 22477 net.cpp:139] Memory required for data: 1794800
I0428 19:31:03.716951 22477 layer_factory.hpp:77] Creating layer ip2
I0428 19:31:03.716965 22477 net.cpp:86] Creating Layer ip2
I0428 19:31:03.716972 22477 net.cpp:408] ip2 <- ip1
I0428 19:31:03.716980 22477 net.cpp:382] ip2 -> ip2
I0428 19:31:03.717111 22477 net.cpp:124] Setting up ip2
I0428 19:31:03.717135 22477 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:31:03.717139 22477 net.cpp:139] Memory required for data: 1798800
I0428 19:31:03.717152 22477 layer_factory.hpp:77] Creating layer relu2
I0428 19:31:03.717178 22477 net.cpp:86] Creating Layer relu2
I0428 19:31:03.717185 22477 net.cpp:408] relu2 <- ip2
I0428 19:31:03.717192 22477 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:31:03.718014 22477 net.cpp:124] Setting up relu2
I0428 19:31:03.718029 22477 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:31:03.718055 22477 net.cpp:139] Memory required for data: 1802800
I0428 19:31:03.718060 22477 layer_factory.hpp:77] Creating layer ip3
I0428 19:31:03.718073 22477 net.cpp:86] Creating Layer ip3
I0428 19:31:03.718080 22477 net.cpp:408] ip3 <- ip2
I0428 19:31:03.718092 22477 net.cpp:382] ip3 -> ip3
I0428 19:31:03.718221 22477 net.cpp:124] Setting up ip3
I0428 19:31:03.718231 22477 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:31:03.718236 22477 net.cpp:139] Memory required for data: 1806800
I0428 19:31:03.718245 22477 layer_factory.hpp:77] Creating layer relu3
I0428 19:31:03.718255 22477 net.cpp:86] Creating Layer relu3
I0428 19:31:03.718260 22477 net.cpp:408] relu3 <- ip3
I0428 19:31:03.718266 22477 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:31:03.718421 22477 net.cpp:124] Setting up relu3
I0428 19:31:03.718430 22477 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:31:03.718436 22477 net.cpp:139] Memory required for data: 1810800
I0428 19:31:03.718441 22477 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:31:03.718447 22477 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:31:03.718453 22477 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:31:03.718461 22477 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:31:03.718468 22477 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:31:03.718508 22477 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:31:03.718516 22477 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:31:03.718538 22477 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:31:03.718554 22477 net.cpp:139] Memory required for data: 1818800
I0428 19:31:03.718559 22477 layer_factory.hpp:77] Creating layer accuracy
I0428 19:31:03.718575 22477 net.cpp:86] Creating Layer accuracy
I0428 19:31:03.718582 22477 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:31:03.718605 22477 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:31:03.718613 22477 net.cpp:382] accuracy -> accuracy
I0428 19:31:03.718624 22477 net.cpp:124] Setting up accuracy
I0428 19:31:03.718633 22477 net.cpp:131] Top shape: (1)
I0428 19:31:03.718638 22477 net.cpp:139] Memory required for data: 1818804
I0428 19:31:03.718643 22477 layer_factory.hpp:77] Creating layer loss
I0428 19:31:03.718652 22477 net.cpp:86] Creating Layer loss
I0428 19:31:03.718658 22477 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:31:03.718665 22477 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:31:03.718673 22477 net.cpp:382] loss -> loss
I0428 19:31:03.718683 22477 layer_factory.hpp:77] Creating layer loss
I0428 19:31:03.718945 22477 net.cpp:124] Setting up loss
I0428 19:31:03.718955 22477 net.cpp:131] Top shape: (1)
I0428 19:31:03.718961 22477 net.cpp:134]     with loss weight 1
I0428 19:31:03.718969 22477 net.cpp:139] Memory required for data: 1818808
I0428 19:31:03.718974 22477 net.cpp:200] loss needs backward computation.
I0428 19:31:03.718981 22477 net.cpp:202] accuracy does not need backward computation.
I0428 19:31:03.718986 22477 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:31:03.718991 22477 net.cpp:200] relu3 needs backward computation.
I0428 19:31:03.718997 22477 net.cpp:200] ip3 needs backward computation.
I0428 19:31:03.719002 22477 net.cpp:200] relu2 needs backward computation.
I0428 19:31:03.719007 22477 net.cpp:200] ip2 needs backward computation.
I0428 19:31:03.719012 22477 net.cpp:200] relu1 needs backward computation.
I0428 19:31:03.719017 22477 net.cpp:200] ip1 needs backward computation.
I0428 19:31:03.719022 22477 net.cpp:200] pool0 needs backward computation.
I0428 19:31:03.719027 22477 net.cpp:200] conv0 needs backward computation.
I0428 19:31:03.719033 22477 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:31:03.719038 22477 net.cpp:202] mnist does not need backward computation.
I0428 19:31:03.719043 22477 net.cpp:244] This network produces output accuracy
I0428 19:31:03.719048 22477 net.cpp:244] This network produces output loss
I0428 19:31:03.719065 22477 net.cpp:257] Network initialization done.
I0428 19:31:03.719108 22477 solver.cpp:56] Solver scaffolding done.
I0428 19:31:03.719379 22477 caffe.cpp:248] Starting Optimization
I0428 19:31:03.719386 22477 solver.cpp:273] Solving LeNet
I0428 19:31:03.719390 22477 solver.cpp:274] Learning Rate Policy: inv
I0428 19:31:03.720185 22477 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:31:03.723428 22477 blocking_queue.cpp:49] Waiting for data
I0428 19:31:03.796438 22484 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:31:03.796918 22477 solver.cpp:398]     Test net output #0: accuracy = 0.0513
I0428 19:31:03.796949 22477 solver.cpp:398]     Test net output #1: loss = 2.32423 (* 1 = 2.32423 loss)
I0428 19:31:03.799434 22477 solver.cpp:219] Iteration 0 (-1.20086e-31 iter/s, 0.0800119s/100 iters), loss = 2.32164
I0428 19:31:03.799487 22477 solver.cpp:238]     Train net output #0: loss = 2.32164 (* 1 = 2.32164 loss)
I0428 19:31:03.799512 22477 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:31:03.867559 22477 solver.cpp:219] Iteration 100 (1469.1 iter/s, 0.0680688s/100 iters), loss = 1.07269
I0428 19:31:03.867593 22477 solver.cpp:238]     Train net output #0: loss = 1.07269 (* 1 = 1.07269 loss)
I0428 19:31:03.867604 22477 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:31:03.928320 22477 solver.cpp:219] Iteration 200 (1646.82 iter/s, 0.0607232s/100 iters), loss = 0.765552
I0428 19:31:03.928371 22477 solver.cpp:238]     Train net output #0: loss = 0.765552 (* 1 = 0.765552 loss)
I0428 19:31:03.928385 22477 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:31:03.987110 22477 solver.cpp:219] Iteration 300 (1702.08 iter/s, 0.0587515s/100 iters), loss = 0.64466
I0428 19:31:03.987154 22477 solver.cpp:238]     Train net output #0: loss = 0.64466 (* 1 = 0.64466 loss)
I0428 19:31:03.987164 22477 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:31:04.045007 22477 solver.cpp:219] Iteration 400 (1728.67 iter/s, 0.0578478s/100 iters), loss = 0.531025
I0428 19:31:04.045035 22477 solver.cpp:238]     Train net output #0: loss = 0.531025 (* 1 = 0.531025 loss)
I0428 19:31:04.045060 22477 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:31:04.113741 22477 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:31:04.168504 22484 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:31:04.168926 22477 solver.cpp:398]     Test net output #0: accuracy = 0.7625
I0428 19:31:04.168946 22477 solver.cpp:398]     Test net output #1: loss = 0.649815 (* 1 = 0.649815 loss)
I0428 19:31:04.169591 22477 solver.cpp:219] Iteration 500 (802.909 iter/s, 0.124547s/100 iters), loss = 0.842549
I0428 19:31:04.169632 22477 solver.cpp:238]     Train net output #0: loss = 0.842549 (* 1 = 0.842549 loss)
I0428 19:31:04.169649 22477 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:31:04.239094 22477 solver.cpp:219] Iteration 600 (1439.45 iter/s, 0.0694711s/100 iters), loss = 0.588829
I0428 19:31:04.239123 22477 solver.cpp:238]     Train net output #0: loss = 0.588829 (* 1 = 0.588829 loss)
I0428 19:31:04.239147 22477 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:31:04.300866 22477 solver.cpp:219] Iteration 700 (1619.78 iter/s, 0.061737s/100 iters), loss = 0.894188
I0428 19:31:04.300894 22477 solver.cpp:238]     Train net output #0: loss = 0.894188 (* 1 = 0.894188 loss)
I0428 19:31:04.300918 22477 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:31:04.358991 22477 solver.cpp:219] Iteration 800 (1721.39 iter/s, 0.0580925s/100 iters), loss = 0.72465
I0428 19:31:04.359019 22477 solver.cpp:238]     Train net output #0: loss = 0.72465 (* 1 = 0.72465 loss)
I0428 19:31:04.359042 22477 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:31:04.424245 22477 solver.cpp:219] Iteration 900 (1533.23 iter/s, 0.0652216s/100 iters), loss = 0.697043
I0428 19:31:04.424273 22477 solver.cpp:238]     Train net output #0: loss = 0.697043 (* 1 = 0.697043 loss)
I0428 19:31:04.424298 22477 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:31:04.447644 22483 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:31:04.487543 22477 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:31:04.488598 22477 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:31:04.489264 22477 solver.cpp:311] Iteration 1000, loss = 0.675092
I0428 19:31:04.489280 22477 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:31:04.541709 22484 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:31:04.542124 22477 solver.cpp:398]     Test net output #0: accuracy = 0.7818
I0428 19:31:04.542145 22477 solver.cpp:398]     Test net output #1: loss = 0.571813 (* 1 = 0.571813 loss)
I0428 19:31:04.542152 22477 solver.cpp:316] Optimization Done.
I0428 19:31:04.542157 22477 caffe.cpp:259] Optimization Done.
