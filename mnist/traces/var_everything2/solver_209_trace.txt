I0428 19:34:41.136044 23373 caffe.cpp:218] Using GPUs 0
I0428 19:34:41.176440 23373 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:34:41.679999 23373 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test209.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:34:41.680162 23373 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test209.prototxt
I0428 19:34:41.680528 23373 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:34:41.680544 23373 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:34:41.680632 23373 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool0"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:34:41.680707 23373 layer_factory.hpp:77] Creating layer mnist
I0428 19:34:41.680809 23373 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:34:41.680850 23373 net.cpp:86] Creating Layer mnist
I0428 19:34:41.680861 23373 net.cpp:382] mnist -> data
I0428 19:34:41.680891 23373 net.cpp:382] mnist -> label
I0428 19:34:41.681988 23373 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:34:41.684438 23373 net.cpp:124] Setting up mnist
I0428 19:34:41.684456 23373 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:34:41.684464 23373 net.cpp:131] Top shape: 64 (64)
I0428 19:34:41.684468 23373 net.cpp:139] Memory required for data: 200960
I0428 19:34:41.684474 23373 layer_factory.hpp:77] Creating layer conv0
I0428 19:34:41.684490 23373 net.cpp:86] Creating Layer conv0
I0428 19:34:41.684496 23373 net.cpp:408] conv0 <- data
I0428 19:34:41.684509 23373 net.cpp:382] conv0 -> conv0
I0428 19:34:41.976176 23373 net.cpp:124] Setting up conv0
I0428 19:34:41.976207 23373 net.cpp:131] Top shape: 64 50 24 24 (1843200)
I0428 19:34:41.976212 23373 net.cpp:139] Memory required for data: 7573760
I0428 19:34:41.976231 23373 layer_factory.hpp:77] Creating layer pool0
I0428 19:34:41.976245 23373 net.cpp:86] Creating Layer pool0
I0428 19:34:41.976274 23373 net.cpp:408] pool0 <- conv0
I0428 19:34:41.976280 23373 net.cpp:382] pool0 -> pool0
I0428 19:34:41.976336 23373 net.cpp:124] Setting up pool0
I0428 19:34:41.976346 23373 net.cpp:131] Top shape: 64 50 12 12 (460800)
I0428 19:34:41.976351 23373 net.cpp:139] Memory required for data: 9416960
I0428 19:34:41.976354 23373 layer_factory.hpp:77] Creating layer ip1
I0428 19:34:41.976363 23373 net.cpp:86] Creating Layer ip1
I0428 19:34:41.976366 23373 net.cpp:408] ip1 <- pool0
I0428 19:34:41.976372 23373 net.cpp:382] ip1 -> ip1
I0428 19:34:41.977942 23373 net.cpp:124] Setting up ip1
I0428 19:34:41.977958 23373 net.cpp:131] Top shape: 64 10 (640)
I0428 19:34:41.977962 23373 net.cpp:139] Memory required for data: 9419520
I0428 19:34:41.977972 23373 layer_factory.hpp:77] Creating layer relu1
I0428 19:34:41.977980 23373 net.cpp:86] Creating Layer relu1
I0428 19:34:41.977984 23373 net.cpp:408] relu1 <- ip1
I0428 19:34:41.977990 23373 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:34:41.978193 23373 net.cpp:124] Setting up relu1
I0428 19:34:41.978204 23373 net.cpp:131] Top shape: 64 10 (640)
I0428 19:34:41.978207 23373 net.cpp:139] Memory required for data: 9422080
I0428 19:34:41.978211 23373 layer_factory.hpp:77] Creating layer ip2
I0428 19:34:41.978219 23373 net.cpp:86] Creating Layer ip2
I0428 19:34:41.978224 23373 net.cpp:408] ip2 <- ip1
I0428 19:34:41.978229 23373 net.cpp:382] ip2 -> ip2
I0428 19:34:41.978335 23373 net.cpp:124] Setting up ip2
I0428 19:34:41.978343 23373 net.cpp:131] Top shape: 64 10 (640)
I0428 19:34:41.978348 23373 net.cpp:139] Memory required for data: 9424640
I0428 19:34:41.978356 23373 layer_factory.hpp:77] Creating layer relu2
I0428 19:34:41.978361 23373 net.cpp:86] Creating Layer relu2
I0428 19:34:41.978365 23373 net.cpp:408] relu2 <- ip2
I0428 19:34:41.978370 23373 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:34:41.979221 23373 net.cpp:124] Setting up relu2
I0428 19:34:41.979236 23373 net.cpp:131] Top shape: 64 10 (640)
I0428 19:34:41.979240 23373 net.cpp:139] Memory required for data: 9427200
I0428 19:34:41.979245 23373 layer_factory.hpp:77] Creating layer ip3
I0428 19:34:41.979254 23373 net.cpp:86] Creating Layer ip3
I0428 19:34:41.979257 23373 net.cpp:408] ip3 <- ip2
I0428 19:34:41.979264 23373 net.cpp:382] ip3 -> ip3
I0428 19:34:41.979377 23373 net.cpp:124] Setting up ip3
I0428 19:34:41.979384 23373 net.cpp:131] Top shape: 64 10 (640)
I0428 19:34:41.979387 23373 net.cpp:139] Memory required for data: 9429760
I0428 19:34:41.979394 23373 layer_factory.hpp:77] Creating layer relu3
I0428 19:34:41.979401 23373 net.cpp:86] Creating Layer relu3
I0428 19:34:41.979405 23373 net.cpp:408] relu3 <- ip3
I0428 19:34:41.979410 23373 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:34:41.979589 23373 net.cpp:124] Setting up relu3
I0428 19:34:41.979598 23373 net.cpp:131] Top shape: 64 10 (640)
I0428 19:34:41.979601 23373 net.cpp:139] Memory required for data: 9432320
I0428 19:34:41.979605 23373 layer_factory.hpp:77] Creating layer loss
I0428 19:34:41.979611 23373 net.cpp:86] Creating Layer loss
I0428 19:34:41.979615 23373 net.cpp:408] loss <- ip3
I0428 19:34:41.979620 23373 net.cpp:408] loss <- label
I0428 19:34:41.979626 23373 net.cpp:382] loss -> loss
I0428 19:34:41.979646 23373 layer_factory.hpp:77] Creating layer loss
I0428 19:34:41.979915 23373 net.cpp:124] Setting up loss
I0428 19:34:41.979925 23373 net.cpp:131] Top shape: (1)
I0428 19:34:41.979929 23373 net.cpp:134]     with loss weight 1
I0428 19:34:41.979944 23373 net.cpp:139] Memory required for data: 9432324
I0428 19:34:41.979949 23373 net.cpp:200] loss needs backward computation.
I0428 19:34:41.979954 23373 net.cpp:200] relu3 needs backward computation.
I0428 19:34:41.979959 23373 net.cpp:200] ip3 needs backward computation.
I0428 19:34:41.979962 23373 net.cpp:200] relu2 needs backward computation.
I0428 19:34:41.979965 23373 net.cpp:200] ip2 needs backward computation.
I0428 19:34:41.979969 23373 net.cpp:200] relu1 needs backward computation.
I0428 19:34:41.979972 23373 net.cpp:200] ip1 needs backward computation.
I0428 19:34:41.979988 23373 net.cpp:200] pool0 needs backward computation.
I0428 19:34:41.979992 23373 net.cpp:200] conv0 needs backward computation.
I0428 19:34:41.979996 23373 net.cpp:202] mnist does not need backward computation.
I0428 19:34:41.980000 23373 net.cpp:244] This network produces output loss
I0428 19:34:41.980010 23373 net.cpp:257] Network initialization done.
I0428 19:34:41.980341 23373 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test209.prototxt
I0428 19:34:41.980370 23373 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:34:41.980464 23373 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool0"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:34:41.980538 23373 layer_factory.hpp:77] Creating layer mnist
I0428 19:34:41.980592 23373 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:34:41.980607 23373 net.cpp:86] Creating Layer mnist
I0428 19:34:41.980612 23373 net.cpp:382] mnist -> data
I0428 19:34:41.980620 23373 net.cpp:382] mnist -> label
I0428 19:34:41.980720 23373 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:34:41.981966 23373 net.cpp:124] Setting up mnist
I0428 19:34:41.981981 23373 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:34:41.981987 23373 net.cpp:131] Top shape: 100 (100)
I0428 19:34:41.981990 23373 net.cpp:139] Memory required for data: 314000
I0428 19:34:41.981995 23373 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:34:41.982007 23373 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:34:41.982013 23373 net.cpp:408] label_mnist_1_split <- label
I0428 19:34:41.982019 23373 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:34:41.982028 23373 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:34:41.982132 23373 net.cpp:124] Setting up label_mnist_1_split
I0428 19:34:41.982141 23373 net.cpp:131] Top shape: 100 (100)
I0428 19:34:41.982146 23373 net.cpp:131] Top shape: 100 (100)
I0428 19:34:41.982149 23373 net.cpp:139] Memory required for data: 314800
I0428 19:34:41.982165 23373 layer_factory.hpp:77] Creating layer conv0
I0428 19:34:41.982177 23373 net.cpp:86] Creating Layer conv0
I0428 19:34:41.982180 23373 net.cpp:408] conv0 <- data
I0428 19:34:41.982187 23373 net.cpp:382] conv0 -> conv0
I0428 19:34:41.983937 23373 net.cpp:124] Setting up conv0
I0428 19:34:41.983953 23373 net.cpp:131] Top shape: 100 50 24 24 (2880000)
I0428 19:34:41.983958 23373 net.cpp:139] Memory required for data: 11834800
I0428 19:34:41.983968 23373 layer_factory.hpp:77] Creating layer pool0
I0428 19:34:41.983976 23373 net.cpp:86] Creating Layer pool0
I0428 19:34:41.983980 23373 net.cpp:408] pool0 <- conv0
I0428 19:34:41.983986 23373 net.cpp:382] pool0 -> pool0
I0428 19:34:41.984028 23373 net.cpp:124] Setting up pool0
I0428 19:34:41.984035 23373 net.cpp:131] Top shape: 100 50 12 12 (720000)
I0428 19:34:41.984038 23373 net.cpp:139] Memory required for data: 14714800
I0428 19:34:41.984041 23373 layer_factory.hpp:77] Creating layer ip1
I0428 19:34:41.984050 23373 net.cpp:86] Creating Layer ip1
I0428 19:34:41.984053 23373 net.cpp:408] ip1 <- pool0
I0428 19:34:41.984058 23373 net.cpp:382] ip1 -> ip1
I0428 19:34:41.984658 23373 net.cpp:124] Setting up ip1
I0428 19:34:41.984668 23373 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:34:41.984673 23373 net.cpp:139] Memory required for data: 14718800
I0428 19:34:41.984681 23373 layer_factory.hpp:77] Creating layer relu1
I0428 19:34:41.984686 23373 net.cpp:86] Creating Layer relu1
I0428 19:34:41.984690 23373 net.cpp:408] relu1 <- ip1
I0428 19:34:41.984696 23373 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:34:41.984895 23373 net.cpp:124] Setting up relu1
I0428 19:34:41.984906 23373 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:34:41.984911 23373 net.cpp:139] Memory required for data: 14722800
I0428 19:34:41.984915 23373 layer_factory.hpp:77] Creating layer ip2
I0428 19:34:41.984921 23373 net.cpp:86] Creating Layer ip2
I0428 19:34:41.984925 23373 net.cpp:408] ip2 <- ip1
I0428 19:34:41.984931 23373 net.cpp:382] ip2 -> ip2
I0428 19:34:41.985043 23373 net.cpp:124] Setting up ip2
I0428 19:34:41.985050 23373 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:34:41.985054 23373 net.cpp:139] Memory required for data: 14726800
I0428 19:34:41.985062 23373 layer_factory.hpp:77] Creating layer relu2
I0428 19:34:41.985069 23373 net.cpp:86] Creating Layer relu2
I0428 19:34:41.985074 23373 net.cpp:408] relu2 <- ip2
I0428 19:34:41.985080 23373 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:34:41.985983 23373 net.cpp:124] Setting up relu2
I0428 19:34:41.985997 23373 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:34:41.986002 23373 net.cpp:139] Memory required for data: 14730800
I0428 19:34:41.986006 23373 layer_factory.hpp:77] Creating layer ip3
I0428 19:34:41.986016 23373 net.cpp:86] Creating Layer ip3
I0428 19:34:41.986021 23373 net.cpp:408] ip3 <- ip2
I0428 19:34:41.986028 23373 net.cpp:382] ip3 -> ip3
I0428 19:34:41.986196 23373 net.cpp:124] Setting up ip3
I0428 19:34:41.986207 23373 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:34:41.986210 23373 net.cpp:139] Memory required for data: 14734800
I0428 19:34:41.986217 23373 layer_factory.hpp:77] Creating layer relu3
I0428 19:34:41.986224 23373 net.cpp:86] Creating Layer relu3
I0428 19:34:41.986228 23373 net.cpp:408] relu3 <- ip3
I0428 19:34:41.986233 23373 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:34:41.986425 23373 net.cpp:124] Setting up relu3
I0428 19:34:41.986435 23373 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:34:41.986439 23373 net.cpp:139] Memory required for data: 14738800
I0428 19:34:41.986443 23373 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:34:41.986450 23373 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:34:41.986454 23373 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:34:41.986461 23373 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:34:41.986469 23373 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:34:41.986510 23373 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:34:41.986517 23373 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:34:41.986522 23373 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:34:41.986536 23373 net.cpp:139] Memory required for data: 14746800
I0428 19:34:41.986541 23373 layer_factory.hpp:77] Creating layer accuracy
I0428 19:34:41.986548 23373 net.cpp:86] Creating Layer accuracy
I0428 19:34:41.986552 23373 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:34:41.986557 23373 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:34:41.986563 23373 net.cpp:382] accuracy -> accuracy
I0428 19:34:41.986572 23373 net.cpp:124] Setting up accuracy
I0428 19:34:41.986575 23373 net.cpp:131] Top shape: (1)
I0428 19:34:41.986579 23373 net.cpp:139] Memory required for data: 14746804
I0428 19:34:41.986582 23373 layer_factory.hpp:77] Creating layer loss
I0428 19:34:41.986590 23373 net.cpp:86] Creating Layer loss
I0428 19:34:41.986594 23373 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:34:41.986599 23373 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:34:41.986611 23373 net.cpp:382] loss -> loss
I0428 19:34:41.986618 23373 layer_factory.hpp:77] Creating layer loss
I0428 19:34:41.986913 23373 net.cpp:124] Setting up loss
I0428 19:34:41.986923 23373 net.cpp:131] Top shape: (1)
I0428 19:34:41.986927 23373 net.cpp:134]     with loss weight 1
I0428 19:34:41.986933 23373 net.cpp:139] Memory required for data: 14746808
I0428 19:34:41.986937 23373 net.cpp:200] loss needs backward computation.
I0428 19:34:41.986943 23373 net.cpp:202] accuracy does not need backward computation.
I0428 19:34:41.986946 23373 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:34:41.986950 23373 net.cpp:200] relu3 needs backward computation.
I0428 19:34:41.986953 23373 net.cpp:200] ip3 needs backward computation.
I0428 19:34:41.986958 23373 net.cpp:200] relu2 needs backward computation.
I0428 19:34:41.986960 23373 net.cpp:200] ip2 needs backward computation.
I0428 19:34:41.986963 23373 net.cpp:200] relu1 needs backward computation.
I0428 19:34:41.986968 23373 net.cpp:200] ip1 needs backward computation.
I0428 19:34:41.986971 23373 net.cpp:200] pool0 needs backward computation.
I0428 19:34:41.986974 23373 net.cpp:200] conv0 needs backward computation.
I0428 19:34:41.986979 23373 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:34:41.986984 23373 net.cpp:202] mnist does not need backward computation.
I0428 19:34:41.986986 23373 net.cpp:244] This network produces output accuracy
I0428 19:34:41.986990 23373 net.cpp:244] This network produces output loss
I0428 19:34:41.987004 23373 net.cpp:257] Network initialization done.
I0428 19:34:41.987047 23373 solver.cpp:56] Solver scaffolding done.
I0428 19:34:41.987381 23373 caffe.cpp:248] Starting Optimization
I0428 19:34:41.987390 23373 solver.cpp:273] Solving LeNet
I0428 19:34:41.987392 23373 solver.cpp:274] Learning Rate Policy: inv
I0428 19:34:41.988306 23373 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:34:42.090150 23380 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:34:42.093654 23373 solver.cpp:398]     Test net output #0: accuracy = 0.0931
I0428 19:34:42.093673 23373 solver.cpp:398]     Test net output #1: loss = 2.31214 (* 1 = 2.31214 loss)
I0428 19:34:42.097872 23373 solver.cpp:219] Iteration 0 (-7.48293e-43 iter/s, 0.110449s/100 iters), loss = 2.31978
I0428 19:34:42.097895 23373 solver.cpp:238]     Train net output #0: loss = 2.31978 (* 1 = 2.31978 loss)
I0428 19:34:42.097906 23373 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:34:42.238945 23373 solver.cpp:219] Iteration 100 (709.057 iter/s, 0.141032s/100 iters), loss = 0.837222
I0428 19:34:42.238987 23373 solver.cpp:238]     Train net output #0: loss = 0.837222 (* 1 = 0.837222 loss)
I0428 19:34:42.238994 23373 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:34:42.386981 23373 solver.cpp:219] Iteration 200 (675.747 iter/s, 0.147984s/100 iters), loss = 0.516645
I0428 19:34:42.387038 23373 solver.cpp:238]     Train net output #0: loss = 0.516645 (* 1 = 0.516645 loss)
I0428 19:34:42.387048 23373 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:34:42.532642 23373 solver.cpp:219] Iteration 300 (686.797 iter/s, 0.145603s/100 iters), loss = 0.749775
I0428 19:34:42.532687 23373 solver.cpp:238]     Train net output #0: loss = 0.749775 (* 1 = 0.749775 loss)
I0428 19:34:42.532696 23373 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:34:42.678879 23373 solver.cpp:219] Iteration 400 (684.092 iter/s, 0.146179s/100 iters), loss = 0.464827
I0428 19:34:42.678907 23373 solver.cpp:238]     Train net output #0: loss = 0.464827 (* 1 = 0.464827 loss)
I0428 19:34:42.678915 23373 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:34:42.826771 23373 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:34:42.927867 23380 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:34:42.930691 23373 solver.cpp:398]     Test net output #0: accuracy = 0.7565
I0428 19:34:42.930716 23373 solver.cpp:398]     Test net output #1: loss = 0.63559 (* 1 = 0.63559 loss)
I0428 19:34:42.932106 23373 solver.cpp:219] Iteration 500 (394.974 iter/s, 0.253181s/100 iters), loss = 0.679914
I0428 19:34:42.932132 23373 solver.cpp:238]     Train net output #0: loss = 0.679914 (* 1 = 0.679914 loss)
I0428 19:34:42.932139 23373 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:34:43.076637 23373 solver.cpp:219] Iteration 600 (692.082 iter/s, 0.144492s/100 iters), loss = 0.737901
I0428 19:34:43.076664 23373 solver.cpp:238]     Train net output #0: loss = 0.737901 (* 1 = 0.737901 loss)
I0428 19:34:43.076671 23373 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:34:43.223350 23373 solver.cpp:219] Iteration 700 (681.809 iter/s, 0.146669s/100 iters), loss = 0.732989
I0428 19:34:43.223397 23373 solver.cpp:238]     Train net output #0: loss = 0.732989 (* 1 = 0.732989 loss)
I0428 19:34:43.223407 23373 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:34:43.369616 23373 solver.cpp:219] Iteration 800 (683.953 iter/s, 0.146209s/100 iters), loss = 0.655996
I0428 19:34:43.369644 23373 solver.cpp:238]     Train net output #0: loss = 0.655996 (* 1 = 0.655996 loss)
I0428 19:34:43.369652 23373 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:34:43.510826 23373 solver.cpp:219] Iteration 900 (708.367 iter/s, 0.14117s/100 iters), loss = 0.500274
I0428 19:34:43.510854 23373 solver.cpp:238]     Train net output #0: loss = 0.500274 (* 1 = 0.500274 loss)
I0428 19:34:43.510861 23373 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:34:43.557742 23379 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:34:43.654886 23373 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:34:43.656883 23373 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:34:43.658443 23373 solver.cpp:311] Iteration 1000, loss = 0.603422
I0428 19:34:43.658460 23373 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:34:43.759300 23380 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:34:43.762162 23373 solver.cpp:398]     Test net output #0: accuracy = 0.7766
I0428 19:34:43.762186 23373 solver.cpp:398]     Test net output #1: loss = 0.556372 (* 1 = 0.556372 loss)
I0428 19:34:43.762190 23373 solver.cpp:316] Optimization Done.
I0428 19:34:43.762194 23373 caffe.cpp:259] Optimization Done.
