I0428 19:31:23.988297 22569 caffe.cpp:218] Using GPUs 0
I0428 19:31:24.023530 22569 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:31:24.537437 22569 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test121.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:31:24.537607 22569 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test121.prototxt
I0428 19:31:24.537935 22569 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:31:24.537955 22569 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:31:24.538038 22569 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool0"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0428 19:31:24.538127 22569 layer_factory.hpp:77] Creating layer mnist
I0428 19:31:24.538261 22569 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:31:24.538295 22569 net.cpp:86] Creating Layer mnist
I0428 19:31:24.538306 22569 net.cpp:382] mnist -> data
I0428 19:31:24.538337 22569 net.cpp:382] mnist -> label
I0428 19:31:24.539532 22569 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:31:24.542207 22569 net.cpp:124] Setting up mnist
I0428 19:31:24.542227 22569 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:31:24.542237 22569 net.cpp:131] Top shape: 64 (64)
I0428 19:31:24.542244 22569 net.cpp:139] Memory required for data: 200960
I0428 19:31:24.542255 22569 layer_factory.hpp:77] Creating layer conv0
I0428 19:31:24.542279 22569 net.cpp:86] Creating Layer conv0
I0428 19:31:24.542289 22569 net.cpp:408] conv0 <- data
I0428 19:31:24.542305 22569 net.cpp:382] conv0 -> conv0
I0428 19:31:24.833379 22569 net.cpp:124] Setting up conv0
I0428 19:31:24.833415 22569 net.cpp:131] Top shape: 64 10 24 24 (368640)
I0428 19:31:24.833421 22569 net.cpp:139] Memory required for data: 1675520
I0428 19:31:24.833447 22569 layer_factory.hpp:77] Creating layer pool0
I0428 19:31:24.833468 22569 net.cpp:86] Creating Layer pool0
I0428 19:31:24.833477 22569 net.cpp:408] pool0 <- conv0
I0428 19:31:24.833488 22569 net.cpp:382] pool0 -> pool0
I0428 19:31:24.833562 22569 net.cpp:124] Setting up pool0
I0428 19:31:24.833573 22569 net.cpp:131] Top shape: 64 10 12 12 (92160)
I0428 19:31:24.833580 22569 net.cpp:139] Memory required for data: 2044160
I0428 19:31:24.833607 22569 layer_factory.hpp:77] Creating layer ip1
I0428 19:31:24.833622 22569 net.cpp:86] Creating Layer ip1
I0428 19:31:24.833631 22569 net.cpp:408] ip1 <- pool0
I0428 19:31:24.833640 22569 net.cpp:382] ip1 -> ip1
I0428 19:31:24.834841 22569 net.cpp:124] Setting up ip1
I0428 19:31:24.834858 22569 net.cpp:131] Top shape: 64 10 (640)
I0428 19:31:24.834867 22569 net.cpp:139] Memory required for data: 2046720
I0428 19:31:24.834882 22569 layer_factory.hpp:77] Creating layer relu1
I0428 19:31:24.834895 22569 net.cpp:86] Creating Layer relu1
I0428 19:31:24.834906 22569 net.cpp:408] relu1 <- ip1
I0428 19:31:24.834916 22569 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:31:24.835124 22569 net.cpp:124] Setting up relu1
I0428 19:31:24.835135 22569 net.cpp:131] Top shape: 64 10 (640)
I0428 19:31:24.835141 22569 net.cpp:139] Memory required for data: 2049280
I0428 19:31:24.835149 22569 layer_factory.hpp:77] Creating layer ip2
I0428 19:31:24.835160 22569 net.cpp:86] Creating Layer ip2
I0428 19:31:24.835167 22569 net.cpp:408] ip2 <- ip1
I0428 19:31:24.835177 22569 net.cpp:382] ip2 -> ip2
I0428 19:31:24.835299 22569 net.cpp:124] Setting up ip2
I0428 19:31:24.835310 22569 net.cpp:131] Top shape: 64 10 (640)
I0428 19:31:24.835316 22569 net.cpp:139] Memory required for data: 2051840
I0428 19:31:24.835331 22569 layer_factory.hpp:77] Creating layer relu2
I0428 19:31:24.835341 22569 net.cpp:86] Creating Layer relu2
I0428 19:31:24.835355 22569 net.cpp:408] relu2 <- ip2
I0428 19:31:24.835363 22569 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:31:24.836223 22569 net.cpp:124] Setting up relu2
I0428 19:31:24.836241 22569 net.cpp:131] Top shape: 64 10 (640)
I0428 19:31:24.836247 22569 net.cpp:139] Memory required for data: 2054400
I0428 19:31:24.836254 22569 layer_factory.hpp:77] Creating layer loss
I0428 19:31:24.836266 22569 net.cpp:86] Creating Layer loss
I0428 19:31:24.836272 22569 net.cpp:408] loss <- ip2
I0428 19:31:24.836282 22569 net.cpp:408] loss <- label
I0428 19:31:24.836292 22569 net.cpp:382] loss -> loss
I0428 19:31:24.836321 22569 layer_factory.hpp:77] Creating layer loss
I0428 19:31:24.836607 22569 net.cpp:124] Setting up loss
I0428 19:31:24.836621 22569 net.cpp:131] Top shape: (1)
I0428 19:31:24.836627 22569 net.cpp:134]     with loss weight 1
I0428 19:31:24.836650 22569 net.cpp:139] Memory required for data: 2054404
I0428 19:31:24.836658 22569 net.cpp:200] loss needs backward computation.
I0428 19:31:24.836665 22569 net.cpp:200] relu2 needs backward computation.
I0428 19:31:24.836671 22569 net.cpp:200] ip2 needs backward computation.
I0428 19:31:24.836678 22569 net.cpp:200] relu1 needs backward computation.
I0428 19:31:24.836684 22569 net.cpp:200] ip1 needs backward computation.
I0428 19:31:24.836690 22569 net.cpp:200] pool0 needs backward computation.
I0428 19:31:24.836697 22569 net.cpp:200] conv0 needs backward computation.
I0428 19:31:24.836704 22569 net.cpp:202] mnist does not need backward computation.
I0428 19:31:24.836710 22569 net.cpp:244] This network produces output loss
I0428 19:31:24.836725 22569 net.cpp:257] Network initialization done.
I0428 19:31:24.837034 22569 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test121.prototxt
I0428 19:31:24.837072 22569 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:31:24.837168 22569 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool0"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0428 19:31:24.837285 22569 layer_factory.hpp:77] Creating layer mnist
I0428 19:31:24.837358 22569 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:31:24.837379 22569 net.cpp:86] Creating Layer mnist
I0428 19:31:24.837388 22569 net.cpp:382] mnist -> data
I0428 19:31:24.837402 22569 net.cpp:382] mnist -> label
I0428 19:31:24.837545 22569 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:31:24.839833 22569 net.cpp:124] Setting up mnist
I0428 19:31:24.839850 22569 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:31:24.839860 22569 net.cpp:131] Top shape: 100 (100)
I0428 19:31:24.839866 22569 net.cpp:139] Memory required for data: 314000
I0428 19:31:24.839874 22569 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:31:24.839886 22569 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:31:24.839893 22569 net.cpp:408] label_mnist_1_split <- label
I0428 19:31:24.839902 22569 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:31:24.839915 22569 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:31:24.840011 22569 net.cpp:124] Setting up label_mnist_1_split
I0428 19:31:24.840023 22569 net.cpp:131] Top shape: 100 (100)
I0428 19:31:24.840030 22569 net.cpp:131] Top shape: 100 (100)
I0428 19:31:24.840036 22569 net.cpp:139] Memory required for data: 314800
I0428 19:31:24.840042 22569 layer_factory.hpp:77] Creating layer conv0
I0428 19:31:24.840059 22569 net.cpp:86] Creating Layer conv0
I0428 19:31:24.840067 22569 net.cpp:408] conv0 <- data
I0428 19:31:24.840078 22569 net.cpp:382] conv0 -> conv0
I0428 19:31:24.841869 22569 net.cpp:124] Setting up conv0
I0428 19:31:24.841888 22569 net.cpp:131] Top shape: 100 10 24 24 (576000)
I0428 19:31:24.841897 22569 net.cpp:139] Memory required for data: 2618800
I0428 19:31:24.841913 22569 layer_factory.hpp:77] Creating layer pool0
I0428 19:31:24.841925 22569 net.cpp:86] Creating Layer pool0
I0428 19:31:24.841933 22569 net.cpp:408] pool0 <- conv0
I0428 19:31:24.841943 22569 net.cpp:382] pool0 -> pool0
I0428 19:31:24.842000 22569 net.cpp:124] Setting up pool0
I0428 19:31:24.842011 22569 net.cpp:131] Top shape: 100 10 12 12 (144000)
I0428 19:31:24.842017 22569 net.cpp:139] Memory required for data: 3194800
I0428 19:31:24.842023 22569 layer_factory.hpp:77] Creating layer ip1
I0428 19:31:24.842036 22569 net.cpp:86] Creating Layer ip1
I0428 19:31:24.842042 22569 net.cpp:408] ip1 <- pool0
I0428 19:31:24.842052 22569 net.cpp:382] ip1 -> ip1
I0428 19:31:24.842267 22569 net.cpp:124] Setting up ip1
I0428 19:31:24.842278 22569 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:31:24.842283 22569 net.cpp:139] Memory required for data: 3198800
I0428 19:31:24.842298 22569 layer_factory.hpp:77] Creating layer relu1
I0428 19:31:24.842308 22569 net.cpp:86] Creating Layer relu1
I0428 19:31:24.842315 22569 net.cpp:408] relu1 <- ip1
I0428 19:31:24.842324 22569 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:31:24.842604 22569 net.cpp:124] Setting up relu1
I0428 19:31:24.842617 22569 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:31:24.842639 22569 net.cpp:139] Memory required for data: 3202800
I0428 19:31:24.842646 22569 layer_factory.hpp:77] Creating layer ip2
I0428 19:31:24.842658 22569 net.cpp:86] Creating Layer ip2
I0428 19:31:24.842664 22569 net.cpp:408] ip2 <- ip1
I0428 19:31:24.842676 22569 net.cpp:382] ip2 -> ip2
I0428 19:31:24.842797 22569 net.cpp:124] Setting up ip2
I0428 19:31:24.842808 22569 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:31:24.842818 22569 net.cpp:139] Memory required for data: 3206800
I0428 19:31:24.842833 22569 layer_factory.hpp:77] Creating layer relu2
I0428 19:31:24.842844 22569 net.cpp:86] Creating Layer relu2
I0428 19:31:24.842850 22569 net.cpp:408] relu2 <- ip2
I0428 19:31:24.842859 22569 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:31:24.843042 22569 net.cpp:124] Setting up relu2
I0428 19:31:24.843055 22569 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:31:24.843060 22569 net.cpp:139] Memory required for data: 3210800
I0428 19:31:24.843066 22569 layer_factory.hpp:77] Creating layer ip2_relu2_0_split
I0428 19:31:24.843077 22569 net.cpp:86] Creating Layer ip2_relu2_0_split
I0428 19:31:24.843086 22569 net.cpp:408] ip2_relu2_0_split <- ip2
I0428 19:31:24.843096 22569 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_0
I0428 19:31:24.843108 22569 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_1
I0428 19:31:24.843158 22569 net.cpp:124] Setting up ip2_relu2_0_split
I0428 19:31:24.843168 22569 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:31:24.843176 22569 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:31:24.843183 22569 net.cpp:139] Memory required for data: 3218800
I0428 19:31:24.843189 22569 layer_factory.hpp:77] Creating layer accuracy
I0428 19:31:24.843199 22569 net.cpp:86] Creating Layer accuracy
I0428 19:31:24.843204 22569 net.cpp:408] accuracy <- ip2_relu2_0_split_0
I0428 19:31:24.843214 22569 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:31:24.843222 22569 net.cpp:382] accuracy -> accuracy
I0428 19:31:24.843237 22569 net.cpp:124] Setting up accuracy
I0428 19:31:24.843247 22569 net.cpp:131] Top shape: (1)
I0428 19:31:24.843255 22569 net.cpp:139] Memory required for data: 3218804
I0428 19:31:24.843261 22569 layer_factory.hpp:77] Creating layer loss
I0428 19:31:24.843269 22569 net.cpp:86] Creating Layer loss
I0428 19:31:24.843276 22569 net.cpp:408] loss <- ip2_relu2_0_split_1
I0428 19:31:24.843284 22569 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:31:24.843293 22569 net.cpp:382] loss -> loss
I0428 19:31:24.843305 22569 layer_factory.hpp:77] Creating layer loss
I0428 19:31:24.844377 22569 net.cpp:124] Setting up loss
I0428 19:31:24.844393 22569 net.cpp:131] Top shape: (1)
I0428 19:31:24.844400 22569 net.cpp:134]     with loss weight 1
I0428 19:31:24.844413 22569 net.cpp:139] Memory required for data: 3218808
I0428 19:31:24.844420 22569 net.cpp:200] loss needs backward computation.
I0428 19:31:24.844429 22569 net.cpp:202] accuracy does not need backward computation.
I0428 19:31:24.844436 22569 net.cpp:200] ip2_relu2_0_split needs backward computation.
I0428 19:31:24.844442 22569 net.cpp:200] relu2 needs backward computation.
I0428 19:31:24.844449 22569 net.cpp:200] ip2 needs backward computation.
I0428 19:31:24.844455 22569 net.cpp:200] relu1 needs backward computation.
I0428 19:31:24.844462 22569 net.cpp:200] ip1 needs backward computation.
I0428 19:31:24.844468 22569 net.cpp:200] pool0 needs backward computation.
I0428 19:31:24.844475 22569 net.cpp:200] conv0 needs backward computation.
I0428 19:31:24.844482 22569 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:31:24.844490 22569 net.cpp:202] mnist does not need backward computation.
I0428 19:31:24.844496 22569 net.cpp:244] This network produces output accuracy
I0428 19:31:24.844503 22569 net.cpp:244] This network produces output loss
I0428 19:31:24.844521 22569 net.cpp:257] Network initialization done.
I0428 19:31:24.844573 22569 solver.cpp:56] Solver scaffolding done.
I0428 19:31:24.844830 22569 caffe.cpp:248] Starting Optimization
I0428 19:31:24.844838 22569 solver.cpp:273] Solving LeNet
I0428 19:31:24.844861 22569 solver.cpp:274] Learning Rate Policy: inv
I0428 19:31:24.844986 22569 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:31:24.848945 22569 blocking_queue.cpp:49] Waiting for data
I0428 19:31:24.921124 22576 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:31:24.921646 22569 solver.cpp:398]     Test net output #0: accuracy = 0.0699
I0428 19:31:24.921671 22569 solver.cpp:398]     Test net output #1: loss = 2.32694 (* 1 = 2.32694 loss)
I0428 19:31:24.923894 22569 solver.cpp:219] Iteration 0 (-1.02855e-42 iter/s, 0.0790013s/100 iters), loss = 2.33488
I0428 19:31:24.923928 22569 solver.cpp:238]     Train net output #0: loss = 2.33488 (* 1 = 2.33488 loss)
I0428 19:31:24.923948 22569 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:31:24.982465 22569 solver.cpp:219] Iteration 100 (1708.48 iter/s, 0.0585317s/100 iters), loss = 1.42321
I0428 19:31:24.982496 22569 solver.cpp:238]     Train net output #0: loss = 1.42321 (* 1 = 1.42321 loss)
I0428 19:31:24.982507 22569 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:31:25.052776 22569 solver.cpp:219] Iteration 200 (1423.05 iter/s, 0.0702718s/100 iters), loss = 1.12804
I0428 19:31:25.052826 22569 solver.cpp:238]     Train net output #0: loss = 1.12804 (* 1 = 1.12804 loss)
I0428 19:31:25.052842 22569 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:31:25.114248 22569 solver.cpp:219] Iteration 300 (1627.98 iter/s, 0.0614258s/100 iters), loss = 1.11731
I0428 19:31:25.114280 22569 solver.cpp:238]     Train net output #0: loss = 1.11731 (* 1 = 1.11731 loss)
I0428 19:31:25.114292 22569 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:31:25.172209 22569 solver.cpp:219] Iteration 400 (1726.38 iter/s, 0.0579246s/100 iters), loss = 0.864577
I0428 19:31:25.172240 22569 solver.cpp:238]     Train net output #0: loss = 0.864577 (* 1 = 0.864577 loss)
I0428 19:31:25.172250 22569 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:31:25.227505 22569 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:31:25.280149 22576 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:31:25.280633 22569 solver.cpp:398]     Test net output #0: accuracy = 0.5441
I0428 19:31:25.280653 22569 solver.cpp:398]     Test net output #1: loss = 1.14261 (* 1 = 1.14261 loss)
I0428 19:31:25.281265 22569 solver.cpp:219] Iteration 500 (917.279 iter/s, 0.109018s/100 iters), loss = 1.15225
I0428 19:31:25.281291 22569 solver.cpp:238]     Train net output #0: loss = 1.15225 (* 1 = 1.15225 loss)
I0428 19:31:25.281301 22569 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:31:25.340116 22569 solver.cpp:219] Iteration 600 (1700.17 iter/s, 0.0588175s/100 iters), loss = 1.37552
I0428 19:31:25.340140 22569 solver.cpp:238]     Train net output #0: loss = 1.37552 (* 1 = 1.37552 loss)
I0428 19:31:25.340164 22569 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:31:25.399096 22569 solver.cpp:219] Iteration 700 (1696.36 iter/s, 0.0589498s/100 iters), loss = 1.19971
I0428 19:31:25.399123 22569 solver.cpp:238]     Train net output #0: loss = 1.19971 (* 1 = 1.19971 loss)
I0428 19:31:25.399147 22569 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:31:25.457170 22569 solver.cpp:219] Iteration 800 (1722.94 iter/s, 0.0580403s/100 iters), loss = 1.19561
I0428 19:31:25.457197 22569 solver.cpp:238]     Train net output #0: loss = 1.19561 (* 1 = 1.19561 loss)
I0428 19:31:25.457207 22569 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:31:25.513432 22569 solver.cpp:219] Iteration 900 (1778.42 iter/s, 0.0562298s/100 iters), loss = 1.2161
I0428 19:31:25.513460 22569 solver.cpp:238]     Train net output #0: loss = 1.2161 (* 1 = 1.2161 loss)
I0428 19:31:25.513485 22569 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:31:25.532807 22575 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:31:25.570781 22569 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:31:25.571481 22569 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:31:25.571974 22569 solver.cpp:311] Iteration 1000, loss = 1.10938
I0428 19:31:25.571990 22569 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:31:25.618294 22576 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:31:25.618775 22569 solver.cpp:398]     Test net output #0: accuracy = 0.5677
I0428 19:31:25.618798 22569 solver.cpp:398]     Test net output #1: loss = 1.06095 (* 1 = 1.06095 loss)
I0428 19:31:25.618806 22569 solver.cpp:316] Optimization Done.
I0428 19:31:25.618811 22569 caffe.cpp:259] Optimization Done.
