I0428 20:21:03.077697  1684 caffe.cpp:218] Using GPUs 0
I0428 20:21:03.113548  1684 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:21:03.628111  1684 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1376.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:21:03.628252  1684 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1376.prototxt
I0428 20:21:03.628628  1684 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:21:03.628648  1684 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:21:03.628737  1684 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0428 20:21:03.628821  1684 layer_factory.hpp:77] Creating layer mnist
I0428 20:21:03.628927  1684 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:21:03.628952  1684 net.cpp:86] Creating Layer mnist
I0428 20:21:03.628958  1684 net.cpp:382] mnist -> data
I0428 20:21:03.628985  1684 net.cpp:382] mnist -> label
I0428 20:21:03.630074  1684 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:21:03.632740  1684 net.cpp:124] Setting up mnist
I0428 20:21:03.632760  1684 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:21:03.632766  1684 net.cpp:131] Top shape: 64 (64)
I0428 20:21:03.632769  1684 net.cpp:139] Memory required for data: 200960
I0428 20:21:03.632776  1684 layer_factory.hpp:77] Creating layer conv0
I0428 20:21:03.632797  1684 net.cpp:86] Creating Layer conv0
I0428 20:21:03.632802  1684 net.cpp:408] conv0 <- data
I0428 20:21:03.632823  1684 net.cpp:382] conv0 -> conv0
I0428 20:21:03.918083  1684 net.cpp:124] Setting up conv0
I0428 20:21:03.918110  1684 net.cpp:131] Top shape: 64 50 24 24 (1843200)
I0428 20:21:03.918115  1684 net.cpp:139] Memory required for data: 7573760
I0428 20:21:03.918151  1684 layer_factory.hpp:77] Creating layer pool0
I0428 20:21:03.918164  1684 net.cpp:86] Creating Layer pool0
I0428 20:21:03.918169  1684 net.cpp:408] pool0 <- conv0
I0428 20:21:03.918174  1684 net.cpp:382] pool0 -> pool0
I0428 20:21:03.918221  1684 net.cpp:124] Setting up pool0
I0428 20:21:03.918227  1684 net.cpp:131] Top shape: 64 50 12 12 (460800)
I0428 20:21:03.918231  1684 net.cpp:139] Memory required for data: 9416960
I0428 20:21:03.918234  1684 layer_factory.hpp:77] Creating layer conv1
I0428 20:21:03.918244  1684 net.cpp:86] Creating Layer conv1
I0428 20:21:03.918247  1684 net.cpp:408] conv1 <- pool0
I0428 20:21:03.918252  1684 net.cpp:382] conv1 -> conv1
I0428 20:21:03.920490  1684 net.cpp:124] Setting up conv1
I0428 20:21:03.920506  1684 net.cpp:131] Top shape: 64 50 8 8 (204800)
I0428 20:21:03.920509  1684 net.cpp:139] Memory required for data: 10236160
I0428 20:21:03.920517  1684 layer_factory.hpp:77] Creating layer pool1
I0428 20:21:03.920524  1684 net.cpp:86] Creating Layer pool1
I0428 20:21:03.920528  1684 net.cpp:408] pool1 <- conv1
I0428 20:21:03.920533  1684 net.cpp:382] pool1 -> pool1
I0428 20:21:03.920584  1684 net.cpp:124] Setting up pool1
I0428 20:21:03.920590  1684 net.cpp:131] Top shape: 64 50 4 4 (51200)
I0428 20:21:03.920593  1684 net.cpp:139] Memory required for data: 10440960
I0428 20:21:03.920596  1684 layer_factory.hpp:77] Creating layer ip1
I0428 20:21:03.920603  1684 net.cpp:86] Creating Layer ip1
I0428 20:21:03.920608  1684 net.cpp:408] ip1 <- pool1
I0428 20:21:03.920611  1684 net.cpp:382] ip1 -> ip1
I0428 20:21:03.920996  1684 net.cpp:124] Setting up ip1
I0428 20:21:03.921005  1684 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:21:03.921025  1684 net.cpp:139] Memory required for data: 10453760
I0428 20:21:03.921032  1684 layer_factory.hpp:77] Creating layer relu1
I0428 20:21:03.921038  1684 net.cpp:86] Creating Layer relu1
I0428 20:21:03.921041  1684 net.cpp:408] relu1 <- ip1
I0428 20:21:03.921046  1684 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:21:03.921241  1684 net.cpp:124] Setting up relu1
I0428 20:21:03.921249  1684 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:21:03.921252  1684 net.cpp:139] Memory required for data: 10466560
I0428 20:21:03.921255  1684 layer_factory.hpp:77] Creating layer ip2
I0428 20:21:03.921262  1684 net.cpp:86] Creating Layer ip2
I0428 20:21:03.921265  1684 net.cpp:408] ip2 <- ip1
I0428 20:21:03.921270  1684 net.cpp:382] ip2 -> ip2
I0428 20:21:03.921365  1684 net.cpp:124] Setting up ip2
I0428 20:21:03.921372  1684 net.cpp:131] Top shape: 64 10 (640)
I0428 20:21:03.921375  1684 net.cpp:139] Memory required for data: 10469120
I0428 20:21:03.921381  1684 layer_factory.hpp:77] Creating layer relu2
I0428 20:21:03.921387  1684 net.cpp:86] Creating Layer relu2
I0428 20:21:03.921391  1684 net.cpp:408] relu2 <- ip2
I0428 20:21:03.921394  1684 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:21:03.922166  1684 net.cpp:124] Setting up relu2
I0428 20:21:03.922178  1684 net.cpp:131] Top shape: 64 10 (640)
I0428 20:21:03.922183  1684 net.cpp:139] Memory required for data: 10471680
I0428 20:21:03.922185  1684 layer_factory.hpp:77] Creating layer loss
I0428 20:21:03.922191  1684 net.cpp:86] Creating Layer loss
I0428 20:21:03.922195  1684 net.cpp:408] loss <- ip2
I0428 20:21:03.922199  1684 net.cpp:408] loss <- label
I0428 20:21:03.922204  1684 net.cpp:382] loss -> loss
I0428 20:21:03.922226  1684 layer_factory.hpp:77] Creating layer loss
I0428 20:21:03.922469  1684 net.cpp:124] Setting up loss
I0428 20:21:03.922479  1684 net.cpp:131] Top shape: (1)
I0428 20:21:03.922483  1684 net.cpp:134]     with loss weight 1
I0428 20:21:03.922497  1684 net.cpp:139] Memory required for data: 10471684
I0428 20:21:03.922500  1684 net.cpp:200] loss needs backward computation.
I0428 20:21:03.922504  1684 net.cpp:200] relu2 needs backward computation.
I0428 20:21:03.922508  1684 net.cpp:200] ip2 needs backward computation.
I0428 20:21:03.922510  1684 net.cpp:200] relu1 needs backward computation.
I0428 20:21:03.922513  1684 net.cpp:200] ip1 needs backward computation.
I0428 20:21:03.922528  1684 net.cpp:200] pool1 needs backward computation.
I0428 20:21:03.922531  1684 net.cpp:200] conv1 needs backward computation.
I0428 20:21:03.922534  1684 net.cpp:200] pool0 needs backward computation.
I0428 20:21:03.922538  1684 net.cpp:200] conv0 needs backward computation.
I0428 20:21:03.922541  1684 net.cpp:202] mnist does not need backward computation.
I0428 20:21:03.922544  1684 net.cpp:244] This network produces output loss
I0428 20:21:03.922554  1684 net.cpp:257] Network initialization done.
I0428 20:21:03.922847  1684 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1376.prototxt
I0428 20:21:03.922884  1684 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:21:03.922978  1684 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0428 20:21:03.923041  1684 layer_factory.hpp:77] Creating layer mnist
I0428 20:21:03.923087  1684 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:21:03.923100  1684 net.cpp:86] Creating Layer mnist
I0428 20:21:03.923105  1684 net.cpp:382] mnist -> data
I0428 20:21:03.923112  1684 net.cpp:382] mnist -> label
I0428 20:21:03.923197  1684 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:21:03.925144  1684 net.cpp:124] Setting up mnist
I0428 20:21:03.925158  1684 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:21:03.925179  1684 net.cpp:131] Top shape: 100 (100)
I0428 20:21:03.925182  1684 net.cpp:139] Memory required for data: 314000
I0428 20:21:03.925186  1684 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:21:03.925212  1684 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:21:03.925216  1684 net.cpp:408] label_mnist_1_split <- label
I0428 20:21:03.925222  1684 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:21:03.925230  1684 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:21:03.925293  1684 net.cpp:124] Setting up label_mnist_1_split
I0428 20:21:03.925302  1684 net.cpp:131] Top shape: 100 (100)
I0428 20:21:03.925307  1684 net.cpp:131] Top shape: 100 (100)
I0428 20:21:03.925310  1684 net.cpp:139] Memory required for data: 314800
I0428 20:21:03.925313  1684 layer_factory.hpp:77] Creating layer conv0
I0428 20:21:03.925323  1684 net.cpp:86] Creating Layer conv0
I0428 20:21:03.925326  1684 net.cpp:408] conv0 <- data
I0428 20:21:03.925333  1684 net.cpp:382] conv0 -> conv0
I0428 20:21:03.927083  1684 net.cpp:124] Setting up conv0
I0428 20:21:03.927098  1684 net.cpp:131] Top shape: 100 50 24 24 (2880000)
I0428 20:21:03.927101  1684 net.cpp:139] Memory required for data: 11834800
I0428 20:21:03.927111  1684 layer_factory.hpp:77] Creating layer pool0
I0428 20:21:03.927117  1684 net.cpp:86] Creating Layer pool0
I0428 20:21:03.927120  1684 net.cpp:408] pool0 <- conv0
I0428 20:21:03.927125  1684 net.cpp:382] pool0 -> pool0
I0428 20:21:03.927160  1684 net.cpp:124] Setting up pool0
I0428 20:21:03.927165  1684 net.cpp:131] Top shape: 100 50 12 12 (720000)
I0428 20:21:03.927168  1684 net.cpp:139] Memory required for data: 14714800
I0428 20:21:03.927171  1684 layer_factory.hpp:77] Creating layer conv1
I0428 20:21:03.927179  1684 net.cpp:86] Creating Layer conv1
I0428 20:21:03.927182  1684 net.cpp:408] conv1 <- pool0
I0428 20:21:03.927187  1684 net.cpp:382] conv1 -> conv1
I0428 20:21:03.929015  1684 net.cpp:124] Setting up conv1
I0428 20:21:03.929044  1684 net.cpp:131] Top shape: 100 50 8 8 (320000)
I0428 20:21:03.929049  1684 net.cpp:139] Memory required for data: 15994800
I0428 20:21:03.929057  1684 layer_factory.hpp:77] Creating layer pool1
I0428 20:21:03.929080  1684 net.cpp:86] Creating Layer pool1
I0428 20:21:03.929090  1684 net.cpp:408] pool1 <- conv1
I0428 20:21:03.929096  1684 net.cpp:382] pool1 -> pool1
I0428 20:21:03.929141  1684 net.cpp:124] Setting up pool1
I0428 20:21:03.929147  1684 net.cpp:131] Top shape: 100 50 4 4 (80000)
I0428 20:21:03.929150  1684 net.cpp:139] Memory required for data: 16314800
I0428 20:21:03.929153  1684 layer_factory.hpp:77] Creating layer ip1
I0428 20:21:03.929160  1684 net.cpp:86] Creating Layer ip1
I0428 20:21:03.929163  1684 net.cpp:408] ip1 <- pool1
I0428 20:21:03.929169  1684 net.cpp:382] ip1 -> ip1
I0428 20:21:03.929549  1684 net.cpp:124] Setting up ip1
I0428 20:21:03.929558  1684 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:21:03.929574  1684 net.cpp:139] Memory required for data: 16334800
I0428 20:21:03.929581  1684 layer_factory.hpp:77] Creating layer relu1
I0428 20:21:03.929586  1684 net.cpp:86] Creating Layer relu1
I0428 20:21:03.929590  1684 net.cpp:408] relu1 <- ip1
I0428 20:21:03.929595  1684 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:21:03.929790  1684 net.cpp:124] Setting up relu1
I0428 20:21:03.929800  1684 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:21:03.929803  1684 net.cpp:139] Memory required for data: 16354800
I0428 20:21:03.929807  1684 layer_factory.hpp:77] Creating layer ip2
I0428 20:21:03.929826  1684 net.cpp:86] Creating Layer ip2
I0428 20:21:03.929829  1684 net.cpp:408] ip2 <- ip1
I0428 20:21:03.929836  1684 net.cpp:382] ip2 -> ip2
I0428 20:21:03.929944  1684 net.cpp:124] Setting up ip2
I0428 20:21:03.929950  1684 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:21:03.929955  1684 net.cpp:139] Memory required for data: 16358800
I0428 20:21:03.929960  1684 layer_factory.hpp:77] Creating layer relu2
I0428 20:21:03.929965  1684 net.cpp:86] Creating Layer relu2
I0428 20:21:03.929970  1684 net.cpp:408] relu2 <- ip2
I0428 20:21:03.929973  1684 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:21:03.930191  1684 net.cpp:124] Setting up relu2
I0428 20:21:03.930200  1684 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:21:03.930204  1684 net.cpp:139] Memory required for data: 16362800
I0428 20:21:03.930208  1684 layer_factory.hpp:77] Creating layer ip2_relu2_0_split
I0428 20:21:03.930213  1684 net.cpp:86] Creating Layer ip2_relu2_0_split
I0428 20:21:03.930217  1684 net.cpp:408] ip2_relu2_0_split <- ip2
I0428 20:21:03.930222  1684 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_0
I0428 20:21:03.930238  1684 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_1
I0428 20:21:03.930300  1684 net.cpp:124] Setting up ip2_relu2_0_split
I0428 20:21:03.930308  1684 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:21:03.930312  1684 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:21:03.930315  1684 net.cpp:139] Memory required for data: 16370800
I0428 20:21:03.930320  1684 layer_factory.hpp:77] Creating layer accuracy
I0428 20:21:03.930326  1684 net.cpp:86] Creating Layer accuracy
I0428 20:21:03.930335  1684 net.cpp:408] accuracy <- ip2_relu2_0_split_0
I0428 20:21:03.930338  1684 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:21:03.930343  1684 net.cpp:382] accuracy -> accuracy
I0428 20:21:03.930356  1684 net.cpp:124] Setting up accuracy
I0428 20:21:03.930361  1684 net.cpp:131] Top shape: (1)
I0428 20:21:03.930363  1684 net.cpp:139] Memory required for data: 16370804
I0428 20:21:03.930366  1684 layer_factory.hpp:77] Creating layer loss
I0428 20:21:03.930383  1684 net.cpp:86] Creating Layer loss
I0428 20:21:03.930387  1684 net.cpp:408] loss <- ip2_relu2_0_split_1
I0428 20:21:03.930392  1684 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:21:03.930395  1684 net.cpp:382] loss -> loss
I0428 20:21:03.930402  1684 layer_factory.hpp:77] Creating layer loss
I0428 20:21:03.930645  1684 net.cpp:124] Setting up loss
I0428 20:21:03.930655  1684 net.cpp:131] Top shape: (1)
I0428 20:21:03.930658  1684 net.cpp:134]     with loss weight 1
I0428 20:21:03.930665  1684 net.cpp:139] Memory required for data: 16370808
I0428 20:21:03.930668  1684 net.cpp:200] loss needs backward computation.
I0428 20:21:03.930672  1684 net.cpp:202] accuracy does not need backward computation.
I0428 20:21:03.930677  1684 net.cpp:200] ip2_relu2_0_split needs backward computation.
I0428 20:21:03.930680  1684 net.cpp:200] relu2 needs backward computation.
I0428 20:21:03.930683  1684 net.cpp:200] ip2 needs backward computation.
I0428 20:21:03.930686  1684 net.cpp:200] relu1 needs backward computation.
I0428 20:21:03.930690  1684 net.cpp:200] ip1 needs backward computation.
I0428 20:21:03.930692  1684 net.cpp:200] pool1 needs backward computation.
I0428 20:21:03.930696  1684 net.cpp:200] conv1 needs backward computation.
I0428 20:21:03.930699  1684 net.cpp:200] pool0 needs backward computation.
I0428 20:21:03.930702  1684 net.cpp:200] conv0 needs backward computation.
I0428 20:21:03.930706  1684 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:21:03.930716  1684 net.cpp:202] mnist does not need backward computation.
I0428 20:21:03.930718  1684 net.cpp:244] This network produces output accuracy
I0428 20:21:03.930722  1684 net.cpp:244] This network produces output loss
I0428 20:21:03.930733  1684 net.cpp:257] Network initialization done.
I0428 20:21:03.930771  1684 solver.cpp:56] Solver scaffolding done.
I0428 20:21:03.931038  1684 caffe.cpp:248] Starting Optimization
I0428 20:21:03.931046  1684 solver.cpp:273] Solving LeNet
I0428 20:21:03.931048  1684 solver.cpp:274] Learning Rate Policy: inv
I0428 20:21:03.931834  1684 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:21:03.938318  1684 blocking_queue.cpp:49] Waiting for data
I0428 20:21:04.011760  1691 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:21:04.012734  1684 solver.cpp:398]     Test net output #0: accuracy = 0.119
I0428 20:21:04.012769  1684 solver.cpp:398]     Test net output #1: loss = 2.3037 (* 1 = 2.3037 loss)
I0428 20:21:04.017449  1684 solver.cpp:219] Iteration 0 (0 iter/s, 0.0863734s/100 iters), loss = 2.28283
I0428 20:21:04.017472  1684 solver.cpp:238]     Train net output #0: loss = 2.28283 (* 1 = 2.28283 loss)
I0428 20:21:04.017482  1684 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:21:04.171649  1684 solver.cpp:219] Iteration 100 (648.676 iter/s, 0.15416s/100 iters), loss = 0.695111
I0428 20:21:04.171689  1684 solver.cpp:238]     Train net output #0: loss = 0.695111 (* 1 = 0.695111 loss)
I0428 20:21:04.171696  1684 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:21:04.325516  1684 solver.cpp:219] Iteration 200 (650.132 iter/s, 0.153815s/100 iters), loss = 0.545296
I0428 20:21:04.325556  1684 solver.cpp:238]     Train net output #0: loss = 0.545296 (* 1 = 0.545296 loss)
I0428 20:21:04.325562  1684 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:21:04.479074  1684 solver.cpp:219] Iteration 300 (651.376 iter/s, 0.153521s/100 iters), loss = 0.419216
I0428 20:21:04.479111  1684 solver.cpp:238]     Train net output #0: loss = 0.419216 (* 1 = 0.419216 loss)
I0428 20:21:04.479117  1684 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:21:04.632499  1684 solver.cpp:219] Iteration 400 (651.932 iter/s, 0.15339s/100 iters), loss = 0.257794
I0428 20:21:04.632539  1684 solver.cpp:238]     Train net output #0: loss = 0.257794 (* 1 = 0.257794 loss)
I0428 20:21:04.632560  1684 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:21:04.784039  1684 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:21:04.853365  1691 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:21:04.855820  1684 solver.cpp:398]     Test net output #0: accuracy = 0.8739
I0428 20:21:04.855856  1684 solver.cpp:398]     Test net output #1: loss = 0.326571 (* 1 = 0.326571 loss)
I0428 20:21:04.857282  1684 solver.cpp:219] Iteration 500 (444.982 iter/s, 0.224728s/100 iters), loss = 0.336727
I0428 20:21:04.857321  1684 solver.cpp:238]     Train net output #0: loss = 0.336727 (* 1 = 0.336727 loss)
I0428 20:21:04.857343  1684 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:21:05.007159  1684 solver.cpp:219] Iteration 600 (667.439 iter/s, 0.149826s/100 iters), loss = 0.408928
I0428 20:21:05.007200  1684 solver.cpp:238]     Train net output #0: loss = 0.408928 (* 1 = 0.408928 loss)
I0428 20:21:05.007205  1684 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:21:05.156474  1684 solver.cpp:219] Iteration 700 (669.884 iter/s, 0.14928s/100 iters), loss = 0.369268
I0428 20:21:05.156500  1684 solver.cpp:238]     Train net output #0: loss = 0.369268 (* 1 = 0.369268 loss)
I0428 20:21:05.156522  1684 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:21:05.309640  1684 solver.cpp:219] Iteration 800 (653.044 iter/s, 0.153129s/100 iters), loss = 0.556176
I0428 20:21:05.309680  1684 solver.cpp:238]     Train net output #0: loss = 0.556176 (* 1 = 0.556176 loss)
I0428 20:21:05.309686  1684 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:21:05.459552  1684 solver.cpp:219] Iteration 900 (667.229 iter/s, 0.149874s/100 iters), loss = 0.490161
I0428 20:21:05.459606  1684 solver.cpp:238]     Train net output #0: loss = 0.490161 (* 1 = 0.490161 loss)
I0428 20:21:05.459614  1684 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:21:05.510463  1690 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:21:05.611593  1684 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:21:05.614068  1684 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:21:05.615466  1684 solver.cpp:311] Iteration 1000, loss = 0.373539
I0428 20:21:05.615481  1684 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:21:05.685680  1691 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:21:05.688262  1684 solver.cpp:398]     Test net output #0: accuracy = 0.8855
I0428 20:21:05.688297  1684 solver.cpp:398]     Test net output #1: loss = 0.294707 (* 1 = 0.294707 loss)
I0428 20:21:05.688303  1684 solver.cpp:316] Optimization Done.
I0428 20:21:05.688307  1684 caffe.cpp:259] Optimization Done.
