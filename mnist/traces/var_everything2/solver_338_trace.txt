I0428 19:40:33.435515 24579 caffe.cpp:218] Using GPUs 0
I0428 19:40:33.464346 24579 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:40:33.913082 24579 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test338.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:40:33.913277 24579 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test338.prototxt
I0428 19:40:33.913617 24579 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:40:33.913646 24579 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:40:33.913727 24579 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:40:33.913789 24579 layer_factory.hpp:77] Creating layer mnist
I0428 19:40:33.913866 24579 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:40:33.913885 24579 net.cpp:86] Creating Layer mnist
I0428 19:40:33.913892 24579 net.cpp:382] mnist -> data
I0428 19:40:33.913908 24579 net.cpp:382] mnist -> label
I0428 19:40:33.914855 24579 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:40:33.917168 24579 net.cpp:124] Setting up mnist
I0428 19:40:33.917213 24579 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:40:33.917218 24579 net.cpp:131] Top shape: 64 (64)
I0428 19:40:33.917222 24579 net.cpp:139] Memory required for data: 200960
I0428 19:40:33.917227 24579 layer_factory.hpp:77] Creating layer conv0
I0428 19:40:33.917273 24579 net.cpp:86] Creating Layer conv0
I0428 19:40:33.917290 24579 net.cpp:408] conv0 <- data
I0428 19:40:33.917302 24579 net.cpp:382] conv0 -> conv0
I0428 19:40:34.144703 24579 net.cpp:124] Setting up conv0
I0428 19:40:34.144744 24579 net.cpp:131] Top shape: 64 2 24 24 (73728)
I0428 19:40:34.144747 24579 net.cpp:139] Memory required for data: 495872
I0428 19:40:34.144762 24579 layer_factory.hpp:77] Creating layer pool0
I0428 19:40:34.144774 24579 net.cpp:86] Creating Layer pool0
I0428 19:40:34.144778 24579 net.cpp:408] pool0 <- conv0
I0428 19:40:34.144783 24579 net.cpp:382] pool0 -> pool0
I0428 19:40:34.144888 24579 net.cpp:124] Setting up pool0
I0428 19:40:34.144918 24579 net.cpp:131] Top shape: 64 2 12 12 (18432)
I0428 19:40:34.144922 24579 net.cpp:139] Memory required for data: 569600
I0428 19:40:34.144924 24579 layer_factory.hpp:77] Creating layer conv1
I0428 19:40:34.144937 24579 net.cpp:86] Creating Layer conv1
I0428 19:40:34.144939 24579 net.cpp:408] conv1 <- pool0
I0428 19:40:34.144944 24579 net.cpp:382] conv1 -> conv1
I0428 19:40:34.146775 24579 net.cpp:124] Setting up conv1
I0428 19:40:34.146805 24579 net.cpp:131] Top shape: 64 5 8 8 (20480)
I0428 19:40:34.146807 24579 net.cpp:139] Memory required for data: 651520
I0428 19:40:34.146831 24579 layer_factory.hpp:77] Creating layer pool1
I0428 19:40:34.146839 24579 net.cpp:86] Creating Layer pool1
I0428 19:40:34.146842 24579 net.cpp:408] pool1 <- conv1
I0428 19:40:34.146847 24579 net.cpp:382] pool1 -> pool1
I0428 19:40:34.146883 24579 net.cpp:124] Setting up pool1
I0428 19:40:34.146889 24579 net.cpp:131] Top shape: 64 5 4 4 (5120)
I0428 19:40:34.146891 24579 net.cpp:139] Memory required for data: 672000
I0428 19:40:34.146894 24579 layer_factory.hpp:77] Creating layer ip1
I0428 19:40:34.146903 24579 net.cpp:86] Creating Layer ip1
I0428 19:40:34.146905 24579 net.cpp:408] ip1 <- pool1
I0428 19:40:34.146909 24579 net.cpp:382] ip1 -> ip1
I0428 19:40:34.147851 24579 net.cpp:124] Setting up ip1
I0428 19:40:34.147864 24579 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:40:34.147882 24579 net.cpp:139] Memory required for data: 678400
I0428 19:40:34.147891 24579 layer_factory.hpp:77] Creating layer relu1
I0428 19:40:34.147897 24579 net.cpp:86] Creating Layer relu1
I0428 19:40:34.147900 24579 net.cpp:408] relu1 <- ip1
I0428 19:40:34.147905 24579 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:40:34.148080 24579 net.cpp:124] Setting up relu1
I0428 19:40:34.148089 24579 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:40:34.148092 24579 net.cpp:139] Memory required for data: 684800
I0428 19:40:34.148095 24579 layer_factory.hpp:77] Creating layer ip2
I0428 19:40:34.148102 24579 net.cpp:86] Creating Layer ip2
I0428 19:40:34.148104 24579 net.cpp:408] ip2 <- ip1
I0428 19:40:34.148109 24579 net.cpp:382] ip2 -> ip2
I0428 19:40:34.148219 24579 net.cpp:124] Setting up ip2
I0428 19:40:34.148226 24579 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:40:34.148229 24579 net.cpp:139] Memory required for data: 691200
I0428 19:40:34.148234 24579 layer_factory.hpp:77] Creating layer relu2
I0428 19:40:34.148241 24579 net.cpp:86] Creating Layer relu2
I0428 19:40:34.148243 24579 net.cpp:408] relu2 <- ip2
I0428 19:40:34.148247 24579 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:40:34.149009 24579 net.cpp:124] Setting up relu2
I0428 19:40:34.149036 24579 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:40:34.149040 24579 net.cpp:139] Memory required for data: 697600
I0428 19:40:34.149044 24579 layer_factory.hpp:77] Creating layer ip3
I0428 19:40:34.149051 24579 net.cpp:86] Creating Layer ip3
I0428 19:40:34.149056 24579 net.cpp:408] ip3 <- ip2
I0428 19:40:34.149061 24579 net.cpp:382] ip3 -> ip3
I0428 19:40:34.149161 24579 net.cpp:124] Setting up ip3
I0428 19:40:34.149169 24579 net.cpp:131] Top shape: 64 10 (640)
I0428 19:40:34.149173 24579 net.cpp:139] Memory required for data: 700160
I0428 19:40:34.149180 24579 layer_factory.hpp:77] Creating layer relu3
I0428 19:40:34.149184 24579 net.cpp:86] Creating Layer relu3
I0428 19:40:34.149188 24579 net.cpp:408] relu3 <- ip3
I0428 19:40:34.149191 24579 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:40:34.149369 24579 net.cpp:124] Setting up relu3
I0428 19:40:34.149379 24579 net.cpp:131] Top shape: 64 10 (640)
I0428 19:40:34.149381 24579 net.cpp:139] Memory required for data: 702720
I0428 19:40:34.149384 24579 layer_factory.hpp:77] Creating layer loss
I0428 19:40:34.149390 24579 net.cpp:86] Creating Layer loss
I0428 19:40:34.149394 24579 net.cpp:408] loss <- ip3
I0428 19:40:34.149396 24579 net.cpp:408] loss <- label
I0428 19:40:34.149401 24579 net.cpp:382] loss -> loss
I0428 19:40:34.149415 24579 layer_factory.hpp:77] Creating layer loss
I0428 19:40:34.149634 24579 net.cpp:124] Setting up loss
I0428 19:40:34.149643 24579 net.cpp:131] Top shape: (1)
I0428 19:40:34.149646 24579 net.cpp:134]     with loss weight 1
I0428 19:40:34.149660 24579 net.cpp:139] Memory required for data: 702724
I0428 19:40:34.149663 24579 net.cpp:200] loss needs backward computation.
I0428 19:40:34.149667 24579 net.cpp:200] relu3 needs backward computation.
I0428 19:40:34.149669 24579 net.cpp:200] ip3 needs backward computation.
I0428 19:40:34.149672 24579 net.cpp:200] relu2 needs backward computation.
I0428 19:40:34.149674 24579 net.cpp:200] ip2 needs backward computation.
I0428 19:40:34.149677 24579 net.cpp:200] relu1 needs backward computation.
I0428 19:40:34.149680 24579 net.cpp:200] ip1 needs backward computation.
I0428 19:40:34.149683 24579 net.cpp:200] pool1 needs backward computation.
I0428 19:40:34.149685 24579 net.cpp:200] conv1 needs backward computation.
I0428 19:40:34.149688 24579 net.cpp:200] pool0 needs backward computation.
I0428 19:40:34.149691 24579 net.cpp:200] conv0 needs backward computation.
I0428 19:40:34.149694 24579 net.cpp:202] mnist does not need backward computation.
I0428 19:40:34.149698 24579 net.cpp:244] This network produces output loss
I0428 19:40:34.149716 24579 net.cpp:257] Network initialization done.
I0428 19:40:34.150066 24579 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test338.prototxt
I0428 19:40:34.150108 24579 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:40:34.150202 24579 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:40:34.150293 24579 layer_factory.hpp:77] Creating layer mnist
I0428 19:40:34.150336 24579 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:40:34.150348 24579 net.cpp:86] Creating Layer mnist
I0428 19:40:34.150352 24579 net.cpp:382] mnist -> data
I0428 19:40:34.150374 24579 net.cpp:382] mnist -> label
I0428 19:40:34.150452 24579 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:40:34.152329 24579 net.cpp:124] Setting up mnist
I0428 19:40:34.152374 24579 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:40:34.152379 24579 net.cpp:131] Top shape: 100 (100)
I0428 19:40:34.152380 24579 net.cpp:139] Memory required for data: 314000
I0428 19:40:34.152384 24579 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:40:34.152391 24579 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:40:34.152395 24579 net.cpp:408] label_mnist_1_split <- label
I0428 19:40:34.152400 24579 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:40:34.152405 24579 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:40:34.152456 24579 net.cpp:124] Setting up label_mnist_1_split
I0428 19:40:34.152460 24579 net.cpp:131] Top shape: 100 (100)
I0428 19:40:34.152464 24579 net.cpp:131] Top shape: 100 (100)
I0428 19:40:34.152467 24579 net.cpp:139] Memory required for data: 314800
I0428 19:40:34.152470 24579 layer_factory.hpp:77] Creating layer conv0
I0428 19:40:34.152493 24579 net.cpp:86] Creating Layer conv0
I0428 19:40:34.152495 24579 net.cpp:408] conv0 <- data
I0428 19:40:34.152500 24579 net.cpp:382] conv0 -> conv0
I0428 19:40:34.154361 24579 net.cpp:124] Setting up conv0
I0428 19:40:34.154391 24579 net.cpp:131] Top shape: 100 2 24 24 (115200)
I0428 19:40:34.154393 24579 net.cpp:139] Memory required for data: 775600
I0428 19:40:34.154402 24579 layer_factory.hpp:77] Creating layer pool0
I0428 19:40:34.154409 24579 net.cpp:86] Creating Layer pool0
I0428 19:40:34.154412 24579 net.cpp:408] pool0 <- conv0
I0428 19:40:34.154417 24579 net.cpp:382] pool0 -> pool0
I0428 19:40:34.154469 24579 net.cpp:124] Setting up pool0
I0428 19:40:34.154474 24579 net.cpp:131] Top shape: 100 2 12 12 (28800)
I0428 19:40:34.154477 24579 net.cpp:139] Memory required for data: 890800
I0428 19:40:34.154480 24579 layer_factory.hpp:77] Creating layer conv1
I0428 19:40:34.154489 24579 net.cpp:86] Creating Layer conv1
I0428 19:40:34.154506 24579 net.cpp:408] conv1 <- pool0
I0428 19:40:34.154511 24579 net.cpp:382] conv1 -> conv1
I0428 19:40:34.156041 24579 net.cpp:124] Setting up conv1
I0428 19:40:34.156070 24579 net.cpp:131] Top shape: 100 5 8 8 (32000)
I0428 19:40:34.156075 24579 net.cpp:139] Memory required for data: 1018800
I0428 19:40:34.156083 24579 layer_factory.hpp:77] Creating layer pool1
I0428 19:40:34.156091 24579 net.cpp:86] Creating Layer pool1
I0428 19:40:34.156095 24579 net.cpp:408] pool1 <- conv1
I0428 19:40:34.156101 24579 net.cpp:382] pool1 -> pool1
I0428 19:40:34.156141 24579 net.cpp:124] Setting up pool1
I0428 19:40:34.156147 24579 net.cpp:131] Top shape: 100 5 4 4 (8000)
I0428 19:40:34.156149 24579 net.cpp:139] Memory required for data: 1050800
I0428 19:40:34.156153 24579 layer_factory.hpp:77] Creating layer ip1
I0428 19:40:34.156159 24579 net.cpp:86] Creating Layer ip1
I0428 19:40:34.156162 24579 net.cpp:408] ip1 <- pool1
I0428 19:40:34.156169 24579 net.cpp:382] ip1 -> ip1
I0428 19:40:34.156286 24579 net.cpp:124] Setting up ip1
I0428 19:40:34.156293 24579 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:40:34.156308 24579 net.cpp:139] Memory required for data: 1060800
I0428 19:40:34.156332 24579 layer_factory.hpp:77] Creating layer relu1
I0428 19:40:34.156339 24579 net.cpp:86] Creating Layer relu1
I0428 19:40:34.156343 24579 net.cpp:408] relu1 <- ip1
I0428 19:40:34.156349 24579 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:40:34.156520 24579 net.cpp:124] Setting up relu1
I0428 19:40:34.156530 24579 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:40:34.156540 24579 net.cpp:139] Memory required for data: 1070800
I0428 19:40:34.156544 24579 layer_factory.hpp:77] Creating layer ip2
I0428 19:40:34.156553 24579 net.cpp:86] Creating Layer ip2
I0428 19:40:34.156556 24579 net.cpp:408] ip2 <- ip1
I0428 19:40:34.156561 24579 net.cpp:382] ip2 -> ip2
I0428 19:40:34.156688 24579 net.cpp:124] Setting up ip2
I0428 19:40:34.156697 24579 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:40:34.156699 24579 net.cpp:139] Memory required for data: 1080800
I0428 19:40:34.156704 24579 layer_factory.hpp:77] Creating layer relu2
I0428 19:40:34.156709 24579 net.cpp:86] Creating Layer relu2
I0428 19:40:34.156713 24579 net.cpp:408] relu2 <- ip2
I0428 19:40:34.156718 24579 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:40:34.156934 24579 net.cpp:124] Setting up relu2
I0428 19:40:34.156942 24579 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:40:34.156945 24579 net.cpp:139] Memory required for data: 1090800
I0428 19:40:34.156949 24579 layer_factory.hpp:77] Creating layer ip3
I0428 19:40:34.156955 24579 net.cpp:86] Creating Layer ip3
I0428 19:40:34.156957 24579 net.cpp:408] ip3 <- ip2
I0428 19:40:34.156963 24579 net.cpp:382] ip3 -> ip3
I0428 19:40:34.157083 24579 net.cpp:124] Setting up ip3
I0428 19:40:34.157089 24579 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:40:34.157094 24579 net.cpp:139] Memory required for data: 1094800
I0428 19:40:34.157101 24579 layer_factory.hpp:77] Creating layer relu3
I0428 19:40:34.157106 24579 net.cpp:86] Creating Layer relu3
I0428 19:40:34.157109 24579 net.cpp:408] relu3 <- ip3
I0428 19:40:34.157114 24579 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:40:34.158025 24579 net.cpp:124] Setting up relu3
I0428 19:40:34.158036 24579 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:40:34.158054 24579 net.cpp:139] Memory required for data: 1098800
I0428 19:40:34.158057 24579 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:40:34.158064 24579 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:40:34.158067 24579 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:40:34.158072 24579 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:40:34.158079 24579 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:40:34.158133 24579 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:40:34.158138 24579 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:40:34.158149 24579 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:40:34.158150 24579 net.cpp:139] Memory required for data: 1106800
I0428 19:40:34.158154 24579 layer_factory.hpp:77] Creating layer accuracy
I0428 19:40:34.158159 24579 net.cpp:86] Creating Layer accuracy
I0428 19:40:34.158161 24579 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:40:34.158165 24579 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:40:34.158171 24579 net.cpp:382] accuracy -> accuracy
I0428 19:40:34.158179 24579 net.cpp:124] Setting up accuracy
I0428 19:40:34.158184 24579 net.cpp:131] Top shape: (1)
I0428 19:40:34.158186 24579 net.cpp:139] Memory required for data: 1106804
I0428 19:40:34.158190 24579 layer_factory.hpp:77] Creating layer loss
I0428 19:40:34.158193 24579 net.cpp:86] Creating Layer loss
I0428 19:40:34.158197 24579 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:40:34.158200 24579 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:40:34.158205 24579 net.cpp:382] loss -> loss
I0428 19:40:34.158217 24579 layer_factory.hpp:77] Creating layer loss
I0428 19:40:34.158459 24579 net.cpp:124] Setting up loss
I0428 19:40:34.158469 24579 net.cpp:131] Top shape: (1)
I0428 19:40:34.158473 24579 net.cpp:134]     with loss weight 1
I0428 19:40:34.158479 24579 net.cpp:139] Memory required for data: 1106808
I0428 19:40:34.158493 24579 net.cpp:200] loss needs backward computation.
I0428 19:40:34.158498 24579 net.cpp:202] accuracy does not need backward computation.
I0428 19:40:34.158500 24579 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:40:34.158504 24579 net.cpp:200] relu3 needs backward computation.
I0428 19:40:34.158507 24579 net.cpp:200] ip3 needs backward computation.
I0428 19:40:34.158510 24579 net.cpp:200] relu2 needs backward computation.
I0428 19:40:34.158512 24579 net.cpp:200] ip2 needs backward computation.
I0428 19:40:34.158516 24579 net.cpp:200] relu1 needs backward computation.
I0428 19:40:34.158519 24579 net.cpp:200] ip1 needs backward computation.
I0428 19:40:34.158522 24579 net.cpp:200] pool1 needs backward computation.
I0428 19:40:34.158525 24579 net.cpp:200] conv1 needs backward computation.
I0428 19:40:34.158529 24579 net.cpp:200] pool0 needs backward computation.
I0428 19:40:34.158546 24579 net.cpp:200] conv0 needs backward computation.
I0428 19:40:34.158550 24579 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:40:34.158553 24579 net.cpp:202] mnist does not need backward computation.
I0428 19:40:34.158556 24579 net.cpp:244] This network produces output accuracy
I0428 19:40:34.158560 24579 net.cpp:244] This network produces output loss
I0428 19:40:34.158571 24579 net.cpp:257] Network initialization done.
I0428 19:40:34.158612 24579 solver.cpp:56] Solver scaffolding done.
I0428 19:40:34.159036 24579 caffe.cpp:248] Starting Optimization
I0428 19:40:34.159042 24579 solver.cpp:273] Solving LeNet
I0428 19:40:34.159045 24579 solver.cpp:274] Learning Rate Policy: inv
I0428 19:40:34.159988 24579 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:40:34.162463 24579 blocking_queue.cpp:49] Waiting for data
I0428 19:40:34.233947 24586 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:40:34.234398 24579 solver.cpp:398]     Test net output #0: accuracy = 0.1079
I0428 19:40:34.234431 24579 solver.cpp:398]     Test net output #1: loss = 2.29775 (* 1 = 2.29775 loss)
I0428 19:40:34.236234 24579 solver.cpp:219] Iteration 0 (0 iter/s, 0.0771421s/100 iters), loss = 2.29928
I0428 19:40:34.236279 24579 solver.cpp:238]     Train net output #0: loss = 2.29928 (* 1 = 2.29928 loss)
I0428 19:40:34.236291 24579 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:40:34.309077 24579 solver.cpp:219] Iteration 100 (1373.87 iter/s, 0.0727872s/100 iters), loss = 1.0425
I0428 19:40:34.309118 24579 solver.cpp:238]     Train net output #0: loss = 1.0425 (* 1 = 1.0425 loss)
I0428 19:40:34.309124 24579 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:40:34.378631 24579 solver.cpp:219] Iteration 200 (1438.79 iter/s, 0.069503s/100 iters), loss = 0.780884
I0428 19:40:34.378674 24579 solver.cpp:238]     Train net output #0: loss = 0.780884 (* 1 = 0.780884 loss)
I0428 19:40:34.378680 24579 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:40:34.447175 24579 solver.cpp:219] Iteration 300 (1459.71 iter/s, 0.0685066s/100 iters), loss = 0.858045
I0428 19:40:34.447216 24579 solver.cpp:238]     Train net output #0: loss = 0.858045 (* 1 = 0.858045 loss)
I0428 19:40:34.447222 24579 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:40:34.516243 24579 solver.cpp:219] Iteration 400 (1448.93 iter/s, 0.0690166s/100 iters), loss = 0.697582
I0428 19:40:34.516284 24579 solver.cpp:238]     Train net output #0: loss = 0.697582 (* 1 = 0.697582 loss)
I0428 19:40:34.516290 24579 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:40:34.585314 24579 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:40:34.660910 24586 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:40:34.661362 24579 solver.cpp:398]     Test net output #0: accuracy = 0.8475
I0428 19:40:34.661383 24579 solver.cpp:398]     Test net output #1: loss = 0.434779 (* 1 = 0.434779 loss)
I0428 19:40:34.662160 24579 solver.cpp:219] Iteration 500 (685.572 iter/s, 0.145864s/100 iters), loss = 0.323711
I0428 19:40:34.662201 24579 solver.cpp:238]     Train net output #0: loss = 0.323711 (* 1 = 0.323711 loss)
I0428 19:40:34.662222 24579 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:40:34.733845 24579 solver.cpp:219] Iteration 600 (1396.01 iter/s, 0.0716329s/100 iters), loss = 0.434466
I0428 19:40:34.733885 24579 solver.cpp:238]     Train net output #0: loss = 0.434466 (* 1 = 0.434466 loss)
I0428 19:40:34.733891 24579 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:40:34.805253 24579 solver.cpp:219] Iteration 700 (1401.13 iter/s, 0.071371s/100 iters), loss = 0.417812
I0428 19:40:34.805300 24579 solver.cpp:238]     Train net output #0: loss = 0.417812 (* 1 = 0.417812 loss)
I0428 19:40:34.805307 24579 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:40:34.874686 24579 solver.cpp:219] Iteration 800 (1441.23 iter/s, 0.0693852s/100 iters), loss = 0.526262
I0428 19:40:34.874725 24579 solver.cpp:238]     Train net output #0: loss = 0.526262 (* 1 = 0.526262 loss)
I0428 19:40:34.874732 24579 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:40:34.943071 24579 solver.cpp:219] Iteration 900 (1463.39 iter/s, 0.0683346s/100 iters), loss = 0.535655
I0428 19:40:34.943111 24579 solver.cpp:238]     Train net output #0: loss = 0.535655 (* 1 = 0.535655 loss)
I0428 19:40:34.943119 24579 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:40:34.966107 24585 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:40:35.011512 24579 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:40:35.012233 24579 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:40:35.012770 24579 solver.cpp:311] Iteration 1000, loss = 0.242046
I0428 19:40:35.012801 24579 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:40:35.087107 24586 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:40:35.087599 24579 solver.cpp:398]     Test net output #0: accuracy = 0.9502
I0428 19:40:35.087633 24579 solver.cpp:398]     Test net output #1: loss = 0.163918 (* 1 = 0.163918 loss)
I0428 19:40:35.087638 24579 solver.cpp:316] Optimization Done.
I0428 19:40:35.087641 24579 caffe.cpp:259] Optimization Done.
