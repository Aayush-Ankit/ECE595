I0428 20:08:50.865134 31499 caffe.cpp:218] Using GPUs 0
I0428 20:08:50.904348 31499 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:08:51.424162 31499 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1094.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:08:51.424298 31499 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1094.prototxt
I0428 20:08:51.424638 31499 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:08:51.424654 31499 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:08:51.424736 31499 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0428 20:08:51.424804 31499 layer_factory.hpp:77] Creating layer mnist
I0428 20:08:51.424926 31499 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:08:51.424952 31499 net.cpp:86] Creating Layer mnist
I0428 20:08:51.424959 31499 net.cpp:382] mnist -> data
I0428 20:08:51.424981 31499 net.cpp:382] mnist -> label
I0428 20:08:51.426089 31499 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:08:51.428557 31499 net.cpp:124] Setting up mnist
I0428 20:08:51.428575 31499 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:08:51.428580 31499 net.cpp:131] Top shape: 64 (64)
I0428 20:08:51.428584 31499 net.cpp:139] Memory required for data: 200960
I0428 20:08:51.428591 31499 layer_factory.hpp:77] Creating layer conv0
I0428 20:08:51.428608 31499 net.cpp:86] Creating Layer conv0
I0428 20:08:51.428613 31499 net.cpp:408] conv0 <- data
I0428 20:08:51.428627 31499 net.cpp:382] conv0 -> conv0
I0428 20:08:51.699645 31499 net.cpp:124] Setting up conv0
I0428 20:08:51.699671 31499 net.cpp:131] Top shape: 64 25 24 24 (921600)
I0428 20:08:51.699674 31499 net.cpp:139] Memory required for data: 3887360
I0428 20:08:51.699689 31499 layer_factory.hpp:77] Creating layer pool0
I0428 20:08:51.699702 31499 net.cpp:86] Creating Layer pool0
I0428 20:08:51.699707 31499 net.cpp:408] pool0 <- conv0
I0428 20:08:51.699710 31499 net.cpp:382] pool0 -> pool0
I0428 20:08:51.699784 31499 net.cpp:124] Setting up pool0
I0428 20:08:51.699790 31499 net.cpp:131] Top shape: 64 25 12 12 (230400)
I0428 20:08:51.699807 31499 net.cpp:139] Memory required for data: 4808960
I0428 20:08:51.699811 31499 layer_factory.hpp:77] Creating layer conv1
I0428 20:08:51.699821 31499 net.cpp:86] Creating Layer conv1
I0428 20:08:51.699825 31499 net.cpp:408] conv1 <- pool0
I0428 20:08:51.699829 31499 net.cpp:382] conv1 -> conv1
I0428 20:08:51.702708 31499 net.cpp:124] Setting up conv1
I0428 20:08:51.702739 31499 net.cpp:131] Top shape: 64 25 8 8 (102400)
I0428 20:08:51.702742 31499 net.cpp:139] Memory required for data: 5218560
I0428 20:08:51.702751 31499 layer_factory.hpp:77] Creating layer pool1
I0428 20:08:51.702760 31499 net.cpp:86] Creating Layer pool1
I0428 20:08:51.702764 31499 net.cpp:408] pool1 <- conv1
I0428 20:08:51.702769 31499 net.cpp:382] pool1 -> pool1
I0428 20:08:51.702818 31499 net.cpp:124] Setting up pool1
I0428 20:08:51.702824 31499 net.cpp:131] Top shape: 64 25 4 4 (25600)
I0428 20:08:51.702827 31499 net.cpp:139] Memory required for data: 5320960
I0428 20:08:51.702829 31499 layer_factory.hpp:77] Creating layer ip1
I0428 20:08:51.702836 31499 net.cpp:86] Creating Layer ip1
I0428 20:08:51.702839 31499 net.cpp:408] ip1 <- pool1
I0428 20:08:51.702844 31499 net.cpp:382] ip1 -> ip1
I0428 20:08:51.703830 31499 net.cpp:124] Setting up ip1
I0428 20:08:51.703842 31499 net.cpp:131] Top shape: 64 10 (640)
I0428 20:08:51.703861 31499 net.cpp:139] Memory required for data: 5323520
I0428 20:08:51.703869 31499 layer_factory.hpp:77] Creating layer relu1
I0428 20:08:51.703876 31499 net.cpp:86] Creating Layer relu1
I0428 20:08:51.703878 31499 net.cpp:408] relu1 <- ip1
I0428 20:08:51.703883 31499 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:08:51.704071 31499 net.cpp:124] Setting up relu1
I0428 20:08:51.704078 31499 net.cpp:131] Top shape: 64 10 (640)
I0428 20:08:51.704082 31499 net.cpp:139] Memory required for data: 5326080
I0428 20:08:51.704084 31499 layer_factory.hpp:77] Creating layer loss
I0428 20:08:51.704090 31499 net.cpp:86] Creating Layer loss
I0428 20:08:51.704093 31499 net.cpp:408] loss <- ip1
I0428 20:08:51.704097 31499 net.cpp:408] loss <- label
I0428 20:08:51.704102 31499 net.cpp:382] loss -> loss
I0428 20:08:51.704114 31499 layer_factory.hpp:77] Creating layer loss
I0428 20:08:51.705023 31499 net.cpp:124] Setting up loss
I0428 20:08:51.705065 31499 net.cpp:131] Top shape: (1)
I0428 20:08:51.705070 31499 net.cpp:134]     with loss weight 1
I0428 20:08:51.705083 31499 net.cpp:139] Memory required for data: 5326084
I0428 20:08:51.705087 31499 net.cpp:200] loss needs backward computation.
I0428 20:08:51.705091 31499 net.cpp:200] relu1 needs backward computation.
I0428 20:08:51.705094 31499 net.cpp:200] ip1 needs backward computation.
I0428 20:08:51.705097 31499 net.cpp:200] pool1 needs backward computation.
I0428 20:08:51.705101 31499 net.cpp:200] conv1 needs backward computation.
I0428 20:08:51.705103 31499 net.cpp:200] pool0 needs backward computation.
I0428 20:08:51.705106 31499 net.cpp:200] conv0 needs backward computation.
I0428 20:08:51.705111 31499 net.cpp:202] mnist does not need backward computation.
I0428 20:08:51.705113 31499 net.cpp:244] This network produces output loss
I0428 20:08:51.705121 31499 net.cpp:257] Network initialization done.
I0428 20:08:51.705426 31499 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1094.prototxt
I0428 20:08:51.705449 31499 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:08:51.705521 31499 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0428 20:08:51.705588 31499 layer_factory.hpp:77] Creating layer mnist
I0428 20:08:51.705629 31499 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:08:51.705641 31499 net.cpp:86] Creating Layer mnist
I0428 20:08:51.705646 31499 net.cpp:382] mnist -> data
I0428 20:08:51.705653 31499 net.cpp:382] mnist -> label
I0428 20:08:51.705731 31499 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:08:51.707721 31499 net.cpp:124] Setting up mnist
I0428 20:08:51.707748 31499 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:08:51.707753 31499 net.cpp:131] Top shape: 100 (100)
I0428 20:08:51.707756 31499 net.cpp:139] Memory required for data: 314000
I0428 20:08:51.707759 31499 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:08:51.707767 31499 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:08:51.707769 31499 net.cpp:408] label_mnist_1_split <- label
I0428 20:08:51.707773 31499 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:08:51.707779 31499 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:08:51.707849 31499 net.cpp:124] Setting up label_mnist_1_split
I0428 20:08:51.707854 31499 net.cpp:131] Top shape: 100 (100)
I0428 20:08:51.707859 31499 net.cpp:131] Top shape: 100 (100)
I0428 20:08:51.707860 31499 net.cpp:139] Memory required for data: 314800
I0428 20:08:51.707864 31499 layer_factory.hpp:77] Creating layer conv0
I0428 20:08:51.707871 31499 net.cpp:86] Creating Layer conv0
I0428 20:08:51.707875 31499 net.cpp:408] conv0 <- data
I0428 20:08:51.707878 31499 net.cpp:382] conv0 -> conv0
I0428 20:08:51.708889 31499 net.cpp:124] Setting up conv0
I0428 20:08:51.708902 31499 net.cpp:131] Top shape: 100 25 24 24 (1440000)
I0428 20:08:51.708906 31499 net.cpp:139] Memory required for data: 6074800
I0428 20:08:51.708915 31499 layer_factory.hpp:77] Creating layer pool0
I0428 20:08:51.708920 31499 net.cpp:86] Creating Layer pool0
I0428 20:08:51.708925 31499 net.cpp:408] pool0 <- conv0
I0428 20:08:51.708928 31499 net.cpp:382] pool0 -> pool0
I0428 20:08:51.708961 31499 net.cpp:124] Setting up pool0
I0428 20:08:51.708982 31499 net.cpp:131] Top shape: 100 25 12 12 (360000)
I0428 20:08:51.708986 31499 net.cpp:139] Memory required for data: 7514800
I0428 20:08:51.708988 31499 layer_factory.hpp:77] Creating layer conv1
I0428 20:08:51.708997 31499 net.cpp:86] Creating Layer conv1
I0428 20:08:51.708999 31499 net.cpp:408] conv1 <- pool0
I0428 20:08:51.709005 31499 net.cpp:382] conv1 -> conv1
I0428 20:08:51.711218 31499 net.cpp:124] Setting up conv1
I0428 20:08:51.711231 31499 net.cpp:131] Top shape: 100 25 8 8 (160000)
I0428 20:08:51.711236 31499 net.cpp:139] Memory required for data: 8154800
I0428 20:08:51.711244 31499 layer_factory.hpp:77] Creating layer pool1
I0428 20:08:51.711266 31499 net.cpp:86] Creating Layer pool1
I0428 20:08:51.711289 31499 net.cpp:408] pool1 <- conv1
I0428 20:08:51.711295 31499 net.cpp:382] pool1 -> pool1
I0428 20:08:51.711334 31499 net.cpp:124] Setting up pool1
I0428 20:08:51.711340 31499 net.cpp:131] Top shape: 100 25 4 4 (40000)
I0428 20:08:51.711344 31499 net.cpp:139] Memory required for data: 8314800
I0428 20:08:51.711346 31499 layer_factory.hpp:77] Creating layer ip1
I0428 20:08:51.711352 31499 net.cpp:86] Creating Layer ip1
I0428 20:08:51.711356 31499 net.cpp:408] ip1 <- pool1
I0428 20:08:51.711361 31499 net.cpp:382] ip1 -> ip1
I0428 20:08:51.711475 31499 net.cpp:124] Setting up ip1
I0428 20:08:51.711484 31499 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:08:51.711503 31499 net.cpp:139] Memory required for data: 8318800
I0428 20:08:51.711524 31499 layer_factory.hpp:77] Creating layer relu1
I0428 20:08:51.711529 31499 net.cpp:86] Creating Layer relu1
I0428 20:08:51.711532 31499 net.cpp:408] relu1 <- ip1
I0428 20:08:51.711542 31499 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:08:51.712394 31499 net.cpp:124] Setting up relu1
I0428 20:08:51.712406 31499 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:08:51.712410 31499 net.cpp:139] Memory required for data: 8322800
I0428 20:08:51.712414 31499 layer_factory.hpp:77] Creating layer ip1_relu1_0_split
I0428 20:08:51.712421 31499 net.cpp:86] Creating Layer ip1_relu1_0_split
I0428 20:08:51.712424 31499 net.cpp:408] ip1_relu1_0_split <- ip1
I0428 20:08:51.712430 31499 net.cpp:382] ip1_relu1_0_split -> ip1_relu1_0_split_0
I0428 20:08:51.712436 31499 net.cpp:382] ip1_relu1_0_split -> ip1_relu1_0_split_1
I0428 20:08:51.712478 31499 net.cpp:124] Setting up ip1_relu1_0_split
I0428 20:08:51.712483 31499 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:08:51.712487 31499 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:08:51.712491 31499 net.cpp:139] Memory required for data: 8330800
I0428 20:08:51.712492 31499 layer_factory.hpp:77] Creating layer accuracy
I0428 20:08:51.712497 31499 net.cpp:86] Creating Layer accuracy
I0428 20:08:51.712502 31499 net.cpp:408] accuracy <- ip1_relu1_0_split_0
I0428 20:08:51.712505 31499 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:08:51.712509 31499 net.cpp:382] accuracy -> accuracy
I0428 20:08:51.712517 31499 net.cpp:124] Setting up accuracy
I0428 20:08:51.712520 31499 net.cpp:131] Top shape: (1)
I0428 20:08:51.712523 31499 net.cpp:139] Memory required for data: 8330804
I0428 20:08:51.712525 31499 layer_factory.hpp:77] Creating layer loss
I0428 20:08:51.712530 31499 net.cpp:86] Creating Layer loss
I0428 20:08:51.712533 31499 net.cpp:408] loss <- ip1_relu1_0_split_1
I0428 20:08:51.712555 31499 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:08:51.712559 31499 net.cpp:382] loss -> loss
I0428 20:08:51.712571 31499 layer_factory.hpp:77] Creating layer loss
I0428 20:08:51.712833 31499 net.cpp:124] Setting up loss
I0428 20:08:51.712843 31499 net.cpp:131] Top shape: (1)
I0428 20:08:51.712847 31499 net.cpp:134]     with loss weight 1
I0428 20:08:51.712853 31499 net.cpp:139] Memory required for data: 8330808
I0428 20:08:51.712857 31499 net.cpp:200] loss needs backward computation.
I0428 20:08:51.712877 31499 net.cpp:202] accuracy does not need backward computation.
I0428 20:08:51.712880 31499 net.cpp:200] ip1_relu1_0_split needs backward computation.
I0428 20:08:51.712883 31499 net.cpp:200] relu1 needs backward computation.
I0428 20:08:51.712887 31499 net.cpp:200] ip1 needs backward computation.
I0428 20:08:51.712889 31499 net.cpp:200] pool1 needs backward computation.
I0428 20:08:51.712893 31499 net.cpp:200] conv1 needs backward computation.
I0428 20:08:51.712919 31499 net.cpp:200] pool0 needs backward computation.
I0428 20:08:51.712939 31499 net.cpp:200] conv0 needs backward computation.
I0428 20:08:51.712942 31499 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:08:51.712946 31499 net.cpp:202] mnist does not need backward computation.
I0428 20:08:51.712949 31499 net.cpp:244] This network produces output accuracy
I0428 20:08:51.712954 31499 net.cpp:244] This network produces output loss
I0428 20:08:51.712973 31499 net.cpp:257] Network initialization done.
I0428 20:08:51.713007 31499 solver.cpp:56] Solver scaffolding done.
I0428 20:08:51.713266 31499 caffe.cpp:248] Starting Optimization
I0428 20:08:51.713274 31499 solver.cpp:273] Solving LeNet
I0428 20:08:51.713275 31499 solver.cpp:274] Learning Rate Policy: inv
I0428 20:08:51.714036 31499 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:08:51.719099 31499 blocking_queue.cpp:49] Waiting for data
I0428 20:08:51.790241 31506 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:08:51.790807 31499 solver.cpp:398]     Test net output #0: accuracy = 0.05
I0428 20:08:51.790824 31499 solver.cpp:398]     Test net output #1: loss = 2.38968 (* 1 = 2.38968 loss)
I0428 20:08:51.794308 31499 solver.cpp:219] Iteration 0 (0 iter/s, 0.0810092s/100 iters), loss = 2.41883
I0428 20:08:51.794332 31499 solver.cpp:238]     Train net output #0: loss = 2.41883 (* 1 = 2.41883 loss)
I0428 20:08:51.794343 31499 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:08:51.894511 31499 solver.cpp:219] Iteration 100 (998.313 iter/s, 0.100169s/100 iters), loss = 1.32467
I0428 20:08:51.894554 31499 solver.cpp:238]     Train net output #0: loss = 1.32467 (* 1 = 1.32467 loss)
I0428 20:08:51.894562 31499 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:08:51.993649 31499 solver.cpp:219] Iteration 200 (1009.26 iter/s, 0.0990822s/100 iters), loss = 0.614084
I0428 20:08:51.993695 31499 solver.cpp:238]     Train net output #0: loss = 0.614084 (* 1 = 0.614084 loss)
I0428 20:08:51.993703 31499 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:08:52.093704 31499 solver.cpp:219] Iteration 300 (1000.06 iter/s, 0.0999939s/100 iters), loss = 0.188791
I0428 20:08:52.093742 31499 solver.cpp:238]     Train net output #0: loss = 0.188791 (* 1 = 0.188791 loss)
I0428 20:08:52.093749 31499 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:08:52.193858 31499 solver.cpp:219] Iteration 400 (998.827 iter/s, 0.100117s/100 iters), loss = 0.0832214
I0428 20:08:52.193902 31499 solver.cpp:238]     Train net output #0: loss = 0.0832214 (* 1 = 0.0832214 loss)
I0428 20:08:52.193912 31499 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:08:52.291661 31499 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:08:52.358981 31506 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:08:52.359992 31499 solver.cpp:398]     Test net output #0: accuracy = 0.962
I0428 20:08:52.360038 31499 solver.cpp:398]     Test net output #1: loss = 0.129735 (* 1 = 0.129735 loss)
I0428 20:08:52.361222 31499 solver.cpp:219] Iteration 500 (597.706 iter/s, 0.167306s/100 iters), loss = 0.152867
I0428 20:08:52.361270 31499 solver.cpp:238]     Train net output #0: loss = 0.152867 (* 1 = 0.152867 loss)
I0428 20:08:52.361286 31499 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:08:52.460021 31499 solver.cpp:219] Iteration 600 (1012.74 iter/s, 0.0987422s/100 iters), loss = 0.0835982
I0428 20:08:52.460062 31499 solver.cpp:238]     Train net output #0: loss = 0.0835982 (* 1 = 0.0835982 loss)
I0428 20:08:52.460078 31499 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:08:52.562264 31499 solver.cpp:219] Iteration 700 (978.56 iter/s, 0.102191s/100 iters), loss = 0.177975
I0428 20:08:52.562310 31499 solver.cpp:238]     Train net output #0: loss = 0.177976 (* 1 = 0.177976 loss)
I0428 20:08:52.562322 31499 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:08:52.658259 31499 solver.cpp:219] Iteration 800 (1042.32 iter/s, 0.0959396s/100 iters), loss = 0.229874
I0428 20:08:52.658295 31499 solver.cpp:238]     Train net output #0: loss = 0.229874 (* 1 = 0.229874 loss)
I0428 20:08:52.658303 31499 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:08:52.754374 31499 solver.cpp:219] Iteration 900 (1040.93 iter/s, 0.0960677s/100 iters), loss = 0.19494
I0428 20:08:52.754410 31499 solver.cpp:238]     Train net output #0: loss = 0.19494 (* 1 = 0.19494 loss)
I0428 20:08:52.754417 31499 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:08:52.786521 31505 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:08:52.845305 31499 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:08:52.846212 31499 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:08:52.846840 31499 solver.cpp:311] Iteration 1000, loss = 0.0900543
I0428 20:08:52.846858 31499 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:08:52.922765 31506 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:08:52.923341 31499 solver.cpp:398]     Test net output #0: accuracy = 0.9751
I0428 20:08:52.923378 31499 solver.cpp:398]     Test net output #1: loss = 0.0785523 (* 1 = 0.0785523 loss)
I0428 20:08:52.923385 31499 solver.cpp:316] Optimization Done.
I0428 20:08:52.923388 31499 caffe.cpp:259] Optimization Done.
