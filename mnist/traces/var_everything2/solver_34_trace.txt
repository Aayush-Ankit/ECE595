I0428 19:28:42.142336 21755 caffe.cpp:218] Using GPUs 0
I0428 19:28:42.180694 21755 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:28:42.700824 21755 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test34.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:28:42.700963 21755 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test34.prototxt
I0428 19:28:42.701262 21755 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:28:42.701277 21755 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:28:42.701349 21755 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:28:42.701411 21755 layer_factory.hpp:77] Creating layer mnist
I0428 19:28:42.701509 21755 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:28:42.701532 21755 net.cpp:86] Creating Layer mnist
I0428 19:28:42.701539 21755 net.cpp:382] mnist -> data
I0428 19:28:42.701562 21755 net.cpp:382] mnist -> label
I0428 19:28:42.702647 21755 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:28:42.705312 21755 net.cpp:124] Setting up mnist
I0428 19:28:42.705329 21755 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:28:42.705335 21755 net.cpp:131] Top shape: 64 (64)
I0428 19:28:42.705339 21755 net.cpp:139] Memory required for data: 200960
I0428 19:28:42.705348 21755 layer_factory.hpp:77] Creating layer ip1
I0428 19:28:42.705358 21755 net.cpp:86] Creating Layer ip1
I0428 19:28:42.705363 21755 net.cpp:408] ip1 <- data
I0428 19:28:42.705375 21755 net.cpp:382] ip1 -> ip1
I0428 19:28:42.705822 21755 net.cpp:124] Setting up ip1
I0428 19:28:42.705833 21755 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:28:42.705837 21755 net.cpp:139] Memory required for data: 213760
I0428 19:28:42.705850 21755 layer_factory.hpp:77] Creating layer relu1
I0428 19:28:42.705858 21755 net.cpp:86] Creating Layer relu1
I0428 19:28:42.705863 21755 net.cpp:408] relu1 <- ip1
I0428 19:28:42.705868 21755 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:28:43.000820 21755 net.cpp:124] Setting up relu1
I0428 19:28:43.000851 21755 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:28:43.000855 21755 net.cpp:139] Memory required for data: 226560
I0428 19:28:43.000861 21755 layer_factory.hpp:77] Creating layer ip2
I0428 19:28:43.000874 21755 net.cpp:86] Creating Layer ip2
I0428 19:28:43.000898 21755 net.cpp:408] ip2 <- ip1
I0428 19:28:43.000907 21755 net.cpp:382] ip2 -> ip2
I0428 19:28:43.001991 21755 net.cpp:124] Setting up ip2
I0428 19:28:43.002005 21755 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:28:43.002009 21755 net.cpp:139] Memory required for data: 232960
I0428 19:28:43.002019 21755 layer_factory.hpp:77] Creating layer relu2
I0428 19:28:43.002028 21755 net.cpp:86] Creating Layer relu2
I0428 19:28:43.002032 21755 net.cpp:408] relu2 <- ip2
I0428 19:28:43.002038 21755 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:28:43.002841 21755 net.cpp:124] Setting up relu2
I0428 19:28:43.002856 21755 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:28:43.002859 21755 net.cpp:139] Memory required for data: 239360
I0428 19:28:43.002863 21755 layer_factory.hpp:77] Creating layer ip3
I0428 19:28:43.002871 21755 net.cpp:86] Creating Layer ip3
I0428 19:28:43.002874 21755 net.cpp:408] ip3 <- ip2
I0428 19:28:43.002881 21755 net.cpp:382] ip3 -> ip3
I0428 19:28:43.002985 21755 net.cpp:124] Setting up ip3
I0428 19:28:43.002995 21755 net.cpp:131] Top shape: 64 10 (640)
I0428 19:28:43.002998 21755 net.cpp:139] Memory required for data: 241920
I0428 19:28:43.003007 21755 layer_factory.hpp:77] Creating layer relu3
I0428 19:28:43.003012 21755 net.cpp:86] Creating Layer relu3
I0428 19:28:43.003017 21755 net.cpp:408] relu3 <- ip3
I0428 19:28:43.003022 21755 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:28:43.003180 21755 net.cpp:124] Setting up relu3
I0428 19:28:43.003190 21755 net.cpp:131] Top shape: 64 10 (640)
I0428 19:28:43.003193 21755 net.cpp:139] Memory required for data: 244480
I0428 19:28:43.003196 21755 layer_factory.hpp:77] Creating layer loss
I0428 19:28:43.003203 21755 net.cpp:86] Creating Layer loss
I0428 19:28:43.003206 21755 net.cpp:408] loss <- ip3
I0428 19:28:43.003211 21755 net.cpp:408] loss <- label
I0428 19:28:43.003216 21755 net.cpp:382] loss -> loss
I0428 19:28:43.003237 21755 layer_factory.hpp:77] Creating layer loss
I0428 19:28:43.004385 21755 net.cpp:124] Setting up loss
I0428 19:28:43.004400 21755 net.cpp:131] Top shape: (1)
I0428 19:28:43.004403 21755 net.cpp:134]     with loss weight 1
I0428 19:28:43.004420 21755 net.cpp:139] Memory required for data: 244484
I0428 19:28:43.004423 21755 net.cpp:200] loss needs backward computation.
I0428 19:28:43.004427 21755 net.cpp:200] relu3 needs backward computation.
I0428 19:28:43.004431 21755 net.cpp:200] ip3 needs backward computation.
I0428 19:28:43.004436 21755 net.cpp:200] relu2 needs backward computation.
I0428 19:28:43.004438 21755 net.cpp:200] ip2 needs backward computation.
I0428 19:28:43.004441 21755 net.cpp:200] relu1 needs backward computation.
I0428 19:28:43.004444 21755 net.cpp:200] ip1 needs backward computation.
I0428 19:28:43.004447 21755 net.cpp:202] mnist does not need backward computation.
I0428 19:28:43.004451 21755 net.cpp:244] This network produces output loss
I0428 19:28:43.004459 21755 net.cpp:257] Network initialization done.
I0428 19:28:43.004716 21755 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test34.prototxt
I0428 19:28:43.004741 21755 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:28:43.004822 21755 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:28:43.004899 21755 layer_factory.hpp:77] Creating layer mnist
I0428 19:28:43.004948 21755 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:28:43.004961 21755 net.cpp:86] Creating Layer mnist
I0428 19:28:43.004966 21755 net.cpp:382] mnist -> data
I0428 19:28:43.004976 21755 net.cpp:382] mnist -> label
I0428 19:28:43.005066 21755 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:28:43.007093 21755 net.cpp:124] Setting up mnist
I0428 19:28:43.007108 21755 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:28:43.007114 21755 net.cpp:131] Top shape: 100 (100)
I0428 19:28:43.007117 21755 net.cpp:139] Memory required for data: 314000
I0428 19:28:43.007122 21755 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:28:43.007133 21755 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:28:43.007138 21755 net.cpp:408] label_mnist_1_split <- label
I0428 19:28:43.007143 21755 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:28:43.007151 21755 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:28:43.007200 21755 net.cpp:124] Setting up label_mnist_1_split
I0428 19:28:43.007206 21755 net.cpp:131] Top shape: 100 (100)
I0428 19:28:43.007210 21755 net.cpp:131] Top shape: 100 (100)
I0428 19:28:43.007213 21755 net.cpp:139] Memory required for data: 314800
I0428 19:28:43.007216 21755 layer_factory.hpp:77] Creating layer ip1
I0428 19:28:43.007223 21755 net.cpp:86] Creating Layer ip1
I0428 19:28:43.007227 21755 net.cpp:408] ip1 <- data
I0428 19:28:43.007232 21755 net.cpp:382] ip1 -> ip1
I0428 19:28:43.007609 21755 net.cpp:124] Setting up ip1
I0428 19:28:43.007619 21755 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:28:43.007622 21755 net.cpp:139] Memory required for data: 334800
I0428 19:28:43.007632 21755 layer_factory.hpp:77] Creating layer relu1
I0428 19:28:43.007637 21755 net.cpp:86] Creating Layer relu1
I0428 19:28:43.007642 21755 net.cpp:408] relu1 <- ip1
I0428 19:28:43.007647 21755 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:28:43.008606 21755 net.cpp:124] Setting up relu1
I0428 19:28:43.008620 21755 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:28:43.008625 21755 net.cpp:139] Memory required for data: 354800
I0428 19:28:43.008628 21755 layer_factory.hpp:77] Creating layer ip2
I0428 19:28:43.008636 21755 net.cpp:86] Creating Layer ip2
I0428 19:28:43.008641 21755 net.cpp:408] ip2 <- ip1
I0428 19:28:43.008646 21755 net.cpp:382] ip2 -> ip2
I0428 19:28:43.008764 21755 net.cpp:124] Setting up ip2
I0428 19:28:43.008772 21755 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:28:43.008776 21755 net.cpp:139] Memory required for data: 364800
I0428 19:28:43.008785 21755 layer_factory.hpp:77] Creating layer relu2
I0428 19:28:43.008790 21755 net.cpp:86] Creating Layer relu2
I0428 19:28:43.008793 21755 net.cpp:408] relu2 <- ip2
I0428 19:28:43.008797 21755 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:28:43.008968 21755 net.cpp:124] Setting up relu2
I0428 19:28:43.008978 21755 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:28:43.008981 21755 net.cpp:139] Memory required for data: 374800
I0428 19:28:43.008985 21755 layer_factory.hpp:77] Creating layer ip3
I0428 19:28:43.008991 21755 net.cpp:86] Creating Layer ip3
I0428 19:28:43.008994 21755 net.cpp:408] ip3 <- ip2
I0428 19:28:43.008999 21755 net.cpp:382] ip3 -> ip3
I0428 19:28:43.009111 21755 net.cpp:124] Setting up ip3
I0428 19:28:43.009119 21755 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:28:43.009124 21755 net.cpp:139] Memory required for data: 378800
I0428 19:28:43.009131 21755 layer_factory.hpp:77] Creating layer relu3
I0428 19:28:43.009136 21755 net.cpp:86] Creating Layer relu3
I0428 19:28:43.009140 21755 net.cpp:408] relu3 <- ip3
I0428 19:28:43.009155 21755 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:28:43.009304 21755 net.cpp:124] Setting up relu3
I0428 19:28:43.009312 21755 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:28:43.009316 21755 net.cpp:139] Memory required for data: 382800
I0428 19:28:43.009320 21755 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:28:43.009325 21755 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:28:43.009328 21755 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:28:43.009333 21755 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:28:43.009341 21755 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:28:43.009380 21755 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:28:43.009387 21755 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:28:43.009392 21755 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:28:43.009395 21755 net.cpp:139] Memory required for data: 390800
I0428 19:28:43.009398 21755 layer_factory.hpp:77] Creating layer accuracy
I0428 19:28:43.009408 21755 net.cpp:86] Creating Layer accuracy
I0428 19:28:43.009412 21755 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:28:43.009415 21755 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:28:43.009420 21755 net.cpp:382] accuracy -> accuracy
I0428 19:28:43.009428 21755 net.cpp:124] Setting up accuracy
I0428 19:28:43.009431 21755 net.cpp:131] Top shape: (1)
I0428 19:28:43.009434 21755 net.cpp:139] Memory required for data: 390804
I0428 19:28:43.009438 21755 layer_factory.hpp:77] Creating layer loss
I0428 19:28:43.009443 21755 net.cpp:86] Creating Layer loss
I0428 19:28:43.009445 21755 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:28:43.009449 21755 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:28:43.009454 21755 net.cpp:382] loss -> loss
I0428 19:28:43.009459 21755 layer_factory.hpp:77] Creating layer loss
I0428 19:28:43.009800 21755 net.cpp:124] Setting up loss
I0428 19:28:43.009811 21755 net.cpp:131] Top shape: (1)
I0428 19:28:43.009814 21755 net.cpp:134]     with loss weight 1
I0428 19:28:43.009821 21755 net.cpp:139] Memory required for data: 390808
I0428 19:28:43.009824 21755 net.cpp:200] loss needs backward computation.
I0428 19:28:43.009829 21755 net.cpp:202] accuracy does not need backward computation.
I0428 19:28:43.009834 21755 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:28:43.009837 21755 net.cpp:200] relu3 needs backward computation.
I0428 19:28:43.009840 21755 net.cpp:200] ip3 needs backward computation.
I0428 19:28:43.009845 21755 net.cpp:200] relu2 needs backward computation.
I0428 19:28:43.009847 21755 net.cpp:200] ip2 needs backward computation.
I0428 19:28:43.009850 21755 net.cpp:200] relu1 needs backward computation.
I0428 19:28:43.009853 21755 net.cpp:200] ip1 needs backward computation.
I0428 19:28:43.009857 21755 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:28:43.009867 21755 net.cpp:202] mnist does not need backward computation.
I0428 19:28:43.009871 21755 net.cpp:244] This network produces output accuracy
I0428 19:28:43.009876 21755 net.cpp:244] This network produces output loss
I0428 19:28:43.009883 21755 net.cpp:257] Network initialization done.
I0428 19:28:43.009917 21755 solver.cpp:56] Solver scaffolding done.
I0428 19:28:43.010116 21755 caffe.cpp:248] Starting Optimization
I0428 19:28:43.010123 21755 solver.cpp:273] Solving LeNet
I0428 19:28:43.010128 21755 solver.cpp:274] Learning Rate Policy: inv
I0428 19:28:43.010882 21755 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:28:43.011023 21755 blocking_queue.cpp:49] Waiting for data
I0428 19:28:43.091074 21763 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:28:43.091519 21755 solver.cpp:398]     Test net output #0: accuracy = 0.1197
I0428 19:28:43.091557 21755 solver.cpp:398]     Test net output #1: loss = 2.29208 (* 1 = 2.29208 loss)
I0428 19:28:43.092216 21755 solver.cpp:219] Iteration 0 (2.77444 iter/s, 0.0820521s/100 iters), loss = 2.29569
I0428 19:28:43.092257 21755 solver.cpp:238]     Train net output #0: loss = 2.29569 (* 1 = 2.29569 loss)
I0428 19:28:43.092268 21755 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:28:43.149605 21755 solver.cpp:219] Iteration 100 (1743.96 iter/s, 0.0573409s/100 iters), loss = 1.33183
I0428 19:28:43.149632 21755 solver.cpp:238]     Train net output #0: loss = 1.33183 (* 1 = 1.33183 loss)
I0428 19:28:43.149639 21755 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:28:43.196121 21755 solver.cpp:219] Iteration 200 (2151.4 iter/s, 0.0464814s/100 iters), loss = 0.976089
I0428 19:28:43.196147 21755 solver.cpp:238]     Train net output #0: loss = 0.976089 (* 1 = 0.976089 loss)
I0428 19:28:43.196171 21755 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:28:43.241786 21755 solver.cpp:219] Iteration 300 (2191.43 iter/s, 0.0456323s/100 iters), loss = 1.32248
I0428 19:28:43.241809 21755 solver.cpp:238]     Train net output #0: loss = 1.32248 (* 1 = 1.32248 loss)
I0428 19:28:43.241833 21755 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:28:43.288913 21755 solver.cpp:219] Iteration 400 (2123.52 iter/s, 0.0470917s/100 iters), loss = 0.874302
I0428 19:28:43.288949 21755 solver.cpp:238]     Train net output #0: loss = 0.874302 (* 1 = 0.874302 loss)
I0428 19:28:43.288956 21755 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:28:43.336298 21755 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:28:43.408140 21763 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:28:43.408656 21755 solver.cpp:398]     Test net output #0: accuracy = 0.7434
I0428 19:28:43.408684 21755 solver.cpp:398]     Test net output #1: loss = 0.823084 (* 1 = 0.823084 loss)
I0428 19:28:43.409252 21755 solver.cpp:219] Iteration 500 (831.379 iter/s, 0.120282s/100 iters), loss = 0.720813
I0428 19:28:43.409313 21755 solver.cpp:238]     Train net output #0: loss = 0.720813 (* 1 = 0.720813 loss)
I0428 19:28:43.409332 21755 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:28:43.466528 21755 solver.cpp:219] Iteration 600 (1747.91 iter/s, 0.0572112s/100 iters), loss = 0.613812
I0428 19:28:43.466560 21755 solver.cpp:238]     Train net output #0: loss = 0.613812 (* 1 = 0.613812 loss)
I0428 19:28:43.466567 21755 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:28:43.514803 21755 solver.cpp:219] Iteration 700 (2073.08 iter/s, 0.0482375s/100 iters), loss = 0.685034
I0428 19:28:43.514832 21755 solver.cpp:238]     Train net output #0: loss = 0.685034 (* 1 = 0.685034 loss)
I0428 19:28:43.514840 21755 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:28:43.567862 21755 solver.cpp:219] Iteration 800 (1885.98 iter/s, 0.0530227s/100 iters), loss = 0.649299
I0428 19:28:43.567893 21755 solver.cpp:238]     Train net output #0: loss = 0.649299 (* 1 = 0.649299 loss)
I0428 19:28:43.567900 21755 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:28:43.625816 21755 solver.cpp:219] Iteration 900 (1726.63 iter/s, 0.0579164s/100 iters), loss = 0.682007
I0428 19:28:43.625844 21755 solver.cpp:238]     Train net output #0: loss = 0.682007 (* 1 = 0.682007 loss)
I0428 19:28:43.625851 21755 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:28:43.643556 21761 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:28:43.644678 21755 blocking_queue.cpp:49] Waiting for data
I0428 19:28:43.683465 21755 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:28:43.684892 21755 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:28:43.685799 21755 solver.cpp:311] Iteration 1000, loss = 0.493333
I0428 19:28:43.685824 21755 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:28:43.733021 21763 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:28:43.733481 21755 solver.cpp:398]     Test net output #0: accuracy = 0.8453
I0428 19:28:43.733507 21755 solver.cpp:398]     Test net output #1: loss = 0.464358 (* 1 = 0.464358 loss)
I0428 19:28:43.733525 21755 solver.cpp:316] Optimization Done.
I0428 19:28:43.733530 21755 caffe.cpp:259] Optimization Done.
