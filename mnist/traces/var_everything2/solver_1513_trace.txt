I0428 20:28:54.784353  3447 caffe.cpp:218] Using GPUs 0
I0428 20:28:54.825412  3447 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:28:55.349053  3447 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1513.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:28:55.349196  3447 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1513.prototxt
I0428 20:28:55.349616  3447 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:28:55.349633  3447 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:28:55.349736  3447 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:28:55.349817  3447 layer_factory.hpp:77] Creating layer mnist
I0428 20:28:55.349921  3447 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:28:55.349944  3447 net.cpp:86] Creating Layer mnist
I0428 20:28:55.349954  3447 net.cpp:382] mnist -> data
I0428 20:28:55.349977  3447 net.cpp:382] mnist -> label
I0428 20:28:55.351078  3447 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:28:55.353543  3447 net.cpp:124] Setting up mnist
I0428 20:28:55.353561  3447 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:28:55.353567  3447 net.cpp:131] Top shape: 64 (64)
I0428 20:28:55.353570  3447 net.cpp:139] Memory required for data: 200960
I0428 20:28:55.353579  3447 layer_factory.hpp:77] Creating layer conv0
I0428 20:28:55.353595  3447 net.cpp:86] Creating Layer conv0
I0428 20:28:55.353617  3447 net.cpp:408] conv0 <- data
I0428 20:28:55.353637  3447 net.cpp:382] conv0 -> conv0
I0428 20:28:55.645454  3447 net.cpp:124] Setting up conv0
I0428 20:28:55.645485  3447 net.cpp:131] Top shape: 64 100 24 24 (3686400)
I0428 20:28:55.645491  3447 net.cpp:139] Memory required for data: 14946560
I0428 20:28:55.645509  3447 layer_factory.hpp:77] Creating layer pool0
I0428 20:28:55.645524  3447 net.cpp:86] Creating Layer pool0
I0428 20:28:55.645529  3447 net.cpp:408] pool0 <- conv0
I0428 20:28:55.645535  3447 net.cpp:382] pool0 -> pool0
I0428 20:28:55.645591  3447 net.cpp:124] Setting up pool0
I0428 20:28:55.645599  3447 net.cpp:131] Top shape: 64 100 12 12 (921600)
I0428 20:28:55.645603  3447 net.cpp:139] Memory required for data: 18632960
I0428 20:28:55.645607  3447 layer_factory.hpp:77] Creating layer conv1
I0428 20:28:55.645620  3447 net.cpp:86] Creating Layer conv1
I0428 20:28:55.645627  3447 net.cpp:408] conv1 <- pool0
I0428 20:28:55.645632  3447 net.cpp:382] conv1 -> conv1
I0428 20:28:55.648859  3447 net.cpp:124] Setting up conv1
I0428 20:28:55.648877  3447 net.cpp:131] Top shape: 64 5 8 8 (20480)
I0428 20:28:55.648882  3447 net.cpp:139] Memory required for data: 18714880
I0428 20:28:55.648893  3447 layer_factory.hpp:77] Creating layer pool1
I0428 20:28:55.648902  3447 net.cpp:86] Creating Layer pool1
I0428 20:28:55.648906  3447 net.cpp:408] pool1 <- conv1
I0428 20:28:55.648913  3447 net.cpp:382] pool1 -> pool1
I0428 20:28:55.648957  3447 net.cpp:124] Setting up pool1
I0428 20:28:55.648967  3447 net.cpp:131] Top shape: 64 5 4 4 (5120)
I0428 20:28:55.648970  3447 net.cpp:139] Memory required for data: 18735360
I0428 20:28:55.648973  3447 layer_factory.hpp:77] Creating layer ip1
I0428 20:28:55.648983  3447 net.cpp:86] Creating Layer ip1
I0428 20:28:55.648986  3447 net.cpp:408] ip1 <- pool1
I0428 20:28:55.648993  3447 net.cpp:382] ip1 -> ip1
I0428 20:28:55.649142  3447 net.cpp:124] Setting up ip1
I0428 20:28:55.649152  3447 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:28:55.649155  3447 net.cpp:139] Memory required for data: 18748160
I0428 20:28:55.649164  3447 layer_factory.hpp:77] Creating layer relu1
I0428 20:28:55.649170  3447 net.cpp:86] Creating Layer relu1
I0428 20:28:55.649174  3447 net.cpp:408] relu1 <- ip1
I0428 20:28:55.649180  3447 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:28:55.649386  3447 net.cpp:124] Setting up relu1
I0428 20:28:55.649396  3447 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:28:55.649400  3447 net.cpp:139] Memory required for data: 18760960
I0428 20:28:55.649405  3447 layer_factory.hpp:77] Creating layer ip2
I0428 20:28:55.649412  3447 net.cpp:86] Creating Layer ip2
I0428 20:28:55.649415  3447 net.cpp:408] ip2 <- ip1
I0428 20:28:55.649421  3447 net.cpp:382] ip2 -> ip2
I0428 20:28:55.649539  3447 net.cpp:124] Setting up ip2
I0428 20:28:55.649547  3447 net.cpp:131] Top shape: 64 10 (640)
I0428 20:28:55.649551  3447 net.cpp:139] Memory required for data: 18763520
I0428 20:28:55.649557  3447 layer_factory.hpp:77] Creating layer relu2
I0428 20:28:55.649564  3447 net.cpp:86] Creating Layer relu2
I0428 20:28:55.649569  3447 net.cpp:408] relu2 <- ip2
I0428 20:28:55.649574  3447 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:28:55.650421  3447 net.cpp:124] Setting up relu2
I0428 20:28:55.650437  3447 net.cpp:131] Top shape: 64 10 (640)
I0428 20:28:55.650441  3447 net.cpp:139] Memory required for data: 18766080
I0428 20:28:55.650445  3447 layer_factory.hpp:77] Creating layer ip3
I0428 20:28:55.650454  3447 net.cpp:86] Creating Layer ip3
I0428 20:28:55.650459  3447 net.cpp:408] ip3 <- ip2
I0428 20:28:55.650465  3447 net.cpp:382] ip3 -> ip3
I0428 20:28:55.650583  3447 net.cpp:124] Setting up ip3
I0428 20:28:55.650593  3447 net.cpp:131] Top shape: 64 10 (640)
I0428 20:28:55.650595  3447 net.cpp:139] Memory required for data: 18768640
I0428 20:28:55.650605  3447 layer_factory.hpp:77] Creating layer relu3
I0428 20:28:55.650614  3447 net.cpp:86] Creating Layer relu3
I0428 20:28:55.650617  3447 net.cpp:408] relu3 <- ip3
I0428 20:28:55.650622  3447 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:28:55.650822  3447 net.cpp:124] Setting up relu3
I0428 20:28:55.650833  3447 net.cpp:131] Top shape: 64 10 (640)
I0428 20:28:55.650837  3447 net.cpp:139] Memory required for data: 18771200
I0428 20:28:55.650841  3447 layer_factory.hpp:77] Creating layer loss
I0428 20:28:55.650848  3447 net.cpp:86] Creating Layer loss
I0428 20:28:55.650856  3447 net.cpp:408] loss <- ip3
I0428 20:28:55.650861  3447 net.cpp:408] loss <- label
I0428 20:28:55.650866  3447 net.cpp:382] loss -> loss
I0428 20:28:55.650883  3447 layer_factory.hpp:77] Creating layer loss
I0428 20:28:55.651149  3447 net.cpp:124] Setting up loss
I0428 20:28:55.651160  3447 net.cpp:131] Top shape: (1)
I0428 20:28:55.651163  3447 net.cpp:134]     with loss weight 1
I0428 20:28:55.651178  3447 net.cpp:139] Memory required for data: 18771204
I0428 20:28:55.651182  3447 net.cpp:200] loss needs backward computation.
I0428 20:28:55.651187  3447 net.cpp:200] relu3 needs backward computation.
I0428 20:28:55.651190  3447 net.cpp:200] ip3 needs backward computation.
I0428 20:28:55.651195  3447 net.cpp:200] relu2 needs backward computation.
I0428 20:28:55.651197  3447 net.cpp:200] ip2 needs backward computation.
I0428 20:28:55.651201  3447 net.cpp:200] relu1 needs backward computation.
I0428 20:28:55.651204  3447 net.cpp:200] ip1 needs backward computation.
I0428 20:28:55.651208  3447 net.cpp:200] pool1 needs backward computation.
I0428 20:28:55.651212  3447 net.cpp:200] conv1 needs backward computation.
I0428 20:28:55.651216  3447 net.cpp:200] pool0 needs backward computation.
I0428 20:28:55.651219  3447 net.cpp:200] conv0 needs backward computation.
I0428 20:28:55.651223  3447 net.cpp:202] mnist does not need backward computation.
I0428 20:28:55.651226  3447 net.cpp:244] This network produces output loss
I0428 20:28:55.651237  3447 net.cpp:257] Network initialization done.
I0428 20:28:55.651635  3447 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1513.prototxt
I0428 20:28:55.651665  3447 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:28:55.651769  3447 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:28:55.651861  3447 layer_factory.hpp:77] Creating layer mnist
I0428 20:28:55.651913  3447 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:28:55.651929  3447 net.cpp:86] Creating Layer mnist
I0428 20:28:55.651937  3447 net.cpp:382] mnist -> data
I0428 20:28:55.651947  3447 net.cpp:382] mnist -> label
I0428 20:28:55.652042  3447 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:28:55.654402  3447 net.cpp:124] Setting up mnist
I0428 20:28:55.654417  3447 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:28:55.654423  3447 net.cpp:131] Top shape: 100 (100)
I0428 20:28:55.654428  3447 net.cpp:139] Memory required for data: 314000
I0428 20:28:55.654431  3447 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:28:55.654439  3447 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:28:55.654443  3447 net.cpp:408] label_mnist_1_split <- label
I0428 20:28:55.654449  3447 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:28:55.654456  3447 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:28:55.654563  3447 net.cpp:124] Setting up label_mnist_1_split
I0428 20:28:55.654572  3447 net.cpp:131] Top shape: 100 (100)
I0428 20:28:55.654577  3447 net.cpp:131] Top shape: 100 (100)
I0428 20:28:55.654582  3447 net.cpp:139] Memory required for data: 314800
I0428 20:28:55.654584  3447 layer_factory.hpp:77] Creating layer conv0
I0428 20:28:55.654594  3447 net.cpp:86] Creating Layer conv0
I0428 20:28:55.654600  3447 net.cpp:408] conv0 <- data
I0428 20:28:55.654606  3447 net.cpp:382] conv0 -> conv0
I0428 20:28:55.656530  3447 net.cpp:124] Setting up conv0
I0428 20:28:55.656548  3447 net.cpp:131] Top shape: 100 100 24 24 (5760000)
I0428 20:28:55.656551  3447 net.cpp:139] Memory required for data: 23354800
I0428 20:28:55.656563  3447 layer_factory.hpp:77] Creating layer pool0
I0428 20:28:55.656569  3447 net.cpp:86] Creating Layer pool0
I0428 20:28:55.656574  3447 net.cpp:408] pool0 <- conv0
I0428 20:28:55.656579  3447 net.cpp:382] pool0 -> pool0
I0428 20:28:55.656620  3447 net.cpp:124] Setting up pool0
I0428 20:28:55.656628  3447 net.cpp:131] Top shape: 100 100 12 12 (1440000)
I0428 20:28:55.656632  3447 net.cpp:139] Memory required for data: 29114800
I0428 20:28:55.656635  3447 layer_factory.hpp:77] Creating layer conv1
I0428 20:28:55.656646  3447 net.cpp:86] Creating Layer conv1
I0428 20:28:55.656652  3447 net.cpp:408] conv1 <- pool0
I0428 20:28:55.656658  3447 net.cpp:382] conv1 -> conv1
I0428 20:28:55.658394  3447 net.cpp:124] Setting up conv1
I0428 20:28:55.658413  3447 net.cpp:131] Top shape: 100 5 8 8 (32000)
I0428 20:28:55.658417  3447 net.cpp:139] Memory required for data: 29242800
I0428 20:28:55.658428  3447 layer_factory.hpp:77] Creating layer pool1
I0428 20:28:55.658435  3447 net.cpp:86] Creating Layer pool1
I0428 20:28:55.658439  3447 net.cpp:408] pool1 <- conv1
I0428 20:28:55.658447  3447 net.cpp:382] pool1 -> pool1
I0428 20:28:55.658486  3447 net.cpp:124] Setting up pool1
I0428 20:28:55.658495  3447 net.cpp:131] Top shape: 100 5 4 4 (8000)
I0428 20:28:55.658499  3447 net.cpp:139] Memory required for data: 29274800
I0428 20:28:55.658504  3447 layer_factory.hpp:77] Creating layer ip1
I0428 20:28:55.658509  3447 net.cpp:86] Creating Layer ip1
I0428 20:28:55.658514  3447 net.cpp:408] ip1 <- pool1
I0428 20:28:55.658519  3447 net.cpp:382] ip1 -> ip1
I0428 20:28:55.658654  3447 net.cpp:124] Setting up ip1
I0428 20:28:55.658663  3447 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:28:55.658680  3447 net.cpp:139] Memory required for data: 29294800
I0428 20:28:55.658689  3447 layer_factory.hpp:77] Creating layer relu1
I0428 20:28:55.658697  3447 net.cpp:86] Creating Layer relu1
I0428 20:28:55.658701  3447 net.cpp:408] relu1 <- ip1
I0428 20:28:55.658706  3447 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:28:55.658885  3447 net.cpp:124] Setting up relu1
I0428 20:28:55.658895  3447 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:28:55.658900  3447 net.cpp:139] Memory required for data: 29314800
I0428 20:28:55.658903  3447 layer_factory.hpp:77] Creating layer ip2
I0428 20:28:55.658912  3447 net.cpp:86] Creating Layer ip2
I0428 20:28:55.658916  3447 net.cpp:408] ip2 <- ip1
I0428 20:28:55.658922  3447 net.cpp:382] ip2 -> ip2
I0428 20:28:55.659042  3447 net.cpp:124] Setting up ip2
I0428 20:28:55.659050  3447 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:28:55.659054  3447 net.cpp:139] Memory required for data: 29318800
I0428 20:28:55.659060  3447 layer_factory.hpp:77] Creating layer relu2
I0428 20:28:55.659065  3447 net.cpp:86] Creating Layer relu2
I0428 20:28:55.659070  3447 net.cpp:408] relu2 <- ip2
I0428 20:28:55.659073  3447 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:28:55.659312  3447 net.cpp:124] Setting up relu2
I0428 20:28:55.659322  3447 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:28:55.659327  3447 net.cpp:139] Memory required for data: 29322800
I0428 20:28:55.659330  3447 layer_factory.hpp:77] Creating layer ip3
I0428 20:28:55.659337  3447 net.cpp:86] Creating Layer ip3
I0428 20:28:55.659342  3447 net.cpp:408] ip3 <- ip2
I0428 20:28:55.659346  3447 net.cpp:382] ip3 -> ip3
I0428 20:28:55.659452  3447 net.cpp:124] Setting up ip3
I0428 20:28:55.659459  3447 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:28:55.659463  3447 net.cpp:139] Memory required for data: 29326800
I0428 20:28:55.659472  3447 layer_factory.hpp:77] Creating layer relu3
I0428 20:28:55.659477  3447 net.cpp:86] Creating Layer relu3
I0428 20:28:55.659482  3447 net.cpp:408] relu3 <- ip3
I0428 20:28:55.659487  3447 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:28:55.660437  3447 net.cpp:124] Setting up relu3
I0428 20:28:55.660454  3447 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:28:55.660457  3447 net.cpp:139] Memory required for data: 29330800
I0428 20:28:55.660461  3447 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:28:55.660467  3447 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:28:55.660471  3447 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:28:55.660477  3447 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:28:55.660485  3447 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:28:55.660537  3447 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:28:55.660547  3447 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:28:55.660552  3447 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:28:55.660555  3447 net.cpp:139] Memory required for data: 29338800
I0428 20:28:55.660559  3447 layer_factory.hpp:77] Creating layer accuracy
I0428 20:28:55.660564  3447 net.cpp:86] Creating Layer accuracy
I0428 20:28:55.660568  3447 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:28:55.660573  3447 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:28:55.660578  3447 net.cpp:382] accuracy -> accuracy
I0428 20:28:55.660586  3447 net.cpp:124] Setting up accuracy
I0428 20:28:55.660593  3447 net.cpp:131] Top shape: (1)
I0428 20:28:55.660603  3447 net.cpp:139] Memory required for data: 29338804
I0428 20:28:55.660607  3447 layer_factory.hpp:77] Creating layer loss
I0428 20:28:55.660612  3447 net.cpp:86] Creating Layer loss
I0428 20:28:55.660615  3447 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:28:55.660620  3447 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:28:55.660625  3447 net.cpp:382] loss -> loss
I0428 20:28:55.660632  3447 layer_factory.hpp:77] Creating layer loss
I0428 20:28:55.660907  3447 net.cpp:124] Setting up loss
I0428 20:28:55.660919  3447 net.cpp:131] Top shape: (1)
I0428 20:28:55.660923  3447 net.cpp:134]     with loss weight 1
I0428 20:28:55.660951  3447 net.cpp:139] Memory required for data: 29338808
I0428 20:28:55.660956  3447 net.cpp:200] loss needs backward computation.
I0428 20:28:55.660961  3447 net.cpp:202] accuracy does not need backward computation.
I0428 20:28:55.660966  3447 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:28:55.660977  3447 net.cpp:200] relu3 needs backward computation.
I0428 20:28:55.660981  3447 net.cpp:200] ip3 needs backward computation.
I0428 20:28:55.660985  3447 net.cpp:200] relu2 needs backward computation.
I0428 20:28:55.660989  3447 net.cpp:200] ip2 needs backward computation.
I0428 20:28:55.660991  3447 net.cpp:200] relu1 needs backward computation.
I0428 20:28:55.660995  3447 net.cpp:200] ip1 needs backward computation.
I0428 20:28:55.661000  3447 net.cpp:200] pool1 needs backward computation.
I0428 20:28:55.661002  3447 net.cpp:200] conv1 needs backward computation.
I0428 20:28:55.661006  3447 net.cpp:200] pool0 needs backward computation.
I0428 20:28:55.661010  3447 net.cpp:200] conv0 needs backward computation.
I0428 20:28:55.661015  3447 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:28:55.661020  3447 net.cpp:202] mnist does not need backward computation.
I0428 20:28:55.661025  3447 net.cpp:244] This network produces output accuracy
I0428 20:28:55.661028  3447 net.cpp:244] This network produces output loss
I0428 20:28:55.661041  3447 net.cpp:257] Network initialization done.
I0428 20:28:55.661090  3447 solver.cpp:56] Solver scaffolding done.
I0428 20:28:55.661502  3447 caffe.cpp:248] Starting Optimization
I0428 20:28:55.661509  3447 solver.cpp:273] Solving LeNet
I0428 20:28:55.661514  3447 solver.cpp:274] Learning Rate Policy: inv
I0428 20:28:55.662425  3447 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:28:55.757318  3454 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:28:55.759744  3447 solver.cpp:398]     Test net output #0: accuracy = 0.1028
I0428 20:28:55.759780  3447 solver.cpp:398]     Test net output #1: loss = 2.30336 (* 1 = 2.30336 loss)
I0428 20:28:55.764171  3447 solver.cpp:219] Iteration 0 (-1.74242e-31 iter/s, 0.102628s/100 iters), loss = 2.30396
I0428 20:28:55.764211  3447 solver.cpp:238]     Train net output #0: loss = 2.30396 (* 1 = 2.30396 loss)
I0428 20:28:55.764222  3447 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:28:55.966408  3447 solver.cpp:219] Iteration 100 (494.617 iter/s, 0.202177s/100 iters), loss = 1.19762
I0428 20:28:55.966457  3447 solver.cpp:238]     Train net output #0: loss = 1.19762 (* 1 = 1.19762 loss)
I0428 20:28:55.966470  3447 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:28:56.188488  3447 solver.cpp:219] Iteration 200 (450.419 iter/s, 0.222015s/100 iters), loss = 0.851727
I0428 20:28:56.188531  3447 solver.cpp:238]     Train net output #0: loss = 0.851727 (* 1 = 0.851727 loss)
I0428 20:28:56.188542  3447 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:28:56.407392  3447 solver.cpp:219] Iteration 300 (456.954 iter/s, 0.21884s/100 iters), loss = 1.11251
I0428 20:28:56.407445  3447 solver.cpp:238]     Train net output #0: loss = 1.11251 (* 1 = 1.11251 loss)
I0428 20:28:56.407457  3447 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:28:56.626055  3447 solver.cpp:219] Iteration 400 (457.468 iter/s, 0.218594s/100 iters), loss = 1.27304
I0428 20:28:56.626103  3447 solver.cpp:238]     Train net output #0: loss = 1.27304 (* 1 = 1.27304 loss)
I0428 20:28:56.626116  3447 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:28:56.849298  3447 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:28:56.954094  3454 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:28:56.956908  3447 solver.cpp:398]     Test net output #0: accuracy = 0.6656
I0428 20:28:56.956936  3447 solver.cpp:398]     Test net output #1: loss = 1.05742 (* 1 = 1.05742 loss)
I0428 20:28:56.958868  3447 solver.cpp:219] Iteration 500 (300.529 iter/s, 0.332747s/100 iters), loss = 1.24094
I0428 20:28:56.958895  3447 solver.cpp:238]     Train net output #0: loss = 1.24094 (* 1 = 1.24094 loss)
I0428 20:28:56.958921  3447 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:28:57.167404  3447 solver.cpp:219] Iteration 600 (479.636 iter/s, 0.208492s/100 iters), loss = 0.930441
I0428 20:28:57.167440  3447 solver.cpp:238]     Train net output #0: loss = 0.930441 (* 1 = 0.930441 loss)
I0428 20:28:57.167450  3447 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:28:57.392485  3447 solver.cpp:219] Iteration 700 (444.397 iter/s, 0.225024s/100 iters), loss = 1.05172
I0428 20:28:57.392537  3447 solver.cpp:238]     Train net output #0: loss = 1.05172 (* 1 = 1.05172 loss)
I0428 20:28:57.392551  3447 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:28:57.619793  3447 solver.cpp:219] Iteration 800 (440.071 iter/s, 0.227236s/100 iters), loss = 1.04488
I0428 20:28:57.619853  3447 solver.cpp:238]     Train net output #0: loss = 1.04488 (* 1 = 1.04488 loss)
I0428 20:28:57.619865  3447 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:28:57.844300  3447 solver.cpp:219] Iteration 900 (445.571 iter/s, 0.224431s/100 iters), loss = 0.765253
I0428 20:28:57.844352  3447 solver.cpp:238]     Train net output #0: loss = 0.765253 (* 1 = 0.765253 loss)
I0428 20:28:57.844364  3447 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:28:57.918972  3453 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:28:58.064772  3447 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:28:58.066712  3447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:28:58.068022  3447 solver.cpp:311] Iteration 1000, loss = 0.956421
I0428 20:28:58.068051  3447 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:28:58.174418  3454 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:28:58.177974  3447 solver.cpp:398]     Test net output #0: accuracy = 0.6831
I0428 20:28:58.178000  3447 solver.cpp:398]     Test net output #1: loss = 0.978992 (* 1 = 0.978992 loss)
I0428 20:28:58.178006  3447 solver.cpp:316] Optimization Done.
I0428 20:28:58.178010  3447 caffe.cpp:259] Optimization Done.
