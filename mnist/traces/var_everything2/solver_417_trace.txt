I0428 19:43:16.688680 25308 caffe.cpp:218] Using GPUs 0
I0428 19:43:16.725379 25308 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:43:17.232156 25308 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test417.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:43:17.232306 25308 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test417.prototxt
I0428 19:43:17.232723 25308 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:43:17.232743 25308 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:43:17.232858 25308 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:43:17.232941 25308 layer_factory.hpp:77] Creating layer mnist
I0428 19:43:17.233039 25308 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:43:17.233064 25308 net.cpp:86] Creating Layer mnist
I0428 19:43:17.233073 25308 net.cpp:382] mnist -> data
I0428 19:43:17.233098 25308 net.cpp:382] mnist -> label
I0428 19:43:17.234186 25308 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:43:17.236634 25308 net.cpp:124] Setting up mnist
I0428 19:43:17.236651 25308 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:43:17.236660 25308 net.cpp:131] Top shape: 64 (64)
I0428 19:43:17.236663 25308 net.cpp:139] Memory required for data: 200960
I0428 19:43:17.236670 25308 layer_factory.hpp:77] Creating layer conv0
I0428 19:43:17.236686 25308 net.cpp:86] Creating Layer conv0
I0428 19:43:17.236706 25308 net.cpp:408] conv0 <- data
I0428 19:43:17.236721 25308 net.cpp:382] conv0 -> conv0
I0428 19:43:17.514283 25308 net.cpp:124] Setting up conv0
I0428 19:43:17.514324 25308 net.cpp:131] Top shape: 64 2 24 24 (73728)
I0428 19:43:17.514328 25308 net.cpp:139] Memory required for data: 495872
I0428 19:43:17.514343 25308 layer_factory.hpp:77] Creating layer pool0
I0428 19:43:17.514355 25308 net.cpp:86] Creating Layer pool0
I0428 19:43:17.514360 25308 net.cpp:408] pool0 <- conv0
I0428 19:43:17.514380 25308 net.cpp:382] pool0 -> pool0
I0428 19:43:17.514426 25308 net.cpp:124] Setting up pool0
I0428 19:43:17.514432 25308 net.cpp:131] Top shape: 64 2 12 12 (18432)
I0428 19:43:17.514434 25308 net.cpp:139] Memory required for data: 569600
I0428 19:43:17.514438 25308 layer_factory.hpp:77] Creating layer conv1
I0428 19:43:17.514449 25308 net.cpp:86] Creating Layer conv1
I0428 19:43:17.514452 25308 net.cpp:408] conv1 <- pool0
I0428 19:43:17.514456 25308 net.cpp:382] conv1 -> conv1
I0428 19:43:17.517146 25308 net.cpp:124] Setting up conv1
I0428 19:43:17.517175 25308 net.cpp:131] Top shape: 64 25 8 8 (102400)
I0428 19:43:17.517180 25308 net.cpp:139] Memory required for data: 979200
I0428 19:43:17.517189 25308 layer_factory.hpp:77] Creating layer pool1
I0428 19:43:17.517197 25308 net.cpp:86] Creating Layer pool1
I0428 19:43:17.517201 25308 net.cpp:408] pool1 <- conv1
I0428 19:43:17.517206 25308 net.cpp:382] pool1 -> pool1
I0428 19:43:17.517258 25308 net.cpp:124] Setting up pool1
I0428 19:43:17.517264 25308 net.cpp:131] Top shape: 64 25 4 4 (25600)
I0428 19:43:17.517267 25308 net.cpp:139] Memory required for data: 1081600
I0428 19:43:17.517271 25308 layer_factory.hpp:77] Creating layer ip1
I0428 19:43:17.517277 25308 net.cpp:86] Creating Layer ip1
I0428 19:43:17.517280 25308 net.cpp:408] ip1 <- pool1
I0428 19:43:17.517285 25308 net.cpp:382] ip1 -> ip1
I0428 19:43:17.518240 25308 net.cpp:124] Setting up ip1
I0428 19:43:17.518252 25308 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:43:17.518271 25308 net.cpp:139] Memory required for data: 1088000
I0428 19:43:17.518280 25308 layer_factory.hpp:77] Creating layer relu1
I0428 19:43:17.518285 25308 net.cpp:86] Creating Layer relu1
I0428 19:43:17.518288 25308 net.cpp:408] relu1 <- ip1
I0428 19:43:17.518292 25308 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:43:17.518487 25308 net.cpp:124] Setting up relu1
I0428 19:43:17.518496 25308 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:43:17.518501 25308 net.cpp:139] Memory required for data: 1094400
I0428 19:43:17.518503 25308 layer_factory.hpp:77] Creating layer ip2
I0428 19:43:17.518510 25308 net.cpp:86] Creating Layer ip2
I0428 19:43:17.518513 25308 net.cpp:408] ip2 <- ip1
I0428 19:43:17.518518 25308 net.cpp:382] ip2 -> ip2
I0428 19:43:17.518627 25308 net.cpp:124] Setting up ip2
I0428 19:43:17.518635 25308 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:43:17.518637 25308 net.cpp:139] Memory required for data: 1107200
I0428 19:43:17.518643 25308 layer_factory.hpp:77] Creating layer relu2
I0428 19:43:17.518649 25308 net.cpp:86] Creating Layer relu2
I0428 19:43:17.518652 25308 net.cpp:408] relu2 <- ip2
I0428 19:43:17.518656 25308 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:43:17.519423 25308 net.cpp:124] Setting up relu2
I0428 19:43:17.519435 25308 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:43:17.519454 25308 net.cpp:139] Memory required for data: 1120000
I0428 19:43:17.519457 25308 layer_factory.hpp:77] Creating layer ip3
I0428 19:43:17.519464 25308 net.cpp:86] Creating Layer ip3
I0428 19:43:17.519469 25308 net.cpp:408] ip3 <- ip2
I0428 19:43:17.519474 25308 net.cpp:382] ip3 -> ip3
I0428 19:43:17.519608 25308 net.cpp:124] Setting up ip3
I0428 19:43:17.519618 25308 net.cpp:131] Top shape: 64 10 (640)
I0428 19:43:17.519620 25308 net.cpp:139] Memory required for data: 1122560
I0428 19:43:17.519629 25308 layer_factory.hpp:77] Creating layer relu3
I0428 19:43:17.519635 25308 net.cpp:86] Creating Layer relu3
I0428 19:43:17.519639 25308 net.cpp:408] relu3 <- ip3
I0428 19:43:17.519642 25308 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:43:17.519845 25308 net.cpp:124] Setting up relu3
I0428 19:43:17.519855 25308 net.cpp:131] Top shape: 64 10 (640)
I0428 19:43:17.519857 25308 net.cpp:139] Memory required for data: 1125120
I0428 19:43:17.519860 25308 layer_factory.hpp:77] Creating layer loss
I0428 19:43:17.519866 25308 net.cpp:86] Creating Layer loss
I0428 19:43:17.519870 25308 net.cpp:408] loss <- ip3
I0428 19:43:17.519887 25308 net.cpp:408] loss <- label
I0428 19:43:17.519892 25308 net.cpp:382] loss -> loss
I0428 19:43:17.519909 25308 layer_factory.hpp:77] Creating layer loss
I0428 19:43:17.520148 25308 net.cpp:124] Setting up loss
I0428 19:43:17.520156 25308 net.cpp:131] Top shape: (1)
I0428 19:43:17.520159 25308 net.cpp:134]     with loss weight 1
I0428 19:43:17.520174 25308 net.cpp:139] Memory required for data: 1125124
I0428 19:43:17.520177 25308 net.cpp:200] loss needs backward computation.
I0428 19:43:17.520180 25308 net.cpp:200] relu3 needs backward computation.
I0428 19:43:17.520184 25308 net.cpp:200] ip3 needs backward computation.
I0428 19:43:17.520186 25308 net.cpp:200] relu2 needs backward computation.
I0428 19:43:17.520190 25308 net.cpp:200] ip2 needs backward computation.
I0428 19:43:17.520191 25308 net.cpp:200] relu1 needs backward computation.
I0428 19:43:17.520195 25308 net.cpp:200] ip1 needs backward computation.
I0428 19:43:17.520196 25308 net.cpp:200] pool1 needs backward computation.
I0428 19:43:17.520200 25308 net.cpp:200] conv1 needs backward computation.
I0428 19:43:17.520202 25308 net.cpp:200] pool0 needs backward computation.
I0428 19:43:17.520205 25308 net.cpp:200] conv0 needs backward computation.
I0428 19:43:17.520208 25308 net.cpp:202] mnist does not need backward computation.
I0428 19:43:17.520210 25308 net.cpp:244] This network produces output loss
I0428 19:43:17.520220 25308 net.cpp:257] Network initialization done.
I0428 19:43:17.520593 25308 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test417.prototxt
I0428 19:43:17.520632 25308 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:43:17.520740 25308 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:43:17.520876 25308 layer_factory.hpp:77] Creating layer mnist
I0428 19:43:17.520921 25308 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:43:17.520936 25308 net.cpp:86] Creating Layer mnist
I0428 19:43:17.520941 25308 net.cpp:382] mnist -> data
I0428 19:43:17.520949 25308 net.cpp:382] mnist -> label
I0428 19:43:17.521044 25308 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:43:17.523046 25308 net.cpp:124] Setting up mnist
I0428 19:43:17.523073 25308 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:43:17.523088 25308 net.cpp:131] Top shape: 100 (100)
I0428 19:43:17.523092 25308 net.cpp:139] Memory required for data: 314000
I0428 19:43:17.523109 25308 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:43:17.523119 25308 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:43:17.523123 25308 net.cpp:408] label_mnist_1_split <- label
I0428 19:43:17.523128 25308 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:43:17.523133 25308 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:43:17.523218 25308 net.cpp:124] Setting up label_mnist_1_split
I0428 19:43:17.523226 25308 net.cpp:131] Top shape: 100 (100)
I0428 19:43:17.523229 25308 net.cpp:131] Top shape: 100 (100)
I0428 19:43:17.523232 25308 net.cpp:139] Memory required for data: 314800
I0428 19:43:17.523236 25308 layer_factory.hpp:77] Creating layer conv0
I0428 19:43:17.523244 25308 net.cpp:86] Creating Layer conv0
I0428 19:43:17.523247 25308 net.cpp:408] conv0 <- data
I0428 19:43:17.523253 25308 net.cpp:382] conv0 -> conv0
I0428 19:43:17.525082 25308 net.cpp:124] Setting up conv0
I0428 19:43:17.525106 25308 net.cpp:131] Top shape: 100 2 24 24 (115200)
I0428 19:43:17.525110 25308 net.cpp:139] Memory required for data: 775600
I0428 19:43:17.525135 25308 layer_factory.hpp:77] Creating layer pool0
I0428 19:43:17.525158 25308 net.cpp:86] Creating Layer pool0
I0428 19:43:17.525162 25308 net.cpp:408] pool0 <- conv0
I0428 19:43:17.525182 25308 net.cpp:382] pool0 -> pool0
I0428 19:43:17.525220 25308 net.cpp:124] Setting up pool0
I0428 19:43:17.525233 25308 net.cpp:131] Top shape: 100 2 12 12 (28800)
I0428 19:43:17.525236 25308 net.cpp:139] Memory required for data: 890800
I0428 19:43:17.525239 25308 layer_factory.hpp:77] Creating layer conv1
I0428 19:43:17.525249 25308 net.cpp:86] Creating Layer conv1
I0428 19:43:17.525252 25308 net.cpp:408] conv1 <- pool0
I0428 19:43:17.525259 25308 net.cpp:382] conv1 -> conv1
I0428 19:43:17.526830 25308 net.cpp:124] Setting up conv1
I0428 19:43:17.526860 25308 net.cpp:131] Top shape: 100 25 8 8 (160000)
I0428 19:43:17.526865 25308 net.cpp:139] Memory required for data: 1530800
I0428 19:43:17.526873 25308 layer_factory.hpp:77] Creating layer pool1
I0428 19:43:17.526880 25308 net.cpp:86] Creating Layer pool1
I0428 19:43:17.526885 25308 net.cpp:408] pool1 <- conv1
I0428 19:43:17.526890 25308 net.cpp:382] pool1 -> pool1
I0428 19:43:17.526926 25308 net.cpp:124] Setting up pool1
I0428 19:43:17.526932 25308 net.cpp:131] Top shape: 100 25 4 4 (40000)
I0428 19:43:17.526934 25308 net.cpp:139] Memory required for data: 1690800
I0428 19:43:17.526937 25308 layer_factory.hpp:77] Creating layer ip1
I0428 19:43:17.526944 25308 net.cpp:86] Creating Layer ip1
I0428 19:43:17.526947 25308 net.cpp:408] ip1 <- pool1
I0428 19:43:17.526952 25308 net.cpp:382] ip1 -> ip1
I0428 19:43:17.527107 25308 net.cpp:124] Setting up ip1
I0428 19:43:17.527114 25308 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:43:17.527128 25308 net.cpp:139] Memory required for data: 1700800
I0428 19:43:17.527137 25308 layer_factory.hpp:77] Creating layer relu1
I0428 19:43:17.527143 25308 net.cpp:86] Creating Layer relu1
I0428 19:43:17.527146 25308 net.cpp:408] relu1 <- ip1
I0428 19:43:17.527151 25308 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:43:17.527334 25308 net.cpp:124] Setting up relu1
I0428 19:43:17.527350 25308 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:43:17.527355 25308 net.cpp:139] Memory required for data: 1710800
I0428 19:43:17.527359 25308 layer_factory.hpp:77] Creating layer ip2
I0428 19:43:17.527367 25308 net.cpp:86] Creating Layer ip2
I0428 19:43:17.527370 25308 net.cpp:408] ip2 <- ip1
I0428 19:43:17.527376 25308 net.cpp:382] ip2 -> ip2
I0428 19:43:17.527492 25308 net.cpp:124] Setting up ip2
I0428 19:43:17.527498 25308 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:43:17.527513 25308 net.cpp:139] Memory required for data: 1730800
I0428 19:43:17.527519 25308 layer_factory.hpp:77] Creating layer relu2
I0428 19:43:17.527524 25308 net.cpp:86] Creating Layer relu2
I0428 19:43:17.527529 25308 net.cpp:408] relu2 <- ip2
I0428 19:43:17.527532 25308 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:43:17.527705 25308 net.cpp:124] Setting up relu2
I0428 19:43:17.527714 25308 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:43:17.527716 25308 net.cpp:139] Memory required for data: 1750800
I0428 19:43:17.527719 25308 layer_factory.hpp:77] Creating layer ip3
I0428 19:43:17.527727 25308 net.cpp:86] Creating Layer ip3
I0428 19:43:17.527731 25308 net.cpp:408] ip3 <- ip2
I0428 19:43:17.527736 25308 net.cpp:382] ip3 -> ip3
I0428 19:43:17.527861 25308 net.cpp:124] Setting up ip3
I0428 19:43:17.527868 25308 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:43:17.527871 25308 net.cpp:139] Memory required for data: 1754800
I0428 19:43:17.527878 25308 layer_factory.hpp:77] Creating layer relu3
I0428 19:43:17.527884 25308 net.cpp:86] Creating Layer relu3
I0428 19:43:17.527886 25308 net.cpp:408] relu3 <- ip3
I0428 19:43:17.527891 25308 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:43:17.528770 25308 net.cpp:124] Setting up relu3
I0428 19:43:17.528781 25308 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:43:17.528803 25308 net.cpp:139] Memory required for data: 1758800
I0428 19:43:17.528806 25308 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:43:17.528833 25308 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:43:17.528837 25308 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:43:17.528843 25308 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:43:17.528851 25308 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:43:17.528930 25308 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:43:17.528939 25308 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:43:17.528944 25308 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:43:17.528947 25308 net.cpp:139] Memory required for data: 1766800
I0428 19:43:17.528951 25308 layer_factory.hpp:77] Creating layer accuracy
I0428 19:43:17.528956 25308 net.cpp:86] Creating Layer accuracy
I0428 19:43:17.528960 25308 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:43:17.528964 25308 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:43:17.528969 25308 net.cpp:382] accuracy -> accuracy
I0428 19:43:17.528976 25308 net.cpp:124] Setting up accuracy
I0428 19:43:17.528981 25308 net.cpp:131] Top shape: (1)
I0428 19:43:17.528985 25308 net.cpp:139] Memory required for data: 1766804
I0428 19:43:17.528987 25308 layer_factory.hpp:77] Creating layer loss
I0428 19:43:17.528993 25308 net.cpp:86] Creating Layer loss
I0428 19:43:17.528997 25308 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:43:17.529002 25308 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:43:17.529007 25308 net.cpp:382] loss -> loss
I0428 19:43:17.529013 25308 layer_factory.hpp:77] Creating layer loss
I0428 19:43:17.529304 25308 net.cpp:124] Setting up loss
I0428 19:43:17.529320 25308 net.cpp:131] Top shape: (1)
I0428 19:43:17.529322 25308 net.cpp:134]     with loss weight 1
I0428 19:43:17.529328 25308 net.cpp:139] Memory required for data: 1766808
I0428 19:43:17.529341 25308 net.cpp:200] loss needs backward computation.
I0428 19:43:17.529345 25308 net.cpp:202] accuracy does not need backward computation.
I0428 19:43:17.529348 25308 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:43:17.529351 25308 net.cpp:200] relu3 needs backward computation.
I0428 19:43:17.529355 25308 net.cpp:200] ip3 needs backward computation.
I0428 19:43:17.529356 25308 net.cpp:200] relu2 needs backward computation.
I0428 19:43:17.529369 25308 net.cpp:200] ip2 needs backward computation.
I0428 19:43:17.529372 25308 net.cpp:200] relu1 needs backward computation.
I0428 19:43:17.529376 25308 net.cpp:200] ip1 needs backward computation.
I0428 19:43:17.529378 25308 net.cpp:200] pool1 needs backward computation.
I0428 19:43:17.529381 25308 net.cpp:200] conv1 needs backward computation.
I0428 19:43:17.529383 25308 net.cpp:200] pool0 needs backward computation.
I0428 19:43:17.529386 25308 net.cpp:200] conv0 needs backward computation.
I0428 19:43:17.529392 25308 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:43:17.529395 25308 net.cpp:202] mnist does not need backward computation.
I0428 19:43:17.529397 25308 net.cpp:244] This network produces output accuracy
I0428 19:43:17.529402 25308 net.cpp:244] This network produces output loss
I0428 19:43:17.529412 25308 net.cpp:257] Network initialization done.
I0428 19:43:17.529474 25308 solver.cpp:56] Solver scaffolding done.
I0428 19:43:17.529867 25308 caffe.cpp:248] Starting Optimization
I0428 19:43:17.529875 25308 solver.cpp:273] Solving LeNet
I0428 19:43:17.529877 25308 solver.cpp:274] Learning Rate Policy: inv
I0428 19:43:17.530710 25308 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:43:17.533532 25308 blocking_queue.cpp:49] Waiting for data
I0428 19:43:17.638833 25315 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:43:17.639338 25308 solver.cpp:398]     Test net output #0: accuracy = 0.0994
I0428 19:43:17.639369 25308 solver.cpp:398]     Test net output #1: loss = 2.30101 (* 1 = 2.30101 loss)
I0428 19:43:17.640712 25308 solver.cpp:219] Iteration 0 (-4.00255e-31 iter/s, 0.110788s/100 iters), loss = 2.29643
I0428 19:43:17.640749 25308 solver.cpp:238]     Train net output #0: loss = 2.29643 (* 1 = 2.29643 loss)
I0428 19:43:17.640763 25308 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:43:17.725953 25308 solver.cpp:219] Iteration 100 (1173.64 iter/s, 0.085205s/100 iters), loss = 0.829238
I0428 19:43:17.725977 25308 solver.cpp:238]     Train net output #0: loss = 0.829238 (* 1 = 0.829238 loss)
I0428 19:43:17.725983 25308 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:43:17.811444 25308 solver.cpp:219] Iteration 200 (1170.19 iter/s, 0.0854559s/100 iters), loss = 0.543652
I0428 19:43:17.811481 25308 solver.cpp:238]     Train net output #0: loss = 0.543652 (* 1 = 0.543652 loss)
I0428 19:43:17.811487 25308 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:43:17.899476 25308 solver.cpp:219] Iteration 300 (1136.37 iter/s, 0.0879995s/100 iters), loss = 0.503557
I0428 19:43:17.899513 25308 solver.cpp:238]     Train net output #0: loss = 0.503557 (* 1 = 0.503557 loss)
I0428 19:43:17.899519 25308 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:43:17.983196 25308 solver.cpp:219] Iteration 400 (1194.92 iter/s, 0.0836876s/100 iters), loss = 0.852354
I0428 19:43:17.983235 25308 solver.cpp:238]     Train net output #0: loss = 0.852354 (* 1 = 0.852354 loss)
I0428 19:43:17.983242 25308 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:43:18.067520 25308 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:43:18.120427 25315 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:43:18.120895 25308 solver.cpp:398]     Test net output #0: accuracy = 0.7706
I0428 19:43:18.120916 25308 solver.cpp:398]     Test net output #1: loss = 0.584057 (* 1 = 0.584057 loss)
I0428 19:43:18.121779 25308 solver.cpp:219] Iteration 500 (721.85 iter/s, 0.138533s/100 iters), loss = 0.683921
I0428 19:43:18.121862 25308 solver.cpp:238]     Train net output #0: loss = 0.683921 (* 1 = 0.683921 loss)
I0428 19:43:18.121870 25308 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:43:18.206835 25308 solver.cpp:219] Iteration 600 (1176.81 iter/s, 0.0849756s/100 iters), loss = 0.472756
I0428 19:43:18.206874 25308 solver.cpp:238]     Train net output #0: loss = 0.472756 (* 1 = 0.472756 loss)
I0428 19:43:18.206881 25308 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:43:18.291510 25308 solver.cpp:219] Iteration 700 (1181.54 iter/s, 0.0846357s/100 iters), loss = 0.484236
I0428 19:43:18.291535 25308 solver.cpp:238]     Train net output #0: loss = 0.484236 (* 1 = 0.484236 loss)
I0428 19:43:18.291541 25308 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:43:18.374838 25308 solver.cpp:219] Iteration 800 (1200.62 iter/s, 0.0832901s/100 iters), loss = 0.653538
I0428 19:43:18.374881 25308 solver.cpp:238]     Train net output #0: loss = 0.653538 (* 1 = 0.653538 loss)
I0428 19:43:18.374887 25308 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:43:18.459468 25308 solver.cpp:219] Iteration 900 (1182.36 iter/s, 0.0845763s/100 iters), loss = 0.331886
I0428 19:43:18.459507 25308 solver.cpp:238]     Train net output #0: loss = 0.331886 (* 1 = 0.331886 loss)
I0428 19:43:18.459513 25308 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:43:18.488206 25314 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:43:18.544386 25308 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:43:18.545356 25308 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:43:18.545887 25308 solver.cpp:311] Iteration 1000, loss = 0.474101
I0428 19:43:18.545902 25308 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:43:18.598893 25315 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:43:18.599333 25308 solver.cpp:398]     Test net output #0: accuracy = 0.7838
I0428 19:43:18.599365 25308 solver.cpp:398]     Test net output #1: loss = 0.536437 (* 1 = 0.536437 loss)
I0428 19:43:18.599370 25308 solver.cpp:316] Optimization Done.
I0428 19:43:18.599373 25308 caffe.cpp:259] Optimization Done.
