I0428 19:48:21.571578 26463 caffe.cpp:218] Using GPUs 0
I0428 19:48:21.611948 26463 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:48:22.143849 26463 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test542.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:48:22.144013 26463 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test542.prototxt
I0428 19:48:22.144444 26463 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:48:22.144470 26463 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:48:22.144590 26463 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:48:22.144707 26463 layer_factory.hpp:77] Creating layer mnist
I0428 19:48:22.145227 26463 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:48:22.145259 26463 net.cpp:86] Creating Layer mnist
I0428 19:48:22.145269 26463 net.cpp:382] mnist -> data
I0428 19:48:22.145293 26463 net.cpp:382] mnist -> label
I0428 19:48:22.146406 26463 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:48:22.148871 26463 net.cpp:124] Setting up mnist
I0428 19:48:22.148890 26463 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:48:22.148896 26463 net.cpp:131] Top shape: 64 (64)
I0428 19:48:22.148900 26463 net.cpp:139] Memory required for data: 200960
I0428 19:48:22.148907 26463 layer_factory.hpp:77] Creating layer conv0
I0428 19:48:22.148922 26463 net.cpp:86] Creating Layer conv0
I0428 19:48:22.148943 26463 net.cpp:408] conv0 <- data
I0428 19:48:22.148957 26463 net.cpp:382] conv0 -> conv0
I0428 19:48:22.438611 26463 net.cpp:124] Setting up conv0
I0428 19:48:22.438643 26463 net.cpp:131] Top shape: 64 5 24 24 (184320)
I0428 19:48:22.438647 26463 net.cpp:139] Memory required for data: 938240
I0428 19:48:22.438666 26463 layer_factory.hpp:77] Creating layer pool0
I0428 19:48:22.438681 26463 net.cpp:86] Creating Layer pool0
I0428 19:48:22.438686 26463 net.cpp:408] pool0 <- conv0
I0428 19:48:22.438693 26463 net.cpp:382] pool0 -> pool0
I0428 19:48:22.438752 26463 net.cpp:124] Setting up pool0
I0428 19:48:22.438760 26463 net.cpp:131] Top shape: 64 5 12 12 (46080)
I0428 19:48:22.438762 26463 net.cpp:139] Memory required for data: 1122560
I0428 19:48:22.438766 26463 layer_factory.hpp:77] Creating layer conv1
I0428 19:48:22.438779 26463 net.cpp:86] Creating Layer conv1
I0428 19:48:22.438783 26463 net.cpp:408] conv1 <- pool0
I0428 19:48:22.438791 26463 net.cpp:382] conv1 -> conv1
I0428 19:48:22.441000 26463 net.cpp:124] Setting up conv1
I0428 19:48:22.441017 26463 net.cpp:131] Top shape: 64 2 8 8 (8192)
I0428 19:48:22.441022 26463 net.cpp:139] Memory required for data: 1155328
I0428 19:48:22.441032 26463 layer_factory.hpp:77] Creating layer pool1
I0428 19:48:22.441042 26463 net.cpp:86] Creating Layer pool1
I0428 19:48:22.441047 26463 net.cpp:408] pool1 <- conv1
I0428 19:48:22.441052 26463 net.cpp:382] pool1 -> pool1
I0428 19:48:22.441099 26463 net.cpp:124] Setting up pool1
I0428 19:48:22.441105 26463 net.cpp:131] Top shape: 64 2 4 4 (2048)
I0428 19:48:22.441108 26463 net.cpp:139] Memory required for data: 1163520
I0428 19:48:22.441112 26463 layer_factory.hpp:77] Creating layer ip1
I0428 19:48:22.441120 26463 net.cpp:86] Creating Layer ip1
I0428 19:48:22.441124 26463 net.cpp:408] ip1 <- pool1
I0428 19:48:22.441130 26463 net.cpp:382] ip1 -> ip1
I0428 19:48:22.442248 26463 net.cpp:124] Setting up ip1
I0428 19:48:22.442262 26463 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:48:22.442266 26463 net.cpp:139] Memory required for data: 1176320
I0428 19:48:22.442276 26463 layer_factory.hpp:77] Creating layer relu1
I0428 19:48:22.442283 26463 net.cpp:86] Creating Layer relu1
I0428 19:48:22.442287 26463 net.cpp:408] relu1 <- ip1
I0428 19:48:22.442293 26463 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:48:22.442512 26463 net.cpp:124] Setting up relu1
I0428 19:48:22.442522 26463 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:48:22.442525 26463 net.cpp:139] Memory required for data: 1189120
I0428 19:48:22.442529 26463 layer_factory.hpp:77] Creating layer ip2
I0428 19:48:22.442538 26463 net.cpp:86] Creating Layer ip2
I0428 19:48:22.442543 26463 net.cpp:408] ip2 <- ip1
I0428 19:48:22.442548 26463 net.cpp:382] ip2 -> ip2
I0428 19:48:22.442677 26463 net.cpp:124] Setting up ip2
I0428 19:48:22.442685 26463 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:48:22.442688 26463 net.cpp:139] Memory required for data: 1195520
I0428 19:48:22.442695 26463 layer_factory.hpp:77] Creating layer relu2
I0428 19:48:22.442703 26463 net.cpp:86] Creating Layer relu2
I0428 19:48:22.442706 26463 net.cpp:408] relu2 <- ip2
I0428 19:48:22.442713 26463 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:48:22.443577 26463 net.cpp:124] Setting up relu2
I0428 19:48:22.443590 26463 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:48:22.443594 26463 net.cpp:139] Memory required for data: 1201920
I0428 19:48:22.443598 26463 layer_factory.hpp:77] Creating layer ip3
I0428 19:48:22.443608 26463 net.cpp:86] Creating Layer ip3
I0428 19:48:22.443612 26463 net.cpp:408] ip3 <- ip2
I0428 19:48:22.443619 26463 net.cpp:382] ip3 -> ip3
I0428 19:48:22.443745 26463 net.cpp:124] Setting up ip3
I0428 19:48:22.443754 26463 net.cpp:131] Top shape: 64 10 (640)
I0428 19:48:22.443758 26463 net.cpp:139] Memory required for data: 1204480
I0428 19:48:22.443768 26463 layer_factory.hpp:77] Creating layer relu3
I0428 19:48:22.443774 26463 net.cpp:86] Creating Layer relu3
I0428 19:48:22.443778 26463 net.cpp:408] relu3 <- ip3
I0428 19:48:22.443783 26463 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:48:22.443994 26463 net.cpp:124] Setting up relu3
I0428 19:48:22.444005 26463 net.cpp:131] Top shape: 64 10 (640)
I0428 19:48:22.444008 26463 net.cpp:139] Memory required for data: 1207040
I0428 19:48:22.444011 26463 layer_factory.hpp:77] Creating layer loss
I0428 19:48:22.444020 26463 net.cpp:86] Creating Layer loss
I0428 19:48:22.444023 26463 net.cpp:408] loss <- ip3
I0428 19:48:22.444027 26463 net.cpp:408] loss <- label
I0428 19:48:22.444033 26463 net.cpp:382] loss -> loss
I0428 19:48:22.444051 26463 layer_factory.hpp:77] Creating layer loss
I0428 19:48:22.444340 26463 net.cpp:124] Setting up loss
I0428 19:48:22.444351 26463 net.cpp:131] Top shape: (1)
I0428 19:48:22.444355 26463 net.cpp:134]     with loss weight 1
I0428 19:48:22.444371 26463 net.cpp:139] Memory required for data: 1207044
I0428 19:48:22.444375 26463 net.cpp:200] loss needs backward computation.
I0428 19:48:22.444380 26463 net.cpp:200] relu3 needs backward computation.
I0428 19:48:22.444383 26463 net.cpp:200] ip3 needs backward computation.
I0428 19:48:22.444387 26463 net.cpp:200] relu2 needs backward computation.
I0428 19:48:22.444391 26463 net.cpp:200] ip2 needs backward computation.
I0428 19:48:22.444393 26463 net.cpp:200] relu1 needs backward computation.
I0428 19:48:22.444397 26463 net.cpp:200] ip1 needs backward computation.
I0428 19:48:22.444401 26463 net.cpp:200] pool1 needs backward computation.
I0428 19:48:22.444404 26463 net.cpp:200] conv1 needs backward computation.
I0428 19:48:22.444409 26463 net.cpp:200] pool0 needs backward computation.
I0428 19:48:22.444412 26463 net.cpp:200] conv0 needs backward computation.
I0428 19:48:22.444416 26463 net.cpp:202] mnist does not need backward computation.
I0428 19:48:22.444419 26463 net.cpp:244] This network produces output loss
I0428 19:48:22.444430 26463 net.cpp:257] Network initialization done.
I0428 19:48:22.444824 26463 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test542.prototxt
I0428 19:48:22.444857 26463 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:48:22.444967 26463 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:48:22.445070 26463 layer_factory.hpp:77] Creating layer mnist
I0428 19:48:22.445122 26463 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:48:22.445137 26463 net.cpp:86] Creating Layer mnist
I0428 19:48:22.445144 26463 net.cpp:382] mnist -> data
I0428 19:48:22.445153 26463 net.cpp:382] mnist -> label
I0428 19:48:22.445256 26463 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:48:22.447468 26463 net.cpp:124] Setting up mnist
I0428 19:48:22.447482 26463 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:48:22.447489 26463 net.cpp:131] Top shape: 100 (100)
I0428 19:48:22.447492 26463 net.cpp:139] Memory required for data: 314000
I0428 19:48:22.447496 26463 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:48:22.447504 26463 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:48:22.447507 26463 net.cpp:408] label_mnist_1_split <- label
I0428 19:48:22.447515 26463 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:48:22.447522 26463 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:48:22.447677 26463 net.cpp:124] Setting up label_mnist_1_split
I0428 19:48:22.447686 26463 net.cpp:131] Top shape: 100 (100)
I0428 19:48:22.447690 26463 net.cpp:131] Top shape: 100 (100)
I0428 19:48:22.447695 26463 net.cpp:139] Memory required for data: 314800
I0428 19:48:22.447697 26463 layer_factory.hpp:77] Creating layer conv0
I0428 19:48:22.447710 26463 net.cpp:86] Creating Layer conv0
I0428 19:48:22.447715 26463 net.cpp:408] conv0 <- data
I0428 19:48:22.447721 26463 net.cpp:382] conv0 -> conv0
I0428 19:48:22.449617 26463 net.cpp:124] Setting up conv0
I0428 19:48:22.449635 26463 net.cpp:131] Top shape: 100 5 24 24 (288000)
I0428 19:48:22.449638 26463 net.cpp:139] Memory required for data: 1466800
I0428 19:48:22.449651 26463 layer_factory.hpp:77] Creating layer pool0
I0428 19:48:22.449659 26463 net.cpp:86] Creating Layer pool0
I0428 19:48:22.449663 26463 net.cpp:408] pool0 <- conv0
I0428 19:48:22.449669 26463 net.cpp:382] pool0 -> pool0
I0428 19:48:22.449713 26463 net.cpp:124] Setting up pool0
I0428 19:48:22.449720 26463 net.cpp:131] Top shape: 100 5 12 12 (72000)
I0428 19:48:22.449723 26463 net.cpp:139] Memory required for data: 1754800
I0428 19:48:22.449726 26463 layer_factory.hpp:77] Creating layer conv1
I0428 19:48:22.449738 26463 net.cpp:86] Creating Layer conv1
I0428 19:48:22.449741 26463 net.cpp:408] conv1 <- pool0
I0428 19:48:22.449748 26463 net.cpp:382] conv1 -> conv1
I0428 19:48:22.452113 26463 net.cpp:124] Setting up conv1
I0428 19:48:22.452133 26463 net.cpp:131] Top shape: 100 2 8 8 (12800)
I0428 19:48:22.452145 26463 net.cpp:139] Memory required for data: 1806000
I0428 19:48:22.452158 26463 layer_factory.hpp:77] Creating layer pool1
I0428 19:48:22.452167 26463 net.cpp:86] Creating Layer pool1
I0428 19:48:22.452172 26463 net.cpp:408] pool1 <- conv1
I0428 19:48:22.452177 26463 net.cpp:382] pool1 -> pool1
I0428 19:48:22.452229 26463 net.cpp:124] Setting up pool1
I0428 19:48:22.452241 26463 net.cpp:131] Top shape: 100 2 4 4 (3200)
I0428 19:48:22.452244 26463 net.cpp:139] Memory required for data: 1818800
I0428 19:48:22.452249 26463 layer_factory.hpp:77] Creating layer ip1
I0428 19:48:22.452255 26463 net.cpp:86] Creating Layer ip1
I0428 19:48:22.452258 26463 net.cpp:408] ip1 <- pool1
I0428 19:48:22.452268 26463 net.cpp:382] ip1 -> ip1
I0428 19:48:22.452467 26463 net.cpp:124] Setting up ip1
I0428 19:48:22.452477 26463 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:48:22.452493 26463 net.cpp:139] Memory required for data: 1838800
I0428 19:48:22.452503 26463 layer_factory.hpp:77] Creating layer relu1
I0428 19:48:22.452510 26463 net.cpp:86] Creating Layer relu1
I0428 19:48:22.452515 26463 net.cpp:408] relu1 <- ip1
I0428 19:48:22.452520 26463 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:48:22.452728 26463 net.cpp:124] Setting up relu1
I0428 19:48:22.452739 26463 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:48:22.452742 26463 net.cpp:139] Memory required for data: 1858800
I0428 19:48:22.452746 26463 layer_factory.hpp:77] Creating layer ip2
I0428 19:48:22.452764 26463 net.cpp:86] Creating Layer ip2
I0428 19:48:22.452767 26463 net.cpp:408] ip2 <- ip1
I0428 19:48:22.452774 26463 net.cpp:382] ip2 -> ip2
I0428 19:48:22.452914 26463 net.cpp:124] Setting up ip2
I0428 19:48:22.452924 26463 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:48:22.452927 26463 net.cpp:139] Memory required for data: 1868800
I0428 19:48:22.452934 26463 layer_factory.hpp:77] Creating layer relu2
I0428 19:48:22.452941 26463 net.cpp:86] Creating Layer relu2
I0428 19:48:22.452945 26463 net.cpp:408] relu2 <- ip2
I0428 19:48:22.452950 26463 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:48:22.453146 26463 net.cpp:124] Setting up relu2
I0428 19:48:22.453157 26463 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:48:22.453161 26463 net.cpp:139] Memory required for data: 1878800
I0428 19:48:22.453164 26463 layer_factory.hpp:77] Creating layer ip3
I0428 19:48:22.453172 26463 net.cpp:86] Creating Layer ip3
I0428 19:48:22.453176 26463 net.cpp:408] ip3 <- ip2
I0428 19:48:22.453182 26463 net.cpp:382] ip3 -> ip3
I0428 19:48:22.453301 26463 net.cpp:124] Setting up ip3
I0428 19:48:22.453310 26463 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:48:22.453313 26463 net.cpp:139] Memory required for data: 1882800
I0428 19:48:22.453322 26463 layer_factory.hpp:77] Creating layer relu3
I0428 19:48:22.453328 26463 net.cpp:86] Creating Layer relu3
I0428 19:48:22.453332 26463 net.cpp:408] relu3 <- ip3
I0428 19:48:22.453336 26463 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:48:22.454277 26463 net.cpp:124] Setting up relu3
I0428 19:48:22.454293 26463 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:48:22.454296 26463 net.cpp:139] Memory required for data: 1886800
I0428 19:48:22.454300 26463 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:48:22.454306 26463 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:48:22.454310 26463 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:48:22.454318 26463 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:48:22.454324 26463 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:48:22.454386 26463 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:48:22.454394 26463 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:48:22.454399 26463 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:48:22.454402 26463 net.cpp:139] Memory required for data: 1894800
I0428 19:48:22.454406 26463 layer_factory.hpp:77] Creating layer accuracy
I0428 19:48:22.454413 26463 net.cpp:86] Creating Layer accuracy
I0428 19:48:22.454417 26463 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:48:22.454422 26463 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:48:22.454427 26463 net.cpp:382] accuracy -> accuracy
I0428 19:48:22.454437 26463 net.cpp:124] Setting up accuracy
I0428 19:48:22.454440 26463 net.cpp:131] Top shape: (1)
I0428 19:48:22.454443 26463 net.cpp:139] Memory required for data: 1894804
I0428 19:48:22.454447 26463 layer_factory.hpp:77] Creating layer loss
I0428 19:48:22.454453 26463 net.cpp:86] Creating Layer loss
I0428 19:48:22.454458 26463 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:48:22.454463 26463 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:48:22.454468 26463 net.cpp:382] loss -> loss
I0428 19:48:22.454480 26463 layer_factory.hpp:77] Creating layer loss
I0428 19:48:22.454768 26463 net.cpp:124] Setting up loss
I0428 19:48:22.454779 26463 net.cpp:131] Top shape: (1)
I0428 19:48:22.454783 26463 net.cpp:134]     with loss weight 1
I0428 19:48:22.454790 26463 net.cpp:139] Memory required for data: 1894808
I0428 19:48:22.454808 26463 net.cpp:200] loss needs backward computation.
I0428 19:48:22.454813 26463 net.cpp:202] accuracy does not need backward computation.
I0428 19:48:22.454818 26463 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:48:22.454828 26463 net.cpp:200] relu3 needs backward computation.
I0428 19:48:22.454833 26463 net.cpp:200] ip3 needs backward computation.
I0428 19:48:22.454835 26463 net.cpp:200] relu2 needs backward computation.
I0428 19:48:22.454839 26463 net.cpp:200] ip2 needs backward computation.
I0428 19:48:22.454843 26463 net.cpp:200] relu1 needs backward computation.
I0428 19:48:22.454845 26463 net.cpp:200] ip1 needs backward computation.
I0428 19:48:22.454849 26463 net.cpp:200] pool1 needs backward computation.
I0428 19:48:22.454852 26463 net.cpp:200] conv1 needs backward computation.
I0428 19:48:22.454857 26463 net.cpp:200] pool0 needs backward computation.
I0428 19:48:22.454867 26463 net.cpp:200] conv0 needs backward computation.
I0428 19:48:22.454871 26463 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:48:22.454877 26463 net.cpp:202] mnist does not need backward computation.
I0428 19:48:22.454885 26463 net.cpp:244] This network produces output accuracy
I0428 19:48:22.454890 26463 net.cpp:244] This network produces output loss
I0428 19:48:22.454905 26463 net.cpp:257] Network initialization done.
I0428 19:48:22.454953 26463 solver.cpp:56] Solver scaffolding done.
I0428 19:48:22.455359 26463 caffe.cpp:248] Starting Optimization
I0428 19:48:22.455366 26463 solver.cpp:273] Solving LeNet
I0428 19:48:22.455369 26463 solver.cpp:274] Learning Rate Policy: inv
I0428 19:48:22.455721 26463 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:48:22.460237 26463 blocking_queue.cpp:49] Waiting for data
I0428 19:48:22.531607 26470 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:48:22.532155 26463 solver.cpp:398]     Test net output #0: accuracy = 0.1152
I0428 19:48:22.532182 26463 solver.cpp:398]     Test net output #1: loss = 2.30577 (* 1 = 2.30577 loss)
I0428 19:48:22.534273 26463 solver.cpp:219] Iteration 0 (0 iter/s, 0.0788705s/100 iters), loss = 2.31356
I0428 19:48:22.534304 26463 solver.cpp:238]     Train net output #0: loss = 2.31356 (* 1 = 2.31356 loss)
I0428 19:48:22.534318 26463 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:48:22.608968 26463 solver.cpp:219] Iteration 100 (1339.59 iter/s, 0.0746497s/100 iters), loss = 0.843062
I0428 19:48:22.608999 26463 solver.cpp:238]     Train net output #0: loss = 0.843062 (* 1 = 0.843062 loss)
I0428 19:48:22.609006 26463 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:48:22.680948 26463 solver.cpp:219] Iteration 200 (1390.05 iter/s, 0.0719401s/100 iters), loss = 0.392212
I0428 19:48:22.680972 26463 solver.cpp:238]     Train net output #0: loss = 0.392212 (* 1 = 0.392212 loss)
I0428 19:48:22.680979 26463 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:48:22.749296 26463 solver.cpp:219] Iteration 300 (1463.78 iter/s, 0.0683162s/100 iters), loss = 0.328521
I0428 19:48:22.749320 26463 solver.cpp:238]     Train net output #0: loss = 0.328521 (* 1 = 0.328521 loss)
I0428 19:48:22.749325 26463 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:48:22.818248 26463 solver.cpp:219] Iteration 400 (1450.99 iter/s, 0.0689183s/100 iters), loss = 0.356471
I0428 19:48:22.818270 26463 solver.cpp:238]     Train net output #0: loss = 0.356471 (* 1 = 0.356471 loss)
I0428 19:48:22.818276 26463 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:48:22.885716 26463 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:48:22.938627 26470 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:48:22.939105 26463 solver.cpp:398]     Test net output #0: accuracy = 0.9217
I0428 19:48:22.939123 26463 solver.cpp:398]     Test net output #1: loss = 0.257213 (* 1 = 0.257213 loss)
I0428 19:48:22.939910 26463 solver.cpp:219] Iteration 500 (822.175 iter/s, 0.121629s/100 iters), loss = 0.363817
I0428 19:48:22.939934 26463 solver.cpp:238]     Train net output #0: loss = 0.363817 (* 1 = 0.363817 loss)
I0428 19:48:22.939961 26463 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:48:23.012789 26463 solver.cpp:219] Iteration 600 (1372.79 iter/s, 0.0728443s/100 iters), loss = 0.141804
I0428 19:48:23.012832 26463 solver.cpp:238]     Train net output #0: loss = 0.141804 (* 1 = 0.141804 loss)
I0428 19:48:23.012854 26463 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:48:23.083427 26463 solver.cpp:219] Iteration 700 (1416.34 iter/s, 0.0706048s/100 iters), loss = 0.411907
I0428 19:48:23.083453 26463 solver.cpp:238]     Train net output #0: loss = 0.411907 (* 1 = 0.411907 loss)
I0428 19:48:23.083461 26463 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:48:23.152818 26463 solver.cpp:219] Iteration 800 (1441.93 iter/s, 0.0693514s/100 iters), loss = 0.277274
I0428 19:48:23.152843 26463 solver.cpp:238]     Train net output #0: loss = 0.277274 (* 1 = 0.277274 loss)
I0428 19:48:23.152850 26463 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:48:23.222350 26463 solver.cpp:219] Iteration 900 (1438.89 iter/s, 0.0694979s/100 iters), loss = 0.238907
I0428 19:48:23.222375 26463 solver.cpp:238]     Train net output #0: loss = 0.238907 (* 1 = 0.238907 loss)
I0428 19:48:23.222383 26463 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:48:23.245604 26469 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:48:23.291085 26463 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:48:23.291764 26463 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:48:23.292176 26463 solver.cpp:311] Iteration 1000, loss = 0.217392
I0428 19:48:23.292191 26463 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:48:23.366814 26470 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:48:23.367321 26463 solver.cpp:398]     Test net output #0: accuracy = 0.9461
I0428 19:48:23.367341 26463 solver.cpp:398]     Test net output #1: loss = 0.172676 (* 1 = 0.172676 loss)
I0428 19:48:23.367347 26463 solver.cpp:316] Optimization Done.
I0428 19:48:23.367364 26463 caffe.cpp:259] Optimization Done.
