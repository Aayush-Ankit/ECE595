I0428 19:34:35.496125 23357 caffe.cpp:218] Using GPUs 0
I0428 19:34:35.536756 23357 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:34:35.998383 23357 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test207.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:34:35.998550 23357 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test207.prototxt
I0428 19:34:35.998878 23357 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:34:35.998906 23357 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:34:35.998975 23357 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool0"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:34:35.999035 23357 layer_factory.hpp:77] Creating layer mnist
I0428 19:34:35.999115 23357 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:34:35.999135 23357 net.cpp:86] Creating Layer mnist
I0428 19:34:35.999141 23357 net.cpp:382] mnist -> data
I0428 19:34:35.999160 23357 net.cpp:382] mnist -> label
I0428 19:34:36.000242 23357 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:34:36.002599 23357 net.cpp:124] Setting up mnist
I0428 19:34:36.002611 23357 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:34:36.002616 23357 net.cpp:131] Top shape: 64 (64)
I0428 19:34:36.002619 23357 net.cpp:139] Memory required for data: 200960
I0428 19:34:36.002624 23357 layer_factory.hpp:77] Creating layer conv0
I0428 19:34:36.002637 23357 net.cpp:86] Creating Layer conv0
I0428 19:34:36.002642 23357 net.cpp:408] conv0 <- data
I0428 19:34:36.002667 23357 net.cpp:382] conv0 -> conv0
I0428 19:34:36.232028 23357 net.cpp:124] Setting up conv0
I0428 19:34:36.232069 23357 net.cpp:131] Top shape: 64 50 24 24 (1843200)
I0428 19:34:36.232074 23357 net.cpp:139] Memory required for data: 7573760
I0428 19:34:36.232089 23357 layer_factory.hpp:77] Creating layer pool0
I0428 19:34:36.232101 23357 net.cpp:86] Creating Layer pool0
I0428 19:34:36.232144 23357 net.cpp:408] pool0 <- conv0
I0428 19:34:36.232151 23357 net.cpp:382] pool0 -> pool0
I0428 19:34:36.232195 23357 net.cpp:124] Setting up pool0
I0428 19:34:36.232208 23357 net.cpp:131] Top shape: 64 50 12 12 (460800)
I0428 19:34:36.232210 23357 net.cpp:139] Memory required for data: 9416960
I0428 19:34:36.232213 23357 layer_factory.hpp:77] Creating layer ip1
I0428 19:34:36.232220 23357 net.cpp:86] Creating Layer ip1
I0428 19:34:36.232223 23357 net.cpp:408] ip1 <- pool0
I0428 19:34:36.232228 23357 net.cpp:382] ip1 -> ip1
I0428 19:34:36.233711 23357 net.cpp:124] Setting up ip1
I0428 19:34:36.233724 23357 net.cpp:131] Top shape: 64 10 (640)
I0428 19:34:36.233743 23357 net.cpp:139] Memory required for data: 9419520
I0428 19:34:36.233752 23357 layer_factory.hpp:77] Creating layer relu1
I0428 19:34:36.233758 23357 net.cpp:86] Creating Layer relu1
I0428 19:34:36.233762 23357 net.cpp:408] relu1 <- ip1
I0428 19:34:36.233767 23357 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:34:36.233945 23357 net.cpp:124] Setting up relu1
I0428 19:34:36.233954 23357 net.cpp:131] Top shape: 64 10 (640)
I0428 19:34:36.233958 23357 net.cpp:139] Memory required for data: 9422080
I0428 19:34:36.233960 23357 layer_factory.hpp:77] Creating layer ip2
I0428 19:34:36.233966 23357 net.cpp:86] Creating Layer ip2
I0428 19:34:36.233969 23357 net.cpp:408] ip2 <- ip1
I0428 19:34:36.233973 23357 net.cpp:382] ip2 -> ip2
I0428 19:34:36.234061 23357 net.cpp:124] Setting up ip2
I0428 19:34:36.234067 23357 net.cpp:131] Top shape: 64 10 (640)
I0428 19:34:36.234069 23357 net.cpp:139] Memory required for data: 9424640
I0428 19:34:36.234076 23357 layer_factory.hpp:77] Creating layer relu2
I0428 19:34:36.234081 23357 net.cpp:86] Creating Layer relu2
I0428 19:34:36.234084 23357 net.cpp:408] relu2 <- ip2
I0428 19:34:36.234088 23357 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:34:36.234803 23357 net.cpp:124] Setting up relu2
I0428 19:34:36.234815 23357 net.cpp:131] Top shape: 64 10 (640)
I0428 19:34:36.234834 23357 net.cpp:139] Memory required for data: 9427200
I0428 19:34:36.234838 23357 layer_factory.hpp:77] Creating layer ip3
I0428 19:34:36.234844 23357 net.cpp:86] Creating Layer ip3
I0428 19:34:36.234848 23357 net.cpp:408] ip3 <- ip2
I0428 19:34:36.234853 23357 net.cpp:382] ip3 -> ip3
I0428 19:34:36.234958 23357 net.cpp:124] Setting up ip3
I0428 19:34:36.234966 23357 net.cpp:131] Top shape: 64 10 (640)
I0428 19:34:36.234968 23357 net.cpp:139] Memory required for data: 9429760
I0428 19:34:36.234973 23357 layer_factory.hpp:77] Creating layer relu3
I0428 19:34:36.234980 23357 net.cpp:86] Creating Layer relu3
I0428 19:34:36.234982 23357 net.cpp:408] relu3 <- ip3
I0428 19:34:36.234987 23357 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:34:36.235126 23357 net.cpp:124] Setting up relu3
I0428 19:34:36.235136 23357 net.cpp:131] Top shape: 64 10 (640)
I0428 19:34:36.235154 23357 net.cpp:139] Memory required for data: 9432320
I0428 19:34:36.235157 23357 layer_factory.hpp:77] Creating layer loss
I0428 19:34:36.235162 23357 net.cpp:86] Creating Layer loss
I0428 19:34:36.235165 23357 net.cpp:408] loss <- ip3
I0428 19:34:36.235169 23357 net.cpp:408] loss <- label
I0428 19:34:36.235174 23357 net.cpp:382] loss -> loss
I0428 19:34:36.235188 23357 layer_factory.hpp:77] Creating layer loss
I0428 19:34:36.235405 23357 net.cpp:124] Setting up loss
I0428 19:34:36.235414 23357 net.cpp:131] Top shape: (1)
I0428 19:34:36.235417 23357 net.cpp:134]     with loss weight 1
I0428 19:34:36.235431 23357 net.cpp:139] Memory required for data: 9432324
I0428 19:34:36.235435 23357 net.cpp:200] loss needs backward computation.
I0428 19:34:36.235452 23357 net.cpp:200] relu3 needs backward computation.
I0428 19:34:36.235455 23357 net.cpp:200] ip3 needs backward computation.
I0428 19:34:36.235458 23357 net.cpp:200] relu2 needs backward computation.
I0428 19:34:36.235460 23357 net.cpp:200] ip2 needs backward computation.
I0428 19:34:36.235463 23357 net.cpp:200] relu1 needs backward computation.
I0428 19:34:36.235466 23357 net.cpp:200] ip1 needs backward computation.
I0428 19:34:36.235479 23357 net.cpp:200] pool0 needs backward computation.
I0428 19:34:36.235482 23357 net.cpp:200] conv0 needs backward computation.
I0428 19:34:36.235486 23357 net.cpp:202] mnist does not need backward computation.
I0428 19:34:36.235488 23357 net.cpp:244] This network produces output loss
I0428 19:34:36.235496 23357 net.cpp:257] Network initialization done.
I0428 19:34:36.235781 23357 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test207.prototxt
I0428 19:34:36.235806 23357 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:34:36.235887 23357 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool0"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:34:36.235947 23357 layer_factory.hpp:77] Creating layer mnist
I0428 19:34:36.235992 23357 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:34:36.236006 23357 net.cpp:86] Creating Layer mnist
I0428 19:34:36.236009 23357 net.cpp:382] mnist -> data
I0428 19:34:36.236032 23357 net.cpp:382] mnist -> label
I0428 19:34:36.236110 23357 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:34:36.237237 23357 net.cpp:124] Setting up mnist
I0428 19:34:36.237280 23357 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:34:36.237285 23357 net.cpp:131] Top shape: 100 (100)
I0428 19:34:36.237288 23357 net.cpp:139] Memory required for data: 314000
I0428 19:34:36.237293 23357 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:34:36.237298 23357 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:34:36.237303 23357 net.cpp:408] label_mnist_1_split <- label
I0428 19:34:36.237308 23357 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:34:36.237313 23357 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:34:36.237356 23357 net.cpp:124] Setting up label_mnist_1_split
I0428 19:34:36.237361 23357 net.cpp:131] Top shape: 100 (100)
I0428 19:34:36.237365 23357 net.cpp:131] Top shape: 100 (100)
I0428 19:34:36.237367 23357 net.cpp:139] Memory required for data: 314800
I0428 19:34:36.237380 23357 layer_factory.hpp:77] Creating layer conv0
I0428 19:34:36.237390 23357 net.cpp:86] Creating Layer conv0
I0428 19:34:36.237392 23357 net.cpp:408] conv0 <- data
I0428 19:34:36.237397 23357 net.cpp:382] conv0 -> conv0
I0428 19:34:36.238852 23357 net.cpp:124] Setting up conv0
I0428 19:34:36.238883 23357 net.cpp:131] Top shape: 100 50 24 24 (2880000)
I0428 19:34:36.238885 23357 net.cpp:139] Memory required for data: 11834800
I0428 19:34:36.238894 23357 layer_factory.hpp:77] Creating layer pool0
I0428 19:34:36.238903 23357 net.cpp:86] Creating Layer pool0
I0428 19:34:36.238921 23357 net.cpp:408] pool0 <- conv0
I0428 19:34:36.238926 23357 net.cpp:382] pool0 -> pool0
I0428 19:34:36.238977 23357 net.cpp:124] Setting up pool0
I0428 19:34:36.238998 23357 net.cpp:131] Top shape: 100 50 12 12 (720000)
I0428 19:34:36.239002 23357 net.cpp:139] Memory required for data: 14714800
I0428 19:34:36.239006 23357 layer_factory.hpp:77] Creating layer ip1
I0428 19:34:36.239012 23357 net.cpp:86] Creating Layer ip1
I0428 19:34:36.239015 23357 net.cpp:408] ip1 <- pool0
I0428 19:34:36.239020 23357 net.cpp:382] ip1 -> ip1
I0428 19:34:36.239596 23357 net.cpp:124] Setting up ip1
I0428 19:34:36.239604 23357 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:34:36.239624 23357 net.cpp:139] Memory required for data: 14718800
I0428 19:34:36.239631 23357 layer_factory.hpp:77] Creating layer relu1
I0428 19:34:36.239636 23357 net.cpp:86] Creating Layer relu1
I0428 19:34:36.239640 23357 net.cpp:408] relu1 <- ip1
I0428 19:34:36.239645 23357 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:34:36.239816 23357 net.cpp:124] Setting up relu1
I0428 19:34:36.239826 23357 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:34:36.239830 23357 net.cpp:139] Memory required for data: 14722800
I0428 19:34:36.239833 23357 layer_factory.hpp:77] Creating layer ip2
I0428 19:34:36.239838 23357 net.cpp:86] Creating Layer ip2
I0428 19:34:36.239842 23357 net.cpp:408] ip2 <- ip1
I0428 19:34:36.239847 23357 net.cpp:382] ip2 -> ip2
I0428 19:34:36.239944 23357 net.cpp:124] Setting up ip2
I0428 19:34:36.239951 23357 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:34:36.239955 23357 net.cpp:139] Memory required for data: 14726800
I0428 19:34:36.239962 23357 layer_factory.hpp:77] Creating layer relu2
I0428 19:34:36.239967 23357 net.cpp:86] Creating Layer relu2
I0428 19:34:36.239970 23357 net.cpp:408] relu2 <- ip2
I0428 19:34:36.239975 23357 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:34:36.240880 23357 net.cpp:124] Setting up relu2
I0428 19:34:36.240892 23357 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:34:36.240896 23357 net.cpp:139] Memory required for data: 14730800
I0428 19:34:36.240900 23357 layer_factory.hpp:77] Creating layer ip3
I0428 19:34:36.240908 23357 net.cpp:86] Creating Layer ip3
I0428 19:34:36.240912 23357 net.cpp:408] ip3 <- ip2
I0428 19:34:36.240918 23357 net.cpp:382] ip3 -> ip3
I0428 19:34:36.241039 23357 net.cpp:124] Setting up ip3
I0428 19:34:36.241047 23357 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:34:36.241050 23357 net.cpp:139] Memory required for data: 14734800
I0428 19:34:36.241065 23357 layer_factory.hpp:77] Creating layer relu3
I0428 19:34:36.241076 23357 net.cpp:86] Creating Layer relu3
I0428 19:34:36.241080 23357 net.cpp:408] relu3 <- ip3
I0428 19:34:36.241089 23357 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:34:36.241255 23357 net.cpp:124] Setting up relu3
I0428 19:34:36.241266 23357 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:34:36.241271 23357 net.cpp:139] Memory required for data: 14738800
I0428 19:34:36.241273 23357 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:34:36.241278 23357 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:34:36.241282 23357 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:34:36.241287 23357 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:34:36.241298 23357 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:34:36.241334 23357 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:34:36.241341 23357 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:34:36.241345 23357 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:34:36.241360 23357 net.cpp:139] Memory required for data: 14746800
I0428 19:34:36.241364 23357 layer_factory.hpp:77] Creating layer accuracy
I0428 19:34:36.241370 23357 net.cpp:86] Creating Layer accuracy
I0428 19:34:36.241374 23357 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:34:36.241379 23357 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:34:36.241384 23357 net.cpp:382] accuracy -> accuracy
I0428 19:34:36.241391 23357 net.cpp:124] Setting up accuracy
I0428 19:34:36.241395 23357 net.cpp:131] Top shape: (1)
I0428 19:34:36.241399 23357 net.cpp:139] Memory required for data: 14746804
I0428 19:34:36.241401 23357 layer_factory.hpp:77] Creating layer loss
I0428 19:34:36.241406 23357 net.cpp:86] Creating Layer loss
I0428 19:34:36.241410 23357 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:34:36.241413 23357 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:34:36.241418 23357 net.cpp:382] loss -> loss
I0428 19:34:36.241425 23357 layer_factory.hpp:77] Creating layer loss
I0428 19:34:36.241672 23357 net.cpp:124] Setting up loss
I0428 19:34:36.241683 23357 net.cpp:131] Top shape: (1)
I0428 19:34:36.241688 23357 net.cpp:134]     with loss weight 1
I0428 19:34:36.241694 23357 net.cpp:139] Memory required for data: 14746808
I0428 19:34:36.241699 23357 net.cpp:200] loss needs backward computation.
I0428 19:34:36.241703 23357 net.cpp:202] accuracy does not need backward computation.
I0428 19:34:36.241708 23357 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:34:36.241711 23357 net.cpp:200] relu3 needs backward computation.
I0428 19:34:36.241714 23357 net.cpp:200] ip3 needs backward computation.
I0428 19:34:36.241717 23357 net.cpp:200] relu2 needs backward computation.
I0428 19:34:36.241721 23357 net.cpp:200] ip2 needs backward computation.
I0428 19:34:36.241725 23357 net.cpp:200] relu1 needs backward computation.
I0428 19:34:36.241729 23357 net.cpp:200] ip1 needs backward computation.
I0428 19:34:36.241739 23357 net.cpp:200] pool0 needs backward computation.
I0428 19:34:36.241742 23357 net.cpp:200] conv0 needs backward computation.
I0428 19:34:36.241746 23357 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:34:36.241750 23357 net.cpp:202] mnist does not need backward computation.
I0428 19:34:36.241753 23357 net.cpp:244] This network produces output accuracy
I0428 19:34:36.241758 23357 net.cpp:244] This network produces output loss
I0428 19:34:36.241768 23357 net.cpp:257] Network initialization done.
I0428 19:34:36.241808 23357 solver.cpp:56] Solver scaffolding done.
I0428 19:34:36.242151 23357 caffe.cpp:248] Starting Optimization
I0428 19:34:36.242156 23357 solver.cpp:273] Solving LeNet
I0428 19:34:36.242159 23357 solver.cpp:274] Learning Rate Policy: inv
I0428 19:34:36.243069 23357 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:34:36.343408 23364 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:34:36.346066 23357 solver.cpp:398]     Test net output #0: accuracy = 0.1026
I0428 19:34:36.346103 23357 solver.cpp:398]     Test net output #1: loss = 2.3108 (* 1 = 2.3108 loss)
I0428 19:34:36.350534 23357 solver.cpp:219] Iteration 0 (-7.49695e-43 iter/s, 0.108319s/100 iters), loss = 2.31016
I0428 19:34:36.350574 23357 solver.cpp:238]     Train net output #0: loss = 2.31016 (* 1 = 2.31016 loss)
I0428 19:34:36.350585 23357 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:34:36.494686 23357 solver.cpp:219] Iteration 100 (693.975 iter/s, 0.144098s/100 iters), loss = 0.854114
I0428 19:34:36.494741 23357 solver.cpp:238]     Train net output #0: loss = 0.854114 (* 1 = 0.854114 loss)
I0428 19:34:36.494748 23357 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:34:36.637224 23357 solver.cpp:219] Iteration 200 (701.912 iter/s, 0.142468s/100 iters), loss = 0.84471
I0428 19:34:36.637264 23357 solver.cpp:238]     Train net output #0: loss = 0.84471 (* 1 = 0.84471 loss)
I0428 19:34:36.637284 23357 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:34:36.786674 23357 solver.cpp:219] Iteration 300 (669.291 iter/s, 0.149412s/100 iters), loss = 0.545901
I0428 19:34:36.786736 23357 solver.cpp:238]     Train net output #0: loss = 0.545901 (* 1 = 0.545901 loss)
I0428 19:34:36.786744 23357 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:34:36.927479 23357 solver.cpp:219] Iteration 400 (710.574 iter/s, 0.140731s/100 iters), loss = 0.492418
I0428 19:34:36.927508 23357 solver.cpp:238]     Train net output #0: loss = 0.492418 (* 1 = 0.492418 loss)
I0428 19:34:36.927518 23357 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:34:37.068251 23357 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:34:37.169114 23364 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:34:37.172710 23357 solver.cpp:398]     Test net output #0: accuracy = 0.8445
I0428 19:34:37.172734 23357 solver.cpp:398]     Test net output #1: loss = 0.433919 (* 1 = 0.433919 loss)
I0428 19:34:37.174103 23357 solver.cpp:219] Iteration 500 (405.553 iter/s, 0.246577s/100 iters), loss = 0.53543
I0428 19:34:37.174130 23357 solver.cpp:238]     Train net output #0: loss = 0.53543 (* 1 = 0.53543 loss)
I0428 19:34:37.174137 23357 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:34:37.318226 23357 solver.cpp:219] Iteration 600 (694.05 iter/s, 0.144082s/100 iters), loss = 0.457994
I0428 19:34:37.318256 23357 solver.cpp:238]     Train net output #0: loss = 0.457994 (* 1 = 0.457994 loss)
I0428 19:34:37.318264 23357 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:34:37.461819 23357 solver.cpp:219] Iteration 700 (696.618 iter/s, 0.143551s/100 iters), loss = 0.424811
I0428 19:34:37.461849 23357 solver.cpp:238]     Train net output #0: loss = 0.424811 (* 1 = 0.424811 loss)
I0428 19:34:37.461858 23357 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:34:37.601256 23357 solver.cpp:219] Iteration 800 (717.386 iter/s, 0.139395s/100 iters), loss = 0.697968
I0428 19:34:37.601296 23357 solver.cpp:238]     Train net output #0: loss = 0.697968 (* 1 = 0.697968 loss)
I0428 19:34:37.601302 23357 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:34:37.742100 23357 solver.cpp:219] Iteration 900 (710.186 iter/s, 0.140808s/100 iters), loss = 0.830926
I0428 19:34:37.742139 23357 solver.cpp:238]     Train net output #0: loss = 0.830926 (* 1 = 0.830926 loss)
I0428 19:34:37.742146 23357 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:34:37.788940 23363 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:34:37.882098 23357 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:34:37.884078 23357 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:34:37.885746 23357 solver.cpp:311] Iteration 1000, loss = 0.371772
I0428 19:34:37.885761 23357 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:34:37.983108 23364 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:34:37.985682 23357 solver.cpp:398]     Test net output #0: accuracy = 0.8591
I0428 19:34:37.985702 23357 solver.cpp:398]     Test net output #1: loss = 0.375951 (* 1 = 0.375951 loss)
I0428 19:34:37.985707 23357 solver.cpp:316] Optimization Done.
I0428 19:34:37.985725 23357 caffe.cpp:259] Optimization Done.
