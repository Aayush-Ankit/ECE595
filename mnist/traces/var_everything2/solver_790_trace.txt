I0428 19:57:29.031292 28693 caffe.cpp:218] Using GPUs 0
I0428 19:57:29.071377 28693 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:57:29.591418 28693 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test790.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:57:29.591560 28693 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test790.prototxt
I0428 19:57:29.591941 28693 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:57:29.591959 28693 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:57:29.592051 28693 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0428 19:57:29.592124 28693 layer_factory.hpp:77] Creating layer mnist
I0428 19:57:29.592224 28693 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:57:29.592250 28693 net.cpp:86] Creating Layer mnist
I0428 19:57:29.592258 28693 net.cpp:382] mnist -> data
I0428 19:57:29.592280 28693 net.cpp:382] mnist -> label
I0428 19:57:29.593375 28693 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:57:29.595829 28693 net.cpp:124] Setting up mnist
I0428 19:57:29.595846 28693 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:57:29.595854 28693 net.cpp:131] Top shape: 64 (64)
I0428 19:57:29.595857 28693 net.cpp:139] Memory required for data: 200960
I0428 19:57:29.595865 28693 layer_factory.hpp:77] Creating layer conv0
I0428 19:57:29.595880 28693 net.cpp:86] Creating Layer conv0
I0428 19:57:29.595886 28693 net.cpp:408] conv0 <- data
I0428 19:57:29.595901 28693 net.cpp:382] conv0 -> conv0
I0428 19:57:29.887120 28693 net.cpp:124] Setting up conv0
I0428 19:57:29.887151 28693 net.cpp:131] Top shape: 64 10 24 24 (368640)
I0428 19:57:29.887156 28693 net.cpp:139] Memory required for data: 1675520
I0428 19:57:29.887195 28693 layer_factory.hpp:77] Creating layer pool0
I0428 19:57:29.887209 28693 net.cpp:86] Creating Layer pool0
I0428 19:57:29.887214 28693 net.cpp:408] pool0 <- conv0
I0428 19:57:29.887220 28693 net.cpp:382] pool0 -> pool0
I0428 19:57:29.887279 28693 net.cpp:124] Setting up pool0
I0428 19:57:29.887289 28693 net.cpp:131] Top shape: 64 10 12 12 (92160)
I0428 19:57:29.887291 28693 net.cpp:139] Memory required for data: 2044160
I0428 19:57:29.887295 28693 layer_factory.hpp:77] Creating layer conv1
I0428 19:57:29.887308 28693 net.cpp:86] Creating Layer conv1
I0428 19:57:29.887312 28693 net.cpp:408] conv1 <- pool0
I0428 19:57:29.887318 28693 net.cpp:382] conv1 -> conv1
I0428 19:57:29.890504 28693 net.cpp:124] Setting up conv1
I0428 19:57:29.890522 28693 net.cpp:131] Top shape: 64 5 8 8 (20480)
I0428 19:57:29.890527 28693 net.cpp:139] Memory required for data: 2126080
I0428 19:57:29.890537 28693 layer_factory.hpp:77] Creating layer pool1
I0428 19:57:29.890545 28693 net.cpp:86] Creating Layer pool1
I0428 19:57:29.890550 28693 net.cpp:408] pool1 <- conv1
I0428 19:57:29.890558 28693 net.cpp:382] pool1 -> pool1
I0428 19:57:29.890604 28693 net.cpp:124] Setting up pool1
I0428 19:57:29.890610 28693 net.cpp:131] Top shape: 64 5 4 4 (5120)
I0428 19:57:29.890614 28693 net.cpp:139] Memory required for data: 2146560
I0428 19:57:29.890617 28693 layer_factory.hpp:77] Creating layer ip1
I0428 19:57:29.890627 28693 net.cpp:86] Creating Layer ip1
I0428 19:57:29.890631 28693 net.cpp:408] ip1 <- pool1
I0428 19:57:29.890637 28693 net.cpp:382] ip1 -> ip1
I0428 19:57:29.890785 28693 net.cpp:124] Setting up ip1
I0428 19:57:29.890795 28693 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:57:29.890799 28693 net.cpp:139] Memory required for data: 2159360
I0428 19:57:29.890806 28693 layer_factory.hpp:77] Creating layer relu1
I0428 19:57:29.890813 28693 net.cpp:86] Creating Layer relu1
I0428 19:57:29.890817 28693 net.cpp:408] relu1 <- ip1
I0428 19:57:29.890822 28693 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:57:29.891039 28693 net.cpp:124] Setting up relu1
I0428 19:57:29.891050 28693 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:57:29.891054 28693 net.cpp:139] Memory required for data: 2172160
I0428 19:57:29.891058 28693 layer_factory.hpp:77] Creating layer ip2
I0428 19:57:29.891069 28693 net.cpp:86] Creating Layer ip2
I0428 19:57:29.891072 28693 net.cpp:408] ip2 <- ip1
I0428 19:57:29.891078 28693 net.cpp:382] ip2 -> ip2
I0428 19:57:29.891203 28693 net.cpp:124] Setting up ip2
I0428 19:57:29.891212 28693 net.cpp:131] Top shape: 64 10 (640)
I0428 19:57:29.891216 28693 net.cpp:139] Memory required for data: 2174720
I0428 19:57:29.891222 28693 layer_factory.hpp:77] Creating layer relu2
I0428 19:57:29.891229 28693 net.cpp:86] Creating Layer relu2
I0428 19:57:29.891233 28693 net.cpp:408] relu2 <- ip2
I0428 19:57:29.891239 28693 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:57:29.892094 28693 net.cpp:124] Setting up relu2
I0428 19:57:29.892108 28693 net.cpp:131] Top shape: 64 10 (640)
I0428 19:57:29.892112 28693 net.cpp:139] Memory required for data: 2177280
I0428 19:57:29.892117 28693 layer_factory.hpp:77] Creating layer loss
I0428 19:57:29.892125 28693 net.cpp:86] Creating Layer loss
I0428 19:57:29.892129 28693 net.cpp:408] loss <- ip2
I0428 19:57:29.892134 28693 net.cpp:408] loss <- label
I0428 19:57:29.892141 28693 net.cpp:382] loss -> loss
I0428 19:57:29.892159 28693 layer_factory.hpp:77] Creating layer loss
I0428 19:57:29.892459 28693 net.cpp:124] Setting up loss
I0428 19:57:29.892470 28693 net.cpp:131] Top shape: (1)
I0428 19:57:29.892474 28693 net.cpp:134]     with loss weight 1
I0428 19:57:29.892491 28693 net.cpp:139] Memory required for data: 2177284
I0428 19:57:29.892495 28693 net.cpp:200] loss needs backward computation.
I0428 19:57:29.892499 28693 net.cpp:200] relu2 needs backward computation.
I0428 19:57:29.892503 28693 net.cpp:200] ip2 needs backward computation.
I0428 19:57:29.892506 28693 net.cpp:200] relu1 needs backward computation.
I0428 19:57:29.892509 28693 net.cpp:200] ip1 needs backward computation.
I0428 19:57:29.892524 28693 net.cpp:200] pool1 needs backward computation.
I0428 19:57:29.892529 28693 net.cpp:200] conv1 needs backward computation.
I0428 19:57:29.892532 28693 net.cpp:200] pool0 needs backward computation.
I0428 19:57:29.892535 28693 net.cpp:200] conv0 needs backward computation.
I0428 19:57:29.892539 28693 net.cpp:202] mnist does not need backward computation.
I0428 19:57:29.892542 28693 net.cpp:244] This network produces output loss
I0428 19:57:29.892554 28693 net.cpp:257] Network initialization done.
I0428 19:57:29.892907 28693 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test790.prototxt
I0428 19:57:29.892938 28693 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:57:29.893039 28693 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0428 19:57:29.893117 28693 layer_factory.hpp:77] Creating layer mnist
I0428 19:57:29.893169 28693 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:57:29.893183 28693 net.cpp:86] Creating Layer mnist
I0428 19:57:29.893191 28693 net.cpp:382] mnist -> data
I0428 19:57:29.893200 28693 net.cpp:382] mnist -> label
I0428 19:57:29.893308 28693 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:57:29.895714 28693 net.cpp:124] Setting up mnist
I0428 19:57:29.895728 28693 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:57:29.895735 28693 net.cpp:131] Top shape: 100 (100)
I0428 19:57:29.895738 28693 net.cpp:139] Memory required for data: 314000
I0428 19:57:29.895742 28693 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:57:29.895750 28693 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:57:29.895754 28693 net.cpp:408] label_mnist_1_split <- label
I0428 19:57:29.895761 28693 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:57:29.895769 28693 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:57:29.895855 28693 net.cpp:124] Setting up label_mnist_1_split
I0428 19:57:29.895879 28693 net.cpp:131] Top shape: 100 (100)
I0428 19:57:29.895884 28693 net.cpp:131] Top shape: 100 (100)
I0428 19:57:29.895886 28693 net.cpp:139] Memory required for data: 314800
I0428 19:57:29.895890 28693 layer_factory.hpp:77] Creating layer conv0
I0428 19:57:29.895903 28693 net.cpp:86] Creating Layer conv0
I0428 19:57:29.895906 28693 net.cpp:408] conv0 <- data
I0428 19:57:29.895912 28693 net.cpp:382] conv0 -> conv0
I0428 19:57:29.897804 28693 net.cpp:124] Setting up conv0
I0428 19:57:29.897821 28693 net.cpp:131] Top shape: 100 10 24 24 (576000)
I0428 19:57:29.897827 28693 net.cpp:139] Memory required for data: 2618800
I0428 19:57:29.897838 28693 layer_factory.hpp:77] Creating layer pool0
I0428 19:57:29.897846 28693 net.cpp:86] Creating Layer pool0
I0428 19:57:29.897850 28693 net.cpp:408] pool0 <- conv0
I0428 19:57:29.897857 28693 net.cpp:382] pool0 -> pool0
I0428 19:57:29.897902 28693 net.cpp:124] Setting up pool0
I0428 19:57:29.897912 28693 net.cpp:131] Top shape: 100 10 12 12 (144000)
I0428 19:57:29.897915 28693 net.cpp:139] Memory required for data: 3194800
I0428 19:57:29.897918 28693 layer_factory.hpp:77] Creating layer conv1
I0428 19:57:29.897930 28693 net.cpp:86] Creating Layer conv1
I0428 19:57:29.897934 28693 net.cpp:408] conv1 <- pool0
I0428 19:57:29.897939 28693 net.cpp:382] conv1 -> conv1
I0428 19:57:29.900171 28693 net.cpp:124] Setting up conv1
I0428 19:57:29.900187 28693 net.cpp:131] Top shape: 100 5 8 8 (32000)
I0428 19:57:29.900200 28693 net.cpp:139] Memory required for data: 3322800
I0428 19:57:29.900212 28693 layer_factory.hpp:77] Creating layer pool1
I0428 19:57:29.900218 28693 net.cpp:86] Creating Layer pool1
I0428 19:57:29.900223 28693 net.cpp:408] pool1 <- conv1
I0428 19:57:29.900228 28693 net.cpp:382] pool1 -> pool1
I0428 19:57:29.900281 28693 net.cpp:124] Setting up pool1
I0428 19:57:29.900290 28693 net.cpp:131] Top shape: 100 5 4 4 (8000)
I0428 19:57:29.900293 28693 net.cpp:139] Memory required for data: 3354800
I0428 19:57:29.900297 28693 layer_factory.hpp:77] Creating layer ip1
I0428 19:57:29.900305 28693 net.cpp:86] Creating Layer ip1
I0428 19:57:29.900310 28693 net.cpp:408] ip1 <- pool1
I0428 19:57:29.900315 28693 net.cpp:382] ip1 -> ip1
I0428 19:57:29.900463 28693 net.cpp:124] Setting up ip1
I0428 19:57:29.900472 28693 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:57:29.900476 28693 net.cpp:139] Memory required for data: 3374800
I0428 19:57:29.900485 28693 layer_factory.hpp:77] Creating layer relu1
I0428 19:57:29.900492 28693 net.cpp:86] Creating Layer relu1
I0428 19:57:29.900496 28693 net.cpp:408] relu1 <- ip1
I0428 19:57:29.900501 28693 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:57:29.900704 28693 net.cpp:124] Setting up relu1
I0428 19:57:29.900717 28693 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:57:29.900720 28693 net.cpp:139] Memory required for data: 3394800
I0428 19:57:29.900723 28693 layer_factory.hpp:77] Creating layer ip2
I0428 19:57:29.900732 28693 net.cpp:86] Creating Layer ip2
I0428 19:57:29.900737 28693 net.cpp:408] ip2 <- ip1
I0428 19:57:29.900743 28693 net.cpp:382] ip2 -> ip2
I0428 19:57:29.900879 28693 net.cpp:124] Setting up ip2
I0428 19:57:29.900895 28693 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:57:29.900898 28693 net.cpp:139] Memory required for data: 3398800
I0428 19:57:29.900905 28693 layer_factory.hpp:77] Creating layer relu2
I0428 19:57:29.900920 28693 net.cpp:86] Creating Layer relu2
I0428 19:57:29.900924 28693 net.cpp:408] relu2 <- ip2
I0428 19:57:29.900929 28693 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:57:29.901140 28693 net.cpp:124] Setting up relu2
I0428 19:57:29.901150 28693 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:57:29.901154 28693 net.cpp:139] Memory required for data: 3402800
I0428 19:57:29.901157 28693 layer_factory.hpp:77] Creating layer ip2_relu2_0_split
I0428 19:57:29.901165 28693 net.cpp:86] Creating Layer ip2_relu2_0_split
I0428 19:57:29.901170 28693 net.cpp:408] ip2_relu2_0_split <- ip2
I0428 19:57:29.901175 28693 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_0
I0428 19:57:29.901195 28693 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_1
I0428 19:57:29.901278 28693 net.cpp:124] Setting up ip2_relu2_0_split
I0428 19:57:29.901286 28693 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:57:29.901290 28693 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:57:29.901294 28693 net.cpp:139] Memory required for data: 3410800
I0428 19:57:29.901298 28693 layer_factory.hpp:77] Creating layer accuracy
I0428 19:57:29.901305 28693 net.cpp:86] Creating Layer accuracy
I0428 19:57:29.901309 28693 net.cpp:408] accuracy <- ip2_relu2_0_split_0
I0428 19:57:29.901314 28693 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:57:29.901319 28693 net.cpp:382] accuracy -> accuracy
I0428 19:57:29.901327 28693 net.cpp:124] Setting up accuracy
I0428 19:57:29.901332 28693 net.cpp:131] Top shape: (1)
I0428 19:57:29.901336 28693 net.cpp:139] Memory required for data: 3410804
I0428 19:57:29.901340 28693 layer_factory.hpp:77] Creating layer loss
I0428 19:57:29.901346 28693 net.cpp:86] Creating Layer loss
I0428 19:57:29.901350 28693 net.cpp:408] loss <- ip2_relu2_0_split_1
I0428 19:57:29.901355 28693 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:57:29.901360 28693 net.cpp:382] loss -> loss
I0428 19:57:29.901367 28693 layer_factory.hpp:77] Creating layer loss
I0428 19:57:29.901648 28693 net.cpp:124] Setting up loss
I0428 19:57:29.901659 28693 net.cpp:131] Top shape: (1)
I0428 19:57:29.901661 28693 net.cpp:134]     with loss weight 1
I0428 19:57:29.901669 28693 net.cpp:139] Memory required for data: 3410808
I0428 19:57:29.901672 28693 net.cpp:200] loss needs backward computation.
I0428 19:57:29.901677 28693 net.cpp:202] accuracy does not need backward computation.
I0428 19:57:29.901681 28693 net.cpp:200] ip2_relu2_0_split needs backward computation.
I0428 19:57:29.901685 28693 net.cpp:200] relu2 needs backward computation.
I0428 19:57:29.901688 28693 net.cpp:200] ip2 needs backward computation.
I0428 19:57:29.901692 28693 net.cpp:200] relu1 needs backward computation.
I0428 19:57:29.901696 28693 net.cpp:200] ip1 needs backward computation.
I0428 19:57:29.901700 28693 net.cpp:200] pool1 needs backward computation.
I0428 19:57:29.901710 28693 net.cpp:200] conv1 needs backward computation.
I0428 19:57:29.901720 28693 net.cpp:200] pool0 needs backward computation.
I0428 19:57:29.901723 28693 net.cpp:200] conv0 needs backward computation.
I0428 19:57:29.901727 28693 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:57:29.901731 28693 net.cpp:202] mnist does not need backward computation.
I0428 19:57:29.901736 28693 net.cpp:244] This network produces output accuracy
I0428 19:57:29.901738 28693 net.cpp:244] This network produces output loss
I0428 19:57:29.901752 28693 net.cpp:257] Network initialization done.
I0428 19:57:29.901803 28693 solver.cpp:56] Solver scaffolding done.
I0428 19:57:29.902119 28693 caffe.cpp:248] Starting Optimization
I0428 19:57:29.902127 28693 solver.cpp:273] Solving LeNet
I0428 19:57:29.902129 28693 solver.cpp:274] Learning Rate Policy: inv
I0428 19:57:29.903059 28693 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:57:29.906697 28693 blocking_queue.cpp:49] Waiting for data
I0428 19:57:29.976855 28700 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:57:29.977366 28693 solver.cpp:398]     Test net output #0: accuracy = 0.0553
I0428 19:57:29.977386 28693 solver.cpp:398]     Test net output #1: loss = 2.32823 (* 1 = 2.32823 loss)
I0428 19:57:29.979782 28693 solver.cpp:219] Iteration 0 (0 iter/s, 0.0776212s/100 iters), loss = 2.32935
I0428 19:57:29.979807 28693 solver.cpp:238]     Train net output #0: loss = 2.32935 (* 1 = 2.32935 loss)
I0428 19:57:29.979820 28693 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:57:30.047741 28693 solver.cpp:219] Iteration 100 (1472.26 iter/s, 0.067923s/100 iters), loss = 1.13627
I0428 19:57:30.047765 28693 solver.cpp:238]     Train net output #0: loss = 1.13627 (* 1 = 1.13627 loss)
I0428 19:57:30.047770 28693 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:57:30.115546 28693 solver.cpp:219] Iteration 200 (1475.53 iter/s, 0.0677724s/100 iters), loss = 0.621519
I0428 19:57:30.115597 28693 solver.cpp:238]     Train net output #0: loss = 0.621519 (* 1 = 0.621519 loss)
I0428 19:57:30.115603 28693 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:57:30.189045 28693 solver.cpp:219] Iteration 300 (1361.46 iter/s, 0.0734503s/100 iters), loss = 0.700596
I0428 19:57:30.189075 28693 solver.cpp:238]     Train net output #0: loss = 0.700596 (* 1 = 0.700596 loss)
I0428 19:57:30.189081 28693 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:57:30.263988 28693 solver.cpp:219] Iteration 400 (1335.01 iter/s, 0.0749056s/100 iters), loss = 0.95185
I0428 19:57:30.264027 28693 solver.cpp:238]     Train net output #0: loss = 0.95185 (* 1 = 0.95185 loss)
I0428 19:57:30.264034 28693 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:57:30.331388 28693 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:57:30.384779 28700 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:57:30.385298 28693 solver.cpp:398]     Test net output #0: accuracy = 0.7651
I0428 19:57:30.385334 28693 solver.cpp:398]     Test net output #1: loss = 0.634089 (* 1 = 0.634089 loss)
I0428 19:57:30.386109 28693 solver.cpp:219] Iteration 500 (819.099 iter/s, 0.122085s/100 iters), loss = 0.801708
I0428 19:57:30.386133 28693 solver.cpp:238]     Train net output #0: loss = 0.801708 (* 1 = 0.801708 loss)
I0428 19:57:30.386142 28693 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:57:30.471920 28693 solver.cpp:219] Iteration 600 (1165.82 iter/s, 0.0857764s/100 iters), loss = 0.543635
I0428 19:57:30.471962 28693 solver.cpp:238]     Train net output #0: loss = 0.543635 (* 1 = 0.543635 loss)
I0428 19:57:30.471968 28693 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:57:30.546051 28693 solver.cpp:219] Iteration 700 (1349.89 iter/s, 0.0740804s/100 iters), loss = 0.6423
I0428 19:57:30.546092 28693 solver.cpp:238]     Train net output #0: loss = 0.6423 (* 1 = 0.6423 loss)
I0428 19:57:30.546097 28693 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:57:30.614214 28693 solver.cpp:219] Iteration 800 (1467.78 iter/s, 0.0681299s/100 iters), loss = 0.656616
I0428 19:57:30.614253 28693 solver.cpp:238]     Train net output #0: loss = 0.656616 (* 1 = 0.656616 loss)
I0428 19:57:30.614259 28693 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:57:30.682095 28693 solver.cpp:219] Iteration 900 (1473.88 iter/s, 0.0678483s/100 iters), loss = 0.436356
I0428 19:57:30.682116 28693 solver.cpp:238]     Train net output #0: loss = 0.436356 (* 1 = 0.436356 loss)
I0428 19:57:30.682138 28693 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:57:30.705090 28699 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:57:30.749691 28693 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:57:30.750422 28693 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:57:30.750913 28693 solver.cpp:311] Iteration 1000, loss = 0.227381
I0428 19:57:30.750928 28693 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:57:30.826742 28700 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:57:30.827260 28693 solver.cpp:398]     Test net output #0: accuracy = 0.8686
I0428 19:57:30.827292 28693 solver.cpp:398]     Test net output #1: loss = 0.35903 (* 1 = 0.35903 loss)
I0428 19:57:30.827297 28693 solver.cpp:316] Optimization Done.
I0428 19:57:30.827301 28693 caffe.cpp:259] Optimization Done.
