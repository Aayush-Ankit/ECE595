I0428 20:35:00.631935  4561 caffe.cpp:218] Using GPUs 0
I0428 20:35:00.664988  4561 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:35:01.174269  4561 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1614.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:35:01.174417  4561 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1614.prototxt
I0428 20:35:01.174837  4561 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:35:01.174857  4561 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:35:01.174959  4561 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:35:01.175042  4561 layer_factory.hpp:77] Creating layer mnist
I0428 20:35:01.175143  4561 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:35:01.175168  4561 net.cpp:86] Creating Layer mnist
I0428 20:35:01.175178  4561 net.cpp:382] mnist -> data
I0428 20:35:01.175199  4561 net.cpp:382] mnist -> label
I0428 20:35:01.176296  4561 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:35:01.178764  4561 net.cpp:124] Setting up mnist
I0428 20:35:01.178782  4561 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:35:01.178788  4561 net.cpp:131] Top shape: 64 (64)
I0428 20:35:01.178792  4561 net.cpp:139] Memory required for data: 200960
I0428 20:35:01.178799  4561 layer_factory.hpp:77] Creating layer conv0
I0428 20:35:01.178828  4561 net.cpp:86] Creating Layer conv0
I0428 20:35:01.178848  4561 net.cpp:408] conv0 <- data
I0428 20:35:01.178864  4561 net.cpp:382] conv0 -> conv0
I0428 20:35:01.431537  4561 net.cpp:124] Setting up conv0
I0428 20:35:01.431565  4561 net.cpp:131] Top shape: 64 100 24 24 (3686400)
I0428 20:35:01.431568  4561 net.cpp:139] Memory required for data: 14946560
I0428 20:35:01.431582  4561 layer_factory.hpp:77] Creating layer pool0
I0428 20:35:01.431593  4561 net.cpp:86] Creating Layer pool0
I0428 20:35:01.431597  4561 net.cpp:408] pool0 <- conv0
I0428 20:35:01.431602  4561 net.cpp:382] pool0 -> pool0
I0428 20:35:01.431661  4561 net.cpp:124] Setting up pool0
I0428 20:35:01.431666  4561 net.cpp:131] Top shape: 64 100 12 12 (921600)
I0428 20:35:01.431669  4561 net.cpp:139] Memory required for data: 18632960
I0428 20:35:01.431673  4561 layer_factory.hpp:77] Creating layer conv1
I0428 20:35:01.431682  4561 net.cpp:86] Creating Layer conv1
I0428 20:35:01.431685  4561 net.cpp:408] conv1 <- pool0
I0428 20:35:01.431689  4561 net.cpp:382] conv1 -> conv1
I0428 20:35:01.435135  4561 net.cpp:124] Setting up conv1
I0428 20:35:01.435165  4561 net.cpp:131] Top shape: 64 50 8 8 (204800)
I0428 20:35:01.435169  4561 net.cpp:139] Memory required for data: 19452160
I0428 20:35:01.435178  4561 layer_factory.hpp:77] Creating layer pool1
I0428 20:35:01.435184  4561 net.cpp:86] Creating Layer pool1
I0428 20:35:01.435189  4561 net.cpp:408] pool1 <- conv1
I0428 20:35:01.435192  4561 net.cpp:382] pool1 -> pool1
I0428 20:35:01.435243  4561 net.cpp:124] Setting up pool1
I0428 20:35:01.435248  4561 net.cpp:131] Top shape: 64 50 4 4 (51200)
I0428 20:35:01.435251  4561 net.cpp:139] Memory required for data: 19656960
I0428 20:35:01.435255  4561 layer_factory.hpp:77] Creating layer ip1
I0428 20:35:01.435261  4561 net.cpp:86] Creating Layer ip1
I0428 20:35:01.435264  4561 net.cpp:408] ip1 <- pool1
I0428 20:35:01.435268  4561 net.cpp:382] ip1 -> ip1
I0428 20:35:01.436292  4561 net.cpp:124] Setting up ip1
I0428 20:35:01.436305  4561 net.cpp:131] Top shape: 64 10 (640)
I0428 20:35:01.436324  4561 net.cpp:139] Memory required for data: 19659520
I0428 20:35:01.436332  4561 layer_factory.hpp:77] Creating layer relu1
I0428 20:35:01.436339  4561 net.cpp:86] Creating Layer relu1
I0428 20:35:01.436343  4561 net.cpp:408] relu1 <- ip1
I0428 20:35:01.436347  4561 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:35:01.436533  4561 net.cpp:124] Setting up relu1
I0428 20:35:01.436542  4561 net.cpp:131] Top shape: 64 10 (640)
I0428 20:35:01.436547  4561 net.cpp:139] Memory required for data: 19662080
I0428 20:35:01.436564  4561 layer_factory.hpp:77] Creating layer ip2
I0428 20:35:01.436570  4561 net.cpp:86] Creating Layer ip2
I0428 20:35:01.436573  4561 net.cpp:408] ip2 <- ip1
I0428 20:35:01.436578  4561 net.cpp:382] ip2 -> ip2
I0428 20:35:01.436678  4561 net.cpp:124] Setting up ip2
I0428 20:35:01.436686  4561 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:35:01.436688  4561 net.cpp:139] Memory required for data: 19668480
I0428 20:35:01.436694  4561 layer_factory.hpp:77] Creating layer relu2
I0428 20:35:01.436700  4561 net.cpp:86] Creating Layer relu2
I0428 20:35:01.436703  4561 net.cpp:408] relu2 <- ip2
I0428 20:35:01.436707  4561 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:35:01.437602  4561 net.cpp:124] Setting up relu2
I0428 20:35:01.437630  4561 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:35:01.437634  4561 net.cpp:139] Memory required for data: 19674880
I0428 20:35:01.437638  4561 layer_factory.hpp:77] Creating layer ip3
I0428 20:35:01.437644  4561 net.cpp:86] Creating Layer ip3
I0428 20:35:01.437647  4561 net.cpp:408] ip3 <- ip2
I0428 20:35:01.437654  4561 net.cpp:382] ip3 -> ip3
I0428 20:35:01.437757  4561 net.cpp:124] Setting up ip3
I0428 20:35:01.437764  4561 net.cpp:131] Top shape: 64 10 (640)
I0428 20:35:01.437767  4561 net.cpp:139] Memory required for data: 19677440
I0428 20:35:01.437775  4561 layer_factory.hpp:77] Creating layer relu3
I0428 20:35:01.437780  4561 net.cpp:86] Creating Layer relu3
I0428 20:35:01.437783  4561 net.cpp:408] relu3 <- ip3
I0428 20:35:01.437788  4561 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:35:01.437980  4561 net.cpp:124] Setting up relu3
I0428 20:35:01.437990  4561 net.cpp:131] Top shape: 64 10 (640)
I0428 20:35:01.437994  4561 net.cpp:139] Memory required for data: 19680000
I0428 20:35:01.437997  4561 layer_factory.hpp:77] Creating layer loss
I0428 20:35:01.438004  4561 net.cpp:86] Creating Layer loss
I0428 20:35:01.438006  4561 net.cpp:408] loss <- ip3
I0428 20:35:01.438011  4561 net.cpp:408] loss <- label
I0428 20:35:01.438016  4561 net.cpp:382] loss -> loss
I0428 20:35:01.438030  4561 layer_factory.hpp:77] Creating layer loss
I0428 20:35:01.438259  4561 net.cpp:124] Setting up loss
I0428 20:35:01.438268  4561 net.cpp:131] Top shape: (1)
I0428 20:35:01.438272  4561 net.cpp:134]     with loss weight 1
I0428 20:35:01.438287  4561 net.cpp:139] Memory required for data: 19680004
I0428 20:35:01.438292  4561 net.cpp:200] loss needs backward computation.
I0428 20:35:01.438294  4561 net.cpp:200] relu3 needs backward computation.
I0428 20:35:01.438297  4561 net.cpp:200] ip3 needs backward computation.
I0428 20:35:01.438300  4561 net.cpp:200] relu2 needs backward computation.
I0428 20:35:01.438303  4561 net.cpp:200] ip2 needs backward computation.
I0428 20:35:01.438307  4561 net.cpp:200] relu1 needs backward computation.
I0428 20:35:01.438308  4561 net.cpp:200] ip1 needs backward computation.
I0428 20:35:01.438311  4561 net.cpp:200] pool1 needs backward computation.
I0428 20:35:01.438314  4561 net.cpp:200] conv1 needs backward computation.
I0428 20:35:01.438318  4561 net.cpp:200] pool0 needs backward computation.
I0428 20:35:01.438320  4561 net.cpp:200] conv0 needs backward computation.
I0428 20:35:01.438324  4561 net.cpp:202] mnist does not need backward computation.
I0428 20:35:01.438326  4561 net.cpp:244] This network produces output loss
I0428 20:35:01.438350  4561 net.cpp:257] Network initialization done.
I0428 20:35:01.438766  4561 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1614.prototxt
I0428 20:35:01.438807  4561 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:35:01.438908  4561 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:35:01.439003  4561 layer_factory.hpp:77] Creating layer mnist
I0428 20:35:01.439049  4561 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:35:01.439061  4561 net.cpp:86] Creating Layer mnist
I0428 20:35:01.439066  4561 net.cpp:382] mnist -> data
I0428 20:35:01.439074  4561 net.cpp:382] mnist -> label
I0428 20:35:01.439169  4561 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:35:01.440640  4561 net.cpp:124] Setting up mnist
I0428 20:35:01.440670  4561 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:35:01.440690  4561 net.cpp:131] Top shape: 100 (100)
I0428 20:35:01.440692  4561 net.cpp:139] Memory required for data: 314000
I0428 20:35:01.440696  4561 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:35:01.440703  4561 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:35:01.440706  4561 net.cpp:408] label_mnist_1_split <- label
I0428 20:35:01.440711  4561 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:35:01.440718  4561 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:35:01.440760  4561 net.cpp:124] Setting up label_mnist_1_split
I0428 20:35:01.440767  4561 net.cpp:131] Top shape: 100 (100)
I0428 20:35:01.440770  4561 net.cpp:131] Top shape: 100 (100)
I0428 20:35:01.440773  4561 net.cpp:139] Memory required for data: 314800
I0428 20:35:01.440775  4561 layer_factory.hpp:77] Creating layer conv0
I0428 20:35:01.440783  4561 net.cpp:86] Creating Layer conv0
I0428 20:35:01.440788  4561 net.cpp:408] conv0 <- data
I0428 20:35:01.440793  4561 net.cpp:382] conv0 -> conv0
I0428 20:35:01.442662  4561 net.cpp:124] Setting up conv0
I0428 20:35:01.442690  4561 net.cpp:131] Top shape: 100 100 24 24 (5760000)
I0428 20:35:01.442694  4561 net.cpp:139] Memory required for data: 23354800
I0428 20:35:01.442703  4561 layer_factory.hpp:77] Creating layer pool0
I0428 20:35:01.442709  4561 net.cpp:86] Creating Layer pool0
I0428 20:35:01.442713  4561 net.cpp:408] pool0 <- conv0
I0428 20:35:01.442718  4561 net.cpp:382] pool0 -> pool0
I0428 20:35:01.442752  4561 net.cpp:124] Setting up pool0
I0428 20:35:01.442759  4561 net.cpp:131] Top shape: 100 100 12 12 (1440000)
I0428 20:35:01.442761  4561 net.cpp:139] Memory required for data: 29114800
I0428 20:35:01.442764  4561 layer_factory.hpp:77] Creating layer conv1
I0428 20:35:01.442772  4561 net.cpp:86] Creating Layer conv1
I0428 20:35:01.442775  4561 net.cpp:408] conv1 <- pool0
I0428 20:35:01.442780  4561 net.cpp:382] conv1 -> conv1
I0428 20:35:01.446081  4561 net.cpp:124] Setting up conv1
I0428 20:35:01.446110  4561 net.cpp:131] Top shape: 100 50 8 8 (320000)
I0428 20:35:01.446115  4561 net.cpp:139] Memory required for data: 30394800
I0428 20:35:01.446122  4561 layer_factory.hpp:77] Creating layer pool1
I0428 20:35:01.446131  4561 net.cpp:86] Creating Layer pool1
I0428 20:35:01.446135  4561 net.cpp:408] pool1 <- conv1
I0428 20:35:01.446140  4561 net.cpp:382] pool1 -> pool1
I0428 20:35:01.446190  4561 net.cpp:124] Setting up pool1
I0428 20:35:01.446197  4561 net.cpp:131] Top shape: 100 50 4 4 (80000)
I0428 20:35:01.446199  4561 net.cpp:139] Memory required for data: 30714800
I0428 20:35:01.446202  4561 layer_factory.hpp:77] Creating layer ip1
I0428 20:35:01.446225  4561 net.cpp:86] Creating Layer ip1
I0428 20:35:01.446228  4561 net.cpp:408] ip1 <- pool1
I0428 20:35:01.446233  4561 net.cpp:382] ip1 -> ip1
I0428 20:35:01.446367  4561 net.cpp:124] Setting up ip1
I0428 20:35:01.446385  4561 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:35:01.446389  4561 net.cpp:139] Memory required for data: 30718800
I0428 20:35:01.446396  4561 layer_factory.hpp:77] Creating layer relu1
I0428 20:35:01.446401  4561 net.cpp:86] Creating Layer relu1
I0428 20:35:01.446405  4561 net.cpp:408] relu1 <- ip1
I0428 20:35:01.446409  4561 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:35:01.446579  4561 net.cpp:124] Setting up relu1
I0428 20:35:01.446588  4561 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:35:01.446593  4561 net.cpp:139] Memory required for data: 30722800
I0428 20:35:01.446595  4561 layer_factory.hpp:77] Creating layer ip2
I0428 20:35:01.446602  4561 net.cpp:86] Creating Layer ip2
I0428 20:35:01.446606  4561 net.cpp:408] ip2 <- ip1
I0428 20:35:01.446611  4561 net.cpp:382] ip2 -> ip2
I0428 20:35:01.446705  4561 net.cpp:124] Setting up ip2
I0428 20:35:01.446712  4561 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:35:01.446715  4561 net.cpp:139] Memory required for data: 30732800
I0428 20:35:01.446720  4561 layer_factory.hpp:77] Creating layer relu2
I0428 20:35:01.446725  4561 net.cpp:86] Creating Layer relu2
I0428 20:35:01.446728  4561 net.cpp:408] relu2 <- ip2
I0428 20:35:01.446732  4561 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:35:01.446928  4561 net.cpp:124] Setting up relu2
I0428 20:35:01.446935  4561 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:35:01.446939  4561 net.cpp:139] Memory required for data: 30742800
I0428 20:35:01.446943  4561 layer_factory.hpp:77] Creating layer ip3
I0428 20:35:01.446948  4561 net.cpp:86] Creating Layer ip3
I0428 20:35:01.446951  4561 net.cpp:408] ip3 <- ip2
I0428 20:35:01.446956  4561 net.cpp:382] ip3 -> ip3
I0428 20:35:01.447088  4561 net.cpp:124] Setting up ip3
I0428 20:35:01.447095  4561 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:35:01.447098  4561 net.cpp:139] Memory required for data: 30746800
I0428 20:35:01.447106  4561 layer_factory.hpp:77] Creating layer relu3
I0428 20:35:01.447111  4561 net.cpp:86] Creating Layer relu3
I0428 20:35:01.447130  4561 net.cpp:408] relu3 <- ip3
I0428 20:35:01.447134  4561 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:35:01.448016  4561 net.cpp:124] Setting up relu3
I0428 20:35:01.448029  4561 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:35:01.448048  4561 net.cpp:139] Memory required for data: 30750800
I0428 20:35:01.448051  4561 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:35:01.448057  4561 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:35:01.448061  4561 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:35:01.448067  4561 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:35:01.448073  4561 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:35:01.448109  4561 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:35:01.448115  4561 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:35:01.448119  4561 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:35:01.448122  4561 net.cpp:139] Memory required for data: 30758800
I0428 20:35:01.448124  4561 layer_factory.hpp:77] Creating layer accuracy
I0428 20:35:01.448130  4561 net.cpp:86] Creating Layer accuracy
I0428 20:35:01.448133  4561 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:35:01.448137  4561 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:35:01.448141  4561 net.cpp:382] accuracy -> accuracy
I0428 20:35:01.448149  4561 net.cpp:124] Setting up accuracy
I0428 20:35:01.448153  4561 net.cpp:131] Top shape: (1)
I0428 20:35:01.448156  4561 net.cpp:139] Memory required for data: 30758804
I0428 20:35:01.448159  4561 layer_factory.hpp:77] Creating layer loss
I0428 20:35:01.448164  4561 net.cpp:86] Creating Layer loss
I0428 20:35:01.448168  4561 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:35:01.448171  4561 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:35:01.448175  4561 net.cpp:382] loss -> loss
I0428 20:35:01.448189  4561 layer_factory.hpp:77] Creating layer loss
I0428 20:35:01.448478  4561 net.cpp:124] Setting up loss
I0428 20:35:01.448488  4561 net.cpp:131] Top shape: (1)
I0428 20:35:01.448493  4561 net.cpp:134]     with loss weight 1
I0428 20:35:01.448514  4561 net.cpp:139] Memory required for data: 30758808
I0428 20:35:01.448521  4561 net.cpp:200] loss needs backward computation.
I0428 20:35:01.448526  4561 net.cpp:202] accuracy does not need backward computation.
I0428 20:35:01.448531  4561 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:35:01.448535  4561 net.cpp:200] relu3 needs backward computation.
I0428 20:35:01.448539  4561 net.cpp:200] ip3 needs backward computation.
I0428 20:35:01.448541  4561 net.cpp:200] relu2 needs backward computation.
I0428 20:35:01.448545  4561 net.cpp:200] ip2 needs backward computation.
I0428 20:35:01.448549  4561 net.cpp:200] relu1 needs backward computation.
I0428 20:35:01.448551  4561 net.cpp:200] ip1 needs backward computation.
I0428 20:35:01.448555  4561 net.cpp:200] pool1 needs backward computation.
I0428 20:35:01.448559  4561 net.cpp:200] conv1 needs backward computation.
I0428 20:35:01.448562  4561 net.cpp:200] pool0 needs backward computation.
I0428 20:35:01.448565  4561 net.cpp:200] conv0 needs backward computation.
I0428 20:35:01.448570  4561 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:35:01.448575  4561 net.cpp:202] mnist does not need backward computation.
I0428 20:35:01.448583  4561 net.cpp:244] This network produces output accuracy
I0428 20:35:01.448586  4561 net.cpp:244] This network produces output loss
I0428 20:35:01.448598  4561 net.cpp:257] Network initialization done.
I0428 20:35:01.448643  4561 solver.cpp:56] Solver scaffolding done.
I0428 20:35:01.449120  4561 caffe.cpp:248] Starting Optimization
I0428 20:35:01.449126  4561 solver.cpp:273] Solving LeNet
I0428 20:35:01.449129  4561 solver.cpp:274] Learning Rate Policy: inv
I0428 20:35:01.449415  4561 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:35:01.560523  4568 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:35:01.563433  4561 solver.cpp:398]     Test net output #0: accuracy = 0.1003
I0428 20:35:01.563470  4561 solver.cpp:398]     Test net output #1: loss = 2.31951 (* 1 = 2.31951 loss)
I0428 20:35:01.568140  4561 solver.cpp:219] Iteration 0 (0 iter/s, 0.118951s/100 iters), loss = 2.3142
I0428 20:35:01.568181  4561 solver.cpp:238]     Train net output #0: loss = 2.3142 (* 1 = 2.3142 loss)
I0428 20:35:01.568193  4561 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:35:01.804589  4561 solver.cpp:219] Iteration 100 (423.024 iter/s, 0.236393s/100 iters), loss = 0.900055
I0428 20:35:01.804616  4561 solver.cpp:238]     Train net output #0: loss = 0.900055 (* 1 = 0.900055 loss)
I0428 20:35:01.804638  4561 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:35:02.035473  4561 solver.cpp:219] Iteration 200 (433.2 iter/s, 0.23084s/100 iters), loss = 0.459843
I0428 20:35:02.035501  4561 solver.cpp:238]     Train net output #0: loss = 0.459843 (* 1 = 0.459843 loss)
I0428 20:35:02.035508  4561 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:35:02.266687  4561 solver.cpp:219] Iteration 300 (432.582 iter/s, 0.23117s/100 iters), loss = 0.436526
I0428 20:35:02.266716  4561 solver.cpp:238]     Train net output #0: loss = 0.436526 (* 1 = 0.436526 loss)
I0428 20:35:02.266738  4561 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:35:02.499717  4561 solver.cpp:219] Iteration 400 (429.213 iter/s, 0.232985s/100 iters), loss = 0.168028
I0428 20:35:02.499743  4561 solver.cpp:238]     Train net output #0: loss = 0.168028 (* 1 = 0.168028 loss)
I0428 20:35:02.499750  4561 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:35:02.731853  4561 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:35:02.839606  4568 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:35:02.842617  4561 solver.cpp:398]     Test net output #0: accuracy = 0.9576
I0428 20:35:02.842656  4561 solver.cpp:398]     Test net output #1: loss = 0.142736 (* 1 = 0.142736 loss)
I0428 20:35:02.844921  4561 solver.cpp:219] Iteration 500 (289.723 iter/s, 0.345157s/100 iters), loss = 0.20336
I0428 20:35:02.844960  4561 solver.cpp:238]     Train net output #0: loss = 0.20336 (* 1 = 0.20336 loss)
I0428 20:35:02.844980  4561 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:35:03.101009  4561 solver.cpp:219] Iteration 600 (390.561 iter/s, 0.256042s/100 iters), loss = 0.12818
I0428 20:35:03.101059  4561 solver.cpp:238]     Train net output #0: loss = 0.12818 (* 1 = 0.12818 loss)
I0428 20:35:03.101073  4561 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:35:03.357570  4561 solver.cpp:219] Iteration 700 (389.873 iter/s, 0.256494s/100 iters), loss = 0.148629
I0428 20:35:03.357623  4561 solver.cpp:238]     Train net output #0: loss = 0.148629 (* 1 = 0.148629 loss)
I0428 20:35:03.357640  4561 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:35:03.615079  4561 solver.cpp:219] Iteration 800 (388.445 iter/s, 0.257437s/100 iters), loss = 0.275721
I0428 20:35:03.615139  4561 solver.cpp:238]     Train net output #0: loss = 0.275721 (* 1 = 0.275721 loss)
I0428 20:35:03.615152  4561 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:35:03.865772  4561 solver.cpp:219] Iteration 900 (399.012 iter/s, 0.250619s/100 iters), loss = 0.177475
I0428 20:35:03.865806  4561 solver.cpp:238]     Train net output #0: loss = 0.177475 (* 1 = 0.177475 loss)
I0428 20:35:03.865815  4561 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:35:03.945255  4567 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:35:04.103276  4561 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:35:04.107791  4561 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:35:04.109930  4561 solver.cpp:311] Iteration 1000, loss = 0.203483
I0428 20:35:04.109949  4561 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:35:04.226833  4568 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:35:04.230557  4561 solver.cpp:398]     Test net output #0: accuracy = 0.9677
I0428 20:35:04.230579  4561 solver.cpp:398]     Test net output #1: loss = 0.108123 (* 1 = 0.108123 loss)
I0428 20:35:04.230584  4561 solver.cpp:316] Optimization Done.
I0428 20:35:04.230588  4561 caffe.cpp:259] Optimization Done.
