I0428 20:07:48.792088 31236 caffe.cpp:218] Using GPUs 0
I0428 20:07:48.830001 31236 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:07:49.292059 31236 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1066.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:07:49.292223 31236 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1066.prototxt
I0428 20:07:49.292615 31236 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:07:49.292646 31236 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:07:49.292745 31236 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:07:49.292832 31236 layer_factory.hpp:77] Creating layer mnist
I0428 20:07:49.292917 31236 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:07:49.292937 31236 net.cpp:86] Creating Layer mnist
I0428 20:07:49.292944 31236 net.cpp:382] mnist -> data
I0428 20:07:49.292963 31236 net.cpp:382] mnist -> label
I0428 20:07:49.294051 31236 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:07:49.296245 31236 net.cpp:124] Setting up mnist
I0428 20:07:49.296277 31236 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:07:49.296283 31236 net.cpp:131] Top shape: 64 (64)
I0428 20:07:49.296286 31236 net.cpp:139] Memory required for data: 200960
I0428 20:07:49.296293 31236 layer_factory.hpp:77] Creating layer conv0
I0428 20:07:49.296306 31236 net.cpp:86] Creating Layer conv0
I0428 20:07:49.296324 31236 net.cpp:408] conv0 <- data
I0428 20:07:49.296334 31236 net.cpp:382] conv0 -> conv0
I0428 20:07:49.524214 31236 net.cpp:124] Setting up conv0
I0428 20:07:49.524271 31236 net.cpp:131] Top shape: 64 25 24 24 (921600)
I0428 20:07:49.524274 31236 net.cpp:139] Memory required for data: 3887360
I0428 20:07:49.524289 31236 layer_factory.hpp:77] Creating layer pool0
I0428 20:07:49.524302 31236 net.cpp:86] Creating Layer pool0
I0428 20:07:49.524305 31236 net.cpp:408] pool0 <- conv0
I0428 20:07:49.524310 31236 net.cpp:382] pool0 -> pool0
I0428 20:07:49.524369 31236 net.cpp:124] Setting up pool0
I0428 20:07:49.524374 31236 net.cpp:131] Top shape: 64 25 12 12 (230400)
I0428 20:07:49.524377 31236 net.cpp:139] Memory required for data: 4808960
I0428 20:07:49.524380 31236 layer_factory.hpp:77] Creating layer conv1
I0428 20:07:49.524391 31236 net.cpp:86] Creating Layer conv1
I0428 20:07:49.524394 31236 net.cpp:408] conv1 <- pool0
I0428 20:07:49.524399 31236 net.cpp:382] conv1 -> conv1
I0428 20:07:49.527222 31236 net.cpp:124] Setting up conv1
I0428 20:07:49.527251 31236 net.cpp:131] Top shape: 64 10 8 8 (40960)
I0428 20:07:49.527256 31236 net.cpp:139] Memory required for data: 4972800
I0428 20:07:49.527263 31236 layer_factory.hpp:77] Creating layer pool1
I0428 20:07:49.527271 31236 net.cpp:86] Creating Layer pool1
I0428 20:07:49.527274 31236 net.cpp:408] pool1 <- conv1
I0428 20:07:49.527278 31236 net.cpp:382] pool1 -> pool1
I0428 20:07:49.527329 31236 net.cpp:124] Setting up pool1
I0428 20:07:49.527334 31236 net.cpp:131] Top shape: 64 10 4 4 (10240)
I0428 20:07:49.527338 31236 net.cpp:139] Memory required for data: 5013760
I0428 20:07:49.527340 31236 layer_factory.hpp:77] Creating layer ip1
I0428 20:07:49.527346 31236 net.cpp:86] Creating Layer ip1
I0428 20:07:49.527349 31236 net.cpp:408] ip1 <- pool1
I0428 20:07:49.527354 31236 net.cpp:382] ip1 -> ip1
I0428 20:07:49.528403 31236 net.cpp:124] Setting up ip1
I0428 20:07:49.528415 31236 net.cpp:131] Top shape: 64 10 (640)
I0428 20:07:49.528420 31236 net.cpp:139] Memory required for data: 5016320
I0428 20:07:49.528429 31236 layer_factory.hpp:77] Creating layer relu1
I0428 20:07:49.528435 31236 net.cpp:86] Creating Layer relu1
I0428 20:07:49.528439 31236 net.cpp:408] relu1 <- ip1
I0428 20:07:49.528443 31236 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:07:49.528611 31236 net.cpp:124] Setting up relu1
I0428 20:07:49.528635 31236 net.cpp:131] Top shape: 64 10 (640)
I0428 20:07:49.528654 31236 net.cpp:139] Memory required for data: 5018880
I0428 20:07:49.528658 31236 layer_factory.hpp:77] Creating layer ip2
I0428 20:07:49.528664 31236 net.cpp:86] Creating Layer ip2
I0428 20:07:49.528668 31236 net.cpp:408] ip2 <- ip1
I0428 20:07:49.528673 31236 net.cpp:382] ip2 -> ip2
I0428 20:07:49.528786 31236 net.cpp:124] Setting up ip2
I0428 20:07:49.528795 31236 net.cpp:131] Top shape: 64 10 (640)
I0428 20:07:49.528797 31236 net.cpp:139] Memory required for data: 5021440
I0428 20:07:49.528803 31236 layer_factory.hpp:77] Creating layer relu2
I0428 20:07:49.528817 31236 net.cpp:86] Creating Layer relu2
I0428 20:07:49.528821 31236 net.cpp:408] relu2 <- ip2
I0428 20:07:49.528825 31236 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:07:49.529688 31236 net.cpp:124] Setting up relu2
I0428 20:07:49.529700 31236 net.cpp:131] Top shape: 64 10 (640)
I0428 20:07:49.529719 31236 net.cpp:139] Memory required for data: 5024000
I0428 20:07:49.529722 31236 layer_factory.hpp:77] Creating layer ip3
I0428 20:07:49.529729 31236 net.cpp:86] Creating Layer ip3
I0428 20:07:49.529734 31236 net.cpp:408] ip3 <- ip2
I0428 20:07:49.529741 31236 net.cpp:382] ip3 -> ip3
I0428 20:07:49.529850 31236 net.cpp:124] Setting up ip3
I0428 20:07:49.529857 31236 net.cpp:131] Top shape: 64 10 (640)
I0428 20:07:49.529860 31236 net.cpp:139] Memory required for data: 5026560
I0428 20:07:49.529868 31236 layer_factory.hpp:77] Creating layer relu3
I0428 20:07:49.529873 31236 net.cpp:86] Creating Layer relu3
I0428 20:07:49.529875 31236 net.cpp:408] relu3 <- ip3
I0428 20:07:49.529880 31236 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:07:49.530074 31236 net.cpp:124] Setting up relu3
I0428 20:07:49.530083 31236 net.cpp:131] Top shape: 64 10 (640)
I0428 20:07:49.530086 31236 net.cpp:139] Memory required for data: 5029120
I0428 20:07:49.530089 31236 layer_factory.hpp:77] Creating layer loss
I0428 20:07:49.530110 31236 net.cpp:86] Creating Layer loss
I0428 20:07:49.530113 31236 net.cpp:408] loss <- ip3
I0428 20:07:49.530118 31236 net.cpp:408] loss <- label
I0428 20:07:49.530123 31236 net.cpp:382] loss -> loss
I0428 20:07:49.530141 31236 layer_factory.hpp:77] Creating layer loss
I0428 20:07:49.530377 31236 net.cpp:124] Setting up loss
I0428 20:07:49.530386 31236 net.cpp:131] Top shape: (1)
I0428 20:07:49.530390 31236 net.cpp:134]     with loss weight 1
I0428 20:07:49.530403 31236 net.cpp:139] Memory required for data: 5029124
I0428 20:07:49.530407 31236 net.cpp:200] loss needs backward computation.
I0428 20:07:49.530411 31236 net.cpp:200] relu3 needs backward computation.
I0428 20:07:49.530413 31236 net.cpp:200] ip3 needs backward computation.
I0428 20:07:49.530417 31236 net.cpp:200] relu2 needs backward computation.
I0428 20:07:49.530419 31236 net.cpp:200] ip2 needs backward computation.
I0428 20:07:49.530422 31236 net.cpp:200] relu1 needs backward computation.
I0428 20:07:49.530439 31236 net.cpp:200] ip1 needs backward computation.
I0428 20:07:49.530442 31236 net.cpp:200] pool1 needs backward computation.
I0428 20:07:49.530447 31236 net.cpp:200] conv1 needs backward computation.
I0428 20:07:49.530463 31236 net.cpp:200] pool0 needs backward computation.
I0428 20:07:49.530467 31236 net.cpp:200] conv0 needs backward computation.
I0428 20:07:49.530469 31236 net.cpp:202] mnist does not need backward computation.
I0428 20:07:49.530488 31236 net.cpp:244] This network produces output loss
I0428 20:07:49.530496 31236 net.cpp:257] Network initialization done.
I0428 20:07:49.530854 31236 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1066.prototxt
I0428 20:07:49.530896 31236 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:07:49.530997 31236 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:07:49.531076 31236 layer_factory.hpp:77] Creating layer mnist
I0428 20:07:49.531136 31236 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:07:49.531148 31236 net.cpp:86] Creating Layer mnist
I0428 20:07:49.531152 31236 net.cpp:382] mnist -> data
I0428 20:07:49.531160 31236 net.cpp:382] mnist -> label
I0428 20:07:49.531255 31236 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:07:49.533313 31236 net.cpp:124] Setting up mnist
I0428 20:07:49.533325 31236 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:07:49.533330 31236 net.cpp:131] Top shape: 100 (100)
I0428 20:07:49.533349 31236 net.cpp:139] Memory required for data: 314000
I0428 20:07:49.533352 31236 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:07:49.533360 31236 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:07:49.533362 31236 net.cpp:408] label_mnist_1_split <- label
I0428 20:07:49.533366 31236 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:07:49.533373 31236 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:07:49.533438 31236 net.cpp:124] Setting up label_mnist_1_split
I0428 20:07:49.533444 31236 net.cpp:131] Top shape: 100 (100)
I0428 20:07:49.533448 31236 net.cpp:131] Top shape: 100 (100)
I0428 20:07:49.533452 31236 net.cpp:139] Memory required for data: 314800
I0428 20:07:49.533454 31236 layer_factory.hpp:77] Creating layer conv0
I0428 20:07:49.533463 31236 net.cpp:86] Creating Layer conv0
I0428 20:07:49.533465 31236 net.cpp:408] conv0 <- data
I0428 20:07:49.533470 31236 net.cpp:382] conv0 -> conv0
I0428 20:07:49.535069 31236 net.cpp:124] Setting up conv0
I0428 20:07:49.535099 31236 net.cpp:131] Top shape: 100 25 24 24 (1440000)
I0428 20:07:49.535104 31236 net.cpp:139] Memory required for data: 6074800
I0428 20:07:49.535114 31236 layer_factory.hpp:77] Creating layer pool0
I0428 20:07:49.535120 31236 net.cpp:86] Creating Layer pool0
I0428 20:07:49.535123 31236 net.cpp:408] pool0 <- conv0
I0428 20:07:49.535128 31236 net.cpp:382] pool0 -> pool0
I0428 20:07:49.535166 31236 net.cpp:124] Setting up pool0
I0428 20:07:49.535171 31236 net.cpp:131] Top shape: 100 25 12 12 (360000)
I0428 20:07:49.535173 31236 net.cpp:139] Memory required for data: 7514800
I0428 20:07:49.535192 31236 layer_factory.hpp:77] Creating layer conv1
I0428 20:07:49.535200 31236 net.cpp:86] Creating Layer conv1
I0428 20:07:49.535204 31236 net.cpp:408] conv1 <- pool0
I0428 20:07:49.535209 31236 net.cpp:382] conv1 -> conv1
I0428 20:07:49.537402 31236 net.cpp:124] Setting up conv1
I0428 20:07:49.537432 31236 net.cpp:131] Top shape: 100 10 8 8 (64000)
I0428 20:07:49.537436 31236 net.cpp:139] Memory required for data: 7770800
I0428 20:07:49.537446 31236 layer_factory.hpp:77] Creating layer pool1
I0428 20:07:49.537454 31236 net.cpp:86] Creating Layer pool1
I0428 20:07:49.537457 31236 net.cpp:408] pool1 <- conv1
I0428 20:07:49.537462 31236 net.cpp:382] pool1 -> pool1
I0428 20:07:49.537528 31236 net.cpp:124] Setting up pool1
I0428 20:07:49.537533 31236 net.cpp:131] Top shape: 100 10 4 4 (16000)
I0428 20:07:49.537538 31236 net.cpp:139] Memory required for data: 7834800
I0428 20:07:49.537541 31236 layer_factory.hpp:77] Creating layer ip1
I0428 20:07:49.537547 31236 net.cpp:86] Creating Layer ip1
I0428 20:07:49.537551 31236 net.cpp:408] ip1 <- pool1
I0428 20:07:49.537562 31236 net.cpp:382] ip1 -> ip1
I0428 20:07:49.537688 31236 net.cpp:124] Setting up ip1
I0428 20:07:49.537711 31236 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:07:49.537724 31236 net.cpp:139] Memory required for data: 7838800
I0428 20:07:49.537731 31236 layer_factory.hpp:77] Creating layer relu1
I0428 20:07:49.537737 31236 net.cpp:86] Creating Layer relu1
I0428 20:07:49.537741 31236 net.cpp:408] relu1 <- ip1
I0428 20:07:49.537745 31236 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:07:49.537951 31236 net.cpp:124] Setting up relu1
I0428 20:07:49.537961 31236 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:07:49.537963 31236 net.cpp:139] Memory required for data: 7842800
I0428 20:07:49.537966 31236 layer_factory.hpp:77] Creating layer ip2
I0428 20:07:49.537974 31236 net.cpp:86] Creating Layer ip2
I0428 20:07:49.537978 31236 net.cpp:408] ip2 <- ip1
I0428 20:07:49.537982 31236 net.cpp:382] ip2 -> ip2
I0428 20:07:49.538094 31236 net.cpp:124] Setting up ip2
I0428 20:07:49.538100 31236 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:07:49.538103 31236 net.cpp:139] Memory required for data: 7846800
I0428 20:07:49.538108 31236 layer_factory.hpp:77] Creating layer relu2
I0428 20:07:49.538112 31236 net.cpp:86] Creating Layer relu2
I0428 20:07:49.538116 31236 net.cpp:408] relu2 <- ip2
I0428 20:07:49.538121 31236 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:07:49.538283 31236 net.cpp:124] Setting up relu2
I0428 20:07:49.538292 31236 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:07:49.538295 31236 net.cpp:139] Memory required for data: 7850800
I0428 20:07:49.538298 31236 layer_factory.hpp:77] Creating layer ip3
I0428 20:07:49.538305 31236 net.cpp:86] Creating Layer ip3
I0428 20:07:49.538313 31236 net.cpp:408] ip3 <- ip2
I0428 20:07:49.538319 31236 net.cpp:382] ip3 -> ip3
I0428 20:07:49.538422 31236 net.cpp:124] Setting up ip3
I0428 20:07:49.538441 31236 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:07:49.538445 31236 net.cpp:139] Memory required for data: 7854800
I0428 20:07:49.538453 31236 layer_factory.hpp:77] Creating layer relu3
I0428 20:07:49.538458 31236 net.cpp:86] Creating Layer relu3
I0428 20:07:49.538461 31236 net.cpp:408] relu3 <- ip3
I0428 20:07:49.538465 31236 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:07:49.539368 31236 net.cpp:124] Setting up relu3
I0428 20:07:49.539396 31236 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:07:49.539399 31236 net.cpp:139] Memory required for data: 7858800
I0428 20:07:49.539403 31236 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:07:49.539409 31236 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:07:49.539414 31236 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:07:49.539420 31236 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:07:49.539427 31236 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:07:49.539474 31236 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:07:49.539479 31236 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:07:49.539484 31236 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:07:49.539485 31236 net.cpp:139] Memory required for data: 7866800
I0428 20:07:49.539489 31236 layer_factory.hpp:77] Creating layer accuracy
I0428 20:07:49.539499 31236 net.cpp:86] Creating Layer accuracy
I0428 20:07:49.539502 31236 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:07:49.539507 31236 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:07:49.539511 31236 net.cpp:382] accuracy -> accuracy
I0428 20:07:49.539518 31236 net.cpp:124] Setting up accuracy
I0428 20:07:49.539522 31236 net.cpp:131] Top shape: (1)
I0428 20:07:49.539525 31236 net.cpp:139] Memory required for data: 7866804
I0428 20:07:49.539527 31236 layer_factory.hpp:77] Creating layer loss
I0428 20:07:49.539539 31236 net.cpp:86] Creating Layer loss
I0428 20:07:49.539542 31236 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:07:49.539546 31236 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:07:49.539556 31236 net.cpp:382] loss -> loss
I0428 20:07:49.539561 31236 layer_factory.hpp:77] Creating layer loss
I0428 20:07:49.539830 31236 net.cpp:124] Setting up loss
I0428 20:07:49.539840 31236 net.cpp:131] Top shape: (1)
I0428 20:07:49.539844 31236 net.cpp:134]     with loss weight 1
I0428 20:07:49.539860 31236 net.cpp:139] Memory required for data: 7866808
I0428 20:07:49.539863 31236 net.cpp:200] loss needs backward computation.
I0428 20:07:49.539867 31236 net.cpp:202] accuracy does not need backward computation.
I0428 20:07:49.539871 31236 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:07:49.539875 31236 net.cpp:200] relu3 needs backward computation.
I0428 20:07:49.539877 31236 net.cpp:200] ip3 needs backward computation.
I0428 20:07:49.539880 31236 net.cpp:200] relu2 needs backward computation.
I0428 20:07:49.539883 31236 net.cpp:200] ip2 needs backward computation.
I0428 20:07:49.539906 31236 net.cpp:200] relu1 needs backward computation.
I0428 20:07:49.539909 31236 net.cpp:200] ip1 needs backward computation.
I0428 20:07:49.539913 31236 net.cpp:200] pool1 needs backward computation.
I0428 20:07:49.539916 31236 net.cpp:200] conv1 needs backward computation.
I0428 20:07:49.539919 31236 net.cpp:200] pool0 needs backward computation.
I0428 20:07:49.539922 31236 net.cpp:200] conv0 needs backward computation.
I0428 20:07:49.539925 31236 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:07:49.539930 31236 net.cpp:202] mnist does not need backward computation.
I0428 20:07:49.539932 31236 net.cpp:244] This network produces output accuracy
I0428 20:07:49.539937 31236 net.cpp:244] This network produces output loss
I0428 20:07:49.539948 31236 net.cpp:257] Network initialization done.
I0428 20:07:49.539991 31236 solver.cpp:56] Solver scaffolding done.
I0428 20:07:49.540344 31236 caffe.cpp:248] Starting Optimization
I0428 20:07:49.540349 31236 solver.cpp:273] Solving LeNet
I0428 20:07:49.540352 31236 solver.cpp:274] Learning Rate Policy: inv
I0428 20:07:49.541307 31236 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:07:49.546530 31236 blocking_queue.cpp:49] Waiting for data
I0428 20:07:49.614943 31243 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:07:49.615566 31236 solver.cpp:398]     Test net output #0: accuracy = 0.1002
I0428 20:07:49.615600 31236 solver.cpp:398]     Test net output #1: loss = 2.33436 (* 1 = 2.33436 loss)
I0428 20:07:49.619081 31236 solver.cpp:219] Iteration 0 (0 iter/s, 0.0786812s/100 iters), loss = 2.3152
I0428 20:07:49.619119 31236 solver.cpp:238]     Train net output #0: loss = 2.3152 (* 1 = 2.3152 loss)
I0428 20:07:49.619130 31236 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:07:49.710439 31236 solver.cpp:219] Iteration 100 (1095.19 iter/s, 0.091308s/100 iters), loss = 1.57081
I0428 20:07:49.710479 31236 solver.cpp:238]     Train net output #0: loss = 1.57081 (* 1 = 1.57081 loss)
I0428 20:07:49.710485 31236 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:07:49.800915 31236 solver.cpp:219] Iteration 200 (1105.87 iter/s, 0.0904264s/100 iters), loss = 1.63877
I0428 20:07:49.800956 31236 solver.cpp:238]     Train net output #0: loss = 1.63877 (* 1 = 1.63877 loss)
I0428 20:07:49.800961 31236 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:07:49.891551 31236 solver.cpp:219] Iteration 300 (1103.9 iter/s, 0.0905882s/100 iters), loss = 1.54015
I0428 20:07:49.891590 31236 solver.cpp:238]     Train net output #0: loss = 1.54015 (* 1 = 1.54015 loss)
I0428 20:07:49.891597 31236 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:07:49.982868 31236 solver.cpp:219] Iteration 400 (1095.49 iter/s, 0.0912835s/100 iters), loss = 1.21021
I0428 20:07:49.982893 31236 solver.cpp:238]     Train net output #0: loss = 1.21021 (* 1 = 1.21021 loss)
I0428 20:07:49.982899 31236 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:07:50.072374 31236 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:07:50.127516 31243 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:07:50.128110 31236 solver.cpp:398]     Test net output #0: accuracy = 0.6383
I0428 20:07:50.128129 31236 solver.cpp:398]     Test net output #1: loss = 1.10173 (* 1 = 1.10173 loss)
I0428 20:07:50.129042 31236 solver.cpp:219] Iteration 500 (684.289 iter/s, 0.146137s/100 iters), loss = 0.981949
I0428 20:07:50.129068 31236 solver.cpp:238]     Train net output #0: loss = 0.981949 (* 1 = 0.981949 loss)
I0428 20:07:50.129096 31236 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:07:50.219909 31236 solver.cpp:219] Iteration 600 (1100.94 iter/s, 0.0908319s/100 iters), loss = 0.965912
I0428 20:07:50.219949 31236 solver.cpp:238]     Train net output #0: loss = 0.965912 (* 1 = 0.965912 loss)
I0428 20:07:50.219954 31236 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:07:50.310118 31236 solver.cpp:219] Iteration 700 (1108.95 iter/s, 0.0901754s/100 iters), loss = 1.05339
I0428 20:07:50.310143 31236 solver.cpp:238]     Train net output #0: loss = 1.05339 (* 1 = 1.05339 loss)
I0428 20:07:50.310150 31236 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:07:50.399677 31236 solver.cpp:219] Iteration 800 (1117 iter/s, 0.0895252s/100 iters), loss = 1.10017
I0428 20:07:50.399718 31236 solver.cpp:238]     Train net output #0: loss = 1.10017 (* 1 = 1.10017 loss)
I0428 20:07:50.399724 31236 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:07:50.489150 31236 solver.cpp:219] Iteration 900 (1118.09 iter/s, 0.0894382s/100 iters), loss = 1.1444
I0428 20:07:50.489173 31236 solver.cpp:238]     Train net output #0: loss = 1.1444 (* 1 = 1.1444 loss)
I0428 20:07:50.489179 31236 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:07:50.518774 31242 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:07:50.576962 31236 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:07:50.577808 31236 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:07:50.578426 31236 solver.cpp:311] Iteration 1000, loss = 0.970192
I0428 20:07:50.578471 31236 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:07:50.630086 31243 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:07:50.631592 31236 solver.cpp:398]     Test net output #0: accuracy = 0.6903
I0428 20:07:50.631628 31236 solver.cpp:398]     Test net output #1: loss = 0.94137 (* 1 = 0.94137 loss)
I0428 20:07:50.631633 31236 solver.cpp:316] Optimization Done.
I0428 20:07:50.631647 31236 caffe.cpp:259] Optimization Done.
