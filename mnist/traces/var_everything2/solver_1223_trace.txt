I0428 20:14:29.443948 32689 caffe.cpp:218] Using GPUs 0
I0428 20:14:29.480561 32689 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:14:29.935011 32689 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1223.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:14:29.935145 32689 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1223.prototxt
I0428 20:14:29.935485 32689 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:14:29.935516 32689 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:14:29.935595 32689 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:14:29.935658 32689 layer_factory.hpp:77] Creating layer mnist
I0428 20:14:29.935735 32689 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:14:29.935755 32689 net.cpp:86] Creating Layer mnist
I0428 20:14:29.935761 32689 net.cpp:382] mnist -> data
I0428 20:14:29.935781 32689 net.cpp:382] mnist -> label
I0428 20:14:29.936718 32689 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:14:29.939038 32689 net.cpp:124] Setting up mnist
I0428 20:14:29.939074 32689 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:14:29.939079 32689 net.cpp:131] Top shape: 64 (64)
I0428 20:14:29.939081 32689 net.cpp:139] Memory required for data: 200960
I0428 20:14:29.939087 32689 layer_factory.hpp:77] Creating layer conv0
I0428 20:14:29.939103 32689 net.cpp:86] Creating Layer conv0
I0428 20:14:29.939121 32689 net.cpp:408] conv0 <- data
I0428 20:14:29.939133 32689 net.cpp:382] conv0 -> conv0
I0428 20:14:30.167062 32689 net.cpp:124] Setting up conv0
I0428 20:14:30.167104 32689 net.cpp:131] Top shape: 64 50 24 24 (1843200)
I0428 20:14:30.167107 32689 net.cpp:139] Memory required for data: 7573760
I0428 20:14:30.167124 32689 layer_factory.hpp:77] Creating layer pool0
I0428 20:14:30.167135 32689 net.cpp:86] Creating Layer pool0
I0428 20:14:30.167140 32689 net.cpp:408] pool0 <- conv0
I0428 20:14:30.167145 32689 net.cpp:382] pool0 -> pool0
I0428 20:14:30.167218 32689 net.cpp:124] Setting up pool0
I0428 20:14:30.167224 32689 net.cpp:131] Top shape: 64 50 12 12 (460800)
I0428 20:14:30.167227 32689 net.cpp:139] Memory required for data: 9416960
I0428 20:14:30.167229 32689 layer_factory.hpp:77] Creating layer conv1
I0428 20:14:30.167240 32689 net.cpp:86] Creating Layer conv1
I0428 20:14:30.167243 32689 net.cpp:408] conv1 <- pool0
I0428 20:14:30.167263 32689 net.cpp:382] conv1 -> conv1
I0428 20:14:30.169175 32689 net.cpp:124] Setting up conv1
I0428 20:14:30.169190 32689 net.cpp:131] Top shape: 64 2 8 8 (8192)
I0428 20:14:30.169194 32689 net.cpp:139] Memory required for data: 9449728
I0428 20:14:30.169203 32689 layer_factory.hpp:77] Creating layer pool1
I0428 20:14:30.169212 32689 net.cpp:86] Creating Layer pool1
I0428 20:14:30.169215 32689 net.cpp:408] pool1 <- conv1
I0428 20:14:30.169220 32689 net.cpp:382] pool1 -> pool1
I0428 20:14:30.169257 32689 net.cpp:124] Setting up pool1
I0428 20:14:30.169265 32689 net.cpp:131] Top shape: 64 2 4 4 (2048)
I0428 20:14:30.169268 32689 net.cpp:139] Memory required for data: 9457920
I0428 20:14:30.169271 32689 layer_factory.hpp:77] Creating layer ip1
I0428 20:14:30.169278 32689 net.cpp:86] Creating Layer ip1
I0428 20:14:30.169282 32689 net.cpp:408] ip1 <- pool1
I0428 20:14:30.169286 32689 net.cpp:382] ip1 -> ip1
I0428 20:14:30.169383 32689 net.cpp:124] Setting up ip1
I0428 20:14:30.169389 32689 net.cpp:131] Top shape: 64 10 (640)
I0428 20:14:30.169392 32689 net.cpp:139] Memory required for data: 9460480
I0428 20:14:30.169399 32689 layer_factory.hpp:77] Creating layer relu1
I0428 20:14:30.169409 32689 net.cpp:86] Creating Layer relu1
I0428 20:14:30.169427 32689 net.cpp:408] relu1 <- ip1
I0428 20:14:30.169432 32689 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:14:30.169622 32689 net.cpp:124] Setting up relu1
I0428 20:14:30.169631 32689 net.cpp:131] Top shape: 64 10 (640)
I0428 20:14:30.169634 32689 net.cpp:139] Memory required for data: 9463040
I0428 20:14:30.169637 32689 layer_factory.hpp:77] Creating layer ip2
I0428 20:14:30.169643 32689 net.cpp:86] Creating Layer ip2
I0428 20:14:30.169647 32689 net.cpp:408] ip2 <- ip1
I0428 20:14:30.169651 32689 net.cpp:382] ip2 -> ip2
I0428 20:14:30.169754 32689 net.cpp:124] Setting up ip2
I0428 20:14:30.169760 32689 net.cpp:131] Top shape: 64 10 (640)
I0428 20:14:30.169764 32689 net.cpp:139] Memory required for data: 9465600
I0428 20:14:30.169769 32689 layer_factory.hpp:77] Creating layer relu2
I0428 20:14:30.169775 32689 net.cpp:86] Creating Layer relu2
I0428 20:14:30.169777 32689 net.cpp:408] relu2 <- ip2
I0428 20:14:30.169781 32689 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:14:30.170594 32689 net.cpp:124] Setting up relu2
I0428 20:14:30.170605 32689 net.cpp:131] Top shape: 64 10 (640)
I0428 20:14:30.170625 32689 net.cpp:139] Memory required for data: 9468160
I0428 20:14:30.170629 32689 layer_factory.hpp:77] Creating layer ip3
I0428 20:14:30.170635 32689 net.cpp:86] Creating Layer ip3
I0428 20:14:30.170639 32689 net.cpp:408] ip3 <- ip2
I0428 20:14:30.170644 32689 net.cpp:382] ip3 -> ip3
I0428 20:14:30.170734 32689 net.cpp:124] Setting up ip3
I0428 20:14:30.170742 32689 net.cpp:131] Top shape: 64 10 (640)
I0428 20:14:30.170744 32689 net.cpp:139] Memory required for data: 9470720
I0428 20:14:30.170753 32689 layer_factory.hpp:77] Creating layer relu3
I0428 20:14:30.170758 32689 net.cpp:86] Creating Layer relu3
I0428 20:14:30.170760 32689 net.cpp:408] relu3 <- ip3
I0428 20:14:30.170763 32689 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:14:30.170918 32689 net.cpp:124] Setting up relu3
I0428 20:14:30.170928 32689 net.cpp:131] Top shape: 64 10 (640)
I0428 20:14:30.170930 32689 net.cpp:139] Memory required for data: 9473280
I0428 20:14:30.170933 32689 layer_factory.hpp:77] Creating layer loss
I0428 20:14:30.170938 32689 net.cpp:86] Creating Layer loss
I0428 20:14:30.170941 32689 net.cpp:408] loss <- ip3
I0428 20:14:30.170945 32689 net.cpp:408] loss <- label
I0428 20:14:30.170949 32689 net.cpp:382] loss -> loss
I0428 20:14:30.170965 32689 layer_factory.hpp:77] Creating layer loss
I0428 20:14:30.171195 32689 net.cpp:124] Setting up loss
I0428 20:14:30.171203 32689 net.cpp:131] Top shape: (1)
I0428 20:14:30.171206 32689 net.cpp:134]     with loss weight 1
I0428 20:14:30.171221 32689 net.cpp:139] Memory required for data: 9473284
I0428 20:14:30.171223 32689 net.cpp:200] loss needs backward computation.
I0428 20:14:30.171227 32689 net.cpp:200] relu3 needs backward computation.
I0428 20:14:30.171231 32689 net.cpp:200] ip3 needs backward computation.
I0428 20:14:30.171233 32689 net.cpp:200] relu2 needs backward computation.
I0428 20:14:30.171236 32689 net.cpp:200] ip2 needs backward computation.
I0428 20:14:30.171239 32689 net.cpp:200] relu1 needs backward computation.
I0428 20:14:30.171242 32689 net.cpp:200] ip1 needs backward computation.
I0428 20:14:30.171245 32689 net.cpp:200] pool1 needs backward computation.
I0428 20:14:30.171248 32689 net.cpp:200] conv1 needs backward computation.
I0428 20:14:30.171252 32689 net.cpp:200] pool0 needs backward computation.
I0428 20:14:30.171254 32689 net.cpp:200] conv0 needs backward computation.
I0428 20:14:30.171257 32689 net.cpp:202] mnist does not need backward computation.
I0428 20:14:30.171260 32689 net.cpp:244] This network produces output loss
I0428 20:14:30.171269 32689 net.cpp:257] Network initialization done.
I0428 20:14:30.171650 32689 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1223.prototxt
I0428 20:14:30.171694 32689 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:14:30.171798 32689 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:14:30.171878 32689 layer_factory.hpp:77] Creating layer mnist
I0428 20:14:30.171922 32689 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:14:30.171950 32689 net.cpp:86] Creating Layer mnist
I0428 20:14:30.171955 32689 net.cpp:382] mnist -> data
I0428 20:14:30.171962 32689 net.cpp:382] mnist -> label
I0428 20:14:30.172058 32689 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:14:30.174136 32689 net.cpp:124] Setting up mnist
I0428 20:14:30.174180 32689 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:14:30.174185 32689 net.cpp:131] Top shape: 100 (100)
I0428 20:14:30.174188 32689 net.cpp:139] Memory required for data: 314000
I0428 20:14:30.174192 32689 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:14:30.174228 32689 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:14:30.174232 32689 net.cpp:408] label_mnist_1_split <- label
I0428 20:14:30.174237 32689 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:14:30.174244 32689 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:14:30.174365 32689 net.cpp:124] Setting up label_mnist_1_split
I0428 20:14:30.174373 32689 net.cpp:131] Top shape: 100 (100)
I0428 20:14:30.174377 32689 net.cpp:131] Top shape: 100 (100)
I0428 20:14:30.174381 32689 net.cpp:139] Memory required for data: 314800
I0428 20:14:30.174383 32689 layer_factory.hpp:77] Creating layer conv0
I0428 20:14:30.174393 32689 net.cpp:86] Creating Layer conv0
I0428 20:14:30.174397 32689 net.cpp:408] conv0 <- data
I0428 20:14:30.174402 32689 net.cpp:382] conv0 -> conv0
I0428 20:14:30.176003 32689 net.cpp:124] Setting up conv0
I0428 20:14:30.176017 32689 net.cpp:131] Top shape: 100 50 24 24 (2880000)
I0428 20:14:30.176036 32689 net.cpp:139] Memory required for data: 11834800
I0428 20:14:30.176046 32689 layer_factory.hpp:77] Creating layer pool0
I0428 20:14:30.176054 32689 net.cpp:86] Creating Layer pool0
I0428 20:14:30.176057 32689 net.cpp:408] pool0 <- conv0
I0428 20:14:30.176064 32689 net.cpp:382] pool0 -> pool0
I0428 20:14:30.176132 32689 net.cpp:124] Setting up pool0
I0428 20:14:30.176146 32689 net.cpp:131] Top shape: 100 50 12 12 (720000)
I0428 20:14:30.176148 32689 net.cpp:139] Memory required for data: 14714800
I0428 20:14:30.176151 32689 layer_factory.hpp:77] Creating layer conv1
I0428 20:14:30.176161 32689 net.cpp:86] Creating Layer conv1
I0428 20:14:30.176164 32689 net.cpp:408] conv1 <- pool0
I0428 20:14:30.176169 32689 net.cpp:382] conv1 -> conv1
I0428 20:14:30.177770 32689 net.cpp:124] Setting up conv1
I0428 20:14:30.177784 32689 net.cpp:131] Top shape: 100 2 8 8 (12800)
I0428 20:14:30.177805 32689 net.cpp:139] Memory required for data: 14766000
I0428 20:14:30.177814 32689 layer_factory.hpp:77] Creating layer pool1
I0428 20:14:30.177822 32689 net.cpp:86] Creating Layer pool1
I0428 20:14:30.177825 32689 net.cpp:408] pool1 <- conv1
I0428 20:14:30.177830 32689 net.cpp:382] pool1 -> pool1
I0428 20:14:30.177870 32689 net.cpp:124] Setting up pool1
I0428 20:14:30.177875 32689 net.cpp:131] Top shape: 100 2 4 4 (3200)
I0428 20:14:30.177877 32689 net.cpp:139] Memory required for data: 14778800
I0428 20:14:30.177881 32689 layer_factory.hpp:77] Creating layer ip1
I0428 20:14:30.177888 32689 net.cpp:86] Creating Layer ip1
I0428 20:14:30.177891 32689 net.cpp:408] ip1 <- pool1
I0428 20:14:30.177896 32689 net.cpp:382] ip1 -> ip1
I0428 20:14:30.178027 32689 net.cpp:124] Setting up ip1
I0428 20:14:30.178035 32689 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:14:30.178050 32689 net.cpp:139] Memory required for data: 14782800
I0428 20:14:30.178057 32689 layer_factory.hpp:77] Creating layer relu1
I0428 20:14:30.178064 32689 net.cpp:86] Creating Layer relu1
I0428 20:14:30.178067 32689 net.cpp:408] relu1 <- ip1
I0428 20:14:30.178072 32689 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:14:30.178272 32689 net.cpp:124] Setting up relu1
I0428 20:14:30.178280 32689 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:14:30.178284 32689 net.cpp:139] Memory required for data: 14786800
I0428 20:14:30.178287 32689 layer_factory.hpp:77] Creating layer ip2
I0428 20:14:30.178297 32689 net.cpp:86] Creating Layer ip2
I0428 20:14:30.178300 32689 net.cpp:408] ip2 <- ip1
I0428 20:14:30.178306 32689 net.cpp:382] ip2 -> ip2
I0428 20:14:30.178457 32689 net.cpp:124] Setting up ip2
I0428 20:14:30.178465 32689 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:14:30.178468 32689 net.cpp:139] Memory required for data: 14790800
I0428 20:14:30.178474 32689 layer_factory.hpp:77] Creating layer relu2
I0428 20:14:30.178479 32689 net.cpp:86] Creating Layer relu2
I0428 20:14:30.178483 32689 net.cpp:408] relu2 <- ip2
I0428 20:14:30.178486 32689 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:14:30.178642 32689 net.cpp:124] Setting up relu2
I0428 20:14:30.178650 32689 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:14:30.178653 32689 net.cpp:139] Memory required for data: 14794800
I0428 20:14:30.178656 32689 layer_factory.hpp:77] Creating layer ip3
I0428 20:14:30.178663 32689 net.cpp:86] Creating Layer ip3
I0428 20:14:30.178665 32689 net.cpp:408] ip3 <- ip2
I0428 20:14:30.178670 32689 net.cpp:382] ip3 -> ip3
I0428 20:14:30.178767 32689 net.cpp:124] Setting up ip3
I0428 20:14:30.178777 32689 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:14:30.178781 32689 net.cpp:139] Memory required for data: 14798800
I0428 20:14:30.178789 32689 layer_factory.hpp:77] Creating layer relu3
I0428 20:14:30.178795 32689 net.cpp:86] Creating Layer relu3
I0428 20:14:30.178798 32689 net.cpp:408] relu3 <- ip3
I0428 20:14:30.178803 32689 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:14:30.179608 32689 net.cpp:124] Setting up relu3
I0428 20:14:30.179620 32689 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:14:30.179638 32689 net.cpp:139] Memory required for data: 14802800
I0428 20:14:30.179641 32689 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:14:30.179646 32689 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:14:30.179649 32689 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:14:30.179656 32689 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:14:30.179661 32689 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:14:30.179698 32689 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:14:30.179703 32689 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:14:30.179708 32689 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:14:30.179710 32689 net.cpp:139] Memory required for data: 14810800
I0428 20:14:30.179718 32689 layer_factory.hpp:77] Creating layer accuracy
I0428 20:14:30.179724 32689 net.cpp:86] Creating Layer accuracy
I0428 20:14:30.179728 32689 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:14:30.179731 32689 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:14:30.179735 32689 net.cpp:382] accuracy -> accuracy
I0428 20:14:30.179741 32689 net.cpp:124] Setting up accuracy
I0428 20:14:30.179747 32689 net.cpp:131] Top shape: (1)
I0428 20:14:30.179764 32689 net.cpp:139] Memory required for data: 14810804
I0428 20:14:30.179767 32689 layer_factory.hpp:77] Creating layer loss
I0428 20:14:30.179771 32689 net.cpp:86] Creating Layer loss
I0428 20:14:30.179775 32689 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:14:30.179778 32689 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:14:30.179782 32689 net.cpp:382] loss -> loss
I0428 20:14:30.179790 32689 layer_factory.hpp:77] Creating layer loss
I0428 20:14:30.180022 32689 net.cpp:124] Setting up loss
I0428 20:14:30.180032 32689 net.cpp:131] Top shape: (1)
I0428 20:14:30.180034 32689 net.cpp:134]     with loss weight 1
I0428 20:14:30.180050 32689 net.cpp:139] Memory required for data: 14810808
I0428 20:14:30.180054 32689 net.cpp:200] loss needs backward computation.
I0428 20:14:30.180058 32689 net.cpp:202] accuracy does not need backward computation.
I0428 20:14:30.180063 32689 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:14:30.180066 32689 net.cpp:200] relu3 needs backward computation.
I0428 20:14:30.180068 32689 net.cpp:200] ip3 needs backward computation.
I0428 20:14:30.180071 32689 net.cpp:200] relu2 needs backward computation.
I0428 20:14:30.180074 32689 net.cpp:200] ip2 needs backward computation.
I0428 20:14:30.180078 32689 net.cpp:200] relu1 needs backward computation.
I0428 20:14:30.180081 32689 net.cpp:200] ip1 needs backward computation.
I0428 20:14:30.180084 32689 net.cpp:200] pool1 needs backward computation.
I0428 20:14:30.180088 32689 net.cpp:200] conv1 needs backward computation.
I0428 20:14:30.180090 32689 net.cpp:200] pool0 needs backward computation.
I0428 20:14:30.180094 32689 net.cpp:200] conv0 needs backward computation.
I0428 20:14:30.180097 32689 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:14:30.180100 32689 net.cpp:202] mnist does not need backward computation.
I0428 20:14:30.180104 32689 net.cpp:244] This network produces output accuracy
I0428 20:14:30.180106 32689 net.cpp:244] This network produces output loss
I0428 20:14:30.180116 32689 net.cpp:257] Network initialization done.
I0428 20:14:30.180157 32689 solver.cpp:56] Solver scaffolding done.
I0428 20:14:30.180572 32689 caffe.cpp:248] Starting Optimization
I0428 20:14:30.180578 32689 solver.cpp:273] Solving LeNet
I0428 20:14:30.180582 32689 solver.cpp:274] Learning Rate Policy: inv
I0428 20:14:30.181370 32689 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:14:30.247517 32696 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:14:30.249156 32689 solver.cpp:398]     Test net output #0: accuracy = 0.0872
I0428 20:14:30.249191 32689 solver.cpp:398]     Test net output #1: loss = 2.31006 (* 1 = 2.31006 loss)
I0428 20:14:30.253149 32689 solver.cpp:219] Iteration 0 (0 iter/s, 0.0725228s/100 iters), loss = 2.31605
I0428 20:14:30.253187 32689 solver.cpp:238]     Train net output #0: loss = 2.31605 (* 1 = 2.31605 loss)
I0428 20:14:30.253198 32689 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:14:30.368068 32689 solver.cpp:219] Iteration 100 (870.454 iter/s, 0.114883s/100 iters), loss = 0.844853
I0428 20:14:30.368110 32689 solver.cpp:238]     Train net output #0: loss = 0.844853 (* 1 = 0.844853 loss)
I0428 20:14:30.368116 32689 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:14:30.484560 32689 solver.cpp:219] Iteration 200 (858.871 iter/s, 0.116432s/100 iters), loss = 0.465808
I0428 20:14:30.484591 32689 solver.cpp:238]     Train net output #0: loss = 0.465808 (* 1 = 0.465808 loss)
I0428 20:14:30.484598 32689 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:14:30.598955 32689 solver.cpp:219] Iteration 300 (874.594 iter/s, 0.114339s/100 iters), loss = 0.361952
I0428 20:14:30.598999 32689 solver.cpp:238]     Train net output #0: loss = 0.361952 (* 1 = 0.361952 loss)
I0428 20:14:30.599005 32689 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:14:30.712127 32689 solver.cpp:219] Iteration 400 (883.903 iter/s, 0.113135s/100 iters), loss = 0.376359
I0428 20:14:30.712169 32689 solver.cpp:238]     Train net output #0: loss = 0.376359 (* 1 = 0.376359 loss)
I0428 20:14:30.712177 32689 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:14:30.824095 32689 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:14:30.889741 32696 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:14:30.892024 32689 solver.cpp:398]     Test net output #0: accuracy = 0.8923
I0428 20:14:30.892042 32689 solver.cpp:398]     Test net output #1: loss = 0.355329 (* 1 = 0.355329 loss)
I0428 20:14:30.893151 32689 solver.cpp:219] Iteration 500 (552.583 iter/s, 0.180968s/100 iters), loss = 0.276138
I0428 20:14:30.893175 32689 solver.cpp:238]     Train net output #0: loss = 0.276138 (* 1 = 0.276138 loss)
I0428 20:14:30.893196 32689 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:14:31.008144 32689 solver.cpp:219] Iteration 600 (869.88 iter/s, 0.114958s/100 iters), loss = 0.351872
I0428 20:14:31.008172 32689 solver.cpp:238]     Train net output #0: loss = 0.351872 (* 1 = 0.351872 loss)
I0428 20:14:31.008178 32689 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:14:31.124794 32689 solver.cpp:219] Iteration 700 (857.59 iter/s, 0.116606s/100 iters), loss = 0.377675
I0428 20:14:31.124850 32689 solver.cpp:238]     Train net output #0: loss = 0.377675 (* 1 = 0.377675 loss)
I0428 20:14:31.124862 32689 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:14:31.251360 32689 solver.cpp:219] Iteration 800 (790.508 iter/s, 0.126501s/100 iters), loss = 0.356122
I0428 20:14:31.251394 32689 solver.cpp:238]     Train net output #0: loss = 0.356122 (* 1 = 0.356122 loss)
I0428 20:14:31.251402 32689 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:14:31.369606 32689 solver.cpp:219] Iteration 900 (845.999 iter/s, 0.118203s/100 iters), loss = 0.338041
I0428 20:14:31.369637 32689 solver.cpp:238]     Train net output #0: loss = 0.338041 (* 1 = 0.338041 loss)
I0428 20:14:31.369644 32689 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:14:31.407807 32695 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:14:31.484866 32689 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:14:31.485831 32689 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:14:31.486552 32689 solver.cpp:311] Iteration 1000, loss = 0.279475
I0428 20:14:31.486568 32689 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:14:31.554909 32696 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:14:31.557368 32689 solver.cpp:398]     Test net output #0: accuracy = 0.9247
I0428 20:14:31.557390 32689 solver.cpp:398]     Test net output #1: loss = 0.242527 (* 1 = 0.242527 loss)
I0428 20:14:31.557405 32689 solver.cpp:316] Optimization Done.
I0428 20:14:31.557410 32689 caffe.cpp:259] Optimization Done.
