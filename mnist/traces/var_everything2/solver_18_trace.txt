I0428 19:28:13.173606 21566 caffe.cpp:218] Using GPUs 0
I0428 19:28:13.206234 21566 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:28:13.652601 21566 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test18.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:28:13.652751 21566 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test18.prototxt
I0428 19:28:13.653090 21566 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:28:13.653103 21566 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:28:13.653192 21566 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:28:13.653260 21566 layer_factory.hpp:77] Creating layer mnist
I0428 19:28:13.653342 21566 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:28:13.653360 21566 net.cpp:86] Creating Layer mnist
I0428 19:28:13.653367 21566 net.cpp:382] mnist -> data
I0428 19:28:13.653384 21566 net.cpp:382] mnist -> label
I0428 19:28:13.654309 21566 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:28:13.656483 21566 net.cpp:124] Setting up mnist
I0428 19:28:13.656513 21566 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:28:13.656518 21566 net.cpp:131] Top shape: 64 (64)
I0428 19:28:13.656522 21566 net.cpp:139] Memory required for data: 200960
I0428 19:28:13.656527 21566 layer_factory.hpp:77] Creating layer ip1
I0428 19:28:13.656570 21566 net.cpp:86] Creating Layer ip1
I0428 19:28:13.656576 21566 net.cpp:408] ip1 <- data
I0428 19:28:13.656584 21566 net.cpp:382] ip1 -> ip1
I0428 19:28:13.657778 21566 net.cpp:124] Setting up ip1
I0428 19:28:13.657790 21566 net.cpp:131] Top shape: 64 10 (640)
I0428 19:28:13.657793 21566 net.cpp:139] Memory required for data: 203520
I0428 19:28:13.657805 21566 layer_factory.hpp:77] Creating layer relu1
I0428 19:28:13.657814 21566 net.cpp:86] Creating Layer relu1
I0428 19:28:13.657816 21566 net.cpp:408] relu1 <- ip1
I0428 19:28:13.657821 21566 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:28:13.886565 21566 net.cpp:124] Setting up relu1
I0428 19:28:13.886593 21566 net.cpp:131] Top shape: 64 10 (640)
I0428 19:28:13.886597 21566 net.cpp:139] Memory required for data: 206080
I0428 19:28:13.886602 21566 layer_factory.hpp:77] Creating layer ip2
I0428 19:28:13.886615 21566 net.cpp:86] Creating Layer ip2
I0428 19:28:13.886638 21566 net.cpp:408] ip2 <- ip1
I0428 19:28:13.886660 21566 net.cpp:382] ip2 -> ip2
I0428 19:28:13.887698 21566 net.cpp:124] Setting up ip2
I0428 19:28:13.887727 21566 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:28:13.887730 21566 net.cpp:139] Memory required for data: 218880
I0428 19:28:13.887740 21566 layer_factory.hpp:77] Creating layer relu2
I0428 19:28:13.887749 21566 net.cpp:86] Creating Layer relu2
I0428 19:28:13.887753 21566 net.cpp:408] relu2 <- ip2
I0428 19:28:13.887756 21566 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:28:13.888558 21566 net.cpp:124] Setting up relu2
I0428 19:28:13.888571 21566 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:28:13.888589 21566 net.cpp:139] Memory required for data: 231680
I0428 19:28:13.888592 21566 layer_factory.hpp:77] Creating layer ip3
I0428 19:28:13.888614 21566 net.cpp:86] Creating Layer ip3
I0428 19:28:13.888618 21566 net.cpp:408] ip3 <- ip2
I0428 19:28:13.888623 21566 net.cpp:382] ip3 -> ip3
I0428 19:28:13.888744 21566 net.cpp:124] Setting up ip3
I0428 19:28:13.888753 21566 net.cpp:131] Top shape: 64 10 (640)
I0428 19:28:13.888756 21566 net.cpp:139] Memory required for data: 234240
I0428 19:28:13.888764 21566 layer_factory.hpp:77] Creating layer relu3
I0428 19:28:13.888770 21566 net.cpp:86] Creating Layer relu3
I0428 19:28:13.888773 21566 net.cpp:408] relu3 <- ip3
I0428 19:28:13.888777 21566 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:28:13.888974 21566 net.cpp:124] Setting up relu3
I0428 19:28:13.888983 21566 net.cpp:131] Top shape: 64 10 (640)
I0428 19:28:13.888988 21566 net.cpp:139] Memory required for data: 236800
I0428 19:28:13.888990 21566 layer_factory.hpp:77] Creating layer loss
I0428 19:28:13.888996 21566 net.cpp:86] Creating Layer loss
I0428 19:28:13.888999 21566 net.cpp:408] loss <- ip3
I0428 19:28:13.889004 21566 net.cpp:408] loss <- label
I0428 19:28:13.889010 21566 net.cpp:382] loss -> loss
I0428 19:28:13.889029 21566 layer_factory.hpp:77] Creating layer loss
I0428 19:28:13.889292 21566 net.cpp:124] Setting up loss
I0428 19:28:13.889302 21566 net.cpp:131] Top shape: (1)
I0428 19:28:13.889304 21566 net.cpp:134]     with loss weight 1
I0428 19:28:13.889318 21566 net.cpp:139] Memory required for data: 236804
I0428 19:28:13.889322 21566 net.cpp:200] loss needs backward computation.
I0428 19:28:13.889325 21566 net.cpp:200] relu3 needs backward computation.
I0428 19:28:13.889328 21566 net.cpp:200] ip3 needs backward computation.
I0428 19:28:13.889341 21566 net.cpp:200] relu2 needs backward computation.
I0428 19:28:13.889344 21566 net.cpp:200] ip2 needs backward computation.
I0428 19:28:13.889346 21566 net.cpp:200] relu1 needs backward computation.
I0428 19:28:13.889349 21566 net.cpp:200] ip1 needs backward computation.
I0428 19:28:13.889353 21566 net.cpp:202] mnist does not need backward computation.
I0428 19:28:13.889355 21566 net.cpp:244] This network produces output loss
I0428 19:28:13.889363 21566 net.cpp:257] Network initialization done.
I0428 19:28:13.889622 21566 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test18.prototxt
I0428 19:28:13.889644 21566 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:28:13.889710 21566 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:28:13.889781 21566 layer_factory.hpp:77] Creating layer mnist
I0428 19:28:13.889824 21566 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:28:13.889837 21566 net.cpp:86] Creating Layer mnist
I0428 19:28:13.889842 21566 net.cpp:382] mnist -> data
I0428 19:28:13.889849 21566 net.cpp:382] mnist -> label
I0428 19:28:13.889933 21566 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:28:13.892089 21566 net.cpp:124] Setting up mnist
I0428 19:28:13.892102 21566 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:28:13.892107 21566 net.cpp:131] Top shape: 100 (100)
I0428 19:28:13.892109 21566 net.cpp:139] Memory required for data: 314000
I0428 19:28:13.892112 21566 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:28:13.892160 21566 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:28:13.892186 21566 net.cpp:408] label_mnist_1_split <- label
I0428 19:28:13.892191 21566 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:28:13.892197 21566 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:28:13.892328 21566 net.cpp:124] Setting up label_mnist_1_split
I0428 19:28:13.892335 21566 net.cpp:131] Top shape: 100 (100)
I0428 19:28:13.892340 21566 net.cpp:131] Top shape: 100 (100)
I0428 19:28:13.892343 21566 net.cpp:139] Memory required for data: 314800
I0428 19:28:13.892346 21566 layer_factory.hpp:77] Creating layer ip1
I0428 19:28:13.892352 21566 net.cpp:86] Creating Layer ip1
I0428 19:28:13.892355 21566 net.cpp:408] ip1 <- data
I0428 19:28:13.892361 21566 net.cpp:382] ip1 -> ip1
I0428 19:28:13.892570 21566 net.cpp:124] Setting up ip1
I0428 19:28:13.892578 21566 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:28:13.892581 21566 net.cpp:139] Memory required for data: 318800
I0428 19:28:13.892590 21566 layer_factory.hpp:77] Creating layer relu1
I0428 19:28:13.892594 21566 net.cpp:86] Creating Layer relu1
I0428 19:28:13.892597 21566 net.cpp:408] relu1 <- ip1
I0428 19:28:13.892603 21566 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:28:13.893520 21566 net.cpp:124] Setting up relu1
I0428 19:28:13.893532 21566 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:28:13.893551 21566 net.cpp:139] Memory required for data: 322800
I0428 19:28:13.893554 21566 layer_factory.hpp:77] Creating layer ip2
I0428 19:28:13.893563 21566 net.cpp:86] Creating Layer ip2
I0428 19:28:13.893565 21566 net.cpp:408] ip2 <- ip1
I0428 19:28:13.893570 21566 net.cpp:382] ip2 -> ip2
I0428 19:28:13.893679 21566 net.cpp:124] Setting up ip2
I0428 19:28:13.893687 21566 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:28:13.893692 21566 net.cpp:139] Memory required for data: 342800
I0428 19:28:13.893698 21566 layer_factory.hpp:77] Creating layer relu2
I0428 19:28:13.893702 21566 net.cpp:86] Creating Layer relu2
I0428 19:28:13.893705 21566 net.cpp:408] relu2 <- ip2
I0428 19:28:13.893710 21566 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:28:13.893867 21566 net.cpp:124] Setting up relu2
I0428 19:28:13.893875 21566 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:28:13.893878 21566 net.cpp:139] Memory required for data: 362800
I0428 19:28:13.893880 21566 layer_factory.hpp:77] Creating layer ip3
I0428 19:28:13.893887 21566 net.cpp:86] Creating Layer ip3
I0428 19:28:13.893889 21566 net.cpp:408] ip3 <- ip2
I0428 19:28:13.893893 21566 net.cpp:382] ip3 -> ip3
I0428 19:28:13.893998 21566 net.cpp:124] Setting up ip3
I0428 19:28:13.894006 21566 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:28:13.894008 21566 net.cpp:139] Memory required for data: 366800
I0428 19:28:13.894016 21566 layer_factory.hpp:77] Creating layer relu3
I0428 19:28:13.894021 21566 net.cpp:86] Creating Layer relu3
I0428 19:28:13.894024 21566 net.cpp:408] relu3 <- ip3
I0428 19:28:13.894028 21566 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:28:13.894170 21566 net.cpp:124] Setting up relu3
I0428 19:28:13.894178 21566 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:28:13.894181 21566 net.cpp:139] Memory required for data: 370800
I0428 19:28:13.894184 21566 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:28:13.894191 21566 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:28:13.894193 21566 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:28:13.894197 21566 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:28:13.894202 21566 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:28:13.894235 21566 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:28:13.894239 21566 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:28:13.894243 21566 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:28:13.894245 21566 net.cpp:139] Memory required for data: 378800
I0428 19:28:13.894248 21566 layer_factory.hpp:77] Creating layer accuracy
I0428 19:28:13.894256 21566 net.cpp:86] Creating Layer accuracy
I0428 19:28:13.894259 21566 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:28:13.894263 21566 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:28:13.894266 21566 net.cpp:382] accuracy -> accuracy
I0428 19:28:13.894273 21566 net.cpp:124] Setting up accuracy
I0428 19:28:13.894278 21566 net.cpp:131] Top shape: (1)
I0428 19:28:13.894279 21566 net.cpp:139] Memory required for data: 378804
I0428 19:28:13.894281 21566 layer_factory.hpp:77] Creating layer loss
I0428 19:28:13.894285 21566 net.cpp:86] Creating Layer loss
I0428 19:28:13.894289 21566 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:28:13.894291 21566 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:28:13.894296 21566 net.cpp:382] loss -> loss
I0428 19:28:13.894301 21566 layer_factory.hpp:77] Creating layer loss
I0428 19:28:13.894579 21566 net.cpp:124] Setting up loss
I0428 19:28:13.894588 21566 net.cpp:131] Top shape: (1)
I0428 19:28:13.894592 21566 net.cpp:134]     with loss weight 1
I0428 19:28:13.894598 21566 net.cpp:139] Memory required for data: 378808
I0428 19:28:13.894601 21566 net.cpp:200] loss needs backward computation.
I0428 19:28:13.894604 21566 net.cpp:202] accuracy does not need backward computation.
I0428 19:28:13.894608 21566 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:28:13.894611 21566 net.cpp:200] relu3 needs backward computation.
I0428 19:28:13.894614 21566 net.cpp:200] ip3 needs backward computation.
I0428 19:28:13.894618 21566 net.cpp:200] relu2 needs backward computation.
I0428 19:28:13.894619 21566 net.cpp:200] ip2 needs backward computation.
I0428 19:28:13.894623 21566 net.cpp:200] relu1 needs backward computation.
I0428 19:28:13.894625 21566 net.cpp:200] ip1 needs backward computation.
I0428 19:28:13.894629 21566 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:28:13.894632 21566 net.cpp:202] mnist does not need backward computation.
I0428 19:28:13.894635 21566 net.cpp:244] This network produces output accuracy
I0428 19:28:13.894639 21566 net.cpp:244] This network produces output loss
I0428 19:28:13.894662 21566 net.cpp:257] Network initialization done.
I0428 19:28:13.894695 21566 solver.cpp:56] Solver scaffolding done.
I0428 19:28:13.895015 21566 caffe.cpp:248] Starting Optimization
I0428 19:28:13.895020 21566 solver.cpp:273] Solving LeNet
I0428 19:28:13.895023 21566 solver.cpp:274] Learning Rate Policy: inv
I0428 19:28:13.895836 21566 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:28:13.895925 21566 blocking_queue.cpp:49] Waiting for data
I0428 19:28:13.973948 21573 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:28:13.974380 21566 solver.cpp:398]     Test net output #0: accuracy = 0.1135
I0428 19:28:13.974400 21566 solver.cpp:398]     Test net output #1: loss = 2.30097 (* 1 = 2.30097 loss)
I0428 19:28:13.975785 21566 solver.cpp:219] Iteration 0 (-9.89949e-31 iter/s, 0.0807363s/100 iters), loss = 2.29714
I0428 19:28:13.975811 21566 solver.cpp:238]     Train net output #0: loss = 2.29714 (* 1 = 2.29714 loss)
I0428 19:28:13.975822 21566 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:28:14.034646 21566 solver.cpp:219] Iteration 100 (1699.97 iter/s, 0.0588247s/100 iters), loss = 0.935607
I0428 19:28:14.034669 21566 solver.cpp:238]     Train net output #0: loss = 0.935607 (* 1 = 0.935607 loss)
I0428 19:28:14.034677 21566 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:28:14.082525 21566 solver.cpp:219] Iteration 200 (2089.98 iter/s, 0.0478474s/100 iters), loss = 0.818923
I0428 19:28:14.082550 21566 solver.cpp:238]     Train net output #0: loss = 0.818923 (* 1 = 0.818923 loss)
I0428 19:28:14.082556 21566 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:28:14.126312 21566 solver.cpp:219] Iteration 300 (2285.37 iter/s, 0.0437565s/100 iters), loss = 0.597034
I0428 19:28:14.126334 21566 solver.cpp:238]     Train net output #0: loss = 0.597034 (* 1 = 0.597034 loss)
I0428 19:28:14.126343 21566 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:28:14.170570 21566 solver.cpp:219] Iteration 400 (2261.03 iter/s, 0.0442276s/100 iters), loss = 0.49909
I0428 19:28:14.170593 21566 solver.cpp:238]     Train net output #0: loss = 0.49909 (* 1 = 0.49909 loss)
I0428 19:28:14.170598 21566 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:28:14.214501 21566 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:28:14.291744 21573 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:28:14.292215 21566 solver.cpp:398]     Test net output #0: accuracy = 0.8145
I0428 19:28:14.292242 21566 solver.cpp:398]     Test net output #1: loss = 0.546906 (* 1 = 0.546906 loss)
I0428 19:28:14.292790 21566 solver.cpp:219] Iteration 500 (818.426 iter/s, 0.122186s/100 iters), loss = 0.622671
I0428 19:28:14.292824 21566 solver.cpp:238]     Train net output #0: loss = 0.622671 (* 1 = 0.622671 loss)
I0428 19:28:14.292834 21566 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:28:14.350412 21566 solver.cpp:219] Iteration 600 (1736.72 iter/s, 0.0575796s/100 iters), loss = 0.573158
I0428 19:28:14.350440 21566 solver.cpp:238]     Train net output #0: loss = 0.573158 (* 1 = 0.573158 loss)
I0428 19:28:14.350450 21566 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:28:14.399847 21566 solver.cpp:219] Iteration 700 (2024.34 iter/s, 0.0493988s/100 iters), loss = 0.619144
I0428 19:28:14.399869 21566 solver.cpp:238]     Train net output #0: loss = 0.619144 (* 1 = 0.619144 loss)
I0428 19:28:14.399895 21566 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:28:14.446637 21566 solver.cpp:219] Iteration 800 (2138.62 iter/s, 0.046759s/100 iters), loss = 0.602887
I0428 19:28:14.446661 21566 solver.cpp:238]     Train net output #0: loss = 0.602887 (* 1 = 0.602887 loss)
I0428 19:28:14.446668 21566 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:28:14.463193 21566 blocking_queue.cpp:49] Waiting for data
I0428 19:28:14.493738 21566 solver.cpp:219] Iteration 900 (2124.53 iter/s, 0.0470692s/100 iters), loss = 0.408512
I0428 19:28:14.493763 21566 solver.cpp:238]     Train net output #0: loss = 0.408512 (* 1 = 0.408512 loss)
I0428 19:28:14.493772 21566 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:28:14.510084 21572 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:28:14.539631 21566 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:28:14.540280 21566 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:28:14.540683 21566 solver.cpp:311] Iteration 1000, loss = 0.522532
I0428 19:28:14.540699 21566 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:28:14.596396 21573 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:28:14.596801 21566 solver.cpp:398]     Test net output #0: accuracy = 0.8243
I0428 19:28:14.596848 21566 solver.cpp:398]     Test net output #1: loss = 0.505633 (* 1 = 0.505633 loss)
I0428 19:28:14.596856 21566 solver.cpp:316] Optimization Done.
I0428 19:28:14.596860 21566 caffe.cpp:259] Optimization Done.
