I0428 20:31:47.723629  4006 caffe.cpp:218] Using GPUs 0
I0428 20:31:47.764528  4006 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:31:48.313988  4006 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1561.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:31:48.314129  4006 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1561.prototxt
I0428 20:31:48.314473  4006 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:31:48.314491  4006 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:31:48.314571  4006 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0428 20:31:48.314643  4006 layer_factory.hpp:77] Creating layer mnist
I0428 20:31:48.314744  4006 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:31:48.314769  4006 net.cpp:86] Creating Layer mnist
I0428 20:31:48.314777  4006 net.cpp:382] mnist -> data
I0428 20:31:48.314800  4006 net.cpp:382] mnist -> label
I0428 20:31:48.315898  4006 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:31:48.318373  4006 net.cpp:124] Setting up mnist
I0428 20:31:48.318392  4006 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:31:48.318397  4006 net.cpp:131] Top shape: 64 (64)
I0428 20:31:48.318401  4006 net.cpp:139] Memory required for data: 200960
I0428 20:31:48.318408  4006 layer_factory.hpp:77] Creating layer conv0
I0428 20:31:48.318451  4006 net.cpp:86] Creating Layer conv0
I0428 20:31:48.318467  4006 net.cpp:408] conv0 <- data
I0428 20:31:48.318482  4006 net.cpp:382] conv0 -> conv0
I0428 20:31:48.611393  4006 net.cpp:124] Setting up conv0
I0428 20:31:48.611426  4006 net.cpp:131] Top shape: 64 100 24 24 (3686400)
I0428 20:31:48.611431  4006 net.cpp:139] Memory required for data: 14946560
I0428 20:31:48.611449  4006 layer_factory.hpp:77] Creating layer pool0
I0428 20:31:48.611464  4006 net.cpp:86] Creating Layer pool0
I0428 20:31:48.611469  4006 net.cpp:408] pool0 <- conv0
I0428 20:31:48.611475  4006 net.cpp:382] pool0 -> pool0
I0428 20:31:48.611532  4006 net.cpp:124] Setting up pool0
I0428 20:31:48.611539  4006 net.cpp:131] Top shape: 64 100 12 12 (921600)
I0428 20:31:48.611562  4006 net.cpp:139] Memory required for data: 18632960
I0428 20:31:48.611565  4006 layer_factory.hpp:77] Creating layer conv1
I0428 20:31:48.611578  4006 net.cpp:86] Creating Layer conv1
I0428 20:31:48.611582  4006 net.cpp:408] conv1 <- pool0
I0428 20:31:48.611589  4006 net.cpp:382] conv1 -> conv1
I0428 20:31:48.614187  4006 net.cpp:124] Setting up conv1
I0428 20:31:48.614205  4006 net.cpp:131] Top shape: 64 25 8 8 (102400)
I0428 20:31:48.614210  4006 net.cpp:139] Memory required for data: 19042560
I0428 20:31:48.614222  4006 layer_factory.hpp:77] Creating layer pool1
I0428 20:31:48.614229  4006 net.cpp:86] Creating Layer pool1
I0428 20:31:48.614234  4006 net.cpp:408] pool1 <- conv1
I0428 20:31:48.614241  4006 net.cpp:382] pool1 -> pool1
I0428 20:31:48.614287  4006 net.cpp:124] Setting up pool1
I0428 20:31:48.614295  4006 net.cpp:131] Top shape: 64 25 4 4 (25600)
I0428 20:31:48.614298  4006 net.cpp:139] Memory required for data: 19144960
I0428 20:31:48.614302  4006 layer_factory.hpp:77] Creating layer ip1
I0428 20:31:48.614315  4006 net.cpp:86] Creating Layer ip1
I0428 20:31:48.614318  4006 net.cpp:408] ip1 <- pool1
I0428 20:31:48.614325  4006 net.cpp:382] ip1 -> ip1
I0428 20:31:48.614478  4006 net.cpp:124] Setting up ip1
I0428 20:31:48.614490  4006 net.cpp:131] Top shape: 64 10 (640)
I0428 20:31:48.614492  4006 net.cpp:139] Memory required for data: 19147520
I0428 20:31:48.614501  4006 layer_factory.hpp:77] Creating layer relu1
I0428 20:31:48.614509  4006 net.cpp:86] Creating Layer relu1
I0428 20:31:48.614513  4006 net.cpp:408] relu1 <- ip1
I0428 20:31:48.614518  4006 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:31:48.614729  4006 net.cpp:124] Setting up relu1
I0428 20:31:48.614742  4006 net.cpp:131] Top shape: 64 10 (640)
I0428 20:31:48.614745  4006 net.cpp:139] Memory required for data: 19150080
I0428 20:31:48.614748  4006 layer_factory.hpp:77] Creating layer loss
I0428 20:31:48.614758  4006 net.cpp:86] Creating Layer loss
I0428 20:31:48.614761  4006 net.cpp:408] loss <- ip1
I0428 20:31:48.614766  4006 net.cpp:408] loss <- label
I0428 20:31:48.614773  4006 net.cpp:382] loss -> loss
I0428 20:31:48.614792  4006 layer_factory.hpp:77] Creating layer loss
I0428 20:31:48.615752  4006 net.cpp:124] Setting up loss
I0428 20:31:48.615768  4006 net.cpp:131] Top shape: (1)
I0428 20:31:48.615772  4006 net.cpp:134]     with loss weight 1
I0428 20:31:48.615789  4006 net.cpp:139] Memory required for data: 19150084
I0428 20:31:48.615793  4006 net.cpp:200] loss needs backward computation.
I0428 20:31:48.615799  4006 net.cpp:200] relu1 needs backward computation.
I0428 20:31:48.615803  4006 net.cpp:200] ip1 needs backward computation.
I0428 20:31:48.615806  4006 net.cpp:200] pool1 needs backward computation.
I0428 20:31:48.615810  4006 net.cpp:200] conv1 needs backward computation.
I0428 20:31:48.615813  4006 net.cpp:200] pool0 needs backward computation.
I0428 20:31:48.615818  4006 net.cpp:200] conv0 needs backward computation.
I0428 20:31:48.615821  4006 net.cpp:202] mnist does not need backward computation.
I0428 20:31:48.615824  4006 net.cpp:244] This network produces output loss
I0428 20:31:48.615833  4006 net.cpp:257] Network initialization done.
I0428 20:31:48.616140  4006 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1561.prototxt
I0428 20:31:48.616168  4006 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:31:48.616256  4006 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0428 20:31:48.616348  4006 layer_factory.hpp:77] Creating layer mnist
I0428 20:31:48.616397  4006 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:31:48.616415  4006 net.cpp:86] Creating Layer mnist
I0428 20:31:48.616421  4006 net.cpp:382] mnist -> data
I0428 20:31:48.616430  4006 net.cpp:382] mnist -> label
I0428 20:31:48.616539  4006 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:31:48.618738  4006 net.cpp:124] Setting up mnist
I0428 20:31:48.618757  4006 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:31:48.618762  4006 net.cpp:131] Top shape: 100 (100)
I0428 20:31:48.618767  4006 net.cpp:139] Memory required for data: 314000
I0428 20:31:48.618770  4006 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:31:48.618788  4006 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:31:48.618791  4006 net.cpp:408] label_mnist_1_split <- label
I0428 20:31:48.618798  4006 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:31:48.618805  4006 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:31:48.618857  4006 net.cpp:124] Setting up label_mnist_1_split
I0428 20:31:48.618865  4006 net.cpp:131] Top shape: 100 (100)
I0428 20:31:48.618870  4006 net.cpp:131] Top shape: 100 (100)
I0428 20:31:48.618872  4006 net.cpp:139] Memory required for data: 314800
I0428 20:31:48.618876  4006 layer_factory.hpp:77] Creating layer conv0
I0428 20:31:48.618887  4006 net.cpp:86] Creating Layer conv0
I0428 20:31:48.618892  4006 net.cpp:408] conv0 <- data
I0428 20:31:48.618898  4006 net.cpp:382] conv0 -> conv0
I0428 20:31:48.620009  4006 net.cpp:124] Setting up conv0
I0428 20:31:48.620028  4006 net.cpp:131] Top shape: 100 100 24 24 (5760000)
I0428 20:31:48.620031  4006 net.cpp:139] Memory required for data: 23354800
I0428 20:31:48.620043  4006 layer_factory.hpp:77] Creating layer pool0
I0428 20:31:48.620050  4006 net.cpp:86] Creating Layer pool0
I0428 20:31:48.620054  4006 net.cpp:408] pool0 <- conv0
I0428 20:31:48.620060  4006 net.cpp:382] pool0 -> pool0
I0428 20:31:48.620105  4006 net.cpp:124] Setting up pool0
I0428 20:31:48.620112  4006 net.cpp:131] Top shape: 100 100 12 12 (1440000)
I0428 20:31:48.620116  4006 net.cpp:139] Memory required for data: 29114800
I0428 20:31:48.620120  4006 layer_factory.hpp:77] Creating layer conv1
I0428 20:31:48.620131  4006 net.cpp:86] Creating Layer conv1
I0428 20:31:48.620136  4006 net.cpp:408] conv1 <- pool0
I0428 20:31:48.620141  4006 net.cpp:382] conv1 -> conv1
I0428 20:31:48.622452  4006 net.cpp:124] Setting up conv1
I0428 20:31:48.622468  4006 net.cpp:131] Top shape: 100 25 8 8 (160000)
I0428 20:31:48.622473  4006 net.cpp:139] Memory required for data: 29754800
I0428 20:31:48.622484  4006 layer_factory.hpp:77] Creating layer pool1
I0428 20:31:48.622493  4006 net.cpp:86] Creating Layer pool1
I0428 20:31:48.622509  4006 net.cpp:408] pool1 <- conv1
I0428 20:31:48.622517  4006 net.cpp:382] pool1 -> pool1
I0428 20:31:48.622563  4006 net.cpp:124] Setting up pool1
I0428 20:31:48.622572  4006 net.cpp:131] Top shape: 100 25 4 4 (40000)
I0428 20:31:48.622584  4006 net.cpp:139] Memory required for data: 29914800
I0428 20:31:48.622588  4006 layer_factory.hpp:77] Creating layer ip1
I0428 20:31:48.622596  4006 net.cpp:86] Creating Layer ip1
I0428 20:31:48.622601  4006 net.cpp:408] ip1 <- pool1
I0428 20:31:48.622607  4006 net.cpp:382] ip1 -> ip1
I0428 20:31:48.622758  4006 net.cpp:124] Setting up ip1
I0428 20:31:48.622768  4006 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:31:48.622772  4006 net.cpp:139] Memory required for data: 29918800
I0428 20:31:48.622782  4006 layer_factory.hpp:77] Creating layer relu1
I0428 20:31:48.622788  4006 net.cpp:86] Creating Layer relu1
I0428 20:31:48.622792  4006 net.cpp:408] relu1 <- ip1
I0428 20:31:48.622799  4006 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:31:48.623755  4006 net.cpp:124] Setting up relu1
I0428 20:31:48.623770  4006 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:31:48.623775  4006 net.cpp:139] Memory required for data: 29922800
I0428 20:31:48.623778  4006 layer_factory.hpp:77] Creating layer ip1_relu1_0_split
I0428 20:31:48.623786  4006 net.cpp:86] Creating Layer ip1_relu1_0_split
I0428 20:31:48.623791  4006 net.cpp:408] ip1_relu1_0_split <- ip1
I0428 20:31:48.623798  4006 net.cpp:382] ip1_relu1_0_split -> ip1_relu1_0_split_0
I0428 20:31:48.623806  4006 net.cpp:382] ip1_relu1_0_split -> ip1_relu1_0_split_1
I0428 20:31:48.623854  4006 net.cpp:124] Setting up ip1_relu1_0_split
I0428 20:31:48.623863  4006 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:31:48.623868  4006 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:31:48.623872  4006 net.cpp:139] Memory required for data: 29930800
I0428 20:31:48.623877  4006 layer_factory.hpp:77] Creating layer accuracy
I0428 20:31:48.623883  4006 net.cpp:86] Creating Layer accuracy
I0428 20:31:48.623888  4006 net.cpp:408] accuracy <- ip1_relu1_0_split_0
I0428 20:31:48.623893  4006 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:31:48.623898  4006 net.cpp:382] accuracy -> accuracy
I0428 20:31:48.623906  4006 net.cpp:124] Setting up accuracy
I0428 20:31:48.623911  4006 net.cpp:131] Top shape: (1)
I0428 20:31:48.623915  4006 net.cpp:139] Memory required for data: 29930804
I0428 20:31:48.623919  4006 layer_factory.hpp:77] Creating layer loss
I0428 20:31:48.623924  4006 net.cpp:86] Creating Layer loss
I0428 20:31:48.623929  4006 net.cpp:408] loss <- ip1_relu1_0_split_1
I0428 20:31:48.623934  4006 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:31:48.623939  4006 net.cpp:382] loss -> loss
I0428 20:31:48.623953  4006 layer_factory.hpp:77] Creating layer loss
I0428 20:31:48.624251  4006 net.cpp:124] Setting up loss
I0428 20:31:48.624265  4006 net.cpp:131] Top shape: (1)
I0428 20:31:48.624269  4006 net.cpp:134]     with loss weight 1
I0428 20:31:48.624277  4006 net.cpp:139] Memory required for data: 29930808
I0428 20:31:48.624282  4006 net.cpp:200] loss needs backward computation.
I0428 20:31:48.624286  4006 net.cpp:202] accuracy does not need backward computation.
I0428 20:31:48.624291  4006 net.cpp:200] ip1_relu1_0_split needs backward computation.
I0428 20:31:48.624294  4006 net.cpp:200] relu1 needs backward computation.
I0428 20:31:48.624299  4006 net.cpp:200] ip1 needs backward computation.
I0428 20:31:48.624303  4006 net.cpp:200] pool1 needs backward computation.
I0428 20:31:48.624320  4006 net.cpp:200] conv1 needs backward computation.
I0428 20:31:48.624323  4006 net.cpp:200] pool0 needs backward computation.
I0428 20:31:48.624327  4006 net.cpp:200] conv0 needs backward computation.
I0428 20:31:48.624332  4006 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:31:48.624336  4006 net.cpp:202] mnist does not need backward computation.
I0428 20:31:48.624341  4006 net.cpp:244] This network produces output accuracy
I0428 20:31:48.624344  4006 net.cpp:244] This network produces output loss
I0428 20:31:48.624366  4006 net.cpp:257] Network initialization done.
I0428 20:31:48.624408  4006 solver.cpp:56] Solver scaffolding done.
I0428 20:31:48.624658  4006 caffe.cpp:248] Starting Optimization
I0428 20:31:48.624666  4006 solver.cpp:273] Solving LeNet
I0428 20:31:48.624670  4006 solver.cpp:274] Learning Rate Policy: inv
I0428 20:31:48.624873  4006 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:31:48.720639  4013 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:31:48.723012  4006 solver.cpp:398]     Test net output #0: accuracy = 0.1242
I0428 20:31:48.723037  4006 solver.cpp:398]     Test net output #1: loss = 2.3714 (* 1 = 2.3714 loss)
I0428 20:31:48.727776  4006 solver.cpp:219] Iteration 0 (-4.00546e-31 iter/s, 0.103074s/100 iters), loss = 2.41575
I0428 20:31:48.727807  4006 solver.cpp:238]     Train net output #0: loss = 2.41575 (* 1 = 2.41575 loss)
I0428 20:31:48.727820  4006 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:31:48.939599  4006 solver.cpp:219] Iteration 100 (472.235 iter/s, 0.211759s/100 iters), loss = 0.163725
I0428 20:31:48.939653  4006 solver.cpp:238]     Train net output #0: loss = 0.163725 (* 1 = 0.163725 loss)
I0428 20:31:48.939667  4006 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:31:49.150671  4006 solver.cpp:219] Iteration 200 (473.928 iter/s, 0.211002s/100 iters), loss = 0.119387
I0428 20:31:49.150727  4006 solver.cpp:238]     Train net output #0: loss = 0.119387 (* 1 = 0.119387 loss)
I0428 20:31:49.150740  4006 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:31:49.361572  4006 solver.cpp:219] Iteration 300 (474.32 iter/s, 0.210828s/100 iters), loss = 0.126341
I0428 20:31:49.361624  4006 solver.cpp:238]     Train net output #0: loss = 0.126341 (* 1 = 0.126341 loss)
I0428 20:31:49.361639  4006 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:31:49.571029  4006 solver.cpp:219] Iteration 400 (477.581 iter/s, 0.209389s/100 iters), loss = 0.0482133
I0428 20:31:49.571084  4006 solver.cpp:238]     Train net output #0: loss = 0.0482132 (* 1 = 0.0482132 loss)
I0428 20:31:49.571097  4006 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:31:49.780017  4006 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:31:49.878552  4013 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:31:49.881036  4006 solver.cpp:398]     Test net output #0: accuracy = 0.9678
I0428 20:31:49.881067  4006 solver.cpp:398]     Test net output #1: loss = 0.105483 (* 1 = 0.105483 loss)
I0428 20:31:49.882987  4006 solver.cpp:219] Iteration 500 (320.628 iter/s, 0.311888s/100 iters), loss = 0.142155
I0428 20:31:49.883018  4006 solver.cpp:238]     Train net output #0: loss = 0.142154 (* 1 = 0.142154 loss)
I0428 20:31:49.883028  4006 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:31:50.083755  4006 solver.cpp:219] Iteration 600 (498.206 iter/s, 0.20072s/100 iters), loss = 0.0994646
I0428 20:31:50.083787  4006 solver.cpp:238]     Train net output #0: loss = 0.0994646 (* 1 = 0.0994646 loss)
I0428 20:31:50.083796  4006 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:31:50.281327  4006 solver.cpp:219] Iteration 700 (506.261 iter/s, 0.197526s/100 iters), loss = 0.0944218
I0428 20:31:50.281366  4006 solver.cpp:238]     Train net output #0: loss = 0.0944218 (* 1 = 0.0944218 loss)
I0428 20:31:50.281373  4006 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:31:50.479629  4006 solver.cpp:219] Iteration 800 (504.378 iter/s, 0.198264s/100 iters), loss = 0.232115
I0428 20:31:50.479670  4006 solver.cpp:238]     Train net output #0: loss = 0.232115 (* 1 = 0.232115 loss)
I0428 20:31:50.479676  4006 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:31:50.678717  4006 solver.cpp:219] Iteration 900 (502.388 iter/s, 0.199049s/100 iters), loss = 0.126936
I0428 20:31:50.678757  4006 solver.cpp:238]     Train net output #0: loss = 0.126936 (* 1 = 0.126936 loss)
I0428 20:31:50.678763  4006 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:31:50.745074  4012 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:31:50.875839  4006 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:31:50.878190  4006 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:31:50.879634  4006 solver.cpp:311] Iteration 1000, loss = 0.0856703
I0428 20:31:50.879652  4006 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:31:50.970317  4013 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:31:50.971791  4006 solver.cpp:398]     Test net output #0: accuracy = 0.9751
I0428 20:31:50.971812  4006 solver.cpp:398]     Test net output #1: loss = 0.0732184 (* 1 = 0.0732184 loss)
I0428 20:31:50.971817  4006 solver.cpp:316] Optimization Done.
I0428 20:31:50.971819  4006 caffe.cpp:259] Optimization Done.
