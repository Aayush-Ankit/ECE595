I0428 19:49:09.325140 26687 caffe.cpp:218] Using GPUs 0
I0428 19:49:09.361619 26687 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:49:09.816308 26687 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test566.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:49:09.817010 26687 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test566.prototxt
I0428 19:49:09.817432 26687 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:49:09.817463 26687 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:49:09.817548 26687 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:49:09.817626 26687 layer_factory.hpp:77] Creating layer mnist
I0428 19:49:09.817708 26687 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:49:09.817728 26687 net.cpp:86] Creating Layer mnist
I0428 19:49:09.817734 26687 net.cpp:382] mnist -> data
I0428 19:49:09.817754 26687 net.cpp:382] mnist -> label
I0428 19:49:09.818825 26687 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:49:09.821131 26687 net.cpp:124] Setting up mnist
I0428 19:49:09.821173 26687 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:49:09.821193 26687 net.cpp:131] Top shape: 64 (64)
I0428 19:49:09.821197 26687 net.cpp:139] Memory required for data: 200960
I0428 19:49:09.821216 26687 layer_factory.hpp:77] Creating layer conv0
I0428 19:49:09.821229 26687 net.cpp:86] Creating Layer conv0
I0428 19:49:09.821245 26687 net.cpp:408] conv0 <- data
I0428 19:49:09.821255 26687 net.cpp:382] conv0 -> conv0
I0428 19:49:10.048882 26687 net.cpp:124] Setting up conv0
I0428 19:49:10.048926 26687 net.cpp:131] Top shape: 64 5 24 24 (184320)
I0428 19:49:10.048930 26687 net.cpp:139] Memory required for data: 938240
I0428 19:49:10.048944 26687 layer_factory.hpp:77] Creating layer pool0
I0428 19:49:10.048959 26687 net.cpp:86] Creating Layer pool0
I0428 19:49:10.048977 26687 net.cpp:408] pool0 <- conv0
I0428 19:49:10.048984 26687 net.cpp:382] pool0 -> pool0
I0428 19:49:10.049029 26687 net.cpp:124] Setting up pool0
I0428 19:49:10.049036 26687 net.cpp:131] Top shape: 64 5 12 12 (46080)
I0428 19:49:10.049041 26687 net.cpp:139] Memory required for data: 1122560
I0428 19:49:10.049042 26687 layer_factory.hpp:77] Creating layer conv1
I0428 19:49:10.049053 26687 net.cpp:86] Creating Layer conv1
I0428 19:49:10.049057 26687 net.cpp:408] conv1 <- pool0
I0428 19:49:10.049062 26687 net.cpp:382] conv1 -> conv1
I0428 19:49:10.050920 26687 net.cpp:124] Setting up conv1
I0428 19:49:10.050950 26687 net.cpp:131] Top shape: 64 5 8 8 (20480)
I0428 19:49:10.050953 26687 net.cpp:139] Memory required for data: 1204480
I0428 19:49:10.050962 26687 layer_factory.hpp:77] Creating layer pool1
I0428 19:49:10.050969 26687 net.cpp:86] Creating Layer pool1
I0428 19:49:10.050973 26687 net.cpp:408] pool1 <- conv1
I0428 19:49:10.050978 26687 net.cpp:382] pool1 -> pool1
I0428 19:49:10.051015 26687 net.cpp:124] Setting up pool1
I0428 19:49:10.051023 26687 net.cpp:131] Top shape: 64 5 4 4 (5120)
I0428 19:49:10.051025 26687 net.cpp:139] Memory required for data: 1224960
I0428 19:49:10.051028 26687 layer_factory.hpp:77] Creating layer ip1
I0428 19:49:10.051035 26687 net.cpp:86] Creating Layer ip1
I0428 19:49:10.051039 26687 net.cpp:408] ip1 <- pool1
I0428 19:49:10.051043 26687 net.cpp:382] ip1 -> ip1
I0428 19:49:10.051142 26687 net.cpp:124] Setting up ip1
I0428 19:49:10.051149 26687 net.cpp:131] Top shape: 64 10 (640)
I0428 19:49:10.051152 26687 net.cpp:139] Memory required for data: 1227520
I0428 19:49:10.051159 26687 layer_factory.hpp:77] Creating layer relu1
I0428 19:49:10.051165 26687 net.cpp:86] Creating Layer relu1
I0428 19:49:10.051168 26687 net.cpp:408] relu1 <- ip1
I0428 19:49:10.051173 26687 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:49:10.051348 26687 net.cpp:124] Setting up relu1
I0428 19:49:10.051357 26687 net.cpp:131] Top shape: 64 10 (640)
I0428 19:49:10.051360 26687 net.cpp:139] Memory required for data: 1230080
I0428 19:49:10.051363 26687 layer_factory.hpp:77] Creating layer ip2
I0428 19:49:10.051369 26687 net.cpp:86] Creating Layer ip2
I0428 19:49:10.051373 26687 net.cpp:408] ip2 <- ip1
I0428 19:49:10.051378 26687 net.cpp:382] ip2 -> ip2
I0428 19:49:10.051487 26687 net.cpp:124] Setting up ip2
I0428 19:49:10.051494 26687 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:49:10.051498 26687 net.cpp:139] Memory required for data: 1242880
I0428 19:49:10.051503 26687 layer_factory.hpp:77] Creating layer relu2
I0428 19:49:10.051509 26687 net.cpp:86] Creating Layer relu2
I0428 19:49:10.051513 26687 net.cpp:408] relu2 <- ip2
I0428 19:49:10.051517 26687 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:49:10.052260 26687 net.cpp:124] Setting up relu2
I0428 19:49:10.052273 26687 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:49:10.052291 26687 net.cpp:139] Memory required for data: 1255680
I0428 19:49:10.052294 26687 layer_factory.hpp:77] Creating layer ip3
I0428 19:49:10.052301 26687 net.cpp:86] Creating Layer ip3
I0428 19:49:10.052304 26687 net.cpp:408] ip3 <- ip2
I0428 19:49:10.052310 26687 net.cpp:382] ip3 -> ip3
I0428 19:49:10.052420 26687 net.cpp:124] Setting up ip3
I0428 19:49:10.052428 26687 net.cpp:131] Top shape: 64 10 (640)
I0428 19:49:10.052430 26687 net.cpp:139] Memory required for data: 1258240
I0428 19:49:10.052438 26687 layer_factory.hpp:77] Creating layer relu3
I0428 19:49:10.052443 26687 net.cpp:86] Creating Layer relu3
I0428 19:49:10.052445 26687 net.cpp:408] relu3 <- ip3
I0428 19:49:10.052449 26687 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:49:10.052628 26687 net.cpp:124] Setting up relu3
I0428 19:49:10.052636 26687 net.cpp:131] Top shape: 64 10 (640)
I0428 19:49:10.052639 26687 net.cpp:139] Memory required for data: 1260800
I0428 19:49:10.052642 26687 layer_factory.hpp:77] Creating layer loss
I0428 19:49:10.052649 26687 net.cpp:86] Creating Layer loss
I0428 19:49:10.052651 26687 net.cpp:408] loss <- ip3
I0428 19:49:10.052655 26687 net.cpp:408] loss <- label
I0428 19:49:10.052660 26687 net.cpp:382] loss -> loss
I0428 19:49:10.052673 26687 layer_factory.hpp:77] Creating layer loss
I0428 19:49:10.052922 26687 net.cpp:124] Setting up loss
I0428 19:49:10.052932 26687 net.cpp:131] Top shape: (1)
I0428 19:49:10.052935 26687 net.cpp:134]     with loss weight 1
I0428 19:49:10.052952 26687 net.cpp:139] Memory required for data: 1260804
I0428 19:49:10.052954 26687 net.cpp:200] loss needs backward computation.
I0428 19:49:10.052958 26687 net.cpp:200] relu3 needs backward computation.
I0428 19:49:10.052961 26687 net.cpp:200] ip3 needs backward computation.
I0428 19:49:10.052964 26687 net.cpp:200] relu2 needs backward computation.
I0428 19:49:10.052966 26687 net.cpp:200] ip2 needs backward computation.
I0428 19:49:10.052969 26687 net.cpp:200] relu1 needs backward computation.
I0428 19:49:10.052973 26687 net.cpp:200] ip1 needs backward computation.
I0428 19:49:10.052975 26687 net.cpp:200] pool1 needs backward computation.
I0428 19:49:10.052978 26687 net.cpp:200] conv1 needs backward computation.
I0428 19:49:10.052981 26687 net.cpp:200] pool0 needs backward computation.
I0428 19:49:10.052984 26687 net.cpp:200] conv0 needs backward computation.
I0428 19:49:10.052989 26687 net.cpp:202] mnist does not need backward computation.
I0428 19:49:10.052990 26687 net.cpp:244] This network produces output loss
I0428 19:49:10.052999 26687 net.cpp:257] Network initialization done.
I0428 19:49:10.053375 26687 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test566.prototxt
I0428 19:49:10.053417 26687 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:49:10.053516 26687 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:49:10.053593 26687 layer_factory.hpp:77] Creating layer mnist
I0428 19:49:10.053635 26687 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:49:10.053647 26687 net.cpp:86] Creating Layer mnist
I0428 19:49:10.053652 26687 net.cpp:382] mnist -> data
I0428 19:49:10.053659 26687 net.cpp:382] mnist -> label
I0428 19:49:10.053735 26687 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:49:10.055704 26687 net.cpp:124] Setting up mnist
I0428 19:49:10.055732 26687 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:49:10.055738 26687 net.cpp:131] Top shape: 100 (100)
I0428 19:49:10.055742 26687 net.cpp:139] Memory required for data: 314000
I0428 19:49:10.055744 26687 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:49:10.055750 26687 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:49:10.055753 26687 net.cpp:408] label_mnist_1_split <- label
I0428 19:49:10.055758 26687 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:49:10.055764 26687 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:49:10.055806 26687 net.cpp:124] Setting up label_mnist_1_split
I0428 19:49:10.055811 26687 net.cpp:131] Top shape: 100 (100)
I0428 19:49:10.055815 26687 net.cpp:131] Top shape: 100 (100)
I0428 19:49:10.055817 26687 net.cpp:139] Memory required for data: 314800
I0428 19:49:10.055820 26687 layer_factory.hpp:77] Creating layer conv0
I0428 19:49:10.055827 26687 net.cpp:86] Creating Layer conv0
I0428 19:49:10.055830 26687 net.cpp:408] conv0 <- data
I0428 19:49:10.055835 26687 net.cpp:382] conv0 -> conv0
I0428 19:49:10.057559 26687 net.cpp:124] Setting up conv0
I0428 19:49:10.057572 26687 net.cpp:131] Top shape: 100 5 24 24 (288000)
I0428 19:49:10.057576 26687 net.cpp:139] Memory required for data: 1466800
I0428 19:49:10.057585 26687 layer_factory.hpp:77] Creating layer pool0
I0428 19:49:10.057606 26687 net.cpp:86] Creating Layer pool0
I0428 19:49:10.057610 26687 net.cpp:408] pool0 <- conv0
I0428 19:49:10.057615 26687 net.cpp:382] pool0 -> pool0
I0428 19:49:10.057648 26687 net.cpp:124] Setting up pool0
I0428 19:49:10.057653 26687 net.cpp:131] Top shape: 100 5 12 12 (72000)
I0428 19:49:10.057657 26687 net.cpp:139] Memory required for data: 1754800
I0428 19:49:10.057659 26687 layer_factory.hpp:77] Creating layer conv1
I0428 19:49:10.057667 26687 net.cpp:86] Creating Layer conv1
I0428 19:49:10.057670 26687 net.cpp:408] conv1 <- pool0
I0428 19:49:10.057675 26687 net.cpp:382] conv1 -> conv1
I0428 19:49:10.059727 26687 net.cpp:124] Setting up conv1
I0428 19:49:10.059741 26687 net.cpp:131] Top shape: 100 5 8 8 (32000)
I0428 19:49:10.059744 26687 net.cpp:139] Memory required for data: 1882800
I0428 19:49:10.059753 26687 layer_factory.hpp:77] Creating layer pool1
I0428 19:49:10.059759 26687 net.cpp:86] Creating Layer pool1
I0428 19:49:10.059762 26687 net.cpp:408] pool1 <- conv1
I0428 19:49:10.059768 26687 net.cpp:382] pool1 -> pool1
I0428 19:49:10.059823 26687 net.cpp:124] Setting up pool1
I0428 19:49:10.059829 26687 net.cpp:131] Top shape: 100 5 4 4 (8000)
I0428 19:49:10.059833 26687 net.cpp:139] Memory required for data: 1914800
I0428 19:49:10.059835 26687 layer_factory.hpp:77] Creating layer ip1
I0428 19:49:10.059840 26687 net.cpp:86] Creating Layer ip1
I0428 19:49:10.059844 26687 net.cpp:408] ip1 <- pool1
I0428 19:49:10.059849 26687 net.cpp:382] ip1 -> ip1
I0428 19:49:10.059998 26687 net.cpp:124] Setting up ip1
I0428 19:49:10.060004 26687 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:49:10.060019 26687 net.cpp:139] Memory required for data: 1918800
I0428 19:49:10.060027 26687 layer_factory.hpp:77] Creating layer relu1
I0428 19:49:10.060037 26687 net.cpp:86] Creating Layer relu1
I0428 19:49:10.060040 26687 net.cpp:408] relu1 <- ip1
I0428 19:49:10.060046 26687 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:49:10.060196 26687 net.cpp:124] Setting up relu1
I0428 19:49:10.060204 26687 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:49:10.060207 26687 net.cpp:139] Memory required for data: 1922800
I0428 19:49:10.060210 26687 layer_factory.hpp:77] Creating layer ip2
I0428 19:49:10.060217 26687 net.cpp:86] Creating Layer ip2
I0428 19:49:10.060221 26687 net.cpp:408] ip2 <- ip1
I0428 19:49:10.060225 26687 net.cpp:382] ip2 -> ip2
I0428 19:49:10.060346 26687 net.cpp:124] Setting up ip2
I0428 19:49:10.060353 26687 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:49:10.060356 26687 net.cpp:139] Memory required for data: 1942800
I0428 19:49:10.060361 26687 layer_factory.hpp:77] Creating layer relu2
I0428 19:49:10.060365 26687 net.cpp:86] Creating Layer relu2
I0428 19:49:10.060369 26687 net.cpp:408] relu2 <- ip2
I0428 19:49:10.060372 26687 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:49:10.060505 26687 net.cpp:124] Setting up relu2
I0428 19:49:10.060513 26687 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:49:10.060516 26687 net.cpp:139] Memory required for data: 1962800
I0428 19:49:10.060525 26687 layer_factory.hpp:77] Creating layer ip3
I0428 19:49:10.060530 26687 net.cpp:86] Creating Layer ip3
I0428 19:49:10.060534 26687 net.cpp:408] ip3 <- ip2
I0428 19:49:10.060539 26687 net.cpp:382] ip3 -> ip3
I0428 19:49:10.060631 26687 net.cpp:124] Setting up ip3
I0428 19:49:10.060653 26687 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:49:10.060657 26687 net.cpp:139] Memory required for data: 1966800
I0428 19:49:10.060663 26687 layer_factory.hpp:77] Creating layer relu3
I0428 19:49:10.060667 26687 net.cpp:86] Creating Layer relu3
I0428 19:49:10.060670 26687 net.cpp:408] relu3 <- ip3
I0428 19:49:10.060674 26687 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:49:10.061578 26687 net.cpp:124] Setting up relu3
I0428 19:49:10.061591 26687 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:49:10.061595 26687 net.cpp:139] Memory required for data: 1970800
I0428 19:49:10.061599 26687 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:49:10.061604 26687 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:49:10.061609 26687 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:49:10.061614 26687 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:49:10.061628 26687 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:49:10.061666 26687 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:49:10.061671 26687 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:49:10.061676 26687 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:49:10.061683 26687 net.cpp:139] Memory required for data: 1978800
I0428 19:49:10.061686 26687 layer_factory.hpp:77] Creating layer accuracy
I0428 19:49:10.061691 26687 net.cpp:86] Creating Layer accuracy
I0428 19:49:10.061694 26687 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:49:10.061698 26687 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:49:10.061707 26687 net.cpp:382] accuracy -> accuracy
I0428 19:49:10.061714 26687 net.cpp:124] Setting up accuracy
I0428 19:49:10.061718 26687 net.cpp:131] Top shape: (1)
I0428 19:49:10.061722 26687 net.cpp:139] Memory required for data: 1978804
I0428 19:49:10.061729 26687 layer_factory.hpp:77] Creating layer loss
I0428 19:49:10.061734 26687 net.cpp:86] Creating Layer loss
I0428 19:49:10.061738 26687 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:49:10.061741 26687 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:49:10.061745 26687 net.cpp:382] loss -> loss
I0428 19:49:10.061753 26687 layer_factory.hpp:77] Creating layer loss
I0428 19:49:10.061980 26687 net.cpp:124] Setting up loss
I0428 19:49:10.061990 26687 net.cpp:131] Top shape: (1)
I0428 19:49:10.061993 26687 net.cpp:134]     with loss weight 1
I0428 19:49:10.062000 26687 net.cpp:139] Memory required for data: 1978808
I0428 19:49:10.062012 26687 net.cpp:200] loss needs backward computation.
I0428 19:49:10.062016 26687 net.cpp:202] accuracy does not need backward computation.
I0428 19:49:10.062021 26687 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:49:10.062023 26687 net.cpp:200] relu3 needs backward computation.
I0428 19:49:10.062041 26687 net.cpp:200] ip3 needs backward computation.
I0428 19:49:10.062043 26687 net.cpp:200] relu2 needs backward computation.
I0428 19:49:10.062047 26687 net.cpp:200] ip2 needs backward computation.
I0428 19:49:10.062049 26687 net.cpp:200] relu1 needs backward computation.
I0428 19:49:10.062052 26687 net.cpp:200] ip1 needs backward computation.
I0428 19:49:10.062054 26687 net.cpp:200] pool1 needs backward computation.
I0428 19:49:10.062057 26687 net.cpp:200] conv1 needs backward computation.
I0428 19:49:10.062060 26687 net.cpp:200] pool0 needs backward computation.
I0428 19:49:10.062077 26687 net.cpp:200] conv0 needs backward computation.
I0428 19:49:10.062088 26687 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:49:10.062093 26687 net.cpp:202] mnist does not need backward computation.
I0428 19:49:10.062095 26687 net.cpp:244] This network produces output accuracy
I0428 19:49:10.062098 26687 net.cpp:244] This network produces output loss
I0428 19:49:10.062108 26687 net.cpp:257] Network initialization done.
I0428 19:49:10.062145 26687 solver.cpp:56] Solver scaffolding done.
I0428 19:49:10.062467 26687 caffe.cpp:248] Starting Optimization
I0428 19:49:10.062474 26687 solver.cpp:273] Solving LeNet
I0428 19:49:10.062477 26687 solver.cpp:274] Learning Rate Policy: inv
I0428 19:49:10.063341 26687 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:49:10.066962 26687 blocking_queue.cpp:49] Waiting for data
I0428 19:49:10.121322 26694 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:49:10.121816 26687 solver.cpp:398]     Test net output #0: accuracy = 0.0759
I0428 19:49:10.121835 26687 solver.cpp:398]     Test net output #1: loss = 2.32443 (* 1 = 2.32443 loss)
I0428 19:49:10.123702 26687 solver.cpp:219] Iteration 0 (-1.95018e-31 iter/s, 0.0611973s/100 iters), loss = 2.30104
I0428 19:49:10.123723 26687 solver.cpp:238]     Train net output #0: loss = 2.30104 (* 1 = 2.30104 loss)
I0428 19:49:10.123750 26687 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:49:10.195277 26687 solver.cpp:219] Iteration 100 (1397.72 iter/s, 0.0715448s/100 iters), loss = 0.651212
I0428 19:49:10.195302 26687 solver.cpp:238]     Train net output #0: loss = 0.651212 (* 1 = 0.651212 loss)
I0428 19:49:10.195324 26687 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:49:10.266362 26687 solver.cpp:219] Iteration 200 (1407.45 iter/s, 0.0710504s/100 iters), loss = 0.673553
I0428 19:49:10.266386 26687 solver.cpp:238]     Train net output #0: loss = 0.673553 (* 1 = 0.673553 loss)
I0428 19:49:10.266391 26687 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:49:10.338701 26687 solver.cpp:219] Iteration 300 (1383 iter/s, 0.0723067s/100 iters), loss = 0.36305
I0428 19:49:10.338723 26687 solver.cpp:238]     Train net output #0: loss = 0.36305 (* 1 = 0.36305 loss)
I0428 19:49:10.338729 26687 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:49:10.411797 26687 solver.cpp:219] Iteration 400 (1368.67 iter/s, 0.0730634s/100 iters), loss = 0.283141
I0428 19:49:10.411818 26687 solver.cpp:238]     Train net output #0: loss = 0.283141 (* 1 = 0.283141 loss)
I0428 19:49:10.411824 26687 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:49:10.488426 26687 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:49:10.543421 26694 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:49:10.543915 26687 solver.cpp:398]     Test net output #0: accuracy = 0.9327
I0428 19:49:10.543933 26687 solver.cpp:398]     Test net output #1: loss = 0.213878 (* 1 = 0.213878 loss)
I0428 19:49:10.544689 26687 solver.cpp:219] Iteration 500 (752.672 iter/s, 0.13286s/100 iters), loss = 0.27951
I0428 19:49:10.544711 26687 solver.cpp:238]     Train net output #0: loss = 0.27951 (* 1 = 0.27951 loss)
I0428 19:49:10.544739 26687 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:49:10.622578 26687 solver.cpp:219] Iteration 600 (1284.41 iter/s, 0.0778569s/100 iters), loss = 0.101641
I0428 19:49:10.622602 26687 solver.cpp:238]     Train net output #0: loss = 0.101641 (* 1 = 0.101641 loss)
I0428 19:49:10.622624 26687 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:49:10.699318 26687 solver.cpp:219] Iteration 700 (1303.67 iter/s, 0.0767068s/100 iters), loss = 0.196974
I0428 19:49:10.699358 26687 solver.cpp:238]     Train net output #0: loss = 0.196974 (* 1 = 0.196974 loss)
I0428 19:49:10.699365 26687 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:49:10.770623 26687 solver.cpp:219] Iteration 800 (1403.07 iter/s, 0.0712724s/100 iters), loss = 0.260142
I0428 19:49:10.770647 26687 solver.cpp:238]     Train net output #0: loss = 0.260142 (* 1 = 0.260142 loss)
I0428 19:49:10.770653 26687 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:49:10.844648 26687 solver.cpp:219] Iteration 900 (1351.49 iter/s, 0.0739922s/100 iters), loss = 0.285939
I0428 19:49:10.844687 26687 solver.cpp:238]     Train net output #0: loss = 0.285938 (* 1 = 0.285938 loss)
I0428 19:49:10.844693 26687 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:49:10.869590 26693 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:49:10.917744 26687 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:49:10.918397 26687 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:49:10.918864 26687 solver.cpp:311] Iteration 1000, loss = 0.16752
I0428 19:49:10.918892 26687 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:49:10.973960 26694 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:49:10.974468 26687 solver.cpp:398]     Test net output #0: accuracy = 0.9498
I0428 19:49:10.974485 26687 solver.cpp:398]     Test net output #1: loss = 0.155928 (* 1 = 0.155928 loss)
I0428 19:49:10.974490 26687 solver.cpp:316] Optimization Done.
I0428 19:49:10.974493 26687 caffe.cpp:259] Optimization Done.
