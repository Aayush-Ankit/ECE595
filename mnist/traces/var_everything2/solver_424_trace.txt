I0428 19:43:31.881392 25368 caffe.cpp:218] Using GPUs 0
I0428 19:43:31.916769 25368 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:43:32.436091 25368 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test424.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:43:32.436244 25368 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test424.prototxt
I0428 19:43:32.436669 25368 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:43:32.436698 25368 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:43:32.436820 25368 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:43:32.436933 25368 layer_factory.hpp:77] Creating layer mnist
I0428 19:43:32.437036 25368 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:43:32.437070 25368 net.cpp:86] Creating Layer mnist
I0428 19:43:32.437083 25368 net.cpp:382] mnist -> data
I0428 19:43:32.437113 25368 net.cpp:382] mnist -> label
I0428 19:43:32.438201 25368 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:43:32.440665 25368 net.cpp:124] Setting up mnist
I0428 19:43:32.440685 25368 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:43:32.440708 25368 net.cpp:131] Top shape: 64 (64)
I0428 19:43:32.440716 25368 net.cpp:139] Memory required for data: 200960
I0428 19:43:32.440726 25368 layer_factory.hpp:77] Creating layer conv0
I0428 19:43:32.440747 25368 net.cpp:86] Creating Layer conv0
I0428 19:43:32.440770 25368 net.cpp:408] conv0 <- data
I0428 19:43:32.440793 25368 net.cpp:382] conv0 -> conv0
I0428 19:43:32.730360 25368 net.cpp:124] Setting up conv0
I0428 19:43:32.730393 25368 net.cpp:131] Top shape: 64 2 24 24 (73728)
I0428 19:43:32.730401 25368 net.cpp:139] Memory required for data: 495872
I0428 19:43:32.730423 25368 layer_factory.hpp:77] Creating layer pool0
I0428 19:43:32.730443 25368 net.cpp:86] Creating Layer pool0
I0428 19:43:32.730451 25368 net.cpp:408] pool0 <- conv0
I0428 19:43:32.730465 25368 net.cpp:382] pool0 -> pool0
I0428 19:43:32.730540 25368 net.cpp:124] Setting up pool0
I0428 19:43:32.730551 25368 net.cpp:131] Top shape: 64 2 12 12 (18432)
I0428 19:43:32.730556 25368 net.cpp:139] Memory required for data: 569600
I0428 19:43:32.730562 25368 layer_factory.hpp:77] Creating layer conv1
I0428 19:43:32.730584 25368 net.cpp:86] Creating Layer conv1
I0428 19:43:32.730592 25368 net.cpp:408] conv1 <- pool0
I0428 19:43:32.730603 25368 net.cpp:382] conv1 -> conv1
I0428 19:43:32.733799 25368 net.cpp:124] Setting up conv1
I0428 19:43:32.733821 25368 net.cpp:131] Top shape: 64 25 8 8 (102400)
I0428 19:43:32.733829 25368 net.cpp:139] Memory required for data: 979200
I0428 19:43:32.733844 25368 layer_factory.hpp:77] Creating layer pool1
I0428 19:43:32.733858 25368 net.cpp:86] Creating Layer pool1
I0428 19:43:32.733873 25368 net.cpp:408] pool1 <- conv1
I0428 19:43:32.733886 25368 net.cpp:382] pool1 -> pool1
I0428 19:43:32.733940 25368 net.cpp:124] Setting up pool1
I0428 19:43:32.733950 25368 net.cpp:131] Top shape: 64 25 4 4 (25600)
I0428 19:43:32.733956 25368 net.cpp:139] Memory required for data: 1081600
I0428 19:43:32.733963 25368 layer_factory.hpp:77] Creating layer ip1
I0428 19:43:32.733978 25368 net.cpp:86] Creating Layer ip1
I0428 19:43:32.733989 25368 net.cpp:408] ip1 <- pool1
I0428 19:43:32.734001 25368 net.cpp:382] ip1 -> ip1
I0428 19:43:32.734267 25368 net.cpp:124] Setting up ip1
I0428 19:43:32.734278 25368 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:43:32.734284 25368 net.cpp:139] Memory required for data: 1094400
I0428 19:43:32.734298 25368 layer_factory.hpp:77] Creating layer relu1
I0428 19:43:32.734311 25368 net.cpp:86] Creating Layer relu1
I0428 19:43:32.734319 25368 net.cpp:408] relu1 <- ip1
I0428 19:43:32.734328 25368 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:43:32.734541 25368 net.cpp:124] Setting up relu1
I0428 19:43:32.734555 25368 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:43:32.734560 25368 net.cpp:139] Memory required for data: 1107200
I0428 19:43:32.734566 25368 layer_factory.hpp:77] Creating layer ip2
I0428 19:43:32.734580 25368 net.cpp:86] Creating Layer ip2
I0428 19:43:32.734587 25368 net.cpp:408] ip2 <- ip1
I0428 19:43:32.734599 25368 net.cpp:382] ip2 -> ip2
I0428 19:43:32.734735 25368 net.cpp:124] Setting up ip2
I0428 19:43:32.734746 25368 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:43:32.734752 25368 net.cpp:139] Memory required for data: 1113600
I0428 19:43:32.734762 25368 layer_factory.hpp:77] Creating layer relu2
I0428 19:43:32.734776 25368 net.cpp:86] Creating Layer relu2
I0428 19:43:32.734784 25368 net.cpp:408] relu2 <- ip2
I0428 19:43:32.734792 25368 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:43:32.735662 25368 net.cpp:124] Setting up relu2
I0428 19:43:32.735679 25368 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:43:32.735685 25368 net.cpp:139] Memory required for data: 1120000
I0428 19:43:32.735692 25368 layer_factory.hpp:77] Creating layer ip3
I0428 19:43:32.735707 25368 net.cpp:86] Creating Layer ip3
I0428 19:43:32.735713 25368 net.cpp:408] ip3 <- ip2
I0428 19:43:32.735726 25368 net.cpp:382] ip3 -> ip3
I0428 19:43:32.735865 25368 net.cpp:124] Setting up ip3
I0428 19:43:32.735877 25368 net.cpp:131] Top shape: 64 10 (640)
I0428 19:43:32.735882 25368 net.cpp:139] Memory required for data: 1122560
I0428 19:43:32.735898 25368 layer_factory.hpp:77] Creating layer relu3
I0428 19:43:32.735908 25368 net.cpp:86] Creating Layer relu3
I0428 19:43:32.735915 25368 net.cpp:408] relu3 <- ip3
I0428 19:43:32.735924 25368 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:43:32.736140 25368 net.cpp:124] Setting up relu3
I0428 19:43:32.736151 25368 net.cpp:131] Top shape: 64 10 (640)
I0428 19:43:32.736157 25368 net.cpp:139] Memory required for data: 1125120
I0428 19:43:32.736163 25368 layer_factory.hpp:77] Creating layer loss
I0428 19:43:32.736174 25368 net.cpp:86] Creating Layer loss
I0428 19:43:32.736181 25368 net.cpp:408] loss <- ip3
I0428 19:43:32.736188 25368 net.cpp:408] loss <- label
I0428 19:43:32.736201 25368 net.cpp:382] loss -> loss
I0428 19:43:32.736227 25368 layer_factory.hpp:77] Creating layer loss
I0428 19:43:32.736523 25368 net.cpp:124] Setting up loss
I0428 19:43:32.736536 25368 net.cpp:131] Top shape: (1)
I0428 19:43:32.736542 25368 net.cpp:134]     with loss weight 1
I0428 19:43:32.736565 25368 net.cpp:139] Memory required for data: 1125124
I0428 19:43:32.736572 25368 net.cpp:200] loss needs backward computation.
I0428 19:43:32.736580 25368 net.cpp:200] relu3 needs backward computation.
I0428 19:43:32.736587 25368 net.cpp:200] ip3 needs backward computation.
I0428 19:43:32.736593 25368 net.cpp:200] relu2 needs backward computation.
I0428 19:43:32.736598 25368 net.cpp:200] ip2 needs backward computation.
I0428 19:43:32.736604 25368 net.cpp:200] relu1 needs backward computation.
I0428 19:43:32.736609 25368 net.cpp:200] ip1 needs backward computation.
I0428 19:43:32.736615 25368 net.cpp:200] pool1 needs backward computation.
I0428 19:43:32.736623 25368 net.cpp:200] conv1 needs backward computation.
I0428 19:43:32.736629 25368 net.cpp:200] pool0 needs backward computation.
I0428 19:43:32.736634 25368 net.cpp:200] conv0 needs backward computation.
I0428 19:43:32.736640 25368 net.cpp:202] mnist does not need backward computation.
I0428 19:43:32.736646 25368 net.cpp:244] This network produces output loss
I0428 19:43:32.736668 25368 net.cpp:257] Network initialization done.
I0428 19:43:32.737073 25368 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test424.prototxt
I0428 19:43:32.737116 25368 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:43:32.737236 25368 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:43:32.737381 25368 layer_factory.hpp:77] Creating layer mnist
I0428 19:43:32.737454 25368 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:43:32.737476 25368 net.cpp:86] Creating Layer mnist
I0428 19:43:32.737486 25368 net.cpp:382] mnist -> data
I0428 19:43:32.737499 25368 net.cpp:382] mnist -> label
I0428 19:43:32.737646 25368 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:43:32.740028 25368 net.cpp:124] Setting up mnist
I0428 19:43:32.740048 25368 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:43:32.740058 25368 net.cpp:131] Top shape: 100 (100)
I0428 19:43:32.740064 25368 net.cpp:139] Memory required for data: 314000
I0428 19:43:32.740072 25368 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:43:32.740103 25368 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:43:32.740109 25368 net.cpp:408] label_mnist_1_split <- label
I0428 19:43:32.740118 25368 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:43:32.740131 25368 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:43:32.740232 25368 net.cpp:124] Setting up label_mnist_1_split
I0428 19:43:32.740242 25368 net.cpp:131] Top shape: 100 (100)
I0428 19:43:32.740250 25368 net.cpp:131] Top shape: 100 (100)
I0428 19:43:32.740255 25368 net.cpp:139] Memory required for data: 314800
I0428 19:43:32.740262 25368 layer_factory.hpp:77] Creating layer conv0
I0428 19:43:32.740279 25368 net.cpp:86] Creating Layer conv0
I0428 19:43:32.740288 25368 net.cpp:408] conv0 <- data
I0428 19:43:32.740303 25368 net.cpp:382] conv0 -> conv0
I0428 19:43:32.742117 25368 net.cpp:124] Setting up conv0
I0428 19:43:32.742136 25368 net.cpp:131] Top shape: 100 2 24 24 (115200)
I0428 19:43:32.742143 25368 net.cpp:139] Memory required for data: 775600
I0428 19:43:32.742161 25368 layer_factory.hpp:77] Creating layer pool0
I0428 19:43:32.742174 25368 net.cpp:86] Creating Layer pool0
I0428 19:43:32.742183 25368 net.cpp:408] pool0 <- conv0
I0428 19:43:32.742192 25368 net.cpp:382] pool0 -> pool0
I0428 19:43:32.742245 25368 net.cpp:124] Setting up pool0
I0428 19:43:32.742256 25368 net.cpp:131] Top shape: 100 2 12 12 (28800)
I0428 19:43:32.742261 25368 net.cpp:139] Memory required for data: 890800
I0428 19:43:32.742269 25368 layer_factory.hpp:77] Creating layer conv1
I0428 19:43:32.742285 25368 net.cpp:86] Creating Layer conv1
I0428 19:43:32.742293 25368 net.cpp:408] conv1 <- pool0
I0428 19:43:32.742305 25368 net.cpp:382] conv1 -> conv1
I0428 19:43:32.744204 25368 net.cpp:124] Setting up conv1
I0428 19:43:32.744225 25368 net.cpp:131] Top shape: 100 25 8 8 (160000)
I0428 19:43:32.744230 25368 net.cpp:139] Memory required for data: 1530800
I0428 19:43:32.744246 25368 layer_factory.hpp:77] Creating layer pool1
I0428 19:43:32.744258 25368 net.cpp:86] Creating Layer pool1
I0428 19:43:32.744266 25368 net.cpp:408] pool1 <- conv1
I0428 19:43:32.744277 25368 net.cpp:382] pool1 -> pool1
I0428 19:43:32.744334 25368 net.cpp:124] Setting up pool1
I0428 19:43:32.744345 25368 net.cpp:131] Top shape: 100 25 4 4 (40000)
I0428 19:43:32.744351 25368 net.cpp:139] Memory required for data: 1690800
I0428 19:43:32.744359 25368 layer_factory.hpp:77] Creating layer ip1
I0428 19:43:32.744371 25368 net.cpp:86] Creating Layer ip1
I0428 19:43:32.744380 25368 net.cpp:408] ip1 <- pool1
I0428 19:43:32.744397 25368 net.cpp:382] ip1 -> ip1
I0428 19:43:32.744674 25368 net.cpp:124] Setting up ip1
I0428 19:43:32.744686 25368 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:43:32.744707 25368 net.cpp:139] Memory required for data: 1710800
I0428 19:43:32.744722 25368 layer_factory.hpp:77] Creating layer relu1
I0428 19:43:32.744734 25368 net.cpp:86] Creating Layer relu1
I0428 19:43:32.744741 25368 net.cpp:408] relu1 <- ip1
I0428 19:43:32.744755 25368 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:43:32.745029 25368 net.cpp:124] Setting up relu1
I0428 19:43:32.745044 25368 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:43:32.745050 25368 net.cpp:139] Memory required for data: 1730800
I0428 19:43:32.745056 25368 layer_factory.hpp:77] Creating layer ip2
I0428 19:43:32.745072 25368 net.cpp:86] Creating Layer ip2
I0428 19:43:32.745079 25368 net.cpp:408] ip2 <- ip1
I0428 19:43:32.745090 25368 net.cpp:382] ip2 -> ip2
I0428 19:43:32.745234 25368 net.cpp:124] Setting up ip2
I0428 19:43:32.745252 25368 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:43:32.745260 25368 net.cpp:139] Memory required for data: 1740800
I0428 19:43:32.745270 25368 layer_factory.hpp:77] Creating layer relu2
I0428 19:43:32.745280 25368 net.cpp:86] Creating Layer relu2
I0428 19:43:32.745286 25368 net.cpp:408] relu2 <- ip2
I0428 19:43:32.745297 25368 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:43:32.745497 25368 net.cpp:124] Setting up relu2
I0428 19:43:32.745509 25368 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:43:32.745515 25368 net.cpp:139] Memory required for data: 1750800
I0428 19:43:32.745522 25368 layer_factory.hpp:77] Creating layer ip3
I0428 19:43:32.745535 25368 net.cpp:86] Creating Layer ip3
I0428 19:43:32.745543 25368 net.cpp:408] ip3 <- ip2
I0428 19:43:32.745553 25368 net.cpp:382] ip3 -> ip3
I0428 19:43:32.745684 25368 net.cpp:124] Setting up ip3
I0428 19:43:32.745695 25368 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:43:32.745702 25368 net.cpp:139] Memory required for data: 1754800
I0428 19:43:32.745715 25368 layer_factory.hpp:77] Creating layer relu3
I0428 19:43:32.745728 25368 net.cpp:86] Creating Layer relu3
I0428 19:43:32.745735 25368 net.cpp:408] relu3 <- ip3
I0428 19:43:32.745744 25368 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:43:32.746732 25368 net.cpp:124] Setting up relu3
I0428 19:43:32.746752 25368 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:43:32.746757 25368 net.cpp:139] Memory required for data: 1758800
I0428 19:43:32.746764 25368 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:43:32.746773 25368 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:43:32.746780 25368 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:43:32.746790 25368 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:43:32.746803 25368 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:43:32.746862 25368 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:43:32.746873 25368 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:43:32.746886 25368 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:43:32.746891 25368 net.cpp:139] Memory required for data: 1766800
I0428 19:43:32.746897 25368 layer_factory.hpp:77] Creating layer accuracy
I0428 19:43:32.746907 25368 net.cpp:86] Creating Layer accuracy
I0428 19:43:32.746914 25368 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:43:32.746922 25368 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:43:32.746933 25368 net.cpp:382] accuracy -> accuracy
I0428 19:43:32.746948 25368 net.cpp:124] Setting up accuracy
I0428 19:43:32.746958 25368 net.cpp:131] Top shape: (1)
I0428 19:43:32.746963 25368 net.cpp:139] Memory required for data: 1766804
I0428 19:43:32.746969 25368 layer_factory.hpp:77] Creating layer loss
I0428 19:43:32.746984 25368 net.cpp:86] Creating Layer loss
I0428 19:43:32.746991 25368 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:43:32.746999 25368 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:43:32.747007 25368 net.cpp:382] loss -> loss
I0428 19:43:32.747018 25368 layer_factory.hpp:77] Creating layer loss
I0428 19:43:32.747319 25368 net.cpp:124] Setting up loss
I0428 19:43:32.747333 25368 net.cpp:131] Top shape: (1)
I0428 19:43:32.747339 25368 net.cpp:134]     with loss weight 1
I0428 19:43:32.747349 25368 net.cpp:139] Memory required for data: 1766808
I0428 19:43:32.747371 25368 net.cpp:200] loss needs backward computation.
I0428 19:43:32.747380 25368 net.cpp:202] accuracy does not need backward computation.
I0428 19:43:32.747387 25368 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:43:32.747393 25368 net.cpp:200] relu3 needs backward computation.
I0428 19:43:32.747403 25368 net.cpp:200] ip3 needs backward computation.
I0428 19:43:32.747409 25368 net.cpp:200] relu2 needs backward computation.
I0428 19:43:32.747416 25368 net.cpp:200] ip2 needs backward computation.
I0428 19:43:32.747421 25368 net.cpp:200] relu1 needs backward computation.
I0428 19:43:32.747431 25368 net.cpp:200] ip1 needs backward computation.
I0428 19:43:32.747437 25368 net.cpp:200] pool1 needs backward computation.
I0428 19:43:32.747443 25368 net.cpp:200] conv1 needs backward computation.
I0428 19:43:32.747449 25368 net.cpp:200] pool0 needs backward computation.
I0428 19:43:32.747455 25368 net.cpp:200] conv0 needs backward computation.
I0428 19:43:32.747464 25368 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:43:32.747472 25368 net.cpp:202] mnist does not need backward computation.
I0428 19:43:32.747478 25368 net.cpp:244] This network produces output accuracy
I0428 19:43:32.747484 25368 net.cpp:244] This network produces output loss
I0428 19:43:32.747506 25368 net.cpp:257] Network initialization done.
I0428 19:43:32.747565 25368 solver.cpp:56] Solver scaffolding done.
I0428 19:43:32.747972 25368 caffe.cpp:248] Starting Optimization
I0428 19:43:32.747982 25368 solver.cpp:273] Solving LeNet
I0428 19:43:32.747987 25368 solver.cpp:274] Learning Rate Policy: inv
I0428 19:43:32.748208 25368 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:43:32.752456 25368 blocking_queue.cpp:49] Waiting for data
I0428 19:43:32.820197 25375 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:43:32.820653 25368 solver.cpp:398]     Test net output #0: accuracy = 0.1033
I0428 19:43:32.820680 25368 solver.cpp:398]     Test net output #1: loss = 2.31168 (* 1 = 2.31168 loss)
I0428 19:43:32.822073 25368 solver.cpp:219] Iteration 0 (0 iter/s, 0.0740516s/100 iters), loss = 2.29911
I0428 19:43:32.822106 25368 solver.cpp:238]     Train net output #0: loss = 2.29911 (* 1 = 2.29911 loss)
I0428 19:43:32.822125 25368 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:43:32.912720 25368 solver.cpp:219] Iteration 100 (1103.71 iter/s, 0.0906035s/100 iters), loss = 1.01486
I0428 19:43:32.912751 25368 solver.cpp:238]     Train net output #0: loss = 1.01486 (* 1 = 1.01486 loss)
I0428 19:43:32.912776 25368 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:43:32.997459 25368 solver.cpp:219] Iteration 200 (1180.65 iter/s, 0.0846991s/100 iters), loss = 0.748918
I0428 19:43:32.997486 25368 solver.cpp:238]     Train net output #0: loss = 0.748918 (* 1 = 0.748918 loss)
I0428 19:43:32.997511 25368 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:43:33.080503 25368 solver.cpp:219] Iteration 300 (1204.78 iter/s, 0.0830024s/100 iters), loss = 0.85928
I0428 19:43:33.080530 25368 solver.cpp:238]     Train net output #0: loss = 0.85928 (* 1 = 0.85928 loss)
I0428 19:43:33.080555 25368 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:43:33.166079 25368 solver.cpp:219] Iteration 400 (1169.09 iter/s, 0.0855368s/100 iters), loss = 0.588161
I0428 19:43:33.166105 25368 solver.cpp:238]     Train net output #0: loss = 0.588161 (* 1 = 0.588161 loss)
I0428 19:43:33.166131 25368 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:43:33.249483 25368 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:43:33.323967 25375 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:43:33.324417 25368 solver.cpp:398]     Test net output #0: accuracy = 0.7617
I0428 19:43:33.324445 25368 solver.cpp:398]     Test net output #1: loss = 0.633651 (* 1 = 0.633651 loss)
I0428 19:43:33.325340 25368 solver.cpp:219] Iteration 500 (628.095 iter/s, 0.159212s/100 iters), loss = 0.61111
I0428 19:43:33.325368 25368 solver.cpp:238]     Train net output #0: loss = 0.61111 (* 1 = 0.61111 loss)
I0428 19:43:33.325402 25368 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:43:33.417162 25368 solver.cpp:219] Iteration 600 (1089.6 iter/s, 0.0917771s/100 iters), loss = 0.545499
I0428 19:43:33.417194 25368 solver.cpp:238]     Train net output #0: loss = 0.545499 (* 1 = 0.545499 loss)
I0428 19:43:33.417220 25368 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:43:33.501543 25368 solver.cpp:219] Iteration 700 (1185.68 iter/s, 0.0843397s/100 iters), loss = 0.474657
I0428 19:43:33.501571 25368 solver.cpp:238]     Train net output #0: loss = 0.474657 (* 1 = 0.474657 loss)
I0428 19:43:33.501580 25368 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:43:33.583842 25368 solver.cpp:219] Iteration 800 (1215.64 iter/s, 0.082261s/100 iters), loss = 0.750855
I0428 19:43:33.583868 25368 solver.cpp:238]     Train net output #0: loss = 0.750855 (* 1 = 0.750855 loss)
I0428 19:43:33.583894 25368 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:43:33.666852 25368 solver.cpp:219] Iteration 900 (1205.25 iter/s, 0.0829706s/100 iters), loss = 0.496655
I0428 19:43:33.666879 25368 solver.cpp:238]     Train net output #0: loss = 0.496655 (* 1 = 0.496655 loss)
I0428 19:43:33.666904 25368 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:43:33.695101 25374 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:43:33.749284 25368 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:43:33.750349 25368 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:43:33.750958 25368 solver.cpp:311] Iteration 1000, loss = 0.561159
I0428 19:43:33.750977 25368 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:43:33.827385 25375 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:43:33.828048 25368 solver.cpp:398]     Test net output #0: accuracy = 0.7818
I0428 19:43:33.828092 25368 solver.cpp:398]     Test net output #1: loss = 0.556746 (* 1 = 0.556746 loss)
I0428 19:43:33.828105 25368 solver.cpp:316] Optimization Done.
I0428 19:43:33.828114 25368 caffe.cpp:259] Optimization Done.
