I0428 19:28:29.610568 21642 caffe.cpp:218] Using GPUs 0
I0428 19:28:29.648195 21642 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:28:30.167021 21642 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test27.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:28:30.167167 21642 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test27.prototxt
I0428 19:28:30.167471 21642 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:28:30.167487 21642 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:28:30.167559 21642 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:28:30.167628 21642 layer_factory.hpp:77] Creating layer mnist
I0428 19:28:30.167726 21642 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:28:30.167749 21642 net.cpp:86] Creating Layer mnist
I0428 19:28:30.167757 21642 net.cpp:382] mnist -> data
I0428 19:28:30.167780 21642 net.cpp:382] mnist -> label
I0428 19:28:30.168889 21642 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:28:30.171352 21642 net.cpp:124] Setting up mnist
I0428 19:28:30.171372 21642 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:28:30.171378 21642 net.cpp:131] Top shape: 64 (64)
I0428 19:28:30.171381 21642 net.cpp:139] Memory required for data: 200960
I0428 19:28:30.171388 21642 layer_factory.hpp:77] Creating layer ip1
I0428 19:28:30.171401 21642 net.cpp:86] Creating Layer ip1
I0428 19:28:30.171406 21642 net.cpp:408] ip1 <- data
I0428 19:28:30.171417 21642 net.cpp:382] ip1 -> ip1
I0428 19:28:30.171723 21642 net.cpp:124] Setting up ip1
I0428 19:28:30.171736 21642 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:28:30.171739 21642 net.cpp:139] Memory required for data: 207360
I0428 19:28:30.171752 21642 layer_factory.hpp:77] Creating layer relu1
I0428 19:28:30.171761 21642 net.cpp:86] Creating Layer relu1
I0428 19:28:30.171766 21642 net.cpp:408] relu1 <- ip1
I0428 19:28:30.171771 21642 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:28:30.459154 21642 net.cpp:124] Setting up relu1
I0428 19:28:30.459182 21642 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:28:30.459187 21642 net.cpp:139] Memory required for data: 213760
I0428 19:28:30.459193 21642 layer_factory.hpp:77] Creating layer ip2
I0428 19:28:30.459206 21642 net.cpp:86] Creating Layer ip2
I0428 19:28:30.459228 21642 net.cpp:408] ip2 <- ip1
I0428 19:28:30.459236 21642 net.cpp:382] ip2 -> ip2
I0428 19:28:30.460314 21642 net.cpp:124] Setting up ip2
I0428 19:28:30.460327 21642 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:28:30.460331 21642 net.cpp:139] Memory required for data: 226560
I0428 19:28:30.460342 21642 layer_factory.hpp:77] Creating layer relu2
I0428 19:28:30.460351 21642 net.cpp:86] Creating Layer relu2
I0428 19:28:30.460355 21642 net.cpp:408] relu2 <- ip2
I0428 19:28:30.460360 21642 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:28:30.461174 21642 net.cpp:124] Setting up relu2
I0428 19:28:30.461189 21642 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:28:30.461194 21642 net.cpp:139] Memory required for data: 239360
I0428 19:28:30.461196 21642 layer_factory.hpp:77] Creating layer ip3
I0428 19:28:30.461205 21642 net.cpp:86] Creating Layer ip3
I0428 19:28:30.461210 21642 net.cpp:408] ip3 <- ip2
I0428 19:28:30.461216 21642 net.cpp:382] ip3 -> ip3
I0428 19:28:30.462226 21642 net.cpp:124] Setting up ip3
I0428 19:28:30.462239 21642 net.cpp:131] Top shape: 64 10 (640)
I0428 19:28:30.462244 21642 net.cpp:139] Memory required for data: 241920
I0428 19:28:30.462252 21642 layer_factory.hpp:77] Creating layer relu3
I0428 19:28:30.462260 21642 net.cpp:86] Creating Layer relu3
I0428 19:28:30.462265 21642 net.cpp:408] relu3 <- ip3
I0428 19:28:30.462270 21642 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:28:30.462453 21642 net.cpp:124] Setting up relu3
I0428 19:28:30.462463 21642 net.cpp:131] Top shape: 64 10 (640)
I0428 19:28:30.462467 21642 net.cpp:139] Memory required for data: 244480
I0428 19:28:30.462471 21642 layer_factory.hpp:77] Creating layer loss
I0428 19:28:30.462479 21642 net.cpp:86] Creating Layer loss
I0428 19:28:30.462482 21642 net.cpp:408] loss <- ip3
I0428 19:28:30.462486 21642 net.cpp:408] loss <- label
I0428 19:28:30.462492 21642 net.cpp:382] loss -> loss
I0428 19:28:30.462510 21642 layer_factory.hpp:77] Creating layer loss
I0428 19:28:30.462775 21642 net.cpp:124] Setting up loss
I0428 19:28:30.462787 21642 net.cpp:131] Top shape: (1)
I0428 19:28:30.462790 21642 net.cpp:134]     with loss weight 1
I0428 19:28:30.462805 21642 net.cpp:139] Memory required for data: 244484
I0428 19:28:30.462810 21642 net.cpp:200] loss needs backward computation.
I0428 19:28:30.462812 21642 net.cpp:200] relu3 needs backward computation.
I0428 19:28:30.462815 21642 net.cpp:200] ip3 needs backward computation.
I0428 19:28:30.462819 21642 net.cpp:200] relu2 needs backward computation.
I0428 19:28:30.462822 21642 net.cpp:200] ip2 needs backward computation.
I0428 19:28:30.462824 21642 net.cpp:200] relu1 needs backward computation.
I0428 19:28:30.462827 21642 net.cpp:200] ip1 needs backward computation.
I0428 19:28:30.462831 21642 net.cpp:202] mnist does not need backward computation.
I0428 19:28:30.462834 21642 net.cpp:244] This network produces output loss
I0428 19:28:30.462844 21642 net.cpp:257] Network initialization done.
I0428 19:28:30.463098 21642 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test27.prototxt
I0428 19:28:30.463121 21642 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:28:30.463198 21642 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:28:30.463277 21642 layer_factory.hpp:77] Creating layer mnist
I0428 19:28:30.463325 21642 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:28:30.463338 21642 net.cpp:86] Creating Layer mnist
I0428 19:28:30.463345 21642 net.cpp:382] mnist -> data
I0428 19:28:30.463352 21642 net.cpp:382] mnist -> label
I0428 19:28:30.463449 21642 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:28:30.465497 21642 net.cpp:124] Setting up mnist
I0428 19:28:30.465523 21642 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:28:30.465529 21642 net.cpp:131] Top shape: 100 (100)
I0428 19:28:30.465533 21642 net.cpp:139] Memory required for data: 314000
I0428 19:28:30.465536 21642 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:28:30.465546 21642 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:28:30.465550 21642 net.cpp:408] label_mnist_1_split <- label
I0428 19:28:30.465557 21642 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:28:30.465564 21642 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:28:30.465659 21642 net.cpp:124] Setting up label_mnist_1_split
I0428 19:28:30.465668 21642 net.cpp:131] Top shape: 100 (100)
I0428 19:28:30.465673 21642 net.cpp:131] Top shape: 100 (100)
I0428 19:28:30.465677 21642 net.cpp:139] Memory required for data: 314800
I0428 19:28:30.465680 21642 layer_factory.hpp:77] Creating layer ip1
I0428 19:28:30.465687 21642 net.cpp:86] Creating Layer ip1
I0428 19:28:30.465690 21642 net.cpp:408] ip1 <- data
I0428 19:28:30.465697 21642 net.cpp:382] ip1 -> ip1
I0428 19:28:30.465965 21642 net.cpp:124] Setting up ip1
I0428 19:28:30.465975 21642 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:28:30.465977 21642 net.cpp:139] Memory required for data: 324800
I0428 19:28:30.465988 21642 layer_factory.hpp:77] Creating layer relu1
I0428 19:28:30.465994 21642 net.cpp:86] Creating Layer relu1
I0428 19:28:30.465997 21642 net.cpp:408] relu1 <- ip1
I0428 19:28:30.466001 21642 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:28:30.467048 21642 net.cpp:124] Setting up relu1
I0428 19:28:30.467061 21642 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:28:30.467064 21642 net.cpp:139] Memory required for data: 334800
I0428 19:28:30.467068 21642 layer_factory.hpp:77] Creating layer ip2
I0428 19:28:30.467075 21642 net.cpp:86] Creating Layer ip2
I0428 19:28:30.467079 21642 net.cpp:408] ip2 <- ip1
I0428 19:28:30.467085 21642 net.cpp:382] ip2 -> ip2
I0428 19:28:30.467205 21642 net.cpp:124] Setting up ip2
I0428 19:28:30.467212 21642 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:28:30.467216 21642 net.cpp:139] Memory required for data: 354800
I0428 19:28:30.467223 21642 layer_factory.hpp:77] Creating layer relu2
I0428 19:28:30.467228 21642 net.cpp:86] Creating Layer relu2
I0428 19:28:30.467231 21642 net.cpp:408] relu2 <- ip2
I0428 19:28:30.467237 21642 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:28:30.467402 21642 net.cpp:124] Setting up relu2
I0428 19:28:30.467411 21642 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:28:30.467414 21642 net.cpp:139] Memory required for data: 374800
I0428 19:28:30.467417 21642 layer_factory.hpp:77] Creating layer ip3
I0428 19:28:30.467424 21642 net.cpp:86] Creating Layer ip3
I0428 19:28:30.467427 21642 net.cpp:408] ip3 <- ip2
I0428 19:28:30.467432 21642 net.cpp:382] ip3 -> ip3
I0428 19:28:30.467546 21642 net.cpp:124] Setting up ip3
I0428 19:28:30.467555 21642 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:28:30.467558 21642 net.cpp:139] Memory required for data: 378800
I0428 19:28:30.467566 21642 layer_factory.hpp:77] Creating layer relu3
I0428 19:28:30.467571 21642 net.cpp:86] Creating Layer relu3
I0428 19:28:30.467574 21642 net.cpp:408] relu3 <- ip3
I0428 19:28:30.467581 21642 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:28:30.467746 21642 net.cpp:124] Setting up relu3
I0428 19:28:30.467754 21642 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:28:30.467758 21642 net.cpp:139] Memory required for data: 382800
I0428 19:28:30.467761 21642 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:28:30.467767 21642 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:28:30.467770 21642 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:28:30.467777 21642 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:28:30.467782 21642 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:28:30.467818 21642 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:28:30.467824 21642 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:28:30.467830 21642 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:28:30.467833 21642 net.cpp:139] Memory required for data: 390800
I0428 19:28:30.467836 21642 layer_factory.hpp:77] Creating layer accuracy
I0428 19:28:30.467845 21642 net.cpp:86] Creating Layer accuracy
I0428 19:28:30.467849 21642 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:28:30.467852 21642 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:28:30.467859 21642 net.cpp:382] accuracy -> accuracy
I0428 19:28:30.467865 21642 net.cpp:124] Setting up accuracy
I0428 19:28:30.467870 21642 net.cpp:131] Top shape: (1)
I0428 19:28:30.467874 21642 net.cpp:139] Memory required for data: 390804
I0428 19:28:30.467875 21642 layer_factory.hpp:77] Creating layer loss
I0428 19:28:30.467880 21642 net.cpp:86] Creating Layer loss
I0428 19:28:30.467883 21642 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:28:30.467887 21642 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:28:30.467891 21642 net.cpp:382] loss -> loss
I0428 19:28:30.467897 21642 layer_factory.hpp:77] Creating layer loss
I0428 19:28:30.468140 21642 net.cpp:124] Setting up loss
I0428 19:28:30.468150 21642 net.cpp:131] Top shape: (1)
I0428 19:28:30.468153 21642 net.cpp:134]     with loss weight 1
I0428 19:28:30.468159 21642 net.cpp:139] Memory required for data: 390808
I0428 19:28:30.468163 21642 net.cpp:200] loss needs backward computation.
I0428 19:28:30.468166 21642 net.cpp:202] accuracy does not need backward computation.
I0428 19:28:30.468171 21642 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:28:30.468175 21642 net.cpp:200] relu3 needs backward computation.
I0428 19:28:30.468178 21642 net.cpp:200] ip3 needs backward computation.
I0428 19:28:30.468181 21642 net.cpp:200] relu2 needs backward computation.
I0428 19:28:30.468184 21642 net.cpp:200] ip2 needs backward computation.
I0428 19:28:30.468188 21642 net.cpp:200] relu1 needs backward computation.
I0428 19:28:30.468190 21642 net.cpp:200] ip1 needs backward computation.
I0428 19:28:30.468194 21642 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:28:30.468197 21642 net.cpp:202] mnist does not need backward computation.
I0428 19:28:30.468200 21642 net.cpp:244] This network produces output accuracy
I0428 19:28:30.468204 21642 net.cpp:244] This network produces output loss
I0428 19:28:30.468214 21642 net.cpp:257] Network initialization done.
I0428 19:28:30.468245 21642 solver.cpp:56] Solver scaffolding done.
I0428 19:28:30.468482 21642 caffe.cpp:248] Starting Optimization
I0428 19:28:30.468488 21642 solver.cpp:273] Solving LeNet
I0428 19:28:30.468492 21642 solver.cpp:274] Learning Rate Policy: inv
I0428 19:28:30.469908 21642 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:28:30.470027 21642 blocking_queue.cpp:49] Waiting for data
I0428 19:28:30.548354 21649 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:28:30.548789 21642 solver.cpp:398]     Test net output #0: accuracy = 0.0735
I0428 19:28:30.548818 21642 solver.cpp:398]     Test net output #1: loss = 2.31044 (* 1 = 2.31044 loss)
I0428 19:28:30.549322 21642 solver.cpp:219] Iteration 0 (0 iter/s, 0.0807526s/100 iters), loss = 2.30483
I0428 19:28:30.549345 21642 solver.cpp:238]     Train net output #0: loss = 2.30483 (* 1 = 2.30483 loss)
I0428 19:28:30.549355 21642 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:28:30.605000 21642 solver.cpp:219] Iteration 100 (1797.13 iter/s, 0.0556442s/100 iters), loss = 0.914929
I0428 19:28:30.605024 21642 solver.cpp:238]     Train net output #0: loss = 0.914929 (* 1 = 0.914929 loss)
I0428 19:28:30.605031 21642 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:28:30.651507 21642 solver.cpp:219] Iteration 200 (2151.63 iter/s, 0.0464763s/100 iters), loss = 0.937934
I0428 19:28:30.651531 21642 solver.cpp:238]     Train net output #0: loss = 0.937934 (* 1 = 0.937934 loss)
I0428 19:28:30.651537 21642 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:28:30.694411 21642 solver.cpp:219] Iteration 300 (2332.48 iter/s, 0.0428728s/100 iters), loss = 0.797492
I0428 19:28:30.694448 21642 solver.cpp:238]     Train net output #0: loss = 0.797492 (* 1 = 0.797492 loss)
I0428 19:28:30.694454 21642 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:28:30.736929 21642 solver.cpp:219] Iteration 400 (2353.5 iter/s, 0.0424899s/100 iters), loss = 0.664176
I0428 19:28:30.736968 21642 solver.cpp:238]     Train net output #0: loss = 0.664176 (* 1 = 0.664176 loss)
I0428 19:28:30.736974 21642 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:28:30.779090 21642 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:28:30.845383 21649 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:28:30.845880 21642 solver.cpp:398]     Test net output #0: accuracy = 0.7365
I0428 19:28:30.845907 21642 solver.cpp:398]     Test net output #1: loss = 0.726109 (* 1 = 0.726109 loss)
I0428 19:28:30.846370 21642 solver.cpp:219] Iteration 500 (914.162 iter/s, 0.10939s/100 iters), loss = 0.839474
I0428 19:28:30.846400 21642 solver.cpp:238]     Train net output #0: loss = 0.839474 (* 1 = 0.839474 loss)
I0428 19:28:30.846410 21642 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:28:30.904031 21642 solver.cpp:219] Iteration 600 (1735.43 iter/s, 0.0576225s/100 iters), loss = 0.770848
I0428 19:28:30.904063 21642 solver.cpp:238]     Train net output #0: loss = 0.770848 (* 1 = 0.770848 loss)
I0428 19:28:30.904070 21642 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:28:30.953053 21642 solver.cpp:219] Iteration 700 (2041.52 iter/s, 0.0489832s/100 iters), loss = 0.561432
I0428 19:28:30.953079 21642 solver.cpp:238]     Train net output #0: loss = 0.561432 (* 1 = 0.561432 loss)
I0428 19:28:30.953085 21642 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:28:30.998944 21642 solver.cpp:219] Iteration 800 (2180.65 iter/s, 0.0458579s/100 iters), loss = 0.5219
I0428 19:28:30.998967 21642 solver.cpp:238]     Train net output #0: loss = 0.5219 (* 1 = 0.5219 loss)
I0428 19:28:30.998972 21642 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:28:31.008823 21642 blocking_queue.cpp:49] Waiting for data
I0428 19:28:31.042402 21642 solver.cpp:219] Iteration 900 (2302.66 iter/s, 0.043428s/100 iters), loss = 0.31751
I0428 19:28:31.042424 21642 solver.cpp:238]     Train net output #0: loss = 0.31751 (* 1 = 0.31751 loss)
I0428 19:28:31.042430 21642 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:28:31.058418 21648 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:28:31.086030 21642 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:28:31.086940 21642 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:28:31.087497 21642 solver.cpp:311] Iteration 1000, loss = 0.297033
I0428 19:28:31.087512 21642 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:28:31.145320 21649 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:28:31.145720 21642 solver.cpp:398]     Test net output #0: accuracy = 0.9208
I0428 19:28:31.145753 21642 solver.cpp:398]     Test net output #1: loss = 0.271465 (* 1 = 0.271465 loss)
I0428 19:28:31.145757 21642 solver.cpp:316] Optimization Done.
I0428 19:28:31.145761 21642 caffe.cpp:259] Optimization Done.
