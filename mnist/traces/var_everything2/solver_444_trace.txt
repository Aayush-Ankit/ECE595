I0428 19:44:18.033128 25574 caffe.cpp:218] Using GPUs 0
I0428 19:44:18.072932 25574 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:44:18.590386 25574 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test444.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:44:18.590528 25574 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test444.prototxt
I0428 19:44:18.590946 25574 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:44:18.590966 25574 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:44:18.591069 25574 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:44:18.591150 25574 layer_factory.hpp:77] Creating layer mnist
I0428 19:44:18.591245 25574 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:44:18.591269 25574 net.cpp:86] Creating Layer mnist
I0428 19:44:18.591275 25574 net.cpp:382] mnist -> data
I0428 19:44:18.591297 25574 net.cpp:382] mnist -> label
I0428 19:44:18.592383 25574 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:44:18.594838 25574 net.cpp:124] Setting up mnist
I0428 19:44:18.594856 25574 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:44:18.594861 25574 net.cpp:131] Top shape: 64 (64)
I0428 19:44:18.594866 25574 net.cpp:139] Memory required for data: 200960
I0428 19:44:18.594872 25574 layer_factory.hpp:77] Creating layer conv0
I0428 19:44:18.594925 25574 net.cpp:86] Creating Layer conv0
I0428 19:44:18.594946 25574 net.cpp:408] conv0 <- data
I0428 19:44:18.594959 25574 net.cpp:382] conv0 -> conv0
I0428 19:44:18.882117 25574 net.cpp:124] Setting up conv0
I0428 19:44:18.882144 25574 net.cpp:131] Top shape: 64 2 24 24 (73728)
I0428 19:44:18.882149 25574 net.cpp:139] Memory required for data: 495872
I0428 19:44:18.882164 25574 layer_factory.hpp:77] Creating layer pool0
I0428 19:44:18.882179 25574 net.cpp:86] Creating Layer pool0
I0428 19:44:18.882182 25574 net.cpp:408] pool0 <- conv0
I0428 19:44:18.882187 25574 net.cpp:382] pool0 -> pool0
I0428 19:44:18.882237 25574 net.cpp:124] Setting up pool0
I0428 19:44:18.882244 25574 net.cpp:131] Top shape: 64 2 12 12 (18432)
I0428 19:44:18.882248 25574 net.cpp:139] Memory required for data: 569600
I0428 19:44:18.882251 25574 layer_factory.hpp:77] Creating layer conv1
I0428 19:44:18.882262 25574 net.cpp:86] Creating Layer conv1
I0428 19:44:18.882266 25574 net.cpp:408] conv1 <- pool0
I0428 19:44:18.882271 25574 net.cpp:382] conv1 -> conv1
I0428 19:44:18.885180 25574 net.cpp:124] Setting up conv1
I0428 19:44:18.885195 25574 net.cpp:131] Top shape: 64 50 8 8 (204800)
I0428 19:44:18.885200 25574 net.cpp:139] Memory required for data: 1388800
I0428 19:44:18.885210 25574 layer_factory.hpp:77] Creating layer pool1
I0428 19:44:18.885217 25574 net.cpp:86] Creating Layer pool1
I0428 19:44:18.885221 25574 net.cpp:408] pool1 <- conv1
I0428 19:44:18.885226 25574 net.cpp:382] pool1 -> pool1
I0428 19:44:18.885267 25574 net.cpp:124] Setting up pool1
I0428 19:44:18.885272 25574 net.cpp:131] Top shape: 64 50 4 4 (51200)
I0428 19:44:18.885275 25574 net.cpp:139] Memory required for data: 1593600
I0428 19:44:18.885278 25574 layer_factory.hpp:77] Creating layer ip1
I0428 19:44:18.885287 25574 net.cpp:86] Creating Layer ip1
I0428 19:44:18.885289 25574 net.cpp:408] ip1 <- pool1
I0428 19:44:18.885294 25574 net.cpp:382] ip1 -> ip1
I0428 19:44:18.886268 25574 net.cpp:124] Setting up ip1
I0428 19:44:18.886282 25574 net.cpp:131] Top shape: 64 10 (640)
I0428 19:44:18.886308 25574 net.cpp:139] Memory required for data: 1596160
I0428 19:44:18.886317 25574 layer_factory.hpp:77] Creating layer relu1
I0428 19:44:18.886323 25574 net.cpp:86] Creating Layer relu1
I0428 19:44:18.886327 25574 net.cpp:408] relu1 <- ip1
I0428 19:44:18.886332 25574 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:44:18.886518 25574 net.cpp:124] Setting up relu1
I0428 19:44:18.886526 25574 net.cpp:131] Top shape: 64 10 (640)
I0428 19:44:18.886530 25574 net.cpp:139] Memory required for data: 1598720
I0428 19:44:18.886533 25574 layer_factory.hpp:77] Creating layer ip2
I0428 19:44:18.886541 25574 net.cpp:86] Creating Layer ip2
I0428 19:44:18.886544 25574 net.cpp:408] ip2 <- ip1
I0428 19:44:18.886550 25574 net.cpp:382] ip2 -> ip2
I0428 19:44:18.886654 25574 net.cpp:124] Setting up ip2
I0428 19:44:18.886660 25574 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:44:18.886663 25574 net.cpp:139] Memory required for data: 1605120
I0428 19:44:18.886669 25574 layer_factory.hpp:77] Creating layer relu2
I0428 19:44:18.886674 25574 net.cpp:86] Creating Layer relu2
I0428 19:44:18.886677 25574 net.cpp:408] relu2 <- ip2
I0428 19:44:18.886682 25574 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:44:18.887460 25574 net.cpp:124] Setting up relu2
I0428 19:44:18.887473 25574 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:44:18.887476 25574 net.cpp:139] Memory required for data: 1611520
I0428 19:44:18.887480 25574 layer_factory.hpp:77] Creating layer ip3
I0428 19:44:18.887486 25574 net.cpp:86] Creating Layer ip3
I0428 19:44:18.887490 25574 net.cpp:408] ip3 <- ip2
I0428 19:44:18.887496 25574 net.cpp:382] ip3 -> ip3
I0428 19:44:18.887601 25574 net.cpp:124] Setting up ip3
I0428 19:44:18.887609 25574 net.cpp:131] Top shape: 64 10 (640)
I0428 19:44:18.887611 25574 net.cpp:139] Memory required for data: 1614080
I0428 19:44:18.887619 25574 layer_factory.hpp:77] Creating layer relu3
I0428 19:44:18.887624 25574 net.cpp:86] Creating Layer relu3
I0428 19:44:18.887627 25574 net.cpp:408] relu3 <- ip3
I0428 19:44:18.887635 25574 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:44:18.887811 25574 net.cpp:124] Setting up relu3
I0428 19:44:18.887820 25574 net.cpp:131] Top shape: 64 10 (640)
I0428 19:44:18.887823 25574 net.cpp:139] Memory required for data: 1616640
I0428 19:44:18.887826 25574 layer_factory.hpp:77] Creating layer loss
I0428 19:44:18.887833 25574 net.cpp:86] Creating Layer loss
I0428 19:44:18.887836 25574 net.cpp:408] loss <- ip3
I0428 19:44:18.887840 25574 net.cpp:408] loss <- label
I0428 19:44:18.887845 25574 net.cpp:382] loss -> loss
I0428 19:44:18.887859 25574 layer_factory.hpp:77] Creating layer loss
I0428 19:44:18.888099 25574 net.cpp:124] Setting up loss
I0428 19:44:18.888108 25574 net.cpp:131] Top shape: (1)
I0428 19:44:18.888110 25574 net.cpp:134]     with loss weight 1
I0428 19:44:18.888128 25574 net.cpp:139] Memory required for data: 1616644
I0428 19:44:18.888130 25574 net.cpp:200] loss needs backward computation.
I0428 19:44:18.888134 25574 net.cpp:200] relu3 needs backward computation.
I0428 19:44:18.888137 25574 net.cpp:200] ip3 needs backward computation.
I0428 19:44:18.888139 25574 net.cpp:200] relu2 needs backward computation.
I0428 19:44:18.888142 25574 net.cpp:200] ip2 needs backward computation.
I0428 19:44:18.888145 25574 net.cpp:200] relu1 needs backward computation.
I0428 19:44:18.888147 25574 net.cpp:200] ip1 needs backward computation.
I0428 19:44:18.888150 25574 net.cpp:200] pool1 needs backward computation.
I0428 19:44:18.888154 25574 net.cpp:200] conv1 needs backward computation.
I0428 19:44:18.888156 25574 net.cpp:200] pool0 needs backward computation.
I0428 19:44:18.888159 25574 net.cpp:200] conv0 needs backward computation.
I0428 19:44:18.888162 25574 net.cpp:202] mnist does not need backward computation.
I0428 19:44:18.888165 25574 net.cpp:244] This network produces output loss
I0428 19:44:18.888175 25574 net.cpp:257] Network initialization done.
I0428 19:44:18.888504 25574 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test444.prototxt
I0428 19:44:18.888530 25574 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:44:18.888624 25574 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:44:18.888710 25574 layer_factory.hpp:77] Creating layer mnist
I0428 19:44:18.888754 25574 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:44:18.888767 25574 net.cpp:86] Creating Layer mnist
I0428 19:44:18.888772 25574 net.cpp:382] mnist -> data
I0428 19:44:18.888780 25574 net.cpp:382] mnist -> label
I0428 19:44:18.888895 25574 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:44:18.891037 25574 net.cpp:124] Setting up mnist
I0428 19:44:18.891050 25574 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:44:18.891055 25574 net.cpp:131] Top shape: 100 (100)
I0428 19:44:18.891058 25574 net.cpp:139] Memory required for data: 314000
I0428 19:44:18.891062 25574 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:44:18.891068 25574 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:44:18.891072 25574 net.cpp:408] label_mnist_1_split <- label
I0428 19:44:18.891077 25574 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:44:18.891084 25574 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:44:18.891130 25574 net.cpp:124] Setting up label_mnist_1_split
I0428 19:44:18.891137 25574 net.cpp:131] Top shape: 100 (100)
I0428 19:44:18.891141 25574 net.cpp:131] Top shape: 100 (100)
I0428 19:44:18.891144 25574 net.cpp:139] Memory required for data: 314800
I0428 19:44:18.891147 25574 layer_factory.hpp:77] Creating layer conv0
I0428 19:44:18.891158 25574 net.cpp:86] Creating Layer conv0
I0428 19:44:18.891161 25574 net.cpp:408] conv0 <- data
I0428 19:44:18.891166 25574 net.cpp:382] conv0 -> conv0
I0428 19:44:18.892771 25574 net.cpp:124] Setting up conv0
I0428 19:44:18.892784 25574 net.cpp:131] Top shape: 100 2 24 24 (115200)
I0428 19:44:18.892788 25574 net.cpp:139] Memory required for data: 775600
I0428 19:44:18.892798 25574 layer_factory.hpp:77] Creating layer pool0
I0428 19:44:18.892805 25574 net.cpp:86] Creating Layer pool0
I0428 19:44:18.892814 25574 net.cpp:408] pool0 <- conv0
I0428 19:44:18.892822 25574 net.cpp:382] pool0 -> pool0
I0428 19:44:18.892860 25574 net.cpp:124] Setting up pool0
I0428 19:44:18.892868 25574 net.cpp:131] Top shape: 100 2 12 12 (28800)
I0428 19:44:18.892871 25574 net.cpp:139] Memory required for data: 890800
I0428 19:44:18.892874 25574 layer_factory.hpp:77] Creating layer conv1
I0428 19:44:18.892884 25574 net.cpp:86] Creating Layer conv1
I0428 19:44:18.892886 25574 net.cpp:408] conv1 <- pool0
I0428 19:44:18.892891 25574 net.cpp:382] conv1 -> conv1
I0428 19:44:18.894456 25574 net.cpp:124] Setting up conv1
I0428 19:44:18.894469 25574 net.cpp:131] Top shape: 100 50 8 8 (320000)
I0428 19:44:18.894472 25574 net.cpp:139] Memory required for data: 2170800
I0428 19:44:18.894481 25574 layer_factory.hpp:77] Creating layer pool1
I0428 19:44:18.894490 25574 net.cpp:86] Creating Layer pool1
I0428 19:44:18.894492 25574 net.cpp:408] pool1 <- conv1
I0428 19:44:18.894498 25574 net.cpp:382] pool1 -> pool1
I0428 19:44:18.894541 25574 net.cpp:124] Setting up pool1
I0428 19:44:18.894546 25574 net.cpp:131] Top shape: 100 50 4 4 (80000)
I0428 19:44:18.894548 25574 net.cpp:139] Memory required for data: 2490800
I0428 19:44:18.894551 25574 layer_factory.hpp:77] Creating layer ip1
I0428 19:44:18.894558 25574 net.cpp:86] Creating Layer ip1
I0428 19:44:18.894562 25574 net.cpp:408] ip1 <- pool1
I0428 19:44:18.894567 25574 net.cpp:382] ip1 -> ip1
I0428 19:44:18.894712 25574 net.cpp:124] Setting up ip1
I0428 19:44:18.894721 25574 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:44:18.894733 25574 net.cpp:139] Memory required for data: 2494800
I0428 19:44:18.894742 25574 layer_factory.hpp:77] Creating layer relu1
I0428 19:44:18.894749 25574 net.cpp:86] Creating Layer relu1
I0428 19:44:18.894752 25574 net.cpp:408] relu1 <- ip1
I0428 19:44:18.894757 25574 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:44:18.894933 25574 net.cpp:124] Setting up relu1
I0428 19:44:18.894943 25574 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:44:18.894953 25574 net.cpp:139] Memory required for data: 2498800
I0428 19:44:18.894956 25574 layer_factory.hpp:77] Creating layer ip2
I0428 19:44:18.894964 25574 net.cpp:86] Creating Layer ip2
I0428 19:44:18.894968 25574 net.cpp:408] ip2 <- ip1
I0428 19:44:18.894973 25574 net.cpp:382] ip2 -> ip2
I0428 19:44:18.895081 25574 net.cpp:124] Setting up ip2
I0428 19:44:18.895090 25574 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:44:18.895093 25574 net.cpp:139] Memory required for data: 2508800
I0428 19:44:18.895098 25574 layer_factory.hpp:77] Creating layer relu2
I0428 19:44:18.895103 25574 net.cpp:86] Creating Layer relu2
I0428 19:44:18.895107 25574 net.cpp:408] relu2 <- ip2
I0428 19:44:18.895112 25574 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:44:18.895272 25574 net.cpp:124] Setting up relu2
I0428 19:44:18.895282 25574 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:44:18.895285 25574 net.cpp:139] Memory required for data: 2518800
I0428 19:44:18.895294 25574 layer_factory.hpp:77] Creating layer ip3
I0428 19:44:18.895300 25574 net.cpp:86] Creating Layer ip3
I0428 19:44:18.895303 25574 net.cpp:408] ip3 <- ip2
I0428 19:44:18.895314 25574 net.cpp:382] ip3 -> ip3
I0428 19:44:18.895416 25574 net.cpp:124] Setting up ip3
I0428 19:44:18.895422 25574 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:44:18.895426 25574 net.cpp:139] Memory required for data: 2522800
I0428 19:44:18.895434 25574 layer_factory.hpp:77] Creating layer relu3
I0428 19:44:18.895438 25574 net.cpp:86] Creating Layer relu3
I0428 19:44:18.895442 25574 net.cpp:408] relu3 <- ip3
I0428 19:44:18.895447 25574 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:44:18.896256 25574 net.cpp:124] Setting up relu3
I0428 19:44:18.896270 25574 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:44:18.896275 25574 net.cpp:139] Memory required for data: 2526800
I0428 19:44:18.896277 25574 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:44:18.896282 25574 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:44:18.896286 25574 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:44:18.896292 25574 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:44:18.896298 25574 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:44:18.896342 25574 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:44:18.896347 25574 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:44:18.896350 25574 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:44:18.896353 25574 net.cpp:139] Memory required for data: 2534800
I0428 19:44:18.896356 25574 layer_factory.hpp:77] Creating layer accuracy
I0428 19:44:18.896363 25574 net.cpp:86] Creating Layer accuracy
I0428 19:44:18.896365 25574 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:44:18.896370 25574 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:44:18.896374 25574 net.cpp:382] accuracy -> accuracy
I0428 19:44:18.896383 25574 net.cpp:124] Setting up accuracy
I0428 19:44:18.896392 25574 net.cpp:131] Top shape: (1)
I0428 19:44:18.896395 25574 net.cpp:139] Memory required for data: 2534804
I0428 19:44:18.896397 25574 layer_factory.hpp:77] Creating layer loss
I0428 19:44:18.896402 25574 net.cpp:86] Creating Layer loss
I0428 19:44:18.896405 25574 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:44:18.896409 25574 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:44:18.896414 25574 net.cpp:382] loss -> loss
I0428 19:44:18.896420 25574 layer_factory.hpp:77] Creating layer loss
I0428 19:44:18.896667 25574 net.cpp:124] Setting up loss
I0428 19:44:18.896677 25574 net.cpp:131] Top shape: (1)
I0428 19:44:18.896679 25574 net.cpp:134]     with loss weight 1
I0428 19:44:18.896685 25574 net.cpp:139] Memory required for data: 2534808
I0428 19:44:18.896698 25574 net.cpp:200] loss needs backward computation.
I0428 19:44:18.896703 25574 net.cpp:202] accuracy does not need backward computation.
I0428 19:44:18.896706 25574 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:44:18.896709 25574 net.cpp:200] relu3 needs backward computation.
I0428 19:44:18.896713 25574 net.cpp:200] ip3 needs backward computation.
I0428 19:44:18.896715 25574 net.cpp:200] relu2 needs backward computation.
I0428 19:44:18.896718 25574 net.cpp:200] ip2 needs backward computation.
I0428 19:44:18.896720 25574 net.cpp:200] relu1 needs backward computation.
I0428 19:44:18.896723 25574 net.cpp:200] ip1 needs backward computation.
I0428 19:44:18.896726 25574 net.cpp:200] pool1 needs backward computation.
I0428 19:44:18.896737 25574 net.cpp:200] conv1 needs backward computation.
I0428 19:44:18.896740 25574 net.cpp:200] pool0 needs backward computation.
I0428 19:44:18.896744 25574 net.cpp:200] conv0 needs backward computation.
I0428 19:44:18.896746 25574 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:44:18.896761 25574 net.cpp:202] mnist does not need backward computation.
I0428 19:44:18.896764 25574 net.cpp:244] This network produces output accuracy
I0428 19:44:18.896767 25574 net.cpp:244] This network produces output loss
I0428 19:44:18.896788 25574 net.cpp:257] Network initialization done.
I0428 19:44:18.896836 25574 solver.cpp:56] Solver scaffolding done.
I0428 19:44:18.897176 25574 caffe.cpp:248] Starting Optimization
I0428 19:44:18.897182 25574 solver.cpp:273] Solving LeNet
I0428 19:44:18.897186 25574 solver.cpp:274] Learning Rate Policy: inv
I0428 19:44:18.897348 25574 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:44:18.901523 25574 blocking_queue.cpp:49] Waiting for data
I0428 19:44:18.972470 25581 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:44:18.973007 25574 solver.cpp:398]     Test net output #0: accuracy = 0.1071
I0428 19:44:18.973027 25574 solver.cpp:398]     Test net output #1: loss = 2.30065 (* 1 = 2.30065 loss)
I0428 19:44:18.975174 25574 solver.cpp:219] Iteration 0 (0 iter/s, 0.0779642s/100 iters), loss = 2.29843
I0428 19:44:18.975198 25574 solver.cpp:238]     Train net output #0: loss = 2.29843 (* 1 = 2.29843 loss)
I0428 19:44:18.975224 25574 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:44:19.082242 25574 solver.cpp:219] Iteration 100 (934.304 iter/s, 0.107032s/100 iters), loss = 0.809869
I0428 19:44:19.082267 25574 solver.cpp:238]     Train net output #0: loss = 0.809869 (* 1 = 0.809869 loss)
I0428 19:44:19.082288 25574 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:44:19.186895 25574 solver.cpp:219] Iteration 200 (955.926 iter/s, 0.104611s/100 iters), loss = 0.274667
I0428 19:44:19.186923 25574 solver.cpp:238]     Train net output #0: loss = 0.274667 (* 1 = 0.274667 loss)
I0428 19:44:19.186930 25574 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:44:19.289222 25574 solver.cpp:219] Iteration 300 (977.648 iter/s, 0.102286s/100 iters), loss = 0.351234
I0428 19:44:19.289263 25574 solver.cpp:238]     Train net output #0: loss = 0.351234 (* 1 = 0.351234 loss)
I0428 19:44:19.289268 25574 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:44:19.395681 25574 solver.cpp:219] Iteration 400 (939.798 iter/s, 0.106406s/100 iters), loss = 0.220335
I0428 19:44:19.395721 25574 solver.cpp:238]     Train net output #0: loss = 0.220335 (* 1 = 0.220335 loss)
I0428 19:44:19.395727 25574 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:44:19.505769 25574 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:44:19.559927 25581 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:44:19.560427 25574 solver.cpp:398]     Test net output #0: accuracy = 0.9435
I0428 19:44:19.560449 25574 solver.cpp:398]     Test net output #1: loss = 0.17855 (* 1 = 0.17855 loss)
I0428 19:44:19.561491 25574 solver.cpp:219] Iteration 500 (603.241 iter/s, 0.165771s/100 iters), loss = 0.203368
I0428 19:44:19.561527 25574 solver.cpp:238]     Train net output #0: loss = 0.203369 (* 1 = 0.203369 loss)
I0428 19:44:19.561552 25574 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:44:19.669417 25574 solver.cpp:219] Iteration 600 (926.964 iter/s, 0.107879s/100 iters), loss = 0.105021
I0428 19:44:19.669446 25574 solver.cpp:238]     Train net output #0: loss = 0.105021 (* 1 = 0.105021 loss)
I0428 19:44:19.669453 25574 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:44:19.771993 25574 solver.cpp:219] Iteration 700 (975.246 iter/s, 0.102538s/100 iters), loss = 0.240013
I0428 19:44:19.772033 25574 solver.cpp:238]     Train net output #0: loss = 0.240013 (* 1 = 0.240013 loss)
I0428 19:44:19.772039 25574 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:44:19.879061 25574 solver.cpp:219] Iteration 800 (934.509 iter/s, 0.107008s/100 iters), loss = 0.384225
I0428 19:44:19.879096 25574 solver.cpp:238]     Train net output #0: loss = 0.384225 (* 1 = 0.384225 loss)
I0428 19:44:19.879103 25574 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:44:19.988988 25574 solver.cpp:219] Iteration 900 (910.09 iter/s, 0.109879s/100 iters), loss = 0.254693
I0428 19:44:19.989017 25574 solver.cpp:238]     Train net output #0: loss = 0.254693 (* 1 = 0.254693 loss)
I0428 19:44:19.989023 25574 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:44:20.023418 25580 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:44:20.092489 25574 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:44:20.093565 25574 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:44:20.094324 25574 solver.cpp:311] Iteration 1000, loss = 0.213223
I0428 19:44:20.094357 25574 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:44:20.196676 25581 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:44:20.197355 25574 solver.cpp:398]     Test net output #0: accuracy = 0.9663
I0428 19:44:20.197382 25574 solver.cpp:398]     Test net output #1: loss = 0.106114 (* 1 = 0.106114 loss)
I0428 19:44:20.197392 25574 solver.cpp:316] Optimization Done.
I0428 19:44:20.197397 25574 caffe.cpp:259] Optimization Done.
