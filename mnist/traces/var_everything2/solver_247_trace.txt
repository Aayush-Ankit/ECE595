I0428 19:36:42.586684 23720 caffe.cpp:218] Using GPUs 0
I0428 19:36:42.625033 23720 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:36:43.137534 23720 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test247.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:36:43.137688 23720 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test247.prototxt
I0428 19:36:43.138062 23720 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:36:43.138080 23720 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:36:43.138170 23720 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool0"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:36:43.138245 23720 layer_factory.hpp:77] Creating layer mnist
I0428 19:36:43.138346 23720 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:36:43.138370 23720 net.cpp:86] Creating Layer mnist
I0428 19:36:43.138380 23720 net.cpp:382] mnist -> data
I0428 19:36:43.138401 23720 net.cpp:382] mnist -> label
I0428 19:36:43.139494 23720 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:36:43.142204 23720 net.cpp:124] Setting up mnist
I0428 19:36:43.142221 23720 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:36:43.142230 23720 net.cpp:131] Top shape: 64 (64)
I0428 19:36:43.142233 23720 net.cpp:139] Memory required for data: 200960
I0428 19:36:43.142241 23720 layer_factory.hpp:77] Creating layer conv0
I0428 19:36:43.142256 23720 net.cpp:86] Creating Layer conv0
I0428 19:36:43.142261 23720 net.cpp:408] conv0 <- data
I0428 19:36:43.142276 23720 net.cpp:382] conv0 -> conv0
I0428 19:36:43.431744 23720 net.cpp:124] Setting up conv0
I0428 19:36:43.431773 23720 net.cpp:131] Top shape: 64 100 24 24 (3686400)
I0428 19:36:43.431778 23720 net.cpp:139] Memory required for data: 14946560
I0428 19:36:43.431793 23720 layer_factory.hpp:77] Creating layer pool0
I0428 19:36:43.431808 23720 net.cpp:86] Creating Layer pool0
I0428 19:36:43.431833 23720 net.cpp:408] pool0 <- conv0
I0428 19:36:43.431839 23720 net.cpp:382] pool0 -> pool0
I0428 19:36:43.431890 23720 net.cpp:124] Setting up pool0
I0428 19:36:43.431895 23720 net.cpp:131] Top shape: 64 100 12 12 (921600)
I0428 19:36:43.431898 23720 net.cpp:139] Memory required for data: 18632960
I0428 19:36:43.431901 23720 layer_factory.hpp:77] Creating layer ip1
I0428 19:36:43.431910 23720 net.cpp:86] Creating Layer ip1
I0428 19:36:43.431913 23720 net.cpp:408] ip1 <- pool0
I0428 19:36:43.431918 23720 net.cpp:382] ip1 -> ip1
I0428 19:36:43.433728 23720 net.cpp:124] Setting up ip1
I0428 19:36:43.433744 23720 net.cpp:131] Top shape: 64 10 (640)
I0428 19:36:43.433748 23720 net.cpp:139] Memory required for data: 18635520
I0428 19:36:43.433755 23720 layer_factory.hpp:77] Creating layer relu1
I0428 19:36:43.433763 23720 net.cpp:86] Creating Layer relu1
I0428 19:36:43.433766 23720 net.cpp:408] relu1 <- ip1
I0428 19:36:43.433770 23720 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:36:43.433967 23720 net.cpp:124] Setting up relu1
I0428 19:36:43.433977 23720 net.cpp:131] Top shape: 64 10 (640)
I0428 19:36:43.433981 23720 net.cpp:139] Memory required for data: 18638080
I0428 19:36:43.433984 23720 layer_factory.hpp:77] Creating layer ip2
I0428 19:36:43.433991 23720 net.cpp:86] Creating Layer ip2
I0428 19:36:43.433995 23720 net.cpp:408] ip2 <- ip1
I0428 19:36:43.434000 23720 net.cpp:382] ip2 -> ip2
I0428 19:36:43.434108 23720 net.cpp:124] Setting up ip2
I0428 19:36:43.434115 23720 net.cpp:131] Top shape: 64 10 (640)
I0428 19:36:43.434118 23720 net.cpp:139] Memory required for data: 18640640
I0428 19:36:43.434126 23720 layer_factory.hpp:77] Creating layer relu2
I0428 19:36:43.434131 23720 net.cpp:86] Creating Layer relu2
I0428 19:36:43.434134 23720 net.cpp:408] relu2 <- ip2
I0428 19:36:43.434139 23720 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:36:43.434891 23720 net.cpp:124] Setting up relu2
I0428 19:36:43.434904 23720 net.cpp:131] Top shape: 64 10 (640)
I0428 19:36:43.434923 23720 net.cpp:139] Memory required for data: 18643200
I0428 19:36:43.434927 23720 layer_factory.hpp:77] Creating layer ip3
I0428 19:36:43.434936 23720 net.cpp:86] Creating Layer ip3
I0428 19:36:43.434939 23720 net.cpp:408] ip3 <- ip2
I0428 19:36:43.434945 23720 net.cpp:382] ip3 -> ip3
I0428 19:36:43.435072 23720 net.cpp:124] Setting up ip3
I0428 19:36:43.435080 23720 net.cpp:131] Top shape: 64 10 (640)
I0428 19:36:43.435083 23720 net.cpp:139] Memory required for data: 18645760
I0428 19:36:43.435088 23720 layer_factory.hpp:77] Creating layer relu3
I0428 19:36:43.435096 23720 net.cpp:86] Creating Layer relu3
I0428 19:36:43.435098 23720 net.cpp:408] relu3 <- ip3
I0428 19:36:43.435102 23720 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:36:43.435255 23720 net.cpp:124] Setting up relu3
I0428 19:36:43.435263 23720 net.cpp:131] Top shape: 64 10 (640)
I0428 19:36:43.435266 23720 net.cpp:139] Memory required for data: 18648320
I0428 19:36:43.435269 23720 layer_factory.hpp:77] Creating layer loss
I0428 19:36:43.435276 23720 net.cpp:86] Creating Layer loss
I0428 19:36:43.435278 23720 net.cpp:408] loss <- ip3
I0428 19:36:43.435282 23720 net.cpp:408] loss <- label
I0428 19:36:43.435288 23720 net.cpp:382] loss -> loss
I0428 19:36:43.435302 23720 layer_factory.hpp:77] Creating layer loss
I0428 19:36:43.435525 23720 net.cpp:124] Setting up loss
I0428 19:36:43.435535 23720 net.cpp:131] Top shape: (1)
I0428 19:36:43.435539 23720 net.cpp:134]     with loss weight 1
I0428 19:36:43.435552 23720 net.cpp:139] Memory required for data: 18648324
I0428 19:36:43.435556 23720 net.cpp:200] loss needs backward computation.
I0428 19:36:43.435559 23720 net.cpp:200] relu3 needs backward computation.
I0428 19:36:43.435562 23720 net.cpp:200] ip3 needs backward computation.
I0428 19:36:43.435565 23720 net.cpp:200] relu2 needs backward computation.
I0428 19:36:43.435569 23720 net.cpp:200] ip2 needs backward computation.
I0428 19:36:43.435571 23720 net.cpp:200] relu1 needs backward computation.
I0428 19:36:43.435575 23720 net.cpp:200] ip1 needs backward computation.
I0428 19:36:43.435588 23720 net.cpp:200] pool0 needs backward computation.
I0428 19:36:43.435591 23720 net.cpp:200] conv0 needs backward computation.
I0428 19:36:43.435595 23720 net.cpp:202] mnist does not need backward computation.
I0428 19:36:43.435597 23720 net.cpp:244] This network produces output loss
I0428 19:36:43.435606 23720 net.cpp:257] Network initialization done.
I0428 19:36:43.435904 23720 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test247.prototxt
I0428 19:36:43.435945 23720 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:36:43.436048 23720 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool0"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:36:43.436112 23720 layer_factory.hpp:77] Creating layer mnist
I0428 19:36:43.436161 23720 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:36:43.436187 23720 net.cpp:86] Creating Layer mnist
I0428 19:36:43.436190 23720 net.cpp:382] mnist -> data
I0428 19:36:43.436198 23720 net.cpp:382] mnist -> label
I0428 19:36:43.436285 23720 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:36:43.437367 23720 net.cpp:124] Setting up mnist
I0428 19:36:43.437382 23720 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:36:43.437387 23720 net.cpp:131] Top shape: 100 (100)
I0428 19:36:43.437391 23720 net.cpp:139] Memory required for data: 314000
I0428 19:36:43.437394 23720 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:36:43.437400 23720 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:36:43.437404 23720 net.cpp:408] label_mnist_1_split <- label
I0428 19:36:43.437408 23720 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:36:43.437415 23720 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:36:43.437454 23720 net.cpp:124] Setting up label_mnist_1_split
I0428 19:36:43.437460 23720 net.cpp:131] Top shape: 100 (100)
I0428 19:36:43.437464 23720 net.cpp:131] Top shape: 100 (100)
I0428 19:36:43.437466 23720 net.cpp:139] Memory required for data: 314800
I0428 19:36:43.437479 23720 layer_factory.hpp:77] Creating layer conv0
I0428 19:36:43.437490 23720 net.cpp:86] Creating Layer conv0
I0428 19:36:43.437494 23720 net.cpp:408] conv0 <- data
I0428 19:36:43.437500 23720 net.cpp:382] conv0 -> conv0
I0428 19:36:43.439402 23720 net.cpp:124] Setting up conv0
I0428 19:36:43.439417 23720 net.cpp:131] Top shape: 100 100 24 24 (5760000)
I0428 19:36:43.439421 23720 net.cpp:139] Memory required for data: 23354800
I0428 19:36:43.439430 23720 layer_factory.hpp:77] Creating layer pool0
I0428 19:36:43.439438 23720 net.cpp:86] Creating Layer pool0
I0428 19:36:43.439442 23720 net.cpp:408] pool0 <- conv0
I0428 19:36:43.439448 23720 net.cpp:382] pool0 -> pool0
I0428 19:36:43.439484 23720 net.cpp:124] Setting up pool0
I0428 19:36:43.439491 23720 net.cpp:131] Top shape: 100 100 12 12 (1440000)
I0428 19:36:43.439493 23720 net.cpp:139] Memory required for data: 29114800
I0428 19:36:43.439496 23720 layer_factory.hpp:77] Creating layer ip1
I0428 19:36:43.439504 23720 net.cpp:86] Creating Layer ip1
I0428 19:36:43.439507 23720 net.cpp:408] ip1 <- pool0
I0428 19:36:43.439512 23720 net.cpp:382] ip1 -> ip1
I0428 19:36:43.441314 23720 net.cpp:124] Setting up ip1
I0428 19:36:43.441329 23720 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:36:43.441331 23720 net.cpp:139] Memory required for data: 29118800
I0428 19:36:43.441340 23720 layer_factory.hpp:77] Creating layer relu1
I0428 19:36:43.441347 23720 net.cpp:86] Creating Layer relu1
I0428 19:36:43.441351 23720 net.cpp:408] relu1 <- ip1
I0428 19:36:43.441355 23720 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:36:43.441606 23720 net.cpp:124] Setting up relu1
I0428 19:36:43.441617 23720 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:36:43.441619 23720 net.cpp:139] Memory required for data: 29122800
I0428 19:36:43.441623 23720 layer_factory.hpp:77] Creating layer ip2
I0428 19:36:43.441629 23720 net.cpp:86] Creating Layer ip2
I0428 19:36:43.441632 23720 net.cpp:408] ip2 <- ip1
I0428 19:36:43.441638 23720 net.cpp:382] ip2 -> ip2
I0428 19:36:43.441757 23720 net.cpp:124] Setting up ip2
I0428 19:36:43.441766 23720 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:36:43.441768 23720 net.cpp:139] Memory required for data: 29126800
I0428 19:36:43.441776 23720 layer_factory.hpp:77] Creating layer relu2
I0428 19:36:43.441788 23720 net.cpp:86] Creating Layer relu2
I0428 19:36:43.441792 23720 net.cpp:408] relu2 <- ip2
I0428 19:36:43.441797 23720 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:36:43.442555 23720 net.cpp:124] Setting up relu2
I0428 19:36:43.442569 23720 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:36:43.442600 23720 net.cpp:139] Memory required for data: 29130800
I0428 19:36:43.442605 23720 layer_factory.hpp:77] Creating layer ip3
I0428 19:36:43.442611 23720 net.cpp:86] Creating Layer ip3
I0428 19:36:43.442615 23720 net.cpp:408] ip3 <- ip2
I0428 19:36:43.442622 23720 net.cpp:382] ip3 -> ip3
I0428 19:36:43.442765 23720 net.cpp:124] Setting up ip3
I0428 19:36:43.442775 23720 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:36:43.442778 23720 net.cpp:139] Memory required for data: 29134800
I0428 19:36:43.442797 23720 layer_factory.hpp:77] Creating layer relu3
I0428 19:36:43.442806 23720 net.cpp:86] Creating Layer relu3
I0428 19:36:43.442808 23720 net.cpp:408] relu3 <- ip3
I0428 19:36:43.442812 23720 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:36:43.443056 23720 net.cpp:124] Setting up relu3
I0428 19:36:43.443065 23720 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:36:43.443068 23720 net.cpp:139] Memory required for data: 29138800
I0428 19:36:43.443071 23720 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:36:43.443078 23720 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:36:43.443081 23720 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:36:43.443085 23720 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:36:43.443092 23720 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:36:43.443130 23720 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:36:43.443135 23720 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:36:43.443150 23720 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:36:43.443153 23720 net.cpp:139] Memory required for data: 29146800
I0428 19:36:43.443156 23720 layer_factory.hpp:77] Creating layer accuracy
I0428 19:36:43.443161 23720 net.cpp:86] Creating Layer accuracy
I0428 19:36:43.443171 23720 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:36:43.443174 23720 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:36:43.443181 23720 net.cpp:382] accuracy -> accuracy
I0428 19:36:43.443189 23720 net.cpp:124] Setting up accuracy
I0428 19:36:43.443193 23720 net.cpp:131] Top shape: (1)
I0428 19:36:43.443197 23720 net.cpp:139] Memory required for data: 29146804
I0428 19:36:43.443204 23720 layer_factory.hpp:77] Creating layer loss
I0428 19:36:43.443209 23720 net.cpp:86] Creating Layer loss
I0428 19:36:43.443212 23720 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:36:43.443217 23720 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:36:43.443222 23720 net.cpp:382] loss -> loss
I0428 19:36:43.443228 23720 layer_factory.hpp:77] Creating layer loss
I0428 19:36:43.443482 23720 net.cpp:124] Setting up loss
I0428 19:36:43.443493 23720 net.cpp:131] Top shape: (1)
I0428 19:36:43.443496 23720 net.cpp:134]     with loss weight 1
I0428 19:36:43.443502 23720 net.cpp:139] Memory required for data: 29146808
I0428 19:36:43.443506 23720 net.cpp:200] loss needs backward computation.
I0428 19:36:43.443511 23720 net.cpp:202] accuracy does not need backward computation.
I0428 19:36:43.443516 23720 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:36:43.443518 23720 net.cpp:200] relu3 needs backward computation.
I0428 19:36:43.443521 23720 net.cpp:200] ip3 needs backward computation.
I0428 19:36:43.443524 23720 net.cpp:200] relu2 needs backward computation.
I0428 19:36:43.443527 23720 net.cpp:200] ip2 needs backward computation.
I0428 19:36:43.443531 23720 net.cpp:200] relu1 needs backward computation.
I0428 19:36:43.443539 23720 net.cpp:200] ip1 needs backward computation.
I0428 19:36:43.443542 23720 net.cpp:200] pool0 needs backward computation.
I0428 19:36:43.443545 23720 net.cpp:200] conv0 needs backward computation.
I0428 19:36:43.443549 23720 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:36:43.443552 23720 net.cpp:202] mnist does not need backward computation.
I0428 19:36:43.443555 23720 net.cpp:244] This network produces output accuracy
I0428 19:36:43.443558 23720 net.cpp:244] This network produces output loss
I0428 19:36:43.443569 23720 net.cpp:257] Network initialization done.
I0428 19:36:43.443606 23720 solver.cpp:56] Solver scaffolding done.
I0428 19:36:43.443919 23720 caffe.cpp:248] Starting Optimization
I0428 19:36:43.443928 23720 solver.cpp:273] Solving LeNet
I0428 19:36:43.443930 23720 solver.cpp:274] Learning Rate Policy: inv
I0428 19:36:43.444716 23720 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:36:43.607034 23727 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:36:43.612882 23720 solver.cpp:398]     Test net output #0: accuracy = 0.0987
I0428 19:36:43.612902 23720 solver.cpp:398]     Test net output #1: loss = 2.30592 (* 1 = 2.30592 loss)
I0428 19:36:43.617729 23720 solver.cpp:219] Iteration 0 (1.31008 iter/s, 0.173768s/100 iters), loss = 2.30622
I0428 19:36:43.617768 23720 solver.cpp:238]     Train net output #0: loss = 2.30622 (* 1 = 2.30622 loss)
I0428 19:36:43.617779 23720 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:36:43.843816 23720 solver.cpp:219] Iteration 100 (442.394 iter/s, 0.226043s/100 iters), loss = 0.896491
I0428 19:36:43.843843 23720 solver.cpp:238]     Train net output #0: loss = 0.896491 (* 1 = 0.896491 loss)
I0428 19:36:43.843850 23720 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:36:44.068408 23720 solver.cpp:219] Iteration 200 (445.339 iter/s, 0.224548s/100 iters), loss = 1.00189
I0428 19:36:44.068434 23720 solver.cpp:238]     Train net output #0: loss = 1.00189 (* 1 = 1.00189 loss)
I0428 19:36:44.068440 23720 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:36:44.298857 23720 solver.cpp:219] Iteration 300 (434.016 iter/s, 0.230406s/100 iters), loss = 0.537574
I0428 19:36:44.298895 23720 solver.cpp:238]     Train net output #0: loss = 0.537574 (* 1 = 0.537574 loss)
I0428 19:36:44.298902 23720 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:36:44.528218 23720 solver.cpp:219] Iteration 400 (436.1 iter/s, 0.229305s/100 iters), loss = 0.684205
I0428 19:36:44.528257 23720 solver.cpp:238]     Train net output #0: loss = 0.684205 (* 1 = 0.684205 loss)
I0428 19:36:44.528264 23720 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:36:44.753548 23720 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:36:44.911110 23727 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:36:44.915560 23720 solver.cpp:398]     Test net output #0: accuracy = 0.8831
I0428 19:36:44.915580 23720 solver.cpp:398]     Test net output #1: loss = 0.514971 (* 1 = 0.514971 loss)
I0428 19:36:44.917767 23720 solver.cpp:219] Iteration 500 (256.74 iter/s, 0.3895s/100 iters), loss = 0.590541
I0428 19:36:44.917789 23720 solver.cpp:238]     Train net output #0: loss = 0.590541 (* 1 = 0.590541 loss)
I0428 19:36:44.917795 23720 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:36:45.146024 23720 solver.cpp:219] Iteration 600 (438.178 iter/s, 0.228218s/100 iters), loss = 0.355123
I0428 19:36:45.146054 23720 solver.cpp:238]     Train net output #0: loss = 0.355123 (* 1 = 0.355123 loss)
I0428 19:36:45.146061 23720 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:36:45.372859 23720 solver.cpp:219] Iteration 700 (440.944 iter/s, 0.226786s/100 iters), loss = 0.614727
I0428 19:36:45.372891 23720 solver.cpp:238]     Train net output #0: loss = 0.614727 (* 1 = 0.614727 loss)
I0428 19:36:45.372900 23720 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:36:45.601819 23720 solver.cpp:219] Iteration 800 (436.853 iter/s, 0.22891s/100 iters), loss = 0.514685
I0428 19:36:45.601866 23720 solver.cpp:238]     Train net output #0: loss = 0.514685 (* 1 = 0.514685 loss)
I0428 19:36:45.601874 23720 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:36:45.833380 23720 solver.cpp:219] Iteration 900 (431.968 iter/s, 0.231499s/100 iters), loss = 0.487499
I0428 19:36:45.833408 23720 solver.cpp:238]     Train net output #0: loss = 0.487499 (* 1 = 0.487499 loss)
I0428 19:36:45.833415 23720 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:36:45.909144 23726 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:36:46.057963 23720 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:36:46.062110 23720 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:36:46.064633 23720 solver.cpp:311] Iteration 1000, loss = 0.394175
I0428 19:36:46.064652 23720 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:36:46.222501 23727 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:36:46.227012 23720 solver.cpp:398]     Test net output #0: accuracy = 0.9399
I0428 19:36:46.227033 23720 solver.cpp:398]     Test net output #1: loss = 0.404911 (* 1 = 0.404911 loss)
I0428 19:36:46.227038 23720 solver.cpp:316] Optimization Done.
I0428 19:36:46.227041 23720 caffe.cpp:259] Optimization Done.
