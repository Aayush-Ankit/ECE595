I0428 19:27:48.462731 21454 caffe.cpp:218] Using GPUs 0
I0428 19:27:48.506940 21454 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:27:49.020823 21454 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test4.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:27:49.020968 21454 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test4.prototxt
I0428 19:27:49.021217 21454 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:27:49.021231 21454 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:27:49.021291 21454 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0428 19:27:49.021344 21454 layer_factory.hpp:77] Creating layer mnist
I0428 19:27:49.021440 21454 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:27:49.021461 21454 net.cpp:86] Creating Layer mnist
I0428 19:27:49.021468 21454 net.cpp:382] mnist -> data
I0428 19:27:49.021489 21454 net.cpp:382] mnist -> label
I0428 19:27:49.022547 21454 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:27:49.024914 21454 net.cpp:124] Setting up mnist
I0428 19:27:49.024931 21454 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:27:49.024937 21454 net.cpp:131] Top shape: 64 (64)
I0428 19:27:49.024941 21454 net.cpp:139] Memory required for data: 200960
I0428 19:27:49.024947 21454 layer_factory.hpp:77] Creating layer ip1
I0428 19:27:49.024957 21454 net.cpp:86] Creating Layer ip1
I0428 19:27:49.024962 21454 net.cpp:408] ip1 <- data
I0428 19:27:49.024972 21454 net.cpp:382] ip1 -> ip1
I0428 19:27:49.026201 21454 net.cpp:124] Setting up ip1
I0428 19:27:49.026214 21454 net.cpp:131] Top shape: 64 10 (640)
I0428 19:27:49.026218 21454 net.cpp:139] Memory required for data: 203520
I0428 19:27:49.026232 21454 layer_factory.hpp:77] Creating layer relu1
I0428 19:27:49.026242 21454 net.cpp:86] Creating Layer relu1
I0428 19:27:49.026247 21454 net.cpp:408] relu1 <- ip1
I0428 19:27:49.026252 21454 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:27:49.260283 21454 net.cpp:124] Setting up relu1
I0428 19:27:49.260311 21454 net.cpp:131] Top shape: 64 10 (640)
I0428 19:27:49.260314 21454 net.cpp:139] Memory required for data: 206080
I0428 19:27:49.260320 21454 layer_factory.hpp:77] Creating layer ip2
I0428 19:27:49.260331 21454 net.cpp:86] Creating Layer ip2
I0428 19:27:49.260335 21454 net.cpp:408] ip2 <- ip1
I0428 19:27:49.260342 21454 net.cpp:382] ip2 -> ip2
I0428 19:27:49.260490 21454 net.cpp:124] Setting up ip2
I0428 19:27:49.260499 21454 net.cpp:131] Top shape: 64 10 (640)
I0428 19:27:49.260501 21454 net.cpp:139] Memory required for data: 208640
I0428 19:27:49.260510 21454 layer_factory.hpp:77] Creating layer relu2
I0428 19:27:49.260534 21454 net.cpp:86] Creating Layer relu2
I0428 19:27:49.260537 21454 net.cpp:408] relu2 <- ip2
I0428 19:27:49.260541 21454 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:27:49.261340 21454 net.cpp:124] Setting up relu2
I0428 19:27:49.261353 21454 net.cpp:131] Top shape: 64 10 (640)
I0428 19:27:49.261373 21454 net.cpp:139] Memory required for data: 211200
I0428 19:27:49.261375 21454 layer_factory.hpp:77] Creating layer loss
I0428 19:27:49.261381 21454 net.cpp:86] Creating Layer loss
I0428 19:27:49.261384 21454 net.cpp:408] loss <- ip2
I0428 19:27:49.261389 21454 net.cpp:408] loss <- label
I0428 19:27:49.261394 21454 net.cpp:382] loss -> loss
I0428 19:27:49.261431 21454 layer_factory.hpp:77] Creating layer loss
I0428 19:27:49.262742 21454 net.cpp:124] Setting up loss
I0428 19:27:49.262768 21454 net.cpp:131] Top shape: (1)
I0428 19:27:49.262773 21454 net.cpp:134]     with loss weight 1
I0428 19:27:49.262804 21454 net.cpp:139] Memory required for data: 211204
I0428 19:27:49.262807 21454 net.cpp:200] loss needs backward computation.
I0428 19:27:49.262810 21454 net.cpp:200] relu2 needs backward computation.
I0428 19:27:49.262814 21454 net.cpp:200] ip2 needs backward computation.
I0428 19:27:49.262831 21454 net.cpp:200] relu1 needs backward computation.
I0428 19:27:49.262833 21454 net.cpp:200] ip1 needs backward computation.
I0428 19:27:49.262837 21454 net.cpp:202] mnist does not need backward computation.
I0428 19:27:49.262840 21454 net.cpp:244] This network produces output loss
I0428 19:27:49.262846 21454 net.cpp:257] Network initialization done.
I0428 19:27:49.263053 21454 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test4.prototxt
I0428 19:27:49.263072 21454 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:27:49.263146 21454 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0428 19:27:49.263200 21454 layer_factory.hpp:77] Creating layer mnist
I0428 19:27:49.263262 21454 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:27:49.263274 21454 net.cpp:86] Creating Layer mnist
I0428 19:27:49.263278 21454 net.cpp:382] mnist -> data
I0428 19:27:49.263286 21454 net.cpp:382] mnist -> label
I0428 19:27:49.263371 21454 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:27:49.265668 21454 net.cpp:124] Setting up mnist
I0428 19:27:49.265712 21454 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:27:49.265717 21454 net.cpp:131] Top shape: 100 (100)
I0428 19:27:49.265736 21454 net.cpp:139] Memory required for data: 314000
I0428 19:27:49.265740 21454 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:27:49.265794 21454 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:27:49.265810 21454 net.cpp:408] label_mnist_1_split <- label
I0428 19:27:49.265815 21454 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:27:49.265833 21454 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:27:49.265913 21454 net.cpp:124] Setting up label_mnist_1_split
I0428 19:27:49.265920 21454 net.cpp:131] Top shape: 100 (100)
I0428 19:27:49.265923 21454 net.cpp:131] Top shape: 100 (100)
I0428 19:27:49.265926 21454 net.cpp:139] Memory required for data: 314800
I0428 19:27:49.265929 21454 layer_factory.hpp:77] Creating layer ip1
I0428 19:27:49.265936 21454 net.cpp:86] Creating Layer ip1
I0428 19:27:49.265939 21454 net.cpp:408] ip1 <- data
I0428 19:27:49.265944 21454 net.cpp:382] ip1 -> ip1
I0428 19:27:49.266106 21454 net.cpp:124] Setting up ip1
I0428 19:27:49.266114 21454 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:27:49.266118 21454 net.cpp:139] Memory required for data: 318800
I0428 19:27:49.266126 21454 layer_factory.hpp:77] Creating layer relu1
I0428 19:27:49.266131 21454 net.cpp:86] Creating Layer relu1
I0428 19:27:49.266134 21454 net.cpp:408] relu1 <- ip1
I0428 19:27:49.266139 21454 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:27:49.266340 21454 net.cpp:124] Setting up relu1
I0428 19:27:49.266347 21454 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:27:49.266351 21454 net.cpp:139] Memory required for data: 322800
I0428 19:27:49.266355 21454 layer_factory.hpp:77] Creating layer ip2
I0428 19:27:49.266360 21454 net.cpp:86] Creating Layer ip2
I0428 19:27:49.266365 21454 net.cpp:408] ip2 <- ip1
I0428 19:27:49.266368 21454 net.cpp:382] ip2 -> ip2
I0428 19:27:49.266459 21454 net.cpp:124] Setting up ip2
I0428 19:27:49.266466 21454 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:27:49.266469 21454 net.cpp:139] Memory required for data: 326800
I0428 19:27:49.266476 21454 layer_factory.hpp:77] Creating layer relu2
I0428 19:27:49.266480 21454 net.cpp:86] Creating Layer relu2
I0428 19:27:49.266484 21454 net.cpp:408] relu2 <- ip2
I0428 19:27:49.266487 21454 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:27:49.267379 21454 net.cpp:124] Setting up relu2
I0428 19:27:49.267408 21454 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:27:49.267412 21454 net.cpp:139] Memory required for data: 330800
I0428 19:27:49.267416 21454 layer_factory.hpp:77] Creating layer ip2_relu2_0_split
I0428 19:27:49.267421 21454 net.cpp:86] Creating Layer ip2_relu2_0_split
I0428 19:27:49.267426 21454 net.cpp:408] ip2_relu2_0_split <- ip2
I0428 19:27:49.267431 21454 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_0
I0428 19:27:49.267437 21454 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_1
I0428 19:27:49.267504 21454 net.cpp:124] Setting up ip2_relu2_0_split
I0428 19:27:49.267511 21454 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:27:49.267515 21454 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:27:49.267518 21454 net.cpp:139] Memory required for data: 338800
I0428 19:27:49.267521 21454 layer_factory.hpp:77] Creating layer accuracy
I0428 19:27:49.267526 21454 net.cpp:86] Creating Layer accuracy
I0428 19:27:49.267529 21454 net.cpp:408] accuracy <- ip2_relu2_0_split_0
I0428 19:27:49.267534 21454 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:27:49.267539 21454 net.cpp:382] accuracy -> accuracy
I0428 19:27:49.267546 21454 net.cpp:124] Setting up accuracy
I0428 19:27:49.267550 21454 net.cpp:131] Top shape: (1)
I0428 19:27:49.267554 21454 net.cpp:139] Memory required for data: 338804
I0428 19:27:49.267556 21454 layer_factory.hpp:77] Creating layer loss
I0428 19:27:49.267562 21454 net.cpp:86] Creating Layer loss
I0428 19:27:49.267565 21454 net.cpp:408] loss <- ip2_relu2_0_split_1
I0428 19:27:49.267570 21454 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:27:49.267573 21454 net.cpp:382] loss -> loss
I0428 19:27:49.267580 21454 layer_factory.hpp:77] Creating layer loss
I0428 19:27:49.267834 21454 net.cpp:124] Setting up loss
I0428 19:27:49.267843 21454 net.cpp:131] Top shape: (1)
I0428 19:27:49.267846 21454 net.cpp:134]     with loss weight 1
I0428 19:27:49.267853 21454 net.cpp:139] Memory required for data: 338808
I0428 19:27:49.267855 21454 net.cpp:200] loss needs backward computation.
I0428 19:27:49.267869 21454 net.cpp:202] accuracy does not need backward computation.
I0428 19:27:49.267874 21454 net.cpp:200] ip2_relu2_0_split needs backward computation.
I0428 19:27:49.267876 21454 net.cpp:200] relu2 needs backward computation.
I0428 19:27:49.267879 21454 net.cpp:200] ip2 needs backward computation.
I0428 19:27:49.267882 21454 net.cpp:200] relu1 needs backward computation.
I0428 19:27:49.267884 21454 net.cpp:200] ip1 needs backward computation.
I0428 19:27:49.267889 21454 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:27:49.267892 21454 net.cpp:202] mnist does not need backward computation.
I0428 19:27:49.267894 21454 net.cpp:244] This network produces output accuracy
I0428 19:27:49.267899 21454 net.cpp:244] This network produces output loss
I0428 19:27:49.267906 21454 net.cpp:257] Network initialization done.
I0428 19:27:49.267933 21454 solver.cpp:56] Solver scaffolding done.
I0428 19:27:49.268155 21454 caffe.cpp:248] Starting Optimization
I0428 19:27:49.268162 21454 solver.cpp:273] Solving LeNet
I0428 19:27:49.268164 21454 solver.cpp:274] Learning Rate Policy: inv
I0428 19:27:49.268237 21454 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:27:49.268303 21454 blocking_queue.cpp:49] Waiting for data
I0428 19:27:49.347395 21461 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:27:49.347784 21454 solver.cpp:398]     Test net output #0: accuracy = 0.0919
I0428 19:27:49.347806 21454 solver.cpp:398]     Test net output #1: loss = 2.30867 (* 1 = 2.30867 loss)
I0428 19:27:49.348330 21454 solver.cpp:219] Iteration 0 (-1.03136e-42 iter/s, 0.0801403s/100 iters), loss = 2.31309
I0428 19:27:49.348352 21454 solver.cpp:238]     Train net output #0: loss = 2.31309 (* 1 = 2.31309 loss)
I0428 19:27:49.348364 21454 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:27:49.405177 21454 solver.cpp:219] Iteration 100 (1760.05 iter/s, 0.0568166s/100 iters), loss = 1.5786
I0428 19:27:49.405200 21454 solver.cpp:238]     Train net output #0: loss = 1.5786 (* 1 = 1.5786 loss)
I0428 19:27:49.405206 21454 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:27:49.452769 21454 solver.cpp:219] Iteration 200 (2102.62 iter/s, 0.0475597s/100 iters), loss = 1.07339
I0428 19:27:49.452807 21454 solver.cpp:238]     Train net output #0: loss = 1.07339 (* 1 = 1.07339 loss)
I0428 19:27:49.452819 21454 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:27:49.494418 21454 solver.cpp:219] Iteration 300 (2402.66 iter/s, 0.0416205s/100 iters), loss = 0.974463
I0428 19:27:49.494441 21454 solver.cpp:238]     Train net output #0: loss = 0.974463 (* 1 = 0.974463 loss)
I0428 19:27:49.494446 21454 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:27:49.534914 21454 solver.cpp:219] Iteration 400 (2471.18 iter/s, 0.0404664s/100 iters), loss = 0.819148
I0428 19:27:49.534950 21454 solver.cpp:238]     Train net output #0: loss = 0.819148 (* 1 = 0.819148 loss)
I0428 19:27:49.534956 21454 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:27:49.575222 21454 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:27:49.652968 21461 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:27:49.653408 21454 solver.cpp:398]     Test net output #0: accuracy = 0.7084
I0428 19:27:49.653434 21454 solver.cpp:398]     Test net output #1: loss = 0.838315 (* 1 = 0.838315 loss)
I0428 19:27:49.653957 21454 solver.cpp:219] Iteration 500 (840.277 iter/s, 0.119008s/100 iters), loss = 0.894107
I0428 19:27:49.653988 21454 solver.cpp:238]     Train net output #0: loss = 0.894107 (* 1 = 0.894107 loss)
I0428 19:27:49.654000 21454 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:27:49.710726 21454 solver.cpp:219] Iteration 600 (1762.72 iter/s, 0.0567306s/100 iters), loss = 0.956026
I0428 19:27:49.710758 21454 solver.cpp:238]     Train net output #0: loss = 0.956026 (* 1 = 0.956026 loss)
I0428 19:27:49.710767 21454 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:27:49.758244 21454 solver.cpp:219] Iteration 700 (2106.21 iter/s, 0.0474788s/100 iters), loss = 0.840692
I0428 19:27:49.758285 21454 solver.cpp:238]     Train net output #0: loss = 0.840692 (* 1 = 0.840692 loss)
I0428 19:27:49.758292 21454 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:27:49.799484 21454 solver.cpp:219] Iteration 800 (2427.57 iter/s, 0.0411935s/100 iters), loss = 0.912115
I0428 19:27:49.799506 21454 solver.cpp:238]     Train net output #0: loss = 0.912115 (* 1 = 0.912115 loss)
I0428 19:27:49.799515 21454 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:27:49.807977 21454 blocking_queue.cpp:49] Waiting for data
I0428 19:27:49.839814 21454 solver.cpp:219] Iteration 900 (2481.35 iter/s, 0.0403006s/100 iters), loss = 0.992899
I0428 19:27:49.839835 21454 solver.cpp:238]     Train net output #0: loss = 0.992899 (* 1 = 0.992899 loss)
I0428 19:27:49.839841 21454 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:27:49.854300 21460 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:27:49.884662 21454 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:27:49.885254 21454 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:27:49.885727 21454 solver.cpp:311] Iteration 1000, loss = 0.776652
I0428 19:27:49.885749 21454 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:27:49.940524 21461 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:27:49.941058 21454 solver.cpp:398]     Test net output #0: accuracy = 0.7219
I0428 19:27:49.941100 21454 solver.cpp:398]     Test net output #1: loss = 0.75765 (* 1 = 0.75765 loss)
I0428 19:27:49.941110 21454 solver.cpp:316] Optimization Done.
I0428 19:27:49.941123 21454 caffe.cpp:259] Optimization Done.
