I0428 20:00:32.778028 29501 caffe.cpp:218] Using GPUs 0
I0428 20:00:32.814946 29501 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:00:33.327227 29501 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test879.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:00:33.327368 29501 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test879.prototxt
I0428 20:00:33.327783 29501 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:00:33.327802 29501 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:00:33.327904 29501 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:00:33.327983 29501 layer_factory.hpp:77] Creating layer mnist
I0428 20:00:33.328083 29501 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:00:33.328107 29501 net.cpp:86] Creating Layer mnist
I0428 20:00:33.328117 29501 net.cpp:382] mnist -> data
I0428 20:00:33.328140 29501 net.cpp:382] mnist -> label
I0428 20:00:33.329247 29501 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:00:33.331699 29501 net.cpp:124] Setting up mnist
I0428 20:00:33.331717 29501 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:00:33.331725 29501 net.cpp:131] Top shape: 64 (64)
I0428 20:00:33.331728 29501 net.cpp:139] Memory required for data: 200960
I0428 20:00:33.331737 29501 layer_factory.hpp:77] Creating layer conv0
I0428 20:00:33.331751 29501 net.cpp:86] Creating Layer conv0
I0428 20:00:33.331771 29501 net.cpp:408] conv0 <- data
I0428 20:00:33.331789 29501 net.cpp:382] conv0 -> conv0
I0428 20:00:33.608176 29501 net.cpp:124] Setting up conv0
I0428 20:00:33.608202 29501 net.cpp:131] Top shape: 64 10 24 24 (368640)
I0428 20:00:33.608206 29501 net.cpp:139] Memory required for data: 1675520
I0428 20:00:33.608220 29501 layer_factory.hpp:77] Creating layer pool0
I0428 20:00:33.608233 29501 net.cpp:86] Creating Layer pool0
I0428 20:00:33.608237 29501 net.cpp:408] pool0 <- conv0
I0428 20:00:33.608242 29501 net.cpp:382] pool0 -> pool0
I0428 20:00:33.608300 29501 net.cpp:124] Setting up pool0
I0428 20:00:33.608307 29501 net.cpp:131] Top shape: 64 10 12 12 (92160)
I0428 20:00:33.608310 29501 net.cpp:139] Memory required for data: 2044160
I0428 20:00:33.608314 29501 layer_factory.hpp:77] Creating layer conv1
I0428 20:00:33.608324 29501 net.cpp:86] Creating Layer conv1
I0428 20:00:33.608328 29501 net.cpp:408] conv1 <- pool0
I0428 20:00:33.608347 29501 net.cpp:382] conv1 -> conv1
I0428 20:00:33.611021 29501 net.cpp:124] Setting up conv1
I0428 20:00:33.611035 29501 net.cpp:131] Top shape: 64 25 8 8 (102400)
I0428 20:00:33.611054 29501 net.cpp:139] Memory required for data: 2453760
I0428 20:00:33.611063 29501 layer_factory.hpp:77] Creating layer pool1
I0428 20:00:33.611070 29501 net.cpp:86] Creating Layer pool1
I0428 20:00:33.611073 29501 net.cpp:408] pool1 <- conv1
I0428 20:00:33.611078 29501 net.cpp:382] pool1 -> pool1
I0428 20:00:33.611130 29501 net.cpp:124] Setting up pool1
I0428 20:00:33.611135 29501 net.cpp:131] Top shape: 64 25 4 4 (25600)
I0428 20:00:33.611138 29501 net.cpp:139] Memory required for data: 2556160
I0428 20:00:33.611141 29501 layer_factory.hpp:77] Creating layer ip1
I0428 20:00:33.611148 29501 net.cpp:86] Creating Layer ip1
I0428 20:00:33.611151 29501 net.cpp:408] ip1 <- pool1
I0428 20:00:33.611156 29501 net.cpp:382] ip1 -> ip1
I0428 20:00:33.611304 29501 net.cpp:124] Setting up ip1
I0428 20:00:33.611311 29501 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:00:33.611315 29501 net.cpp:139] Memory required for data: 2562560
I0428 20:00:33.611321 29501 layer_factory.hpp:77] Creating layer relu1
I0428 20:00:33.611328 29501 net.cpp:86] Creating Layer relu1
I0428 20:00:33.611331 29501 net.cpp:408] relu1 <- ip1
I0428 20:00:33.611335 29501 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:00:33.611508 29501 net.cpp:124] Setting up relu1
I0428 20:00:33.611517 29501 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:00:33.611521 29501 net.cpp:139] Memory required for data: 2568960
I0428 20:00:33.611523 29501 layer_factory.hpp:77] Creating layer ip2
I0428 20:00:33.611528 29501 net.cpp:86] Creating Layer ip2
I0428 20:00:33.611531 29501 net.cpp:408] ip2 <- ip1
I0428 20:00:33.611536 29501 net.cpp:382] ip2 -> ip2
I0428 20:00:33.611627 29501 net.cpp:124] Setting up ip2
I0428 20:00:33.611634 29501 net.cpp:131] Top shape: 64 10 (640)
I0428 20:00:33.611637 29501 net.cpp:139] Memory required for data: 2571520
I0428 20:00:33.611642 29501 layer_factory.hpp:77] Creating layer relu2
I0428 20:00:33.611647 29501 net.cpp:86] Creating Layer relu2
I0428 20:00:33.611650 29501 net.cpp:408] relu2 <- ip2
I0428 20:00:33.611654 29501 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:00:33.612409 29501 net.cpp:124] Setting up relu2
I0428 20:00:33.612421 29501 net.cpp:131] Top shape: 64 10 (640)
I0428 20:00:33.612440 29501 net.cpp:139] Memory required for data: 2574080
I0428 20:00:33.612444 29501 layer_factory.hpp:77] Creating layer ip3
I0428 20:00:33.612450 29501 net.cpp:86] Creating Layer ip3
I0428 20:00:33.612453 29501 net.cpp:408] ip3 <- ip2
I0428 20:00:33.612458 29501 net.cpp:382] ip3 -> ip3
I0428 20:00:33.612567 29501 net.cpp:124] Setting up ip3
I0428 20:00:33.612574 29501 net.cpp:131] Top shape: 64 10 (640)
I0428 20:00:33.612577 29501 net.cpp:139] Memory required for data: 2576640
I0428 20:00:33.612586 29501 layer_factory.hpp:77] Creating layer relu3
I0428 20:00:33.612591 29501 net.cpp:86] Creating Layer relu3
I0428 20:00:33.612593 29501 net.cpp:408] relu3 <- ip3
I0428 20:00:33.612597 29501 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:00:33.612778 29501 net.cpp:124] Setting up relu3
I0428 20:00:33.612787 29501 net.cpp:131] Top shape: 64 10 (640)
I0428 20:00:33.612790 29501 net.cpp:139] Memory required for data: 2579200
I0428 20:00:33.612793 29501 layer_factory.hpp:77] Creating layer loss
I0428 20:00:33.612799 29501 net.cpp:86] Creating Layer loss
I0428 20:00:33.612802 29501 net.cpp:408] loss <- ip3
I0428 20:00:33.612807 29501 net.cpp:408] loss <- label
I0428 20:00:33.612834 29501 net.cpp:382] loss -> loss
I0428 20:00:33.612853 29501 layer_factory.hpp:77] Creating layer loss
I0428 20:00:33.613103 29501 net.cpp:124] Setting up loss
I0428 20:00:33.613112 29501 net.cpp:131] Top shape: (1)
I0428 20:00:33.613116 29501 net.cpp:134]     with loss weight 1
I0428 20:00:33.613131 29501 net.cpp:139] Memory required for data: 2579204
I0428 20:00:33.613134 29501 net.cpp:200] loss needs backward computation.
I0428 20:00:33.613137 29501 net.cpp:200] relu3 needs backward computation.
I0428 20:00:33.613140 29501 net.cpp:200] ip3 needs backward computation.
I0428 20:00:33.613144 29501 net.cpp:200] relu2 needs backward computation.
I0428 20:00:33.613162 29501 net.cpp:200] ip2 needs backward computation.
I0428 20:00:33.613164 29501 net.cpp:200] relu1 needs backward computation.
I0428 20:00:33.613168 29501 net.cpp:200] ip1 needs backward computation.
I0428 20:00:33.613170 29501 net.cpp:200] pool1 needs backward computation.
I0428 20:00:33.613173 29501 net.cpp:200] conv1 needs backward computation.
I0428 20:00:33.613176 29501 net.cpp:200] pool0 needs backward computation.
I0428 20:00:33.613179 29501 net.cpp:200] conv0 needs backward computation.
I0428 20:00:33.613183 29501 net.cpp:202] mnist does not need backward computation.
I0428 20:00:33.613185 29501 net.cpp:244] This network produces output loss
I0428 20:00:33.613209 29501 net.cpp:257] Network initialization done.
I0428 20:00:33.613556 29501 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test879.prototxt
I0428 20:00:33.613597 29501 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:00:33.613698 29501 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:00:33.613775 29501 layer_factory.hpp:77] Creating layer mnist
I0428 20:00:33.613817 29501 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:00:33.613829 29501 net.cpp:86] Creating Layer mnist
I0428 20:00:33.613833 29501 net.cpp:382] mnist -> data
I0428 20:00:33.613842 29501 net.cpp:382] mnist -> label
I0428 20:00:33.613931 29501 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:00:33.616065 29501 net.cpp:124] Setting up mnist
I0428 20:00:33.616093 29501 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:00:33.616098 29501 net.cpp:131] Top shape: 100 (100)
I0428 20:00:33.616101 29501 net.cpp:139] Memory required for data: 314000
I0428 20:00:33.616104 29501 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:00:33.616111 29501 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:00:33.616114 29501 net.cpp:408] label_mnist_1_split <- label
I0428 20:00:33.616118 29501 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:00:33.616125 29501 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:00:33.616160 29501 net.cpp:124] Setting up label_mnist_1_split
I0428 20:00:33.616165 29501 net.cpp:131] Top shape: 100 (100)
I0428 20:00:33.616168 29501 net.cpp:131] Top shape: 100 (100)
I0428 20:00:33.616171 29501 net.cpp:139] Memory required for data: 314800
I0428 20:00:33.616174 29501 layer_factory.hpp:77] Creating layer conv0
I0428 20:00:33.616181 29501 net.cpp:86] Creating Layer conv0
I0428 20:00:33.616184 29501 net.cpp:408] conv0 <- data
I0428 20:00:33.616189 29501 net.cpp:382] conv0 -> conv0
I0428 20:00:33.617946 29501 net.cpp:124] Setting up conv0
I0428 20:00:33.617960 29501 net.cpp:131] Top shape: 100 10 24 24 (576000)
I0428 20:00:33.617964 29501 net.cpp:139] Memory required for data: 2618800
I0428 20:00:33.617972 29501 layer_factory.hpp:77] Creating layer pool0
I0428 20:00:33.617979 29501 net.cpp:86] Creating Layer pool0
I0428 20:00:33.617993 29501 net.cpp:408] pool0 <- conv0
I0428 20:00:33.618015 29501 net.cpp:382] pool0 -> pool0
I0428 20:00:33.618053 29501 net.cpp:124] Setting up pool0
I0428 20:00:33.618059 29501 net.cpp:131] Top shape: 100 10 12 12 (144000)
I0428 20:00:33.618062 29501 net.cpp:139] Memory required for data: 3194800
I0428 20:00:33.618065 29501 layer_factory.hpp:77] Creating layer conv1
I0428 20:00:33.618073 29501 net.cpp:86] Creating Layer conv1
I0428 20:00:33.618077 29501 net.cpp:408] conv1 <- pool0
I0428 20:00:33.618080 29501 net.cpp:382] conv1 -> conv1
I0428 20:00:33.620120 29501 net.cpp:124] Setting up conv1
I0428 20:00:33.620133 29501 net.cpp:131] Top shape: 100 25 8 8 (160000)
I0428 20:00:33.620137 29501 net.cpp:139] Memory required for data: 3834800
I0428 20:00:33.620146 29501 layer_factory.hpp:77] Creating layer pool1
I0428 20:00:33.620153 29501 net.cpp:86] Creating Layer pool1
I0428 20:00:33.620157 29501 net.cpp:408] pool1 <- conv1
I0428 20:00:33.620162 29501 net.cpp:382] pool1 -> pool1
I0428 20:00:33.620230 29501 net.cpp:124] Setting up pool1
I0428 20:00:33.620250 29501 net.cpp:131] Top shape: 100 25 4 4 (40000)
I0428 20:00:33.620261 29501 net.cpp:139] Memory required for data: 3994800
I0428 20:00:33.620265 29501 layer_factory.hpp:77] Creating layer ip1
I0428 20:00:33.620270 29501 net.cpp:86] Creating Layer ip1
I0428 20:00:33.620275 29501 net.cpp:408] ip1 <- pool1
I0428 20:00:33.620280 29501 net.cpp:382] ip1 -> ip1
I0428 20:00:33.620429 29501 net.cpp:124] Setting up ip1
I0428 20:00:33.620436 29501 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:00:33.620448 29501 net.cpp:139] Memory required for data: 4004800
I0428 20:00:33.620456 29501 layer_factory.hpp:77] Creating layer relu1
I0428 20:00:33.620461 29501 net.cpp:86] Creating Layer relu1
I0428 20:00:33.620465 29501 net.cpp:408] relu1 <- ip1
I0428 20:00:33.620468 29501 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:00:33.620741 29501 net.cpp:124] Setting up relu1
I0428 20:00:33.620750 29501 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:00:33.620754 29501 net.cpp:139] Memory required for data: 4014800
I0428 20:00:33.620762 29501 layer_factory.hpp:77] Creating layer ip2
I0428 20:00:33.620769 29501 net.cpp:86] Creating Layer ip2
I0428 20:00:33.620772 29501 net.cpp:408] ip2 <- ip1
I0428 20:00:33.620782 29501 net.cpp:382] ip2 -> ip2
I0428 20:00:33.620935 29501 net.cpp:124] Setting up ip2
I0428 20:00:33.620944 29501 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:00:33.620946 29501 net.cpp:139] Memory required for data: 4018800
I0428 20:00:33.620952 29501 layer_factory.hpp:77] Creating layer relu2
I0428 20:00:33.620957 29501 net.cpp:86] Creating Layer relu2
I0428 20:00:33.620961 29501 net.cpp:408] relu2 <- ip2
I0428 20:00:33.620972 29501 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:00:33.621137 29501 net.cpp:124] Setting up relu2
I0428 20:00:33.621146 29501 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:00:33.621150 29501 net.cpp:139] Memory required for data: 4022800
I0428 20:00:33.621160 29501 layer_factory.hpp:77] Creating layer ip3
I0428 20:00:33.621181 29501 net.cpp:86] Creating Layer ip3
I0428 20:00:33.621203 29501 net.cpp:408] ip3 <- ip2
I0428 20:00:33.621208 29501 net.cpp:382] ip3 -> ip3
I0428 20:00:33.621314 29501 net.cpp:124] Setting up ip3
I0428 20:00:33.621320 29501 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:00:33.621323 29501 net.cpp:139] Memory required for data: 4026800
I0428 20:00:33.621330 29501 layer_factory.hpp:77] Creating layer relu3
I0428 20:00:33.621335 29501 net.cpp:86] Creating Layer relu3
I0428 20:00:33.621337 29501 net.cpp:408] relu3 <- ip3
I0428 20:00:33.621341 29501 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:00:33.622099 29501 net.cpp:124] Setting up relu3
I0428 20:00:33.622112 29501 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:00:33.622114 29501 net.cpp:139] Memory required for data: 4030800
I0428 20:00:33.622118 29501 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:00:33.622123 29501 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:00:33.622126 29501 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:00:33.622131 29501 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:00:33.622138 29501 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:00:33.622185 29501 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:00:33.622191 29501 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:00:33.622195 29501 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:00:33.622197 29501 net.cpp:139] Memory required for data: 4038800
I0428 20:00:33.622200 29501 layer_factory.hpp:77] Creating layer accuracy
I0428 20:00:33.622205 29501 net.cpp:86] Creating Layer accuracy
I0428 20:00:33.622208 29501 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:00:33.622212 29501 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:00:33.622217 29501 net.cpp:382] accuracy -> accuracy
I0428 20:00:33.622223 29501 net.cpp:124] Setting up accuracy
I0428 20:00:33.622227 29501 net.cpp:131] Top shape: (1)
I0428 20:00:33.622229 29501 net.cpp:139] Memory required for data: 4038804
I0428 20:00:33.622232 29501 layer_factory.hpp:77] Creating layer loss
I0428 20:00:33.622236 29501 net.cpp:86] Creating Layer loss
I0428 20:00:33.622239 29501 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:00:33.622243 29501 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:00:33.622247 29501 net.cpp:382] loss -> loss
I0428 20:00:33.622252 29501 layer_factory.hpp:77] Creating layer loss
I0428 20:00:33.622510 29501 net.cpp:124] Setting up loss
I0428 20:00:33.622519 29501 net.cpp:131] Top shape: (1)
I0428 20:00:33.622522 29501 net.cpp:134]     with loss weight 1
I0428 20:00:33.622539 29501 net.cpp:139] Memory required for data: 4038808
I0428 20:00:33.622544 29501 net.cpp:200] loss needs backward computation.
I0428 20:00:33.622553 29501 net.cpp:202] accuracy does not need backward computation.
I0428 20:00:33.622557 29501 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:00:33.622566 29501 net.cpp:200] relu3 needs backward computation.
I0428 20:00:33.622568 29501 net.cpp:200] ip3 needs backward computation.
I0428 20:00:33.622571 29501 net.cpp:200] relu2 needs backward computation.
I0428 20:00:33.622575 29501 net.cpp:200] ip2 needs backward computation.
I0428 20:00:33.622576 29501 net.cpp:200] relu1 needs backward computation.
I0428 20:00:33.622586 29501 net.cpp:200] ip1 needs backward computation.
I0428 20:00:33.622588 29501 net.cpp:200] pool1 needs backward computation.
I0428 20:00:33.622591 29501 net.cpp:200] conv1 needs backward computation.
I0428 20:00:33.622611 29501 net.cpp:200] pool0 needs backward computation.
I0428 20:00:33.622613 29501 net.cpp:200] conv0 needs backward computation.
I0428 20:00:33.622617 29501 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:00:33.622622 29501 net.cpp:202] mnist does not need backward computation.
I0428 20:00:33.622623 29501 net.cpp:244] This network produces output accuracy
I0428 20:00:33.622627 29501 net.cpp:244] This network produces output loss
I0428 20:00:33.622643 29501 net.cpp:257] Network initialization done.
I0428 20:00:33.622684 29501 solver.cpp:56] Solver scaffolding done.
I0428 20:00:33.623020 29501 caffe.cpp:248] Starting Optimization
I0428 20:00:33.623028 29501 solver.cpp:273] Solving LeNet
I0428 20:00:33.623029 29501 solver.cpp:274] Learning Rate Policy: inv
I0428 20:00:33.623203 29501 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:00:33.627765 29501 blocking_queue.cpp:49] Waiting for data
I0428 20:00:33.697798 29508 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:00:33.698344 29501 solver.cpp:398]     Test net output #0: accuracy = 0.1107
I0428 20:00:33.698364 29501 solver.cpp:398]     Test net output #1: loss = 2.30704 (* 1 = 2.30704 loss)
I0428 20:00:33.700688 29501 solver.cpp:219] Iteration 0 (-1.5051e-30 iter/s, 0.0776346s/100 iters), loss = 2.3039
I0428 20:00:33.700727 29501 solver.cpp:238]     Train net output #0: loss = 2.3039 (* 1 = 2.3039 loss)
I0428 20:00:33.700740 29501 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:00:33.796887 29501 solver.cpp:219] Iteration 100 (1040.05 iter/s, 0.0961493s/100 iters), loss = 2.03023
I0428 20:00:33.796929 29501 solver.cpp:238]     Train net output #0: loss = 2.03023 (* 1 = 2.03023 loss)
I0428 20:00:33.796952 29501 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:00:33.884218 29501 solver.cpp:219] Iteration 200 (1145.74 iter/s, 0.0872796s/100 iters), loss = 1.8986
I0428 20:00:33.884241 29501 solver.cpp:238]     Train net output #0: loss = 1.8986 (* 1 = 1.8986 loss)
I0428 20:00:33.884248 29501 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:00:33.972379 29501 solver.cpp:219] Iteration 300 (1134.71 iter/s, 0.0881285s/100 iters), loss = 1.39735
I0428 20:00:33.972404 29501 solver.cpp:238]     Train net output #0: loss = 1.39735 (* 1 = 1.39735 loss)
I0428 20:00:33.972410 29501 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:00:34.060303 29501 solver.cpp:219] Iteration 400 (1137.79 iter/s, 0.0878899s/100 iters), loss = 1.80102
I0428 20:00:34.060326 29501 solver.cpp:238]     Train net output #0: loss = 1.80102 (* 1 = 1.80102 loss)
I0428 20:00:34.060348 29501 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:00:34.147011 29501 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:00:34.222566 29508 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:00:34.223116 29501 solver.cpp:398]     Test net output #0: accuracy = 0.4469
I0428 20:00:34.223137 29501 solver.cpp:398]     Test net output #1: loss = 1.59999 (* 1 = 1.59999 loss)
I0428 20:00:34.224092 29501 solver.cpp:219] Iteration 500 (610.68 iter/s, 0.163752s/100 iters), loss = 1.78105
I0428 20:00:34.224133 29501 solver.cpp:238]     Train net output #0: loss = 1.78105 (* 1 = 1.78105 loss)
I0428 20:00:34.224153 29501 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:00:34.323220 29501 solver.cpp:219] Iteration 600 (1009.31 iter/s, 0.0990771s/100 iters), loss = 1.53013
I0428 20:00:34.323246 29501 solver.cpp:238]     Train net output #0: loss = 1.53013 (* 1 = 1.53013 loss)
I0428 20:00:34.323267 29501 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:00:34.416183 29501 solver.cpp:219] Iteration 700 (1076.1 iter/s, 0.092928s/100 iters), loss = 1.40716
I0428 20:00:34.416210 29501 solver.cpp:238]     Train net output #0: loss = 1.40716 (* 1 = 1.40716 loss)
I0428 20:00:34.416231 29501 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:00:34.511068 29501 solver.cpp:219] Iteration 800 (1054.3 iter/s, 0.0948499s/100 iters), loss = 1.17104
I0428 20:00:34.511092 29501 solver.cpp:238]     Train net output #0: loss = 1.17104 (* 1 = 1.17104 loss)
I0428 20:00:34.511109 29501 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:00:34.599390 29501 solver.cpp:219] Iteration 900 (1132.67 iter/s, 0.0882873s/100 iters), loss = 1.0972
I0428 20:00:34.599427 29501 solver.cpp:238]     Train net output #0: loss = 1.0972 (* 1 = 1.0972 loss)
I0428 20:00:34.599433 29501 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:00:34.628978 29507 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:00:34.688302 29501 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:00:34.689344 29501 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:00:34.689924 29501 solver.cpp:311] Iteration 1000, loss = 1.28358
I0428 20:00:34.689937 29501 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:00:34.761785 29508 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:00:34.762333 29501 solver.cpp:398]     Test net output #0: accuracy = 0.5761
I0428 20:00:34.762367 29501 solver.cpp:398]     Test net output #1: loss = 1.24365 (* 1 = 1.24365 loss)
I0428 20:00:34.762372 29501 solver.cpp:316] Optimization Done.
I0428 20:00:34.762374 29501 caffe.cpp:259] Optimization Done.
