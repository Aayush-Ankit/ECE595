I0428 19:33:13.774706 23055 caffe.cpp:218] Using GPUs 0
I0428 19:33:13.810250 23055 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:33:14.319174 23055 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test174.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:33:14.319320 23055 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test174.prototxt
I0428 19:33:14.319689 23055 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:33:14.319706 23055 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:33:14.319794 23055 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool0"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:33:14.319871 23055 layer_factory.hpp:77] Creating layer mnist
I0428 19:33:14.319972 23055 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:33:14.319993 23055 net.cpp:86] Creating Layer mnist
I0428 19:33:14.320003 23055 net.cpp:382] mnist -> data
I0428 19:33:14.320024 23055 net.cpp:382] mnist -> label
I0428 19:33:14.321140 23055 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:33:14.323590 23055 net.cpp:124] Setting up mnist
I0428 19:33:14.323606 23055 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:33:14.323612 23055 net.cpp:131] Top shape: 64 (64)
I0428 19:33:14.323616 23055 net.cpp:139] Memory required for data: 200960
I0428 19:33:14.323622 23055 layer_factory.hpp:77] Creating layer conv0
I0428 19:33:14.323659 23055 net.cpp:86] Creating Layer conv0
I0428 19:33:14.323667 23055 net.cpp:408] conv0 <- data
I0428 19:33:14.323678 23055 net.cpp:382] conv0 -> conv0
I0428 19:33:14.607652 23055 net.cpp:124] Setting up conv0
I0428 19:33:14.607679 23055 net.cpp:131] Top shape: 64 25 24 24 (921600)
I0428 19:33:14.607683 23055 net.cpp:139] Memory required for data: 3887360
I0428 19:33:14.607698 23055 layer_factory.hpp:77] Creating layer pool0
I0428 19:33:14.607710 23055 net.cpp:86] Creating Layer pool0
I0428 19:33:14.607733 23055 net.cpp:408] pool0 <- conv0
I0428 19:33:14.607739 23055 net.cpp:382] pool0 -> pool0
I0428 19:33:14.607785 23055 net.cpp:124] Setting up pool0
I0428 19:33:14.607792 23055 net.cpp:131] Top shape: 64 25 12 12 (230400)
I0428 19:33:14.607795 23055 net.cpp:139] Memory required for data: 4808960
I0428 19:33:14.607797 23055 layer_factory.hpp:77] Creating layer ip1
I0428 19:33:14.607806 23055 net.cpp:86] Creating Layer ip1
I0428 19:33:14.607808 23055 net.cpp:408] ip1 <- pool0
I0428 19:33:14.607815 23055 net.cpp:382] ip1 -> ip1
I0428 19:33:14.608117 23055 net.cpp:124] Setting up ip1
I0428 19:33:14.608140 23055 net.cpp:131] Top shape: 64 10 (640)
I0428 19:33:14.608157 23055 net.cpp:139] Memory required for data: 4811520
I0428 19:33:14.608163 23055 layer_factory.hpp:77] Creating layer relu1
I0428 19:33:14.608191 23055 net.cpp:86] Creating Layer relu1
I0428 19:33:14.608194 23055 net.cpp:408] relu1 <- ip1
I0428 19:33:14.608216 23055 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:33:14.608402 23055 net.cpp:124] Setting up relu1
I0428 19:33:14.608410 23055 net.cpp:131] Top shape: 64 10 (640)
I0428 19:33:14.608413 23055 net.cpp:139] Memory required for data: 4814080
I0428 19:33:14.608417 23055 layer_factory.hpp:77] Creating layer ip2
I0428 19:33:14.608423 23055 net.cpp:86] Creating Layer ip2
I0428 19:33:14.608427 23055 net.cpp:408] ip2 <- ip1
I0428 19:33:14.608433 23055 net.cpp:382] ip2 -> ip2
I0428 19:33:14.608532 23055 net.cpp:124] Setting up ip2
I0428 19:33:14.608539 23055 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:33:14.608542 23055 net.cpp:139] Memory required for data: 4826880
I0428 19:33:14.608549 23055 layer_factory.hpp:77] Creating layer relu2
I0428 19:33:14.608554 23055 net.cpp:86] Creating Layer relu2
I0428 19:33:14.608557 23055 net.cpp:408] relu2 <- ip2
I0428 19:33:14.608562 23055 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:33:14.609411 23055 net.cpp:124] Setting up relu2
I0428 19:33:14.609424 23055 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:33:14.609427 23055 net.cpp:139] Memory required for data: 4839680
I0428 19:33:14.609431 23055 layer_factory.hpp:77] Creating layer ip3
I0428 19:33:14.609438 23055 net.cpp:86] Creating Layer ip3
I0428 19:33:14.609441 23055 net.cpp:408] ip3 <- ip2
I0428 19:33:14.609447 23055 net.cpp:382] ip3 -> ip3
I0428 19:33:14.609549 23055 net.cpp:124] Setting up ip3
I0428 19:33:14.609555 23055 net.cpp:131] Top shape: 64 10 (640)
I0428 19:33:14.609558 23055 net.cpp:139] Memory required for data: 4842240
I0428 19:33:14.609563 23055 layer_factory.hpp:77] Creating layer relu3
I0428 19:33:14.609570 23055 net.cpp:86] Creating Layer relu3
I0428 19:33:14.609572 23055 net.cpp:408] relu3 <- ip3
I0428 19:33:14.609577 23055 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:33:14.609732 23055 net.cpp:124] Setting up relu3
I0428 19:33:14.609740 23055 net.cpp:131] Top shape: 64 10 (640)
I0428 19:33:14.609743 23055 net.cpp:139] Memory required for data: 4844800
I0428 19:33:14.609746 23055 layer_factory.hpp:77] Creating layer loss
I0428 19:33:14.609752 23055 net.cpp:86] Creating Layer loss
I0428 19:33:14.609755 23055 net.cpp:408] loss <- ip3
I0428 19:33:14.609758 23055 net.cpp:408] loss <- label
I0428 19:33:14.609764 23055 net.cpp:382] loss -> loss
I0428 19:33:14.609778 23055 layer_factory.hpp:77] Creating layer loss
I0428 19:33:14.610010 23055 net.cpp:124] Setting up loss
I0428 19:33:14.610018 23055 net.cpp:131] Top shape: (1)
I0428 19:33:14.610021 23055 net.cpp:134]     with loss weight 1
I0428 19:33:14.610035 23055 net.cpp:139] Memory required for data: 4844804
I0428 19:33:14.610038 23055 net.cpp:200] loss needs backward computation.
I0428 19:33:14.610041 23055 net.cpp:200] relu3 needs backward computation.
I0428 19:33:14.610044 23055 net.cpp:200] ip3 needs backward computation.
I0428 19:33:14.610047 23055 net.cpp:200] relu2 needs backward computation.
I0428 19:33:14.610049 23055 net.cpp:200] ip2 needs backward computation.
I0428 19:33:14.610052 23055 net.cpp:200] relu1 needs backward computation.
I0428 19:33:14.610054 23055 net.cpp:200] ip1 needs backward computation.
I0428 19:33:14.610067 23055 net.cpp:200] pool0 needs backward computation.
I0428 19:33:14.610070 23055 net.cpp:200] conv0 needs backward computation.
I0428 19:33:14.610074 23055 net.cpp:202] mnist does not need backward computation.
I0428 19:33:14.610091 23055 net.cpp:244] This network produces output loss
I0428 19:33:14.610116 23055 net.cpp:257] Network initialization done.
I0428 19:33:14.610426 23055 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test174.prototxt
I0428 19:33:14.610466 23055 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:33:14.610543 23055 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool0"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:33:14.610610 23055 layer_factory.hpp:77] Creating layer mnist
I0428 19:33:14.610656 23055 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:33:14.610667 23055 net.cpp:86] Creating Layer mnist
I0428 19:33:14.610671 23055 net.cpp:382] mnist -> data
I0428 19:33:14.610682 23055 net.cpp:382] mnist -> label
I0428 19:33:14.610765 23055 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:33:14.612692 23055 net.cpp:124] Setting up mnist
I0428 19:33:14.612705 23055 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:33:14.612709 23055 net.cpp:131] Top shape: 100 (100)
I0428 19:33:14.612712 23055 net.cpp:139] Memory required for data: 314000
I0428 19:33:14.612716 23055 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:33:14.612722 23055 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:33:14.612725 23055 net.cpp:408] label_mnist_1_split <- label
I0428 19:33:14.612730 23055 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:33:14.612763 23055 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:33:14.612802 23055 net.cpp:124] Setting up label_mnist_1_split
I0428 19:33:14.612807 23055 net.cpp:131] Top shape: 100 (100)
I0428 19:33:14.612817 23055 net.cpp:131] Top shape: 100 (100)
I0428 19:33:14.612819 23055 net.cpp:139] Memory required for data: 314800
I0428 19:33:14.612831 23055 layer_factory.hpp:77] Creating layer conv0
I0428 19:33:14.612843 23055 net.cpp:86] Creating Layer conv0
I0428 19:33:14.612848 23055 net.cpp:408] conv0 <- data
I0428 19:33:14.612851 23055 net.cpp:382] conv0 -> conv0
I0428 19:33:14.614727 23055 net.cpp:124] Setting up conv0
I0428 19:33:14.614740 23055 net.cpp:131] Top shape: 100 25 24 24 (1440000)
I0428 19:33:14.614743 23055 net.cpp:139] Memory required for data: 6074800
I0428 19:33:14.614753 23055 layer_factory.hpp:77] Creating layer pool0
I0428 19:33:14.614759 23055 net.cpp:86] Creating Layer pool0
I0428 19:33:14.614763 23055 net.cpp:408] pool0 <- conv0
I0428 19:33:14.614768 23055 net.cpp:382] pool0 -> pool0
I0428 19:33:14.614802 23055 net.cpp:124] Setting up pool0
I0428 19:33:14.614806 23055 net.cpp:131] Top shape: 100 25 12 12 (360000)
I0428 19:33:14.614809 23055 net.cpp:139] Memory required for data: 7514800
I0428 19:33:14.614812 23055 layer_factory.hpp:77] Creating layer ip1
I0428 19:33:14.614820 23055 net.cpp:86] Creating Layer ip1
I0428 19:33:14.614822 23055 net.cpp:408] ip1 <- pool0
I0428 19:33:14.614827 23055 net.cpp:382] ip1 -> ip1
I0428 19:33:14.615126 23055 net.cpp:124] Setting up ip1
I0428 19:33:14.615134 23055 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:33:14.615137 23055 net.cpp:139] Memory required for data: 7518800
I0428 19:33:14.615144 23055 layer_factory.hpp:77] Creating layer relu1
I0428 19:33:14.615149 23055 net.cpp:86] Creating Layer relu1
I0428 19:33:14.615152 23055 net.cpp:408] relu1 <- ip1
I0428 19:33:14.615156 23055 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:33:14.615329 23055 net.cpp:124] Setting up relu1
I0428 19:33:14.615336 23055 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:33:14.615339 23055 net.cpp:139] Memory required for data: 7522800
I0428 19:33:14.615342 23055 layer_factory.hpp:77] Creating layer ip2
I0428 19:33:14.615347 23055 net.cpp:86] Creating Layer ip2
I0428 19:33:14.615350 23055 net.cpp:408] ip2 <- ip1
I0428 19:33:14.615356 23055 net.cpp:382] ip2 -> ip2
I0428 19:33:14.615469 23055 net.cpp:124] Setting up ip2
I0428 19:33:14.615478 23055 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:33:14.615480 23055 net.cpp:139] Memory required for data: 7542800
I0428 19:33:14.615489 23055 layer_factory.hpp:77] Creating layer relu2
I0428 19:33:14.615494 23055 net.cpp:86] Creating Layer relu2
I0428 19:33:14.615496 23055 net.cpp:408] relu2 <- ip2
I0428 19:33:14.615502 23055 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:33:14.616343 23055 net.cpp:124] Setting up relu2
I0428 19:33:14.616355 23055 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:33:14.616359 23055 net.cpp:139] Memory required for data: 7562800
I0428 19:33:14.616363 23055 layer_factory.hpp:77] Creating layer ip3
I0428 19:33:14.616370 23055 net.cpp:86] Creating Layer ip3
I0428 19:33:14.616374 23055 net.cpp:408] ip3 <- ip2
I0428 19:33:14.616379 23055 net.cpp:382] ip3 -> ip3
I0428 19:33:14.616534 23055 net.cpp:124] Setting up ip3
I0428 19:33:14.616542 23055 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:33:14.616545 23055 net.cpp:139] Memory required for data: 7566800
I0428 19:33:14.616550 23055 layer_factory.hpp:77] Creating layer relu3
I0428 19:33:14.616556 23055 net.cpp:86] Creating Layer relu3
I0428 19:33:14.616559 23055 net.cpp:408] relu3 <- ip3
I0428 19:33:14.616564 23055 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:33:14.616740 23055 net.cpp:124] Setting up relu3
I0428 19:33:14.616749 23055 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:33:14.616752 23055 net.cpp:139] Memory required for data: 7570800
I0428 19:33:14.616756 23055 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:33:14.616761 23055 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:33:14.616765 23055 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:33:14.616783 23055 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:33:14.616789 23055 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:33:14.616860 23055 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:33:14.616873 23055 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:33:14.616878 23055 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:33:14.616889 23055 net.cpp:139] Memory required for data: 7578800
I0428 19:33:14.616892 23055 layer_factory.hpp:77] Creating layer accuracy
I0428 19:33:14.616897 23055 net.cpp:86] Creating Layer accuracy
I0428 19:33:14.616900 23055 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:33:14.616904 23055 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:33:14.616910 23055 net.cpp:382] accuracy -> accuracy
I0428 19:33:14.616919 23055 net.cpp:124] Setting up accuracy
I0428 19:33:14.616922 23055 net.cpp:131] Top shape: (1)
I0428 19:33:14.616925 23055 net.cpp:139] Memory required for data: 7578804
I0428 19:33:14.616933 23055 layer_factory.hpp:77] Creating layer loss
I0428 19:33:14.616938 23055 net.cpp:86] Creating Layer loss
I0428 19:33:14.616941 23055 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:33:14.616945 23055 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:33:14.616950 23055 net.cpp:382] loss -> loss
I0428 19:33:14.616955 23055 layer_factory.hpp:77] Creating layer loss
I0428 19:33:14.617234 23055 net.cpp:124] Setting up loss
I0428 19:33:14.617243 23055 net.cpp:131] Top shape: (1)
I0428 19:33:14.617246 23055 net.cpp:134]     with loss weight 1
I0428 19:33:14.617252 23055 net.cpp:139] Memory required for data: 7578808
I0428 19:33:14.617255 23055 net.cpp:200] loss needs backward computation.
I0428 19:33:14.617270 23055 net.cpp:202] accuracy does not need backward computation.
I0428 19:33:14.617274 23055 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:33:14.617277 23055 net.cpp:200] relu3 needs backward computation.
I0428 19:33:14.617285 23055 net.cpp:200] ip3 needs backward computation.
I0428 19:33:14.617288 23055 net.cpp:200] relu2 needs backward computation.
I0428 19:33:14.617291 23055 net.cpp:200] ip2 needs backward computation.
I0428 19:33:14.617295 23055 net.cpp:200] relu1 needs backward computation.
I0428 19:33:14.617296 23055 net.cpp:200] ip1 needs backward computation.
I0428 19:33:14.617300 23055 net.cpp:200] pool0 needs backward computation.
I0428 19:33:14.617303 23055 net.cpp:200] conv0 needs backward computation.
I0428 19:33:14.617306 23055 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:33:14.617311 23055 net.cpp:202] mnist does not need backward computation.
I0428 19:33:14.617312 23055 net.cpp:244] This network produces output accuracy
I0428 19:33:14.617316 23055 net.cpp:244] This network produces output loss
I0428 19:33:14.617331 23055 net.cpp:257] Network initialization done.
I0428 19:33:14.617373 23055 solver.cpp:56] Solver scaffolding done.
I0428 19:33:14.617626 23055 caffe.cpp:248] Starting Optimization
I0428 19:33:14.617632 23055 solver.cpp:273] Solving LeNet
I0428 19:33:14.617635 23055 solver.cpp:274] Learning Rate Policy: inv
I0428 19:33:14.618497 23055 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:33:14.623128 23055 blocking_queue.cpp:49] Waiting for data
I0428 19:33:14.697165 23062 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:33:14.697899 23055 solver.cpp:398]     Test net output #0: accuracy = 0.1186
I0428 19:33:14.697921 23055 solver.cpp:398]     Test net output #1: loss = 2.29989 (* 1 = 2.29989 loss)
I0428 19:33:14.702266 23055 solver.cpp:219] Iteration 0 (-1.22946e-31 iter/s, 0.0845834s/100 iters), loss = 2.30183
I0428 19:33:14.702332 23055 solver.cpp:238]     Train net output #0: loss = 2.30183 (* 1 = 2.30183 loss)
I0428 19:33:14.702345 23055 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:33:14.805989 23055 solver.cpp:219] Iteration 100 (964.745 iter/s, 0.103654s/100 iters), loss = 0.933054
I0428 19:33:14.806035 23055 solver.cpp:238]     Train net output #0: loss = 0.933054 (* 1 = 0.933054 loss)
I0428 19:33:14.806046 23055 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:33:14.908628 23055 solver.cpp:219] Iteration 200 (974.828 iter/s, 0.102582s/100 iters), loss = 1.51814
I0428 19:33:14.908684 23055 solver.cpp:238]     Train net output #0: loss = 1.51814 (* 1 = 1.51814 loss)
I0428 19:33:14.908700 23055 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:33:15.020619 23055 solver.cpp:219] Iteration 300 (893.437 iter/s, 0.111927s/100 iters), loss = 0.992466
I0428 19:33:15.020661 23055 solver.cpp:238]     Train net output #0: loss = 0.992466 (* 1 = 0.992466 loss)
I0428 19:33:15.020671 23055 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:33:15.124636 23055 solver.cpp:219] Iteration 400 (961.856 iter/s, 0.103966s/100 iters), loss = 1.14524
I0428 19:33:15.124670 23055 solver.cpp:238]     Train net output #0: loss = 1.14524 (* 1 = 1.14524 loss)
I0428 19:33:15.124680 23055 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:33:15.221812 23055 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:33:15.330173 23062 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:33:15.330936 23055 solver.cpp:398]     Test net output #0: accuracy = 0.7543
I0428 19:33:15.330967 23055 solver.cpp:398]     Test net output #1: loss = 0.665791 (* 1 = 0.665791 loss)
I0428 19:33:15.332087 23055 solver.cpp:219] Iteration 500 (482.16 iter/s, 0.2074s/100 iters), loss = 0.722849
I0428 19:33:15.332115 23055 solver.cpp:238]     Train net output #0: loss = 0.722849 (* 1 = 0.722849 loss)
I0428 19:33:15.332124 23055 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:33:15.432937 23055 solver.cpp:219] Iteration 600 (991.959 iter/s, 0.100811s/100 iters), loss = 0.786553
I0428 19:33:15.432968 23055 solver.cpp:238]     Train net output #0: loss = 0.786553 (* 1 = 0.786553 loss)
I0428 19:33:15.432976 23055 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:33:15.534162 23055 solver.cpp:219] Iteration 700 (988.294 iter/s, 0.101185s/100 iters), loss = 0.57784
I0428 19:33:15.534194 23055 solver.cpp:238]     Train net output #0: loss = 0.57784 (* 1 = 0.57784 loss)
I0428 19:33:15.534201 23055 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:33:15.635421 23055 solver.cpp:219] Iteration 800 (987.994 iter/s, 0.101215s/100 iters), loss = 0.79785
I0428 19:33:15.635452 23055 solver.cpp:238]     Train net output #0: loss = 0.79785 (* 1 = 0.79785 loss)
I0428 19:33:15.635459 23055 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:33:15.736085 23055 solver.cpp:219] Iteration 900 (993.796 iter/s, 0.100624s/100 iters), loss = 0.721136
I0428 19:33:15.736115 23055 solver.cpp:238]     Train net output #0: loss = 0.721136 (* 1 = 0.721136 loss)
I0428 19:33:15.736124 23055 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:33:15.771926 23061 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:33:15.837167 23055 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:33:15.838322 23055 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:33:15.839334 23055 solver.cpp:311] Iteration 1000, loss = 0.412987
I0428 19:33:15.839362 23055 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:33:15.915154 23062 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:33:15.915982 23055 solver.cpp:398]     Test net output #0: accuracy = 0.8513
I0428 19:33:15.916003 23055 solver.cpp:398]     Test net output #1: loss = 0.40983 (* 1 = 0.40983 loss)
I0428 19:33:15.916012 23055 solver.cpp:316] Optimization Done.
I0428 19:33:15.916015 23055 caffe.cpp:259] Optimization Done.
