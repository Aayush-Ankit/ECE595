I0428 20:11:50.462754 32152 caffe.cpp:218] Using GPUs 0
I0428 20:11:50.494447 32152 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:11:50.989433 32152 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1168.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:11:50.989583 32152 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1168.prototxt
I0428 20:11:50.989943 32152 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:11:50.989958 32152 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:11:50.990051 32152 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:11:50.990116 32152 layer_factory.hpp:77] Creating layer mnist
I0428 20:11:50.990195 32152 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:11:50.990214 32152 net.cpp:86] Creating Layer mnist
I0428 20:11:50.990221 32152 net.cpp:382] mnist -> data
I0428 20:11:50.990241 32152 net.cpp:382] mnist -> label
I0428 20:11:50.991211 32152 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:11:50.993207 32152 net.cpp:124] Setting up mnist
I0428 20:11:50.993221 32152 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:11:50.993227 32152 net.cpp:131] Top shape: 64 (64)
I0428 20:11:50.993229 32152 net.cpp:139] Memory required for data: 200960
I0428 20:11:50.993234 32152 layer_factory.hpp:77] Creating layer conv0
I0428 20:11:50.993247 32152 net.cpp:86] Creating Layer conv0
I0428 20:11:50.993278 32152 net.cpp:408] conv0 <- data
I0428 20:11:50.993289 32152 net.cpp:382] conv0 -> conv0
I0428 20:11:51.222295 32152 net.cpp:124] Setting up conv0
I0428 20:11:51.222337 32152 net.cpp:131] Top shape: 64 25 24 24 (921600)
I0428 20:11:51.222339 32152 net.cpp:139] Memory required for data: 3887360
I0428 20:11:51.222368 32152 layer_factory.hpp:77] Creating layer pool0
I0428 20:11:51.222381 32152 net.cpp:86] Creating Layer pool0
I0428 20:11:51.222384 32152 net.cpp:408] pool0 <- conv0
I0428 20:11:51.222390 32152 net.cpp:382] pool0 -> pool0
I0428 20:11:51.222448 32152 net.cpp:124] Setting up pool0
I0428 20:11:51.222453 32152 net.cpp:131] Top shape: 64 25 12 12 (230400)
I0428 20:11:51.222456 32152 net.cpp:139] Memory required for data: 4808960
I0428 20:11:51.222460 32152 layer_factory.hpp:77] Creating layer conv1
I0428 20:11:51.222470 32152 net.cpp:86] Creating Layer conv1
I0428 20:11:51.222472 32152 net.cpp:408] conv1 <- pool0
I0428 20:11:51.222477 32152 net.cpp:382] conv1 -> conv1
I0428 20:11:51.224493 32152 net.cpp:124] Setting up conv1
I0428 20:11:51.224524 32152 net.cpp:131] Top shape: 64 50 8 8 (204800)
I0428 20:11:51.224527 32152 net.cpp:139] Memory required for data: 5628160
I0428 20:11:51.224535 32152 layer_factory.hpp:77] Creating layer pool1
I0428 20:11:51.224558 32152 net.cpp:86] Creating Layer pool1
I0428 20:11:51.224562 32152 net.cpp:408] pool1 <- conv1
I0428 20:11:51.224566 32152 net.cpp:382] pool1 -> pool1
I0428 20:11:51.224602 32152 net.cpp:124] Setting up pool1
I0428 20:11:51.224608 32152 net.cpp:131] Top shape: 64 50 4 4 (51200)
I0428 20:11:51.224611 32152 net.cpp:139] Memory required for data: 5832960
I0428 20:11:51.224613 32152 layer_factory.hpp:77] Creating layer ip1
I0428 20:11:51.224622 32152 net.cpp:86] Creating Layer ip1
I0428 20:11:51.224623 32152 net.cpp:408] ip1 <- pool1
I0428 20:11:51.224644 32152 net.cpp:382] ip1 -> ip1
I0428 20:11:51.225014 32152 net.cpp:124] Setting up ip1
I0428 20:11:51.225023 32152 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:11:51.225040 32152 net.cpp:139] Memory required for data: 5845760
I0428 20:11:51.225049 32152 layer_factory.hpp:77] Creating layer relu1
I0428 20:11:51.225054 32152 net.cpp:86] Creating Layer relu1
I0428 20:11:51.225059 32152 net.cpp:408] relu1 <- ip1
I0428 20:11:51.225062 32152 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:11:51.225235 32152 net.cpp:124] Setting up relu1
I0428 20:11:51.225244 32152 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:11:51.225247 32152 net.cpp:139] Memory required for data: 5858560
I0428 20:11:51.225250 32152 layer_factory.hpp:77] Creating layer ip2
I0428 20:11:51.225256 32152 net.cpp:86] Creating Layer ip2
I0428 20:11:51.225260 32152 net.cpp:408] ip2 <- ip1
I0428 20:11:51.225263 32152 net.cpp:382] ip2 -> ip2
I0428 20:11:51.226300 32152 net.cpp:124] Setting up ip2
I0428 20:11:51.226311 32152 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:11:51.226331 32152 net.cpp:139] Memory required for data: 5871360
I0428 20:11:51.226336 32152 layer_factory.hpp:77] Creating layer relu2
I0428 20:11:51.226343 32152 net.cpp:86] Creating Layer relu2
I0428 20:11:51.226347 32152 net.cpp:408] relu2 <- ip2
I0428 20:11:51.226351 32152 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:11:51.227138 32152 net.cpp:124] Setting up relu2
I0428 20:11:51.227149 32152 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:11:51.227167 32152 net.cpp:139] Memory required for data: 5884160
I0428 20:11:51.227170 32152 layer_factory.hpp:77] Creating layer ip3
I0428 20:11:51.227177 32152 net.cpp:86] Creating Layer ip3
I0428 20:11:51.227180 32152 net.cpp:408] ip3 <- ip2
I0428 20:11:51.227186 32152 net.cpp:382] ip3 -> ip3
I0428 20:11:51.227277 32152 net.cpp:124] Setting up ip3
I0428 20:11:51.227284 32152 net.cpp:131] Top shape: 64 10 (640)
I0428 20:11:51.227286 32152 net.cpp:139] Memory required for data: 5886720
I0428 20:11:51.227294 32152 layer_factory.hpp:77] Creating layer relu3
I0428 20:11:51.227298 32152 net.cpp:86] Creating Layer relu3
I0428 20:11:51.227301 32152 net.cpp:408] relu3 <- ip3
I0428 20:11:51.227305 32152 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:11:51.227471 32152 net.cpp:124] Setting up relu3
I0428 20:11:51.227480 32152 net.cpp:131] Top shape: 64 10 (640)
I0428 20:11:51.227483 32152 net.cpp:139] Memory required for data: 5889280
I0428 20:11:51.227486 32152 layer_factory.hpp:77] Creating layer loss
I0428 20:11:51.227493 32152 net.cpp:86] Creating Layer loss
I0428 20:11:51.227495 32152 net.cpp:408] loss <- ip3
I0428 20:11:51.227515 32152 net.cpp:408] loss <- label
I0428 20:11:51.227520 32152 net.cpp:382] loss -> loss
I0428 20:11:51.227538 32152 layer_factory.hpp:77] Creating layer loss
I0428 20:11:51.227756 32152 net.cpp:124] Setting up loss
I0428 20:11:51.227766 32152 net.cpp:131] Top shape: (1)
I0428 20:11:51.227768 32152 net.cpp:134]     with loss weight 1
I0428 20:11:51.227782 32152 net.cpp:139] Memory required for data: 5889284
I0428 20:11:51.227787 32152 net.cpp:200] loss needs backward computation.
I0428 20:11:51.227789 32152 net.cpp:200] relu3 needs backward computation.
I0428 20:11:51.227792 32152 net.cpp:200] ip3 needs backward computation.
I0428 20:11:51.227795 32152 net.cpp:200] relu2 needs backward computation.
I0428 20:11:51.227798 32152 net.cpp:200] ip2 needs backward computation.
I0428 20:11:51.227802 32152 net.cpp:200] relu1 needs backward computation.
I0428 20:11:51.227804 32152 net.cpp:200] ip1 needs backward computation.
I0428 20:11:51.227807 32152 net.cpp:200] pool1 needs backward computation.
I0428 20:11:51.227810 32152 net.cpp:200] conv1 needs backward computation.
I0428 20:11:51.227813 32152 net.cpp:200] pool0 needs backward computation.
I0428 20:11:51.227816 32152 net.cpp:200] conv0 needs backward computation.
I0428 20:11:51.227834 32152 net.cpp:202] mnist does not need backward computation.
I0428 20:11:51.227838 32152 net.cpp:244] This network produces output loss
I0428 20:11:51.227846 32152 net.cpp:257] Network initialization done.
I0428 20:11:51.228183 32152 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1168.prototxt
I0428 20:11:51.228224 32152 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:11:51.228312 32152 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:11:51.228389 32152 layer_factory.hpp:77] Creating layer mnist
I0428 20:11:51.228432 32152 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:11:51.228461 32152 net.cpp:86] Creating Layer mnist
I0428 20:11:51.228464 32152 net.cpp:382] mnist -> data
I0428 20:11:51.228472 32152 net.cpp:382] mnist -> label
I0428 20:11:51.228564 32152 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:11:51.230608 32152 net.cpp:124] Setting up mnist
I0428 20:11:51.230636 32152 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:11:51.230640 32152 net.cpp:131] Top shape: 100 (100)
I0428 20:11:51.230644 32152 net.cpp:139] Memory required for data: 314000
I0428 20:11:51.230664 32152 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:11:51.230696 32152 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:11:51.230700 32152 net.cpp:408] label_mnist_1_split <- label
I0428 20:11:51.230705 32152 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:11:51.230712 32152 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:11:51.230751 32152 net.cpp:124] Setting up label_mnist_1_split
I0428 20:11:51.230756 32152 net.cpp:131] Top shape: 100 (100)
I0428 20:11:51.230761 32152 net.cpp:131] Top shape: 100 (100)
I0428 20:11:51.230762 32152 net.cpp:139] Memory required for data: 314800
I0428 20:11:51.230765 32152 layer_factory.hpp:77] Creating layer conv0
I0428 20:11:51.230773 32152 net.cpp:86] Creating Layer conv0
I0428 20:11:51.230777 32152 net.cpp:408] conv0 <- data
I0428 20:11:51.230782 32152 net.cpp:382] conv0 -> conv0
I0428 20:11:51.232504 32152 net.cpp:124] Setting up conv0
I0428 20:11:51.232517 32152 net.cpp:131] Top shape: 100 25 24 24 (1440000)
I0428 20:11:51.232520 32152 net.cpp:139] Memory required for data: 6074800
I0428 20:11:51.232529 32152 layer_factory.hpp:77] Creating layer pool0
I0428 20:11:51.232535 32152 net.cpp:86] Creating Layer pool0
I0428 20:11:51.232538 32152 net.cpp:408] pool0 <- conv0
I0428 20:11:51.232543 32152 net.cpp:382] pool0 -> pool0
I0428 20:11:51.232576 32152 net.cpp:124] Setting up pool0
I0428 20:11:51.232581 32152 net.cpp:131] Top shape: 100 25 12 12 (360000)
I0428 20:11:51.232583 32152 net.cpp:139] Memory required for data: 7514800
I0428 20:11:51.232586 32152 layer_factory.hpp:77] Creating layer conv1
I0428 20:11:51.232594 32152 net.cpp:86] Creating Layer conv1
I0428 20:11:51.232597 32152 net.cpp:408] conv1 <- pool0
I0428 20:11:51.232601 32152 net.cpp:382] conv1 -> conv1
I0428 20:11:51.234441 32152 net.cpp:124] Setting up conv1
I0428 20:11:51.234455 32152 net.cpp:131] Top shape: 100 50 8 8 (320000)
I0428 20:11:51.234459 32152 net.cpp:139] Memory required for data: 8794800
I0428 20:11:51.234467 32152 layer_factory.hpp:77] Creating layer pool1
I0428 20:11:51.234473 32152 net.cpp:86] Creating Layer pool1
I0428 20:11:51.234477 32152 net.cpp:408] pool1 <- conv1
I0428 20:11:51.234483 32152 net.cpp:382] pool1 -> pool1
I0428 20:11:51.234516 32152 net.cpp:124] Setting up pool1
I0428 20:11:51.234521 32152 net.cpp:131] Top shape: 100 50 4 4 (80000)
I0428 20:11:51.234524 32152 net.cpp:139] Memory required for data: 9114800
I0428 20:11:51.234527 32152 layer_factory.hpp:77] Creating layer ip1
I0428 20:11:51.234534 32152 net.cpp:86] Creating Layer ip1
I0428 20:11:51.234536 32152 net.cpp:408] ip1 <- pool1
I0428 20:11:51.234540 32152 net.cpp:382] ip1 -> ip1
I0428 20:11:51.234868 32152 net.cpp:124] Setting up ip1
I0428 20:11:51.234874 32152 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:11:51.234889 32152 net.cpp:139] Memory required for data: 9134800
I0428 20:11:51.234895 32152 layer_factory.hpp:77] Creating layer relu1
I0428 20:11:51.234900 32152 net.cpp:86] Creating Layer relu1
I0428 20:11:51.234904 32152 net.cpp:408] relu1 <- ip1
I0428 20:11:51.234907 32152 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:11:51.235060 32152 net.cpp:124] Setting up relu1
I0428 20:11:51.235069 32152 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:11:51.235079 32152 net.cpp:139] Memory required for data: 9154800
I0428 20:11:51.235081 32152 layer_factory.hpp:77] Creating layer ip2
I0428 20:11:51.235088 32152 net.cpp:86] Creating Layer ip2
I0428 20:11:51.235097 32152 net.cpp:408] ip2 <- ip1
I0428 20:11:51.235102 32152 net.cpp:382] ip2 -> ip2
I0428 20:11:51.235236 32152 net.cpp:124] Setting up ip2
I0428 20:11:51.235244 32152 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:11:51.235247 32152 net.cpp:139] Memory required for data: 9174800
I0428 20:11:51.235252 32152 layer_factory.hpp:77] Creating layer relu2
I0428 20:11:51.235257 32152 net.cpp:86] Creating Layer relu2
I0428 20:11:51.235260 32152 net.cpp:408] relu2 <- ip2
I0428 20:11:51.235265 32152 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:11:51.235466 32152 net.cpp:124] Setting up relu2
I0428 20:11:51.235474 32152 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:11:51.235478 32152 net.cpp:139] Memory required for data: 9194800
I0428 20:11:51.235481 32152 layer_factory.hpp:77] Creating layer ip3
I0428 20:11:51.235502 32152 net.cpp:86] Creating Layer ip3
I0428 20:11:51.235504 32152 net.cpp:408] ip3 <- ip2
I0428 20:11:51.235509 32152 net.cpp:382] ip3 -> ip3
I0428 20:11:51.235677 32152 net.cpp:124] Setting up ip3
I0428 20:11:51.235684 32152 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:11:51.235688 32152 net.cpp:139] Memory required for data: 9198800
I0428 20:11:51.235695 32152 layer_factory.hpp:77] Creating layer relu3
I0428 20:11:51.235700 32152 net.cpp:86] Creating Layer relu3
I0428 20:11:51.235703 32152 net.cpp:408] relu3 <- ip3
I0428 20:11:51.235707 32152 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:11:51.236441 32152 net.cpp:124] Setting up relu3
I0428 20:11:51.236454 32152 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:11:51.236457 32152 net.cpp:139] Memory required for data: 9202800
I0428 20:11:51.236461 32152 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:11:51.236466 32152 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:11:51.236470 32152 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:11:51.236475 32152 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:11:51.236490 32152 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:11:51.236527 32152 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:11:51.236532 32152 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:11:51.236536 32152 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:11:51.236539 32152 net.cpp:139] Memory required for data: 9210800
I0428 20:11:51.236541 32152 layer_factory.hpp:77] Creating layer accuracy
I0428 20:11:51.236547 32152 net.cpp:86] Creating Layer accuracy
I0428 20:11:51.236551 32152 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:11:51.236555 32152 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:11:51.236559 32152 net.cpp:382] accuracy -> accuracy
I0428 20:11:51.236565 32152 net.cpp:124] Setting up accuracy
I0428 20:11:51.236569 32152 net.cpp:131] Top shape: (1)
I0428 20:11:51.236572 32152 net.cpp:139] Memory required for data: 9210804
I0428 20:11:51.236575 32152 layer_factory.hpp:77] Creating layer loss
I0428 20:11:51.236593 32152 net.cpp:86] Creating Layer loss
I0428 20:11:51.236598 32152 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:11:51.236600 32152 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:11:51.236604 32152 net.cpp:382] loss -> loss
I0428 20:11:51.236610 32152 layer_factory.hpp:77] Creating layer loss
I0428 20:11:51.236951 32152 net.cpp:124] Setting up loss
I0428 20:11:51.236961 32152 net.cpp:131] Top shape: (1)
I0428 20:11:51.236964 32152 net.cpp:134]     with loss weight 1
I0428 20:11:51.236980 32152 net.cpp:139] Memory required for data: 9210808
I0428 20:11:51.236984 32152 net.cpp:200] loss needs backward computation.
I0428 20:11:51.236989 32152 net.cpp:202] accuracy does not need backward computation.
I0428 20:11:51.236994 32152 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:11:51.236996 32152 net.cpp:200] relu3 needs backward computation.
I0428 20:11:51.236999 32152 net.cpp:200] ip3 needs backward computation.
I0428 20:11:51.237009 32152 net.cpp:200] relu2 needs backward computation.
I0428 20:11:51.237011 32152 net.cpp:200] ip2 needs backward computation.
I0428 20:11:51.237015 32152 net.cpp:200] relu1 needs backward computation.
I0428 20:11:51.237017 32152 net.cpp:200] ip1 needs backward computation.
I0428 20:11:51.237021 32152 net.cpp:200] pool1 needs backward computation.
I0428 20:11:51.237025 32152 net.cpp:200] conv1 needs backward computation.
I0428 20:11:51.237028 32152 net.cpp:200] pool0 needs backward computation.
I0428 20:11:51.237031 32152 net.cpp:200] conv0 needs backward computation.
I0428 20:11:51.237035 32152 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:11:51.237040 32152 net.cpp:202] mnist does not need backward computation.
I0428 20:11:51.237042 32152 net.cpp:244] This network produces output accuracy
I0428 20:11:51.237046 32152 net.cpp:244] This network produces output loss
I0428 20:11:51.237057 32152 net.cpp:257] Network initialization done.
I0428 20:11:51.237099 32152 solver.cpp:56] Solver scaffolding done.
I0428 20:11:51.237445 32152 caffe.cpp:248] Starting Optimization
I0428 20:11:51.237452 32152 solver.cpp:273] Solving LeNet
I0428 20:11:51.237457 32152 solver.cpp:274] Learning Rate Policy: inv
I0428 20:11:51.238263 32152 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:11:51.244228 32152 blocking_queue.cpp:49] Waiting for data
I0428 20:11:51.314502 32159 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:11:51.315140 32152 solver.cpp:398]     Test net output #0: accuracy = 0.0996
I0428 20:11:51.315160 32152 solver.cpp:398]     Test net output #1: loss = 2.30369 (* 1 = 2.30369 loss)
I0428 20:11:51.318872 32152 solver.cpp:219] Iteration 0 (0 iter/s, 0.0813936s/100 iters), loss = 2.30611
I0428 20:11:51.318894 32152 solver.cpp:238]     Train net output #0: loss = 2.30611 (* 1 = 2.30611 loss)
I0428 20:11:51.318905 32152 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:11:51.446738 32152 solver.cpp:219] Iteration 100 (782.276 iter/s, 0.127832s/100 iters), loss = 0.540605
I0428 20:11:51.446779 32152 solver.cpp:238]     Train net output #0: loss = 0.540605 (* 1 = 0.540605 loss)
I0428 20:11:51.446786 32152 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:11:51.575024 32152 solver.cpp:219] Iteration 200 (779.759 iter/s, 0.128245s/100 iters), loss = 0.366255
I0428 20:11:51.575072 32152 solver.cpp:238]     Train net output #0: loss = 0.366255 (* 1 = 0.366255 loss)
I0428 20:11:51.575078 32152 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:11:51.699358 32152 solver.cpp:219] Iteration 300 (804.65 iter/s, 0.124278s/100 iters), loss = 0.436898
I0428 20:11:51.699383 32152 solver.cpp:238]     Train net output #0: loss = 0.436898 (* 1 = 0.436898 loss)
I0428 20:11:51.699389 32152 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:11:51.821398 32152 solver.cpp:219] Iteration 400 (819.65 iter/s, 0.122003s/100 iters), loss = 0.433761
I0428 20:11:51.821437 32152 solver.cpp:238]     Train net output #0: loss = 0.433761 (* 1 = 0.433761 loss)
I0428 20:11:51.821444 32152 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:11:51.945163 32152 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:11:52.022384 32159 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:11:52.023035 32152 solver.cpp:398]     Test net output #0: accuracy = 0.9492
I0428 20:11:52.023056 32152 solver.cpp:398]     Test net output #1: loss = 0.332064 (* 1 = 0.332064 loss)
I0428 20:11:52.024303 32152 solver.cpp:219] Iteration 500 (492.974 iter/s, 0.20285s/100 iters), loss = 0.338011
I0428 20:11:52.024355 32152 solver.cpp:238]     Train net output #0: loss = 0.338011 (* 1 = 0.338011 loss)
I0428 20:11:52.024363 32152 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:11:52.154983 32152 solver.cpp:219] Iteration 600 (765.588 iter/s, 0.130619s/100 iters), loss = 0.259548
I0428 20:11:52.155023 32152 solver.cpp:238]     Train net output #0: loss = 0.259548 (* 1 = 0.259548 loss)
I0428 20:11:52.155030 32152 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:11:52.282001 32152 solver.cpp:219] Iteration 700 (787.607 iter/s, 0.126967s/100 iters), loss = 0.367334
I0428 20:11:52.282040 32152 solver.cpp:238]     Train net output #0: loss = 0.367334 (* 1 = 0.367334 loss)
I0428 20:11:52.282047 32152 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:11:52.406951 32152 solver.cpp:219] Iteration 800 (800.638 iter/s, 0.1249s/100 iters), loss = 0.458774
I0428 20:11:52.406991 32152 solver.cpp:238]     Train net output #0: loss = 0.458774 (* 1 = 0.458774 loss)
I0428 20:11:52.406997 32152 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:11:52.529624 32152 solver.cpp:219] Iteration 900 (815.508 iter/s, 0.122623s/100 iters), loss = 0.302984
I0428 20:11:52.529680 32152 solver.cpp:238]     Train net output #0: loss = 0.302984 (* 1 = 0.302984 loss)
I0428 20:11:52.529686 32152 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:11:52.570219 32158 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:11:52.649468 32152 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:11:52.651424 32152 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:11:52.652667 32152 solver.cpp:311] Iteration 1000, loss = 0.349078
I0428 20:11:52.652683 32152 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:11:52.727710 32159 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:11:52.728358 32152 solver.cpp:398]     Test net output #0: accuracy = 0.9709
I0428 20:11:52.728380 32152 solver.cpp:398]     Test net output #1: loss = 0.301103 (* 1 = 0.301103 loss)
I0428 20:11:52.728385 32152 solver.cpp:316] Optimization Done.
I0428 20:11:52.728389 32152 caffe.cpp:259] Optimization Done.
