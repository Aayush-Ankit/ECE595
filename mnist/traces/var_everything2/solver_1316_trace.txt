I0428 20:18:20.013197  1102 caffe.cpp:218] Using GPUs 0
I0428 20:18:20.043244  1102 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:18:20.489783  1102 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1316.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:18:20.489951  1102 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1316.prototxt
I0428 20:18:20.490303  1102 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:18:20.490335  1102 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:18:20.490487  1102 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:18:20.490576  1102 layer_factory.hpp:77] Creating layer mnist
I0428 20:18:20.490674  1102 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:18:20.490694  1102 net.cpp:86] Creating Layer mnist
I0428 20:18:20.490703  1102 net.cpp:382] mnist -> data
I0428 20:18:20.490722  1102 net.cpp:382] mnist -> label
I0428 20:18:20.491693  1102 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:18:20.494065  1102 net.cpp:124] Setting up mnist
I0428 20:18:20.494109  1102 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:18:20.494132  1102 net.cpp:131] Top shape: 64 (64)
I0428 20:18:20.494134  1102 net.cpp:139] Memory required for data: 200960
I0428 20:18:20.494140  1102 layer_factory.hpp:77] Creating layer conv0
I0428 20:18:20.494154  1102 net.cpp:86] Creating Layer conv0
I0428 20:18:20.494171  1102 net.cpp:408] conv0 <- data
I0428 20:18:20.494182  1102 net.cpp:382] conv0 -> conv0
I0428 20:18:20.723145  1102 net.cpp:124] Setting up conv0
I0428 20:18:20.723189  1102 net.cpp:131] Top shape: 64 50 24 24 (1843200)
I0428 20:18:20.723193  1102 net.cpp:139] Memory required for data: 7573760
I0428 20:18:20.723208  1102 layer_factory.hpp:77] Creating layer pool0
I0428 20:18:20.723220  1102 net.cpp:86] Creating Layer pool0
I0428 20:18:20.723240  1102 net.cpp:408] pool0 <- conv0
I0428 20:18:20.723245  1102 net.cpp:382] pool0 -> pool0
I0428 20:18:20.723309  1102 net.cpp:124] Setting up pool0
I0428 20:18:20.723316  1102 net.cpp:131] Top shape: 64 50 12 12 (460800)
I0428 20:18:20.723320  1102 net.cpp:139] Memory required for data: 9416960
I0428 20:18:20.723322  1102 layer_factory.hpp:77] Creating layer conv1
I0428 20:18:20.723335  1102 net.cpp:86] Creating Layer conv1
I0428 20:18:20.723338  1102 net.cpp:408] conv1 <- pool0
I0428 20:18:20.723345  1102 net.cpp:382] conv1 -> conv1
I0428 20:18:20.726393  1102 net.cpp:124] Setting up conv1
I0428 20:18:20.726423  1102 net.cpp:131] Top shape: 64 10 8 8 (40960)
I0428 20:18:20.726428  1102 net.cpp:139] Memory required for data: 9580800
I0428 20:18:20.726435  1102 layer_factory.hpp:77] Creating layer pool1
I0428 20:18:20.726455  1102 net.cpp:86] Creating Layer pool1
I0428 20:18:20.726459  1102 net.cpp:408] pool1 <- conv1
I0428 20:18:20.726480  1102 net.cpp:382] pool1 -> pool1
I0428 20:18:20.726521  1102 net.cpp:124] Setting up pool1
I0428 20:18:20.726528  1102 net.cpp:131] Top shape: 64 10 4 4 (10240)
I0428 20:18:20.726531  1102 net.cpp:139] Memory required for data: 9621760
I0428 20:18:20.726534  1102 layer_factory.hpp:77] Creating layer ip1
I0428 20:18:20.726541  1102 net.cpp:86] Creating Layer ip1
I0428 20:18:20.726547  1102 net.cpp:408] ip1 <- pool1
I0428 20:18:20.726553  1102 net.cpp:382] ip1 -> ip1
I0428 20:18:20.726685  1102 net.cpp:124] Setting up ip1
I0428 20:18:20.726691  1102 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:18:20.726694  1102 net.cpp:139] Memory required for data: 9628160
I0428 20:18:20.726701  1102 layer_factory.hpp:77] Creating layer relu1
I0428 20:18:20.726711  1102 net.cpp:86] Creating Layer relu1
I0428 20:18:20.726713  1102 net.cpp:408] relu1 <- ip1
I0428 20:18:20.726718  1102 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:18:20.726897  1102 net.cpp:124] Setting up relu1
I0428 20:18:20.726907  1102 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:18:20.726909  1102 net.cpp:139] Memory required for data: 9634560
I0428 20:18:20.726912  1102 layer_factory.hpp:77] Creating layer ip2
I0428 20:18:20.726919  1102 net.cpp:86] Creating Layer ip2
I0428 20:18:20.726922  1102 net.cpp:408] ip2 <- ip1
I0428 20:18:20.726927  1102 net.cpp:382] ip2 -> ip2
I0428 20:18:20.727063  1102 net.cpp:124] Setting up ip2
I0428 20:18:20.727071  1102 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:18:20.727073  1102 net.cpp:139] Memory required for data: 9647360
I0428 20:18:20.727079  1102 layer_factory.hpp:77] Creating layer relu2
I0428 20:18:20.727102  1102 net.cpp:86] Creating Layer relu2
I0428 20:18:20.727108  1102 net.cpp:408] relu2 <- ip2
I0428 20:18:20.727115  1102 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:18:20.727901  1102 net.cpp:124] Setting up relu2
I0428 20:18:20.727913  1102 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:18:20.727933  1102 net.cpp:139] Memory required for data: 9660160
I0428 20:18:20.727937  1102 layer_factory.hpp:77] Creating layer ip3
I0428 20:18:20.727946  1102 net.cpp:86] Creating Layer ip3
I0428 20:18:20.727949  1102 net.cpp:408] ip3 <- ip2
I0428 20:18:20.727955  1102 net.cpp:382] ip3 -> ip3
I0428 20:18:20.728055  1102 net.cpp:124] Setting up ip3
I0428 20:18:20.728065  1102 net.cpp:131] Top shape: 64 10 (640)
I0428 20:18:20.728067  1102 net.cpp:139] Memory required for data: 9662720
I0428 20:18:20.728075  1102 layer_factory.hpp:77] Creating layer relu3
I0428 20:18:20.728081  1102 net.cpp:86] Creating Layer relu3
I0428 20:18:20.728085  1102 net.cpp:408] relu3 <- ip3
I0428 20:18:20.728088  1102 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:18:20.728282  1102 net.cpp:124] Setting up relu3
I0428 20:18:20.728291  1102 net.cpp:131] Top shape: 64 10 (640)
I0428 20:18:20.728294  1102 net.cpp:139] Memory required for data: 9665280
I0428 20:18:20.728297  1102 layer_factory.hpp:77] Creating layer loss
I0428 20:18:20.728307  1102 net.cpp:86] Creating Layer loss
I0428 20:18:20.728312  1102 net.cpp:408] loss <- ip3
I0428 20:18:20.728317  1102 net.cpp:408] loss <- label
I0428 20:18:20.728323  1102 net.cpp:382] loss -> loss
I0428 20:18:20.728343  1102 layer_factory.hpp:77] Creating layer loss
I0428 20:18:20.728601  1102 net.cpp:124] Setting up loss
I0428 20:18:20.728610  1102 net.cpp:131] Top shape: (1)
I0428 20:18:20.728613  1102 net.cpp:134]     with loss weight 1
I0428 20:18:20.728628  1102 net.cpp:139] Memory required for data: 9665284
I0428 20:18:20.728631  1102 net.cpp:200] loss needs backward computation.
I0428 20:18:20.728636  1102 net.cpp:200] relu3 needs backward computation.
I0428 20:18:20.728638  1102 net.cpp:200] ip3 needs backward computation.
I0428 20:18:20.728641  1102 net.cpp:200] relu2 needs backward computation.
I0428 20:18:20.728643  1102 net.cpp:200] ip2 needs backward computation.
I0428 20:18:20.728646  1102 net.cpp:200] relu1 needs backward computation.
I0428 20:18:20.728649  1102 net.cpp:200] ip1 needs backward computation.
I0428 20:18:20.728652  1102 net.cpp:200] pool1 needs backward computation.
I0428 20:18:20.728654  1102 net.cpp:200] conv1 needs backward computation.
I0428 20:18:20.728657  1102 net.cpp:200] pool0 needs backward computation.
I0428 20:18:20.728660  1102 net.cpp:200] conv0 needs backward computation.
I0428 20:18:20.728663  1102 net.cpp:202] mnist does not need backward computation.
I0428 20:18:20.728667  1102 net.cpp:244] This network produces output loss
I0428 20:18:20.728675  1102 net.cpp:257] Network initialization done.
I0428 20:18:20.729018  1102 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1316.prototxt
I0428 20:18:20.729046  1102 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:18:20.729158  1102 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:18:20.729256  1102 layer_factory.hpp:77] Creating layer mnist
I0428 20:18:20.729298  1102 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:18:20.729313  1102 net.cpp:86] Creating Layer mnist
I0428 20:18:20.729320  1102 net.cpp:382] mnist -> data
I0428 20:18:20.729326  1102 net.cpp:382] mnist -> label
I0428 20:18:20.729424  1102 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:18:20.731461  1102 net.cpp:124] Setting up mnist
I0428 20:18:20.731492  1102 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:18:20.731514  1102 net.cpp:131] Top shape: 100 (100)
I0428 20:18:20.731518  1102 net.cpp:139] Memory required for data: 314000
I0428 20:18:20.731520  1102 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:18:20.731534  1102 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:18:20.731536  1102 net.cpp:408] label_mnist_1_split <- label
I0428 20:18:20.731541  1102 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:18:20.731549  1102 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:18:20.731650  1102 net.cpp:124] Setting up label_mnist_1_split
I0428 20:18:20.731659  1102 net.cpp:131] Top shape: 100 (100)
I0428 20:18:20.731664  1102 net.cpp:131] Top shape: 100 (100)
I0428 20:18:20.731667  1102 net.cpp:139] Memory required for data: 314800
I0428 20:18:20.731670  1102 layer_factory.hpp:77] Creating layer conv0
I0428 20:18:20.731680  1102 net.cpp:86] Creating Layer conv0
I0428 20:18:20.731684  1102 net.cpp:408] conv0 <- data
I0428 20:18:20.731690  1102 net.cpp:382] conv0 -> conv0
I0428 20:18:20.733394  1102 net.cpp:124] Setting up conv0
I0428 20:18:20.733407  1102 net.cpp:131] Top shape: 100 50 24 24 (2880000)
I0428 20:18:20.733428  1102 net.cpp:139] Memory required for data: 11834800
I0428 20:18:20.733436  1102 layer_factory.hpp:77] Creating layer pool0
I0428 20:18:20.733443  1102 net.cpp:86] Creating Layer pool0
I0428 20:18:20.733446  1102 net.cpp:408] pool0 <- conv0
I0428 20:18:20.733453  1102 net.cpp:382] pool0 -> pool0
I0428 20:18:20.733489  1102 net.cpp:124] Setting up pool0
I0428 20:18:20.733496  1102 net.cpp:131] Top shape: 100 50 12 12 (720000)
I0428 20:18:20.733500  1102 net.cpp:139] Memory required for data: 14714800
I0428 20:18:20.733502  1102 layer_factory.hpp:77] Creating layer conv1
I0428 20:18:20.733512  1102 net.cpp:86] Creating Layer conv1
I0428 20:18:20.733515  1102 net.cpp:408] conv1 <- pool0
I0428 20:18:20.733520  1102 net.cpp:382] conv1 -> conv1
I0428 20:18:20.735110  1102 net.cpp:124] Setting up conv1
I0428 20:18:20.735134  1102 net.cpp:131] Top shape: 100 10 8 8 (64000)
I0428 20:18:20.735157  1102 net.cpp:139] Memory required for data: 14970800
I0428 20:18:20.735165  1102 layer_factory.hpp:77] Creating layer pool1
I0428 20:18:20.735172  1102 net.cpp:86] Creating Layer pool1
I0428 20:18:20.735175  1102 net.cpp:408] pool1 <- conv1
I0428 20:18:20.735180  1102 net.cpp:382] pool1 -> pool1
I0428 20:18:20.735224  1102 net.cpp:124] Setting up pool1
I0428 20:18:20.735229  1102 net.cpp:131] Top shape: 100 10 4 4 (16000)
I0428 20:18:20.735234  1102 net.cpp:139] Memory required for data: 15034800
I0428 20:18:20.735236  1102 layer_factory.hpp:77] Creating layer ip1
I0428 20:18:20.735244  1102 net.cpp:86] Creating Layer ip1
I0428 20:18:20.735246  1102 net.cpp:408] ip1 <- pool1
I0428 20:18:20.735252  1102 net.cpp:382] ip1 -> ip1
I0428 20:18:20.735375  1102 net.cpp:124] Setting up ip1
I0428 20:18:20.735384  1102 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:18:20.735397  1102 net.cpp:139] Memory required for data: 15044800
I0428 20:18:20.735404  1102 layer_factory.hpp:77] Creating layer relu1
I0428 20:18:20.735411  1102 net.cpp:86] Creating Layer relu1
I0428 20:18:20.735415  1102 net.cpp:408] relu1 <- ip1
I0428 20:18:20.735419  1102 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:18:20.735646  1102 net.cpp:124] Setting up relu1
I0428 20:18:20.735656  1102 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:18:20.735658  1102 net.cpp:139] Memory required for data: 15054800
I0428 20:18:20.735662  1102 layer_factory.hpp:77] Creating layer ip2
I0428 20:18:20.735671  1102 net.cpp:86] Creating Layer ip2
I0428 20:18:20.735676  1102 net.cpp:408] ip2 <- ip1
I0428 20:18:20.735682  1102 net.cpp:382] ip2 -> ip2
I0428 20:18:20.735792  1102 net.cpp:124] Setting up ip2
I0428 20:18:20.735800  1102 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:18:20.735803  1102 net.cpp:139] Memory required for data: 15074800
I0428 20:18:20.735808  1102 layer_factory.hpp:77] Creating layer relu2
I0428 20:18:20.735815  1102 net.cpp:86] Creating Layer relu2
I0428 20:18:20.735834  1102 net.cpp:408] relu2 <- ip2
I0428 20:18:20.735838  1102 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:18:20.736035  1102 net.cpp:124] Setting up relu2
I0428 20:18:20.736044  1102 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:18:20.736047  1102 net.cpp:139] Memory required for data: 15094800
I0428 20:18:20.736050  1102 layer_factory.hpp:77] Creating layer ip3
I0428 20:18:20.736057  1102 net.cpp:86] Creating Layer ip3
I0428 20:18:20.736060  1102 net.cpp:408] ip3 <- ip2
I0428 20:18:20.736066  1102 net.cpp:382] ip3 -> ip3
I0428 20:18:20.736201  1102 net.cpp:124] Setting up ip3
I0428 20:18:20.736207  1102 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:18:20.736210  1102 net.cpp:139] Memory required for data: 15098800
I0428 20:18:20.736217  1102 layer_factory.hpp:77] Creating layer relu3
I0428 20:18:20.736222  1102 net.cpp:86] Creating Layer relu3
I0428 20:18:20.736225  1102 net.cpp:408] relu3 <- ip3
I0428 20:18:20.736228  1102 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:18:20.737040  1102 net.cpp:124] Setting up relu3
I0428 20:18:20.737053  1102 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:18:20.737072  1102 net.cpp:139] Memory required for data: 15102800
I0428 20:18:20.737076  1102 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:18:20.737082  1102 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:18:20.737085  1102 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:18:20.737092  1102 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:18:20.737098  1102 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:18:20.737138  1102 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:18:20.737145  1102 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:18:20.737149  1102 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:18:20.737151  1102 net.cpp:139] Memory required for data: 15110800
I0428 20:18:20.737154  1102 layer_factory.hpp:77] Creating layer accuracy
I0428 20:18:20.737159  1102 net.cpp:86] Creating Layer accuracy
I0428 20:18:20.737162  1102 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:18:20.737167  1102 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:18:20.737171  1102 net.cpp:382] accuracy -> accuracy
I0428 20:18:20.737179  1102 net.cpp:124] Setting up accuracy
I0428 20:18:20.737182  1102 net.cpp:131] Top shape: (1)
I0428 20:18:20.737185  1102 net.cpp:139] Memory required for data: 15110804
I0428 20:18:20.737188  1102 layer_factory.hpp:77] Creating layer loss
I0428 20:18:20.737193  1102 net.cpp:86] Creating Layer loss
I0428 20:18:20.737197  1102 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:18:20.737201  1102 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:18:20.737205  1102 net.cpp:382] loss -> loss
I0428 20:18:20.737212  1102 layer_factory.hpp:77] Creating layer loss
I0428 20:18:20.737473  1102 net.cpp:124] Setting up loss
I0428 20:18:20.737481  1102 net.cpp:131] Top shape: (1)
I0428 20:18:20.737484  1102 net.cpp:134]     with loss weight 1
I0428 20:18:20.737500  1102 net.cpp:139] Memory required for data: 15110808
I0428 20:18:20.737504  1102 net.cpp:200] loss needs backward computation.
I0428 20:18:20.737507  1102 net.cpp:202] accuracy does not need backward computation.
I0428 20:18:20.737511  1102 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:18:20.737514  1102 net.cpp:200] relu3 needs backward computation.
I0428 20:18:20.737516  1102 net.cpp:200] ip3 needs backward computation.
I0428 20:18:20.737519  1102 net.cpp:200] relu2 needs backward computation.
I0428 20:18:20.737521  1102 net.cpp:200] ip2 needs backward computation.
I0428 20:18:20.737524  1102 net.cpp:200] relu1 needs backward computation.
I0428 20:18:20.737527  1102 net.cpp:200] ip1 needs backward computation.
I0428 20:18:20.737530  1102 net.cpp:200] pool1 needs backward computation.
I0428 20:18:20.737532  1102 net.cpp:200] conv1 needs backward computation.
I0428 20:18:20.737535  1102 net.cpp:200] pool0 needs backward computation.
I0428 20:18:20.737538  1102 net.cpp:200] conv0 needs backward computation.
I0428 20:18:20.737542  1102 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:18:20.737546  1102 net.cpp:202] mnist does not need backward computation.
I0428 20:18:20.737550  1102 net.cpp:244] This network produces output accuracy
I0428 20:18:20.737552  1102 net.cpp:244] This network produces output loss
I0428 20:18:20.737563  1102 net.cpp:257] Network initialization done.
I0428 20:18:20.737603  1102 solver.cpp:56] Solver scaffolding done.
I0428 20:18:20.737985  1102 caffe.cpp:248] Starting Optimization
I0428 20:18:20.737992  1102 solver.cpp:273] Solving LeNet
I0428 20:18:20.737994  1102 solver.cpp:274] Learning Rate Policy: inv
I0428 20:18:20.738812  1102 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:18:20.747709  1102 blocking_queue.cpp:49] Waiting for data
I0428 20:18:20.816185  1112 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:18:20.816972  1102 solver.cpp:398]     Test net output #0: accuracy = 0.1365
I0428 20:18:20.816989  1102 solver.cpp:398]     Test net output #1: loss = 2.2998 (* 1 = 2.2998 loss)
I0428 20:18:20.821409  1102 solver.cpp:219] Iteration 0 (-1.79896e-33 iter/s, 0.0833731s/100 iters), loss = 2.28627
I0428 20:18:20.821432  1102 solver.cpp:238]     Train net output #0: loss = 2.28627 (* 1 = 2.28627 loss)
I0428 20:18:20.821444  1102 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:18:20.946065  1102 solver.cpp:219] Iteration 100 (802.447 iter/s, 0.124619s/100 iters), loss = 0.576638
I0428 20:18:20.946105  1102 solver.cpp:238]     Train net output #0: loss = 0.576638 (* 1 = 0.576638 loss)
I0428 20:18:20.946112  1102 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:18:21.064096  1102 solver.cpp:219] Iteration 200 (847.71 iter/s, 0.117965s/100 iters), loss = 0.479423
I0428 20:18:21.064136  1102 solver.cpp:238]     Train net output #0: loss = 0.479423 (* 1 = 0.479423 loss)
I0428 20:18:21.064142  1102 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:18:21.182340  1102 solver.cpp:219] Iteration 300 (845.963 iter/s, 0.118209s/100 iters), loss = 0.691667
I0428 20:18:21.182381  1102 solver.cpp:238]     Train net output #0: loss = 0.691667 (* 1 = 0.691667 loss)
I0428 20:18:21.182387  1102 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:18:21.299587  1102 solver.cpp:219] Iteration 400 (853.154 iter/s, 0.117212s/100 iters), loss = 0.653481
I0428 20:18:21.299626  1102 solver.cpp:238]     Train net output #0: loss = 0.653481 (* 1 = 0.653481 loss)
I0428 20:18:21.299633  1102 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:18:21.415662  1102 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:18:21.480494  1112 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:18:21.482707  1102 solver.cpp:398]     Test net output #0: accuracy = 0.8674
I0428 20:18:21.482743  1102 solver.cpp:398]     Test net output #1: loss = 0.570016 (* 1 = 0.570016 loss)
I0428 20:18:21.483901  1102 solver.cpp:219] Iteration 500 (542.664 iter/s, 0.184276s/100 iters), loss = 0.581412
I0428 20:18:21.483955  1102 solver.cpp:238]     Train net output #0: loss = 0.581412 (* 1 = 0.581412 loss)
I0428 20:18:21.483963  1102 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:18:21.608052  1102 solver.cpp:219] Iteration 600 (805.797 iter/s, 0.124101s/100 iters), loss = 0.533434
I0428 20:18:21.608084  1102 solver.cpp:238]     Train net output #0: loss = 0.533434 (* 1 = 0.533434 loss)
I0428 20:18:21.608091  1102 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:18:21.725142  1102 solver.cpp:219] Iteration 700 (854.37 iter/s, 0.117045s/100 iters), loss = 0.480888
I0428 20:18:21.725183  1102 solver.cpp:238]     Train net output #0: loss = 0.480888 (* 1 = 0.480888 loss)
I0428 20:18:21.725204  1102 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:18:21.841225  1102 solver.cpp:219] Iteration 800 (861.826 iter/s, 0.116033s/100 iters), loss = 0.292158
I0428 20:18:21.841248  1102 solver.cpp:238]     Train net output #0: loss = 0.292158 (* 1 = 0.292158 loss)
I0428 20:18:21.841269  1102 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:18:21.973440  1102 solver.cpp:219] Iteration 900 (756.539 iter/s, 0.132181s/100 iters), loss = 0.248026
I0428 20:18:21.973480  1102 solver.cpp:238]     Train net output #0: loss = 0.248026 (* 1 = 0.248026 loss)
I0428 20:18:21.973486  1102 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:18:22.015435  1108 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:18:22.096777  1102 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:18:22.097959  1102 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:18:22.098806  1102 solver.cpp:311] Iteration 1000, loss = 0.113051
I0428 20:18:22.098820  1102 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:18:22.174787  1112 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:18:22.175580  1102 solver.cpp:398]     Test net output #0: accuracy = 0.9694
I0428 20:18:22.175614  1102 solver.cpp:398]     Test net output #1: loss = 0.0941373 (* 1 = 0.0941373 loss)
I0428 20:18:22.175621  1102 solver.cpp:316] Optimization Done.
I0428 20:18:22.175623  1102 caffe.cpp:259] Optimization Done.
