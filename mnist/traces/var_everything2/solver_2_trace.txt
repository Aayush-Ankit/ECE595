I0428 19:27:45.081499 21436 caffe.cpp:218] Using GPUs 0
I0428 19:27:45.122170 21436 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:27:45.641739 21436 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test2.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:27:45.641881 21436 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test2.prototxt
I0428 19:27:45.642112 21436 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:27:45.642124 21436 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:27:45.642177 21436 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0428 19:27:45.642227 21436 layer_factory.hpp:77] Creating layer mnist
I0428 19:27:45.642330 21436 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:27:45.642355 21436 net.cpp:86] Creating Layer mnist
I0428 19:27:45.642365 21436 net.cpp:382] mnist -> data
I0428 19:27:45.642388 21436 net.cpp:382] mnist -> label
I0428 19:27:45.643491 21436 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:27:45.645956 21436 net.cpp:124] Setting up mnist
I0428 19:27:45.645975 21436 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:27:45.645982 21436 net.cpp:131] Top shape: 64 (64)
I0428 19:27:45.645987 21436 net.cpp:139] Memory required for data: 200960
I0428 19:27:45.645993 21436 layer_factory.hpp:77] Creating layer ip1
I0428 19:27:45.646004 21436 net.cpp:86] Creating Layer ip1
I0428 19:27:45.646013 21436 net.cpp:408] ip1 <- data
I0428 19:27:45.646024 21436 net.cpp:382] ip1 -> ip1
I0428 19:27:45.647238 21436 net.cpp:124] Setting up ip1
I0428 19:27:45.647253 21436 net.cpp:131] Top shape: 64 10 (640)
I0428 19:27:45.647258 21436 net.cpp:139] Memory required for data: 203520
I0428 19:27:45.647271 21436 layer_factory.hpp:77] Creating layer relu1
I0428 19:27:45.647280 21436 net.cpp:86] Creating Layer relu1
I0428 19:27:45.647285 21436 net.cpp:408] relu1 <- ip1
I0428 19:27:45.647290 21436 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:27:45.934775 21436 net.cpp:124] Setting up relu1
I0428 19:27:45.934804 21436 net.cpp:131] Top shape: 64 10 (640)
I0428 19:27:45.934809 21436 net.cpp:139] Memory required for data: 206080
I0428 19:27:45.934814 21436 layer_factory.hpp:77] Creating layer loss
I0428 19:27:45.934825 21436 net.cpp:86] Creating Layer loss
I0428 19:27:45.934831 21436 net.cpp:408] loss <- ip1
I0428 19:27:45.934839 21436 net.cpp:408] loss <- label
I0428 19:27:45.934844 21436 net.cpp:382] loss -> loss
I0428 19:27:45.934870 21436 layer_factory.hpp:77] Creating layer loss
I0428 19:27:45.936758 21436 net.cpp:124] Setting up loss
I0428 19:27:45.936774 21436 net.cpp:131] Top shape: (1)
I0428 19:27:45.936779 21436 net.cpp:134]     with loss weight 1
I0428 19:27:45.936794 21436 net.cpp:139] Memory required for data: 206084
I0428 19:27:45.936799 21436 net.cpp:200] loss needs backward computation.
I0428 19:27:45.936802 21436 net.cpp:200] relu1 needs backward computation.
I0428 19:27:45.936806 21436 net.cpp:200] ip1 needs backward computation.
I0428 19:27:45.936836 21436 net.cpp:202] mnist does not need backward computation.
I0428 19:27:45.936841 21436 net.cpp:244] This network produces output loss
I0428 19:27:45.936848 21436 net.cpp:257] Network initialization done.
I0428 19:27:45.937038 21436 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test2.prototxt
I0428 19:27:45.937060 21436 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:27:45.937119 21436 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0428 19:27:45.937170 21436 layer_factory.hpp:77] Creating layer mnist
I0428 19:27:45.937221 21436 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:27:45.937237 21436 net.cpp:86] Creating Layer mnist
I0428 19:27:45.937244 21436 net.cpp:382] mnist -> data
I0428 19:27:45.937254 21436 net.cpp:382] mnist -> label
I0428 19:27:45.937361 21436 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:27:45.939395 21436 net.cpp:124] Setting up mnist
I0428 19:27:45.939410 21436 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:27:45.939416 21436 net.cpp:131] Top shape: 100 (100)
I0428 19:27:45.939420 21436 net.cpp:139] Memory required for data: 314000
I0428 19:27:45.939425 21436 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:27:45.939436 21436 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:27:45.939442 21436 net.cpp:408] label_mnist_1_split <- label
I0428 19:27:45.939448 21436 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:27:45.939455 21436 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:27:45.939548 21436 net.cpp:124] Setting up label_mnist_1_split
I0428 19:27:45.939556 21436 net.cpp:131] Top shape: 100 (100)
I0428 19:27:45.939561 21436 net.cpp:131] Top shape: 100 (100)
I0428 19:27:45.939564 21436 net.cpp:139] Memory required for data: 314800
I0428 19:27:45.939568 21436 layer_factory.hpp:77] Creating layer ip1
I0428 19:27:45.939577 21436 net.cpp:86] Creating Layer ip1
I0428 19:27:45.939582 21436 net.cpp:408] ip1 <- data
I0428 19:27:45.939587 21436 net.cpp:382] ip1 -> ip1
I0428 19:27:45.939743 21436 net.cpp:124] Setting up ip1
I0428 19:27:45.939750 21436 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:27:45.939754 21436 net.cpp:139] Memory required for data: 318800
I0428 19:27:45.939764 21436 layer_factory.hpp:77] Creating layer relu1
I0428 19:27:45.939772 21436 net.cpp:86] Creating Layer relu1
I0428 19:27:45.939776 21436 net.cpp:408] relu1 <- ip1
I0428 19:27:45.939780 21436 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:27:45.939959 21436 net.cpp:124] Setting up relu1
I0428 19:27:45.939967 21436 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:27:45.939971 21436 net.cpp:139] Memory required for data: 322800
I0428 19:27:45.939975 21436 layer_factory.hpp:77] Creating layer ip1_relu1_0_split
I0428 19:27:45.939981 21436 net.cpp:86] Creating Layer ip1_relu1_0_split
I0428 19:27:45.939985 21436 net.cpp:408] ip1_relu1_0_split <- ip1
I0428 19:27:45.939990 21436 net.cpp:382] ip1_relu1_0_split -> ip1_relu1_0_split_0
I0428 19:27:45.939996 21436 net.cpp:382] ip1_relu1_0_split -> ip1_relu1_0_split_1
I0428 19:27:45.940032 21436 net.cpp:124] Setting up ip1_relu1_0_split
I0428 19:27:45.940052 21436 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:27:45.940057 21436 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:27:45.940059 21436 net.cpp:139] Memory required for data: 330800
I0428 19:27:45.940063 21436 layer_factory.hpp:77] Creating layer accuracy
I0428 19:27:45.940069 21436 net.cpp:86] Creating Layer accuracy
I0428 19:27:45.940073 21436 net.cpp:408] accuracy <- ip1_relu1_0_split_0
I0428 19:27:45.940078 21436 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:27:45.940083 21436 net.cpp:382] accuracy -> accuracy
I0428 19:27:45.940095 21436 net.cpp:124] Setting up accuracy
I0428 19:27:45.940101 21436 net.cpp:131] Top shape: (1)
I0428 19:27:45.940104 21436 net.cpp:139] Memory required for data: 330804
I0428 19:27:45.940107 21436 layer_factory.hpp:77] Creating layer loss
I0428 19:27:45.940112 21436 net.cpp:86] Creating Layer loss
I0428 19:27:45.940116 21436 net.cpp:408] loss <- ip1_relu1_0_split_1
I0428 19:27:45.940120 21436 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:27:45.940125 21436 net.cpp:382] loss -> loss
I0428 19:27:45.940134 21436 layer_factory.hpp:77] Creating layer loss
I0428 19:27:45.940377 21436 net.cpp:124] Setting up loss
I0428 19:27:45.940387 21436 net.cpp:131] Top shape: (1)
I0428 19:27:45.940392 21436 net.cpp:134]     with loss weight 1
I0428 19:27:45.940397 21436 net.cpp:139] Memory required for data: 330808
I0428 19:27:45.940402 21436 net.cpp:200] loss needs backward computation.
I0428 19:27:45.940405 21436 net.cpp:202] accuracy does not need backward computation.
I0428 19:27:45.940410 21436 net.cpp:200] ip1_relu1_0_split needs backward computation.
I0428 19:27:45.940413 21436 net.cpp:200] relu1 needs backward computation.
I0428 19:27:45.940417 21436 net.cpp:200] ip1 needs backward computation.
I0428 19:27:45.940420 21436 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:27:45.940424 21436 net.cpp:202] mnist does not need backward computation.
I0428 19:27:45.940428 21436 net.cpp:244] This network produces output accuracy
I0428 19:27:45.940433 21436 net.cpp:244] This network produces output loss
I0428 19:27:45.940440 21436 net.cpp:257] Network initialization done.
I0428 19:27:45.940464 21436 solver.cpp:56] Solver scaffolding done.
I0428 19:27:45.940549 21436 caffe.cpp:248] Starting Optimization
I0428 19:27:45.940557 21436 solver.cpp:273] Solving LeNet
I0428 19:27:45.940559 21436 solver.cpp:274] Learning Rate Policy: inv
I0428 19:27:45.940604 21436 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:27:45.940649 21436 blocking_queue.cpp:49] Waiting for data
I0428 19:27:46.021632 21445 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:27:46.021991 21436 solver.cpp:398]     Test net output #0: accuracy = 0.0431
I0428 19:27:46.022013 21436 solver.cpp:398]     Test net output #1: loss = 2.39831 (* 1 = 2.39831 loss)
I0428 19:27:46.022377 21436 solver.cpp:219] Iteration 0 (0 iter/s, 0.081794s/100 iters), loss = 2.40116
I0428 19:27:46.022423 21436 solver.cpp:238]     Train net output #0: loss = 2.40116 (* 1 = 2.40116 loss)
I0428 19:27:46.022435 21436 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:27:46.063518 21436 solver.cpp:219] Iteration 100 (2433.69 iter/s, 0.04109s/100 iters), loss = 0.799022
I0428 19:27:46.063541 21436 solver.cpp:238]     Train net output #0: loss = 0.799022 (* 1 = 0.799022 loss)
I0428 19:27:46.063549 21436 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:27:46.104667 21436 solver.cpp:219] Iteration 200 (2431.98 iter/s, 0.0411187s/100 iters), loss = 0.710699
I0428 19:27:46.104688 21436 solver.cpp:238]     Train net output #0: loss = 0.710699 (* 1 = 0.710699 loss)
I0428 19:27:46.104694 21436 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:27:46.146358 21436 solver.cpp:219] Iteration 300 (2400.3 iter/s, 0.0416614s/100 iters), loss = 0.510795
I0428 19:27:46.146379 21436 solver.cpp:238]     Train net output #0: loss = 0.510795 (* 1 = 0.510795 loss)
I0428 19:27:46.146386 21436 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:27:46.188041 21436 solver.cpp:219] Iteration 400 (2400.65 iter/s, 0.0416553s/100 iters), loss = 0.35765
I0428 19:27:46.188077 21436 solver.cpp:238]     Train net output #0: loss = 0.35765 (* 1 = 0.35765 loss)
I0428 19:27:46.188083 21436 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:27:46.229043 21436 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:27:46.284150 21445 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:27:46.284502 21436 solver.cpp:398]     Test net output #0: accuracy = 0.8994
I0428 19:27:46.284549 21436 solver.cpp:398]     Test net output #1: loss = 0.385674 (* 1 = 0.385674 loss)
I0428 19:27:46.284835 21436 solver.cpp:219] Iteration 500 (1033.66 iter/s, 0.0967435s/100 iters), loss = 0.426898
I0428 19:27:46.284873 21436 solver.cpp:238]     Train net output #0: loss = 0.426898 (* 1 = 0.426898 loss)
I0428 19:27:46.284895 21436 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:27:46.326092 21436 solver.cpp:219] Iteration 600 (2426.54 iter/s, 0.0412109s/100 iters), loss = 0.283605
I0428 19:27:46.326128 21436 solver.cpp:238]     Train net output #0: loss = 0.283605 (* 1 = 0.283605 loss)
I0428 19:27:46.326133 21436 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:27:46.367223 21436 solver.cpp:219] Iteration 700 (2433.68 iter/s, 0.04109s/100 iters), loss = 0.496755
I0428 19:27:46.367262 21436 solver.cpp:238]     Train net output #0: loss = 0.496755 (* 1 = 0.496755 loss)
I0428 19:27:46.367269 21436 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:27:46.408417 21436 solver.cpp:219] Iteration 800 (2429.26 iter/s, 0.0411647s/100 iters), loss = 0.401241
I0428 19:27:46.408453 21436 solver.cpp:238]     Train net output #0: loss = 0.401241 (* 1 = 0.401241 loss)
I0428 19:27:46.408459 21436 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:27:46.412499 21436 blocking_queue.cpp:49] Waiting for data
I0428 19:27:46.446607 21436 solver.cpp:219] Iteration 900 (2620.38 iter/s, 0.0381623s/100 iters), loss = 0.451604
I0428 19:27:46.446645 21436 solver.cpp:238]     Train net output #0: loss = 0.451604 (* 1 = 0.451604 loss)
I0428 19:27:46.446650 21436 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:27:46.459285 21444 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:27:46.481015 21436 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:27:46.481425 21436 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:27:46.481703 21436 solver.cpp:311] Iteration 1000, loss = 0.349763
I0428 19:27:46.481719 21436 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:27:46.526880 21445 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:27:46.527210 21436 solver.cpp:398]     Test net output #0: accuracy = 0.9059
I0428 19:27:46.527226 21436 solver.cpp:398]     Test net output #1: loss = 0.342513 (* 1 = 0.342513 loss)
I0428 19:27:46.527231 21436 solver.cpp:316] Optimization Done.
I0428 19:27:46.527235 21436 caffe.cpp:259] Optimization Done.
