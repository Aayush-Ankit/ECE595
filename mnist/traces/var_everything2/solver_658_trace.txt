I0428 19:52:18.112159 27484 caffe.cpp:218] Using GPUs 0
I0428 19:52:18.152979 27484 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:52:18.671041 27484 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test658.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:52:18.671181 27484 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test658.prototxt
I0428 19:52:18.671600 27484 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:52:18.671620 27484 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:52:18.671722 27484 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:52:18.671802 27484 layer_factory.hpp:77] Creating layer mnist
I0428 19:52:18.671901 27484 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:52:18.671927 27484 net.cpp:86] Creating Layer mnist
I0428 19:52:18.671934 27484 net.cpp:382] mnist -> data
I0428 19:52:18.671957 27484 net.cpp:382] mnist -> label
I0428 19:52:18.673069 27484 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:52:18.675732 27484 net.cpp:124] Setting up mnist
I0428 19:52:18.675761 27484 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:52:18.675782 27484 net.cpp:131] Top shape: 64 (64)
I0428 19:52:18.675786 27484 net.cpp:139] Memory required for data: 200960
I0428 19:52:18.675794 27484 layer_factory.hpp:77] Creating layer conv0
I0428 19:52:18.675809 27484 net.cpp:86] Creating Layer conv0
I0428 19:52:18.675832 27484 net.cpp:408] conv0 <- data
I0428 19:52:18.675845 27484 net.cpp:382] conv0 -> conv0
I0428 19:52:18.963490 27484 net.cpp:124] Setting up conv0
I0428 19:52:18.963518 27484 net.cpp:131] Top shape: 64 5 24 24 (184320)
I0428 19:52:18.963523 27484 net.cpp:139] Memory required for data: 938240
I0428 19:52:18.963539 27484 layer_factory.hpp:77] Creating layer pool0
I0428 19:52:18.963556 27484 net.cpp:86] Creating Layer pool0
I0428 19:52:18.963559 27484 net.cpp:408] pool0 <- conv0
I0428 19:52:18.963565 27484 net.cpp:382] pool0 -> pool0
I0428 19:52:18.963614 27484 net.cpp:124] Setting up pool0
I0428 19:52:18.963620 27484 net.cpp:131] Top shape: 64 5 12 12 (46080)
I0428 19:52:18.963624 27484 net.cpp:139] Memory required for data: 1122560
I0428 19:52:18.963626 27484 layer_factory.hpp:77] Creating layer conv1
I0428 19:52:18.963637 27484 net.cpp:86] Creating Layer conv1
I0428 19:52:18.963641 27484 net.cpp:408] conv1 <- pool0
I0428 19:52:18.963646 27484 net.cpp:382] conv1 -> conv1
I0428 19:52:18.966559 27484 net.cpp:124] Setting up conv1
I0428 19:52:18.966578 27484 net.cpp:131] Top shape: 64 25 8 8 (102400)
I0428 19:52:18.966581 27484 net.cpp:139] Memory required for data: 1532160
I0428 19:52:18.966590 27484 layer_factory.hpp:77] Creating layer pool1
I0428 19:52:18.966598 27484 net.cpp:86] Creating Layer pool1
I0428 19:52:18.966603 27484 net.cpp:408] pool1 <- conv1
I0428 19:52:18.966608 27484 net.cpp:382] pool1 -> pool1
I0428 19:52:18.966670 27484 net.cpp:124] Setting up pool1
I0428 19:52:18.966675 27484 net.cpp:131] Top shape: 64 25 4 4 (25600)
I0428 19:52:18.966678 27484 net.cpp:139] Memory required for data: 1634560
I0428 19:52:18.966681 27484 layer_factory.hpp:77] Creating layer ip1
I0428 19:52:18.966691 27484 net.cpp:86] Creating Layer ip1
I0428 19:52:18.966693 27484 net.cpp:408] ip1 <- pool1
I0428 19:52:18.966698 27484 net.cpp:382] ip1 -> ip1
I0428 19:52:18.966953 27484 net.cpp:124] Setting up ip1
I0428 19:52:18.966961 27484 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:52:18.966964 27484 net.cpp:139] Memory required for data: 1647360
I0428 19:52:18.966972 27484 layer_factory.hpp:77] Creating layer relu1
I0428 19:52:18.966979 27484 net.cpp:86] Creating Layer relu1
I0428 19:52:18.966984 27484 net.cpp:408] relu1 <- ip1
I0428 19:52:18.966987 27484 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:52:18.967166 27484 net.cpp:124] Setting up relu1
I0428 19:52:18.967176 27484 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:52:18.967180 27484 net.cpp:139] Memory required for data: 1660160
I0428 19:52:18.967182 27484 layer_factory.hpp:77] Creating layer ip2
I0428 19:52:18.967190 27484 net.cpp:86] Creating Layer ip2
I0428 19:52:18.967193 27484 net.cpp:408] ip2 <- ip1
I0428 19:52:18.967198 27484 net.cpp:382] ip2 -> ip2
I0428 19:52:18.967308 27484 net.cpp:124] Setting up ip2
I0428 19:52:18.967314 27484 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:52:18.967317 27484 net.cpp:139] Memory required for data: 1666560
I0428 19:52:18.967324 27484 layer_factory.hpp:77] Creating layer relu2
I0428 19:52:18.967330 27484 net.cpp:86] Creating Layer relu2
I0428 19:52:18.967334 27484 net.cpp:408] relu2 <- ip2
I0428 19:52:18.967339 27484 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:52:18.968101 27484 net.cpp:124] Setting up relu2
I0428 19:52:18.968113 27484 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:52:18.968117 27484 net.cpp:139] Memory required for data: 1672960
I0428 19:52:18.968120 27484 layer_factory.hpp:77] Creating layer ip3
I0428 19:52:18.968129 27484 net.cpp:86] Creating Layer ip3
I0428 19:52:18.968132 27484 net.cpp:408] ip3 <- ip2
I0428 19:52:18.968139 27484 net.cpp:382] ip3 -> ip3
I0428 19:52:18.968250 27484 net.cpp:124] Setting up ip3
I0428 19:52:18.968258 27484 net.cpp:131] Top shape: 64 10 (640)
I0428 19:52:18.968261 27484 net.cpp:139] Memory required for data: 1675520
I0428 19:52:18.968269 27484 layer_factory.hpp:77] Creating layer relu3
I0428 19:52:18.968274 27484 net.cpp:86] Creating Layer relu3
I0428 19:52:18.968278 27484 net.cpp:408] relu3 <- ip3
I0428 19:52:18.968282 27484 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:52:18.968472 27484 net.cpp:124] Setting up relu3
I0428 19:52:18.968482 27484 net.cpp:131] Top shape: 64 10 (640)
I0428 19:52:18.968484 27484 net.cpp:139] Memory required for data: 1678080
I0428 19:52:18.968488 27484 layer_factory.hpp:77] Creating layer loss
I0428 19:52:18.968493 27484 net.cpp:86] Creating Layer loss
I0428 19:52:18.968497 27484 net.cpp:408] loss <- ip3
I0428 19:52:18.968502 27484 net.cpp:408] loss <- label
I0428 19:52:18.968508 27484 net.cpp:382] loss -> loss
I0428 19:52:18.968526 27484 layer_factory.hpp:77] Creating layer loss
I0428 19:52:18.968794 27484 net.cpp:124] Setting up loss
I0428 19:52:18.968804 27484 net.cpp:131] Top shape: (1)
I0428 19:52:18.968807 27484 net.cpp:134]     with loss weight 1
I0428 19:52:18.968828 27484 net.cpp:139] Memory required for data: 1678084
I0428 19:52:18.968832 27484 net.cpp:200] loss needs backward computation.
I0428 19:52:18.968835 27484 net.cpp:200] relu3 needs backward computation.
I0428 19:52:18.968838 27484 net.cpp:200] ip3 needs backward computation.
I0428 19:52:18.968842 27484 net.cpp:200] relu2 needs backward computation.
I0428 19:52:18.968844 27484 net.cpp:200] ip2 needs backward computation.
I0428 19:52:18.968847 27484 net.cpp:200] relu1 needs backward computation.
I0428 19:52:18.968850 27484 net.cpp:200] ip1 needs backward computation.
I0428 19:52:18.968853 27484 net.cpp:200] pool1 needs backward computation.
I0428 19:52:18.968858 27484 net.cpp:200] conv1 needs backward computation.
I0428 19:52:18.968860 27484 net.cpp:200] pool0 needs backward computation.
I0428 19:52:18.968863 27484 net.cpp:200] conv0 needs backward computation.
I0428 19:52:18.968866 27484 net.cpp:202] mnist does not need backward computation.
I0428 19:52:18.968869 27484 net.cpp:244] This network produces output loss
I0428 19:52:18.968880 27484 net.cpp:257] Network initialization done.
I0428 19:52:18.969243 27484 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test658.prototxt
I0428 19:52:18.969270 27484 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:52:18.969367 27484 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:52:18.969455 27484 layer_factory.hpp:77] Creating layer mnist
I0428 19:52:18.969501 27484 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:52:18.969512 27484 net.cpp:86] Creating Layer mnist
I0428 19:52:18.969518 27484 net.cpp:382] mnist -> data
I0428 19:52:18.969527 27484 net.cpp:382] mnist -> label
I0428 19:52:18.969616 27484 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:52:18.971618 27484 net.cpp:124] Setting up mnist
I0428 19:52:18.971632 27484 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:52:18.971648 27484 net.cpp:131] Top shape: 100 (100)
I0428 19:52:18.971652 27484 net.cpp:139] Memory required for data: 314000
I0428 19:52:18.971655 27484 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:52:18.971667 27484 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:52:18.971669 27484 net.cpp:408] label_mnist_1_split <- label
I0428 19:52:18.971675 27484 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:52:18.971681 27484 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:52:18.971770 27484 net.cpp:124] Setting up label_mnist_1_split
I0428 19:52:18.971778 27484 net.cpp:131] Top shape: 100 (100)
I0428 19:52:18.971782 27484 net.cpp:131] Top shape: 100 (100)
I0428 19:52:18.971786 27484 net.cpp:139] Memory required for data: 314800
I0428 19:52:18.971788 27484 layer_factory.hpp:77] Creating layer conv0
I0428 19:52:18.971799 27484 net.cpp:86] Creating Layer conv0
I0428 19:52:18.971803 27484 net.cpp:408] conv0 <- data
I0428 19:52:18.971809 27484 net.cpp:382] conv0 -> conv0
I0428 19:52:18.973363 27484 net.cpp:124] Setting up conv0
I0428 19:52:18.973377 27484 net.cpp:131] Top shape: 100 5 24 24 (288000)
I0428 19:52:18.973381 27484 net.cpp:139] Memory required for data: 1466800
I0428 19:52:18.973392 27484 layer_factory.hpp:77] Creating layer pool0
I0428 19:52:18.973399 27484 net.cpp:86] Creating Layer pool0
I0428 19:52:18.973403 27484 net.cpp:408] pool0 <- conv0
I0428 19:52:18.973408 27484 net.cpp:382] pool0 -> pool0
I0428 19:52:18.973448 27484 net.cpp:124] Setting up pool0
I0428 19:52:18.973453 27484 net.cpp:131] Top shape: 100 5 12 12 (72000)
I0428 19:52:18.973456 27484 net.cpp:139] Memory required for data: 1754800
I0428 19:52:18.973459 27484 layer_factory.hpp:77] Creating layer conv1
I0428 19:52:18.973469 27484 net.cpp:86] Creating Layer conv1
I0428 19:52:18.973472 27484 net.cpp:408] conv1 <- pool0
I0428 19:52:18.973479 27484 net.cpp:382] conv1 -> conv1
I0428 19:52:18.975610 27484 net.cpp:124] Setting up conv1
I0428 19:52:18.975628 27484 net.cpp:131] Top shape: 100 25 8 8 (160000)
I0428 19:52:18.975631 27484 net.cpp:139] Memory required for data: 2394800
I0428 19:52:18.975641 27484 layer_factory.hpp:77] Creating layer pool1
I0428 19:52:18.975649 27484 net.cpp:86] Creating Layer pool1
I0428 19:52:18.975654 27484 net.cpp:408] pool1 <- conv1
I0428 19:52:18.975661 27484 net.cpp:382] pool1 -> pool1
I0428 19:52:18.975703 27484 net.cpp:124] Setting up pool1
I0428 19:52:18.975708 27484 net.cpp:131] Top shape: 100 25 4 4 (40000)
I0428 19:52:18.975710 27484 net.cpp:139] Memory required for data: 2554800
I0428 19:52:18.975713 27484 layer_factory.hpp:77] Creating layer ip1
I0428 19:52:18.975720 27484 net.cpp:86] Creating Layer ip1
I0428 19:52:18.975724 27484 net.cpp:408] ip1 <- pool1
I0428 19:52:18.975729 27484 net.cpp:382] ip1 -> ip1
I0428 19:52:18.975993 27484 net.cpp:124] Setting up ip1
I0428 19:52:18.976003 27484 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:52:18.976017 27484 net.cpp:139] Memory required for data: 2574800
I0428 19:52:18.976027 27484 layer_factory.hpp:77] Creating layer relu1
I0428 19:52:18.976033 27484 net.cpp:86] Creating Layer relu1
I0428 19:52:18.976037 27484 net.cpp:408] relu1 <- ip1
I0428 19:52:18.976042 27484 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:52:18.976233 27484 net.cpp:124] Setting up relu1
I0428 19:52:18.976243 27484 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:52:18.976248 27484 net.cpp:139] Memory required for data: 2594800
I0428 19:52:18.976251 27484 layer_factory.hpp:77] Creating layer ip2
I0428 19:52:18.976261 27484 net.cpp:86] Creating Layer ip2
I0428 19:52:18.976264 27484 net.cpp:408] ip2 <- ip1
I0428 19:52:18.976289 27484 net.cpp:382] ip2 -> ip2
I0428 19:52:18.976405 27484 net.cpp:124] Setting up ip2
I0428 19:52:18.976413 27484 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:52:18.976415 27484 net.cpp:139] Memory required for data: 2604800
I0428 19:52:18.976421 27484 layer_factory.hpp:77] Creating layer relu2
I0428 19:52:18.976426 27484 net.cpp:86] Creating Layer relu2
I0428 19:52:18.976430 27484 net.cpp:408] relu2 <- ip2
I0428 19:52:18.976434 27484 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:52:18.976600 27484 net.cpp:124] Setting up relu2
I0428 19:52:18.976610 27484 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:52:18.976613 27484 net.cpp:139] Memory required for data: 2614800
I0428 19:52:18.976616 27484 layer_factory.hpp:77] Creating layer ip3
I0428 19:52:18.976624 27484 net.cpp:86] Creating Layer ip3
I0428 19:52:18.976626 27484 net.cpp:408] ip3 <- ip2
I0428 19:52:18.976632 27484 net.cpp:382] ip3 -> ip3
I0428 19:52:18.976780 27484 net.cpp:124] Setting up ip3
I0428 19:52:18.976788 27484 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:52:18.976790 27484 net.cpp:139] Memory required for data: 2618800
I0428 19:52:18.976799 27484 layer_factory.hpp:77] Creating layer relu3
I0428 19:52:18.976804 27484 net.cpp:86] Creating Layer relu3
I0428 19:52:18.976806 27484 net.cpp:408] relu3 <- ip3
I0428 19:52:18.976835 27484 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:52:18.977715 27484 net.cpp:124] Setting up relu3
I0428 19:52:18.977742 27484 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:52:18.977746 27484 net.cpp:139] Memory required for data: 2622800
I0428 19:52:18.977749 27484 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:52:18.977756 27484 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:52:18.977761 27484 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:52:18.977785 27484 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:52:18.977793 27484 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:52:18.977834 27484 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:52:18.977840 27484 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:52:18.977844 27484 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:52:18.977847 27484 net.cpp:139] Memory required for data: 2630800
I0428 19:52:18.977850 27484 layer_factory.hpp:77] Creating layer accuracy
I0428 19:52:18.977856 27484 net.cpp:86] Creating Layer accuracy
I0428 19:52:18.977859 27484 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:52:18.977864 27484 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:52:18.977871 27484 net.cpp:382] accuracy -> accuracy
I0428 19:52:18.977879 27484 net.cpp:124] Setting up accuracy
I0428 19:52:18.977882 27484 net.cpp:131] Top shape: (1)
I0428 19:52:18.977886 27484 net.cpp:139] Memory required for data: 2630804
I0428 19:52:18.977890 27484 layer_factory.hpp:77] Creating layer loss
I0428 19:52:18.977895 27484 net.cpp:86] Creating Layer loss
I0428 19:52:18.977897 27484 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:52:18.977901 27484 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:52:18.977905 27484 net.cpp:382] loss -> loss
I0428 19:52:18.977911 27484 layer_factory.hpp:77] Creating layer loss
I0428 19:52:18.978163 27484 net.cpp:124] Setting up loss
I0428 19:52:18.978176 27484 net.cpp:131] Top shape: (1)
I0428 19:52:18.978180 27484 net.cpp:134]     with loss weight 1
I0428 19:52:18.978188 27484 net.cpp:139] Memory required for data: 2630808
I0428 19:52:18.978202 27484 net.cpp:200] loss needs backward computation.
I0428 19:52:18.978212 27484 net.cpp:202] accuracy does not need backward computation.
I0428 19:52:18.978216 27484 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:52:18.978220 27484 net.cpp:200] relu3 needs backward computation.
I0428 19:52:18.978224 27484 net.cpp:200] ip3 needs backward computation.
I0428 19:52:18.978226 27484 net.cpp:200] relu2 needs backward computation.
I0428 19:52:18.978229 27484 net.cpp:200] ip2 needs backward computation.
I0428 19:52:18.978231 27484 net.cpp:200] relu1 needs backward computation.
I0428 19:52:18.978235 27484 net.cpp:200] ip1 needs backward computation.
I0428 19:52:18.978237 27484 net.cpp:200] pool1 needs backward computation.
I0428 19:52:18.978241 27484 net.cpp:200] conv1 needs backward computation.
I0428 19:52:18.978245 27484 net.cpp:200] pool0 needs backward computation.
I0428 19:52:18.978247 27484 net.cpp:200] conv0 needs backward computation.
I0428 19:52:18.978251 27484 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:52:18.978255 27484 net.cpp:202] mnist does not need backward computation.
I0428 19:52:18.978257 27484 net.cpp:244] This network produces output accuracy
I0428 19:52:18.978261 27484 net.cpp:244] This network produces output loss
I0428 19:52:18.978273 27484 net.cpp:257] Network initialization done.
I0428 19:52:18.978317 27484 solver.cpp:56] Solver scaffolding done.
I0428 19:52:18.978690 27484 caffe.cpp:248] Starting Optimization
I0428 19:52:18.978695 27484 solver.cpp:273] Solving LeNet
I0428 19:52:18.978698 27484 solver.cpp:274] Learning Rate Policy: inv
I0428 19:52:18.979029 27484 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:52:18.983332 27484 blocking_queue.cpp:49] Waiting for data
I0428 19:52:19.054208 27491 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:52:19.054718 27484 solver.cpp:398]     Test net output #0: accuracy = 0.0827
I0428 19:52:19.054769 27484 solver.cpp:398]     Test net output #1: loss = 2.31555 (* 1 = 2.31555 loss)
I0428 19:52:19.057251 27484 solver.cpp:219] Iteration 0 (0 iter/s, 0.0785029s/100 iters), loss = 2.3208
I0428 19:52:19.057291 27484 solver.cpp:238]     Train net output #0: loss = 2.3208 (* 1 = 2.3208 loss)
I0428 19:52:19.057301 27484 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:52:19.155231 27484 solver.cpp:219] Iteration 100 (1021 iter/s, 0.0979436s/100 iters), loss = 1.20948
I0428 19:52:19.155272 27484 solver.cpp:238]     Train net output #0: loss = 1.20948 (* 1 = 1.20948 loss)
I0428 19:52:19.155278 27484 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:52:19.250545 27484 solver.cpp:219] Iteration 200 (1049.54 iter/s, 0.0952801s/100 iters), loss = 0.481072
I0428 19:52:19.250583 27484 solver.cpp:238]     Train net output #0: loss = 0.481072 (* 1 = 0.481072 loss)
I0428 19:52:19.250589 27484 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:52:19.341330 27484 solver.cpp:219] Iteration 300 (1101.89 iter/s, 0.090753s/100 iters), loss = 0.504497
I0428 19:52:19.341367 27484 solver.cpp:238]     Train net output #0: loss = 0.504497 (* 1 = 0.504497 loss)
I0428 19:52:19.341372 27484 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:52:19.429760 27484 solver.cpp:219] Iteration 400 (1131.24 iter/s, 0.0883989s/100 iters), loss = 0.25029
I0428 19:52:19.429797 27484 solver.cpp:238]     Train net output #0: loss = 0.25029 (* 1 = 0.25029 loss)
I0428 19:52:19.429818 27484 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:52:19.516906 27484 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:52:19.593662 27491 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:52:19.594192 27484 solver.cpp:398]     Test net output #0: accuracy = 0.862
I0428 19:52:19.594228 27484 solver.cpp:398]     Test net output #1: loss = 0.364258 (* 1 = 0.364258 loss)
I0428 19:52:19.595175 27484 solver.cpp:219] Iteration 500 (604.679 iter/s, 0.165377s/100 iters), loss = 0.365299
I0428 19:52:19.595240 27484 solver.cpp:238]     Train net output #0: loss = 0.365299 (* 1 = 0.365299 loss)
I0428 19:52:19.595260 27484 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:52:19.692109 27484 solver.cpp:219] Iteration 600 (1032.41 iter/s, 0.0968607s/100 iters), loss = 0.308912
I0428 19:52:19.692133 27484 solver.cpp:238]     Train net output #0: loss = 0.308912 (* 1 = 0.308912 loss)
I0428 19:52:19.692139 27484 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:52:19.779729 27484 solver.cpp:219] Iteration 700 (1141.87 iter/s, 0.0875755s/100 iters), loss = 0.486439
I0428 19:52:19.779767 27484 solver.cpp:238]     Train net output #0: loss = 0.486439 (* 1 = 0.486439 loss)
I0428 19:52:19.779773 27484 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:52:19.867543 27484 solver.cpp:219] Iteration 800 (1139.18 iter/s, 0.0877824s/100 iters), loss = 0.453255
I0428 19:52:19.867581 27484 solver.cpp:238]     Train net output #0: loss = 0.453255 (* 1 = 0.453255 loss)
I0428 19:52:19.867588 27484 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:52:19.954581 27484 solver.cpp:219] Iteration 900 (1149.36 iter/s, 0.0870053s/100 iters), loss = 0.422789
I0428 19:52:19.954618 27484 solver.cpp:238]     Train net output #0: loss = 0.422789 (* 1 = 0.422789 loss)
I0428 19:52:19.954623 27484 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:52:19.987414 27490 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:52:20.048460 27484 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:52:20.049556 27484 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:52:20.050181 27484 solver.cpp:311] Iteration 1000, loss = 0.446773
I0428 19:52:20.050196 27484 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:52:20.126814 27491 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:52:20.127749 27484 solver.cpp:398]     Test net output #0: accuracy = 0.879
I0428 19:52:20.127800 27484 solver.cpp:398]     Test net output #1: loss = 0.306025 (* 1 = 0.306025 loss)
I0428 19:52:20.127818 27484 solver.cpp:316] Optimization Done.
I0428 19:52:20.127825 27484 caffe.cpp:259] Optimization Done.
