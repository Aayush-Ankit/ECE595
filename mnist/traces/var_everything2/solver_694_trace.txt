I0428 19:53:42.189463 27828 caffe.cpp:218] Using GPUs 0
I0428 19:53:42.219620 27828 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:53:42.665781 27828 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test694.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:53:42.665921 27828 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test694.prototxt
I0428 19:53:42.666275 27828 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:53:42.666288 27828 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:53:42.666365 27828 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:53:42.666440 27828 layer_factory.hpp:77] Creating layer mnist
I0428 19:53:42.666522 27828 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:53:42.666540 27828 net.cpp:86] Creating Layer mnist
I0428 19:53:42.666546 27828 net.cpp:382] mnist -> data
I0428 19:53:42.666564 27828 net.cpp:382] mnist -> label
I0428 19:53:42.667460 27828 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:53:42.669713 27828 net.cpp:124] Setting up mnist
I0428 19:53:42.669757 27828 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:53:42.669764 27828 net.cpp:131] Top shape: 64 (64)
I0428 19:53:42.669767 27828 net.cpp:139] Memory required for data: 200960
I0428 19:53:42.669772 27828 layer_factory.hpp:77] Creating layer conv0
I0428 19:53:42.669785 27828 net.cpp:86] Creating Layer conv0
I0428 19:53:42.669803 27828 net.cpp:408] conv0 <- data
I0428 19:53:42.669813 27828 net.cpp:382] conv0 -> conv0
I0428 19:53:42.897927 27828 net.cpp:124] Setting up conv0
I0428 19:53:42.897969 27828 net.cpp:131] Top shape: 64 5 24 24 (184320)
I0428 19:53:42.897974 27828 net.cpp:139] Memory required for data: 938240
I0428 19:53:42.897989 27828 layer_factory.hpp:77] Creating layer pool0
I0428 19:53:42.898000 27828 net.cpp:86] Creating Layer pool0
I0428 19:53:42.898018 27828 net.cpp:408] pool0 <- conv0
I0428 19:53:42.898023 27828 net.cpp:382] pool0 -> pool0
I0428 19:53:42.898083 27828 net.cpp:124] Setting up pool0
I0428 19:53:42.898088 27828 net.cpp:131] Top shape: 64 5 12 12 (46080)
I0428 19:53:42.898092 27828 net.cpp:139] Memory required for data: 1122560
I0428 19:53:42.898094 27828 layer_factory.hpp:77] Creating layer conv1
I0428 19:53:42.898104 27828 net.cpp:86] Creating Layer conv1
I0428 19:53:42.898108 27828 net.cpp:408] conv1 <- pool0
I0428 19:53:42.898113 27828 net.cpp:382] conv1 -> conv1
I0428 19:53:42.900894 27828 net.cpp:124] Setting up conv1
I0428 19:53:42.900924 27828 net.cpp:131] Top shape: 64 50 8 8 (204800)
I0428 19:53:42.900928 27828 net.cpp:139] Memory required for data: 1941760
I0428 19:53:42.900936 27828 layer_factory.hpp:77] Creating layer pool1
I0428 19:53:42.900946 27828 net.cpp:86] Creating Layer pool1
I0428 19:53:42.900949 27828 net.cpp:408] pool1 <- conv1
I0428 19:53:42.900954 27828 net.cpp:382] pool1 -> pool1
I0428 19:53:42.901007 27828 net.cpp:124] Setting up pool1
I0428 19:53:42.901012 27828 net.cpp:131] Top shape: 64 50 4 4 (51200)
I0428 19:53:42.901015 27828 net.cpp:139] Memory required for data: 2146560
I0428 19:53:42.901018 27828 layer_factory.hpp:77] Creating layer ip1
I0428 19:53:42.901026 27828 net.cpp:86] Creating Layer ip1
I0428 19:53:42.901029 27828 net.cpp:408] ip1 <- pool1
I0428 19:53:42.901033 27828 net.cpp:382] ip1 -> ip1
I0428 19:53:42.901404 27828 net.cpp:124] Setting up ip1
I0428 19:53:42.901412 27828 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:53:42.901429 27828 net.cpp:139] Memory required for data: 2159360
I0428 19:53:42.901437 27828 layer_factory.hpp:77] Creating layer relu1
I0428 19:53:42.901442 27828 net.cpp:86] Creating Layer relu1
I0428 19:53:42.901444 27828 net.cpp:408] relu1 <- ip1
I0428 19:53:42.901448 27828 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:53:42.901602 27828 net.cpp:124] Setting up relu1
I0428 19:53:42.901610 27828 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:53:42.901613 27828 net.cpp:139] Memory required for data: 2172160
I0428 19:53:42.901617 27828 layer_factory.hpp:77] Creating layer ip2
I0428 19:53:42.901623 27828 net.cpp:86] Creating Layer ip2
I0428 19:53:42.901625 27828 net.cpp:408] ip2 <- ip1
I0428 19:53:42.901629 27828 net.cpp:382] ip2 -> ip2
I0428 19:53:42.901721 27828 net.cpp:124] Setting up ip2
I0428 19:53:42.901728 27828 net.cpp:131] Top shape: 64 10 (640)
I0428 19:53:42.901731 27828 net.cpp:139] Memory required for data: 2174720
I0428 19:53:42.901736 27828 layer_factory.hpp:77] Creating layer relu2
I0428 19:53:42.901742 27828 net.cpp:86] Creating Layer relu2
I0428 19:53:42.901744 27828 net.cpp:408] relu2 <- ip2
I0428 19:53:42.901748 27828 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:53:42.902518 27828 net.cpp:124] Setting up relu2
I0428 19:53:42.902529 27828 net.cpp:131] Top shape: 64 10 (640)
I0428 19:53:42.902547 27828 net.cpp:139] Memory required for data: 2177280
I0428 19:53:42.902550 27828 layer_factory.hpp:77] Creating layer ip3
I0428 19:53:42.902557 27828 net.cpp:86] Creating Layer ip3
I0428 19:53:42.902560 27828 net.cpp:408] ip3 <- ip2
I0428 19:53:42.902565 27828 net.cpp:382] ip3 -> ip3
I0428 19:53:42.902659 27828 net.cpp:124] Setting up ip3
I0428 19:53:42.902667 27828 net.cpp:131] Top shape: 64 10 (640)
I0428 19:53:42.902669 27828 net.cpp:139] Memory required for data: 2179840
I0428 19:53:42.902678 27828 layer_factory.hpp:77] Creating layer relu3
I0428 19:53:42.902683 27828 net.cpp:86] Creating Layer relu3
I0428 19:53:42.902684 27828 net.cpp:408] relu3 <- ip3
I0428 19:53:42.902688 27828 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:53:42.902863 27828 net.cpp:124] Setting up relu3
I0428 19:53:42.902873 27828 net.cpp:131] Top shape: 64 10 (640)
I0428 19:53:42.902876 27828 net.cpp:139] Memory required for data: 2182400
I0428 19:53:42.902879 27828 layer_factory.hpp:77] Creating layer loss
I0428 19:53:42.902885 27828 net.cpp:86] Creating Layer loss
I0428 19:53:42.902889 27828 net.cpp:408] loss <- ip3
I0428 19:53:42.902892 27828 net.cpp:408] loss <- label
I0428 19:53:42.902897 27828 net.cpp:382] loss -> loss
I0428 19:53:42.902915 27828 layer_factory.hpp:77] Creating layer loss
I0428 19:53:42.903148 27828 net.cpp:124] Setting up loss
I0428 19:53:42.903158 27828 net.cpp:131] Top shape: (1)
I0428 19:53:42.903162 27828 net.cpp:134]     with loss weight 1
I0428 19:53:42.903177 27828 net.cpp:139] Memory required for data: 2182404
I0428 19:53:42.903179 27828 net.cpp:200] loss needs backward computation.
I0428 19:53:42.903183 27828 net.cpp:200] relu3 needs backward computation.
I0428 19:53:42.903187 27828 net.cpp:200] ip3 needs backward computation.
I0428 19:53:42.903189 27828 net.cpp:200] relu2 needs backward computation.
I0428 19:53:42.903192 27828 net.cpp:200] ip2 needs backward computation.
I0428 19:53:42.903195 27828 net.cpp:200] relu1 needs backward computation.
I0428 19:53:42.903198 27828 net.cpp:200] ip1 needs backward computation.
I0428 19:53:42.903201 27828 net.cpp:200] pool1 needs backward computation.
I0428 19:53:42.903205 27828 net.cpp:200] conv1 needs backward computation.
I0428 19:53:42.903209 27828 net.cpp:200] pool0 needs backward computation.
I0428 19:53:42.903213 27828 net.cpp:200] conv0 needs backward computation.
I0428 19:53:42.903215 27828 net.cpp:202] mnist does not need backward computation.
I0428 19:53:42.903218 27828 net.cpp:244] This network produces output loss
I0428 19:53:42.903228 27828 net.cpp:257] Network initialization done.
I0428 19:53:42.903612 27828 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test694.prototxt
I0428 19:53:42.903668 27828 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:53:42.903810 27828 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:53:42.903910 27828 layer_factory.hpp:77] Creating layer mnist
I0428 19:53:42.903956 27828 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:53:42.903967 27828 net.cpp:86] Creating Layer mnist
I0428 19:53:42.903972 27828 net.cpp:382] mnist -> data
I0428 19:53:42.903980 27828 net.cpp:382] mnist -> label
I0428 19:53:42.904074 27828 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:53:42.906091 27828 net.cpp:124] Setting up mnist
I0428 19:53:42.906121 27828 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:53:42.906126 27828 net.cpp:131] Top shape: 100 (100)
I0428 19:53:42.906129 27828 net.cpp:139] Memory required for data: 314000
I0428 19:53:42.906133 27828 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:53:42.906168 27828 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:53:42.906172 27828 net.cpp:408] label_mnist_1_split <- label
I0428 19:53:42.906177 27828 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:53:42.906184 27828 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:53:42.906227 27828 net.cpp:124] Setting up label_mnist_1_split
I0428 19:53:42.906234 27828 net.cpp:131] Top shape: 100 (100)
I0428 19:53:42.906237 27828 net.cpp:131] Top shape: 100 (100)
I0428 19:53:42.906240 27828 net.cpp:139] Memory required for data: 314800
I0428 19:53:42.906244 27828 layer_factory.hpp:77] Creating layer conv0
I0428 19:53:42.906270 27828 net.cpp:86] Creating Layer conv0
I0428 19:53:42.906275 27828 net.cpp:408] conv0 <- data
I0428 19:53:42.906282 27828 net.cpp:382] conv0 -> conv0
I0428 19:53:42.908243 27828 net.cpp:124] Setting up conv0
I0428 19:53:42.908268 27828 net.cpp:131] Top shape: 100 5 24 24 (288000)
I0428 19:53:42.908272 27828 net.cpp:139] Memory required for data: 1466800
I0428 19:53:42.908282 27828 layer_factory.hpp:77] Creating layer pool0
I0428 19:53:42.908288 27828 net.cpp:86] Creating Layer pool0
I0428 19:53:42.908291 27828 net.cpp:408] pool0 <- conv0
I0428 19:53:42.908295 27828 net.cpp:382] pool0 -> pool0
I0428 19:53:42.908330 27828 net.cpp:124] Setting up pool0
I0428 19:53:42.908335 27828 net.cpp:131] Top shape: 100 5 12 12 (72000)
I0428 19:53:42.908339 27828 net.cpp:139] Memory required for data: 1754800
I0428 19:53:42.908341 27828 layer_factory.hpp:77] Creating layer conv1
I0428 19:53:42.908349 27828 net.cpp:86] Creating Layer conv1
I0428 19:53:42.908351 27828 net.cpp:408] conv1 <- pool0
I0428 19:53:42.908356 27828 net.cpp:382] conv1 -> conv1
I0428 19:53:42.910030 27828 net.cpp:124] Setting up conv1
I0428 19:53:42.910044 27828 net.cpp:131] Top shape: 100 50 8 8 (320000)
I0428 19:53:42.910048 27828 net.cpp:139] Memory required for data: 3034800
I0428 19:53:42.910056 27828 layer_factory.hpp:77] Creating layer pool1
I0428 19:53:42.910063 27828 net.cpp:86] Creating Layer pool1
I0428 19:53:42.910068 27828 net.cpp:408] pool1 <- conv1
I0428 19:53:42.910073 27828 net.cpp:382] pool1 -> pool1
I0428 19:53:42.910109 27828 net.cpp:124] Setting up pool1
I0428 19:53:42.910114 27828 net.cpp:131] Top shape: 100 50 4 4 (80000)
I0428 19:53:42.910116 27828 net.cpp:139] Memory required for data: 3354800
I0428 19:53:42.910120 27828 layer_factory.hpp:77] Creating layer ip1
I0428 19:53:42.910125 27828 net.cpp:86] Creating Layer ip1
I0428 19:53:42.910128 27828 net.cpp:408] ip1 <- pool1
I0428 19:53:42.910133 27828 net.cpp:382] ip1 -> ip1
I0428 19:53:42.910467 27828 net.cpp:124] Setting up ip1
I0428 19:53:42.910475 27828 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:53:42.910487 27828 net.cpp:139] Memory required for data: 3374800
I0428 19:53:42.910495 27828 layer_factory.hpp:77] Creating layer relu1
I0428 19:53:42.910501 27828 net.cpp:86] Creating Layer relu1
I0428 19:53:42.910503 27828 net.cpp:408] relu1 <- ip1
I0428 19:53:42.910524 27828 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:53:42.910687 27828 net.cpp:124] Setting up relu1
I0428 19:53:42.910696 27828 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:53:42.910708 27828 net.cpp:139] Memory required for data: 3394800
I0428 19:53:42.910712 27828 layer_factory.hpp:77] Creating layer ip2
I0428 19:53:42.910719 27828 net.cpp:86] Creating Layer ip2
I0428 19:53:42.910722 27828 net.cpp:408] ip2 <- ip1
I0428 19:53:42.910728 27828 net.cpp:382] ip2 -> ip2
I0428 19:53:42.910850 27828 net.cpp:124] Setting up ip2
I0428 19:53:42.910858 27828 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:53:42.910861 27828 net.cpp:139] Memory required for data: 3398800
I0428 19:53:42.910866 27828 layer_factory.hpp:77] Creating layer relu2
I0428 19:53:42.910878 27828 net.cpp:86] Creating Layer relu2
I0428 19:53:42.910881 27828 net.cpp:408] relu2 <- ip2
I0428 19:53:42.910886 27828 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:53:42.911144 27828 net.cpp:124] Setting up relu2
I0428 19:53:42.911159 27828 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:53:42.911162 27828 net.cpp:139] Memory required for data: 3402800
I0428 19:53:42.911165 27828 layer_factory.hpp:77] Creating layer ip3
I0428 19:53:42.911170 27828 net.cpp:86] Creating Layer ip3
I0428 19:53:42.911175 27828 net.cpp:408] ip3 <- ip2
I0428 19:53:42.911180 27828 net.cpp:382] ip3 -> ip3
I0428 19:53:42.911335 27828 net.cpp:124] Setting up ip3
I0428 19:53:42.911342 27828 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:53:42.911345 27828 net.cpp:139] Memory required for data: 3406800
I0428 19:53:42.911352 27828 layer_factory.hpp:77] Creating layer relu3
I0428 19:53:42.911358 27828 net.cpp:86] Creating Layer relu3
I0428 19:53:42.911361 27828 net.cpp:408] relu3 <- ip3
I0428 19:53:42.911365 27828 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:53:42.912112 27828 net.cpp:124] Setting up relu3
I0428 19:53:42.912125 27828 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:53:42.912129 27828 net.cpp:139] Memory required for data: 3410800
I0428 19:53:42.912132 27828 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:53:42.912138 27828 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:53:42.912142 27828 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:53:42.912147 27828 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:53:42.912163 27828 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:53:42.912222 27828 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:53:42.912230 27828 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:53:42.912240 27828 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:53:42.912242 27828 net.cpp:139] Memory required for data: 3418800
I0428 19:53:42.912259 27828 layer_factory.hpp:77] Creating layer accuracy
I0428 19:53:42.912266 27828 net.cpp:86] Creating Layer accuracy
I0428 19:53:42.912268 27828 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:53:42.912272 27828 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:53:42.912292 27828 net.cpp:382] accuracy -> accuracy
I0428 19:53:42.912298 27828 net.cpp:124] Setting up accuracy
I0428 19:53:42.912302 27828 net.cpp:131] Top shape: (1)
I0428 19:53:42.912305 27828 net.cpp:139] Memory required for data: 3418804
I0428 19:53:42.912307 27828 layer_factory.hpp:77] Creating layer loss
I0428 19:53:42.912314 27828 net.cpp:86] Creating Layer loss
I0428 19:53:42.912317 27828 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:53:42.912322 27828 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:53:42.912324 27828 net.cpp:382] loss -> loss
I0428 19:53:42.912346 27828 layer_factory.hpp:77] Creating layer loss
I0428 19:53:42.912721 27828 net.cpp:124] Setting up loss
I0428 19:53:42.912731 27828 net.cpp:131] Top shape: (1)
I0428 19:53:42.912734 27828 net.cpp:134]     with loss weight 1
I0428 19:53:42.912740 27828 net.cpp:139] Memory required for data: 3418808
I0428 19:53:42.912752 27828 net.cpp:200] loss needs backward computation.
I0428 19:53:42.912763 27828 net.cpp:202] accuracy does not need backward computation.
I0428 19:53:42.912766 27828 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:53:42.912770 27828 net.cpp:200] relu3 needs backward computation.
I0428 19:53:42.912772 27828 net.cpp:200] ip3 needs backward computation.
I0428 19:53:42.912786 27828 net.cpp:200] relu2 needs backward computation.
I0428 19:53:42.912788 27828 net.cpp:200] ip2 needs backward computation.
I0428 19:53:42.912791 27828 net.cpp:200] relu1 needs backward computation.
I0428 19:53:42.912794 27828 net.cpp:200] ip1 needs backward computation.
I0428 19:53:42.912797 27828 net.cpp:200] pool1 needs backward computation.
I0428 19:53:42.912799 27828 net.cpp:200] conv1 needs backward computation.
I0428 19:53:42.912802 27828 net.cpp:200] pool0 needs backward computation.
I0428 19:53:42.912806 27828 net.cpp:200] conv0 needs backward computation.
I0428 19:53:42.912813 27828 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:53:42.912858 27828 net.cpp:202] mnist does not need backward computation.
I0428 19:53:42.912860 27828 net.cpp:244] This network produces output accuracy
I0428 19:53:42.912863 27828 net.cpp:244] This network produces output loss
I0428 19:53:42.912874 27828 net.cpp:257] Network initialization done.
I0428 19:53:42.912916 27828 solver.cpp:56] Solver scaffolding done.
I0428 19:53:42.913298 27828 caffe.cpp:248] Starting Optimization
I0428 19:53:42.913305 27828 solver.cpp:273] Solving LeNet
I0428 19:53:42.913308 27828 solver.cpp:274] Learning Rate Policy: inv
I0428 19:53:42.914147 27828 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:53:42.918488 27828 blocking_queue.cpp:49] Waiting for data
I0428 19:53:42.989833 27835 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:53:42.990340 27828 solver.cpp:398]     Test net output #0: accuracy = 0.1009
I0428 19:53:42.990360 27828 solver.cpp:398]     Test net output #1: loss = 2.36189 (* 1 = 2.36189 loss)
I0428 19:53:42.993574 27828 solver.cpp:219] Iteration 0 (0 iter/s, 0.0802423s/100 iters), loss = 2.33696
I0428 19:53:42.993597 27828 solver.cpp:238]     Train net output #0: loss = 2.33696 (* 1 = 2.33696 loss)
I0428 19:53:42.993624 27828 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:53:43.099891 27828 solver.cpp:219] Iteration 100 (940.975 iter/s, 0.106273s/100 iters), loss = 0.927753
I0428 19:53:43.099918 27828 solver.cpp:238]     Train net output #0: loss = 0.927753 (* 1 = 0.927753 loss)
I0428 19:53:43.099925 27828 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:53:43.208585 27828 solver.cpp:219] Iteration 200 (920.339 iter/s, 0.108656s/100 iters), loss = 0.535753
I0428 19:53:43.208609 27828 solver.cpp:238]     Train net output #0: loss = 0.535753 (* 1 = 0.535753 loss)
I0428 19:53:43.208616 27828 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:53:43.317226 27828 solver.cpp:219] Iteration 300 (920.756 iter/s, 0.108606s/100 iters), loss = 0.253535
I0428 19:53:43.317265 27828 solver.cpp:238]     Train net output #0: loss = 0.253535 (* 1 = 0.253535 loss)
I0428 19:53:43.317271 27828 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:53:43.423658 27828 solver.cpp:219] Iteration 400 (939.87 iter/s, 0.106398s/100 iters), loss = 0.0864347
I0428 19:53:43.423698 27828 solver.cpp:238]     Train net output #0: loss = 0.0864348 (* 1 = 0.0864348 loss)
I0428 19:53:43.423704 27828 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:53:43.530712 27828 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:53:43.608067 27835 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:53:43.608582 27828 solver.cpp:398]     Test net output #0: accuracy = 0.9548
I0428 19:53:43.608603 27828 solver.cpp:398]     Test net output #1: loss = 0.143987 (* 1 = 0.143987 loss)
I0428 19:53:43.609735 27828 solver.cpp:219] Iteration 500 (537.568 iter/s, 0.186023s/100 iters), loss = 0.145251
I0428 19:53:43.609772 27828 solver.cpp:238]     Train net output #0: loss = 0.145251 (* 1 = 0.145251 loss)
I0428 19:53:43.609810 27828 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:53:43.720322 27828 solver.cpp:219] Iteration 600 (904.648 iter/s, 0.11054s/100 iters), loss = 0.115049
I0428 19:53:43.720346 27828 solver.cpp:238]     Train net output #0: loss = 0.115049 (* 1 = 0.115049 loss)
I0428 19:53:43.720368 27828 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:53:43.827466 27828 solver.cpp:219] Iteration 700 (933.634 iter/s, 0.107108s/100 iters), loss = 0.162773
I0428 19:53:43.827489 27828 solver.cpp:238]     Train net output #0: loss = 0.162773 (* 1 = 0.162773 loss)
I0428 19:53:43.827512 27828 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:53:43.936148 27828 solver.cpp:219] Iteration 800 (920.4 iter/s, 0.108648s/100 iters), loss = 0.296765
I0428 19:53:43.936190 27828 solver.cpp:238]     Train net output #0: loss = 0.296765 (* 1 = 0.296765 loss)
I0428 19:53:43.936197 27828 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:53:44.040724 27828 solver.cpp:219] Iteration 900 (956.738 iter/s, 0.104522s/100 iters), loss = 0.216252
I0428 19:53:44.040763 27828 solver.cpp:238]     Train net output #0: loss = 0.216252 (* 1 = 0.216252 loss)
I0428 19:53:44.040771 27828 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:53:44.076407 27834 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:53:44.146945 27828 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:53:44.148555 27828 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:53:44.149417 27828 solver.cpp:311] Iteration 1000, loss = 0.115529
I0428 19:53:44.149448 27828 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:53:44.220576 27835 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:53:44.221163 27828 solver.cpp:398]     Test net output #0: accuracy = 0.9716
I0428 19:53:44.221194 27828 solver.cpp:398]     Test net output #1: loss = 0.0884172 (* 1 = 0.0884172 loss)
I0428 19:53:44.221201 27828 solver.cpp:316] Optimization Done.
I0428 19:53:44.221206 27828 caffe.cpp:259] Optimization Done.
