I0428 20:12:35.674486 32279 caffe.cpp:218] Using GPUs 0
I0428 20:12:35.710999 32279 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:12:36.219377 32279 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1183.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:12:36.219542 32279 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1183.prototxt
I0428 20:12:36.219959 32279 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:12:36.219980 32279 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:12:36.220084 32279 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:12:36.220161 32279 layer_factory.hpp:77] Creating layer mnist
I0428 20:12:36.220257 32279 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:12:36.220279 32279 net.cpp:86] Creating Layer mnist
I0428 20:12:36.220288 32279 net.cpp:382] mnist -> data
I0428 20:12:36.220309 32279 net.cpp:382] mnist -> label
I0428 20:12:36.221431 32279 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:12:36.223912 32279 net.cpp:124] Setting up mnist
I0428 20:12:36.223932 32279 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:12:36.223937 32279 net.cpp:131] Top shape: 64 (64)
I0428 20:12:36.223940 32279 net.cpp:139] Memory required for data: 200960
I0428 20:12:36.223948 32279 layer_factory.hpp:77] Creating layer conv0
I0428 20:12:36.223964 32279 net.cpp:86] Creating Layer conv0
I0428 20:12:36.223987 32279 net.cpp:408] conv0 <- data
I0428 20:12:36.224001 32279 net.cpp:382] conv0 -> conv0
I0428 20:12:36.515564 32279 net.cpp:124] Setting up conv0
I0428 20:12:36.515595 32279 net.cpp:131] Top shape: 64 25 24 24 (921600)
I0428 20:12:36.515600 32279 net.cpp:139] Memory required for data: 3887360
I0428 20:12:36.515617 32279 layer_factory.hpp:77] Creating layer pool0
I0428 20:12:36.515631 32279 net.cpp:86] Creating Layer pool0
I0428 20:12:36.515636 32279 net.cpp:408] pool0 <- conv0
I0428 20:12:36.515642 32279 net.cpp:382] pool0 -> pool0
I0428 20:12:36.515692 32279 net.cpp:124] Setting up pool0
I0428 20:12:36.515702 32279 net.cpp:131] Top shape: 64 25 12 12 (230400)
I0428 20:12:36.515705 32279 net.cpp:139] Memory required for data: 4808960
I0428 20:12:36.515708 32279 layer_factory.hpp:77] Creating layer conv1
I0428 20:12:36.515722 32279 net.cpp:86] Creating Layer conv1
I0428 20:12:36.515727 32279 net.cpp:408] conv1 <- pool0
I0428 20:12:36.515733 32279 net.cpp:382] conv1 -> conv1
I0428 20:12:36.518232 32279 net.cpp:124] Setting up conv1
I0428 20:12:36.518249 32279 net.cpp:131] Top shape: 64 100 8 8 (409600)
I0428 20:12:36.518254 32279 net.cpp:139] Memory required for data: 6447360
I0428 20:12:36.518265 32279 layer_factory.hpp:77] Creating layer pool1
I0428 20:12:36.518272 32279 net.cpp:86] Creating Layer pool1
I0428 20:12:36.518276 32279 net.cpp:408] pool1 <- conv1
I0428 20:12:36.518282 32279 net.cpp:382] pool1 -> pool1
I0428 20:12:36.518323 32279 net.cpp:124] Setting up pool1
I0428 20:12:36.518332 32279 net.cpp:131] Top shape: 64 100 4 4 (102400)
I0428 20:12:36.518335 32279 net.cpp:139] Memory required for data: 6856960
I0428 20:12:36.518338 32279 layer_factory.hpp:77] Creating layer ip1
I0428 20:12:36.518348 32279 net.cpp:86] Creating Layer ip1
I0428 20:12:36.518354 32279 net.cpp:408] ip1 <- pool1
I0428 20:12:36.518360 32279 net.cpp:382] ip1 -> ip1
I0428 20:12:36.519546 32279 net.cpp:124] Setting up ip1
I0428 20:12:36.519562 32279 net.cpp:131] Top shape: 64 10 (640)
I0428 20:12:36.519565 32279 net.cpp:139] Memory required for data: 6859520
I0428 20:12:36.519577 32279 layer_factory.hpp:77] Creating layer relu1
I0428 20:12:36.519584 32279 net.cpp:86] Creating Layer relu1
I0428 20:12:36.519588 32279 net.cpp:408] relu1 <- ip1
I0428 20:12:36.519593 32279 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:12:36.519784 32279 net.cpp:124] Setting up relu1
I0428 20:12:36.519794 32279 net.cpp:131] Top shape: 64 10 (640)
I0428 20:12:36.519798 32279 net.cpp:139] Memory required for data: 6862080
I0428 20:12:36.519803 32279 layer_factory.hpp:77] Creating layer ip2
I0428 20:12:36.519809 32279 net.cpp:86] Creating Layer ip2
I0428 20:12:36.519816 32279 net.cpp:408] ip2 <- ip1
I0428 20:12:36.519824 32279 net.cpp:382] ip2 -> ip2
I0428 20:12:36.519932 32279 net.cpp:124] Setting up ip2
I0428 20:12:36.519939 32279 net.cpp:131] Top shape: 64 10 (640)
I0428 20:12:36.519942 32279 net.cpp:139] Memory required for data: 6864640
I0428 20:12:36.519948 32279 layer_factory.hpp:77] Creating layer relu2
I0428 20:12:36.519958 32279 net.cpp:86] Creating Layer relu2
I0428 20:12:36.519963 32279 net.cpp:408] relu2 <- ip2
I0428 20:12:36.519968 32279 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:12:36.520783 32279 net.cpp:124] Setting up relu2
I0428 20:12:36.520797 32279 net.cpp:131] Top shape: 64 10 (640)
I0428 20:12:36.520802 32279 net.cpp:139] Memory required for data: 6867200
I0428 20:12:36.520805 32279 layer_factory.hpp:77] Creating layer ip3
I0428 20:12:36.520822 32279 net.cpp:86] Creating Layer ip3
I0428 20:12:36.520836 32279 net.cpp:408] ip3 <- ip2
I0428 20:12:36.520843 32279 net.cpp:382] ip3 -> ip3
I0428 20:12:36.520970 32279 net.cpp:124] Setting up ip3
I0428 20:12:36.520983 32279 net.cpp:131] Top shape: 64 10 (640)
I0428 20:12:36.520987 32279 net.cpp:139] Memory required for data: 6869760
I0428 20:12:36.520997 32279 layer_factory.hpp:77] Creating layer relu3
I0428 20:12:36.521009 32279 net.cpp:86] Creating Layer relu3
I0428 20:12:36.521013 32279 net.cpp:408] relu3 <- ip3
I0428 20:12:36.521018 32279 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:12:36.521210 32279 net.cpp:124] Setting up relu3
I0428 20:12:36.521219 32279 net.cpp:131] Top shape: 64 10 (640)
I0428 20:12:36.521224 32279 net.cpp:139] Memory required for data: 6872320
I0428 20:12:36.521227 32279 layer_factory.hpp:77] Creating layer loss
I0428 20:12:36.521234 32279 net.cpp:86] Creating Layer loss
I0428 20:12:36.521239 32279 net.cpp:408] loss <- ip3
I0428 20:12:36.521244 32279 net.cpp:408] loss <- label
I0428 20:12:36.521250 32279 net.cpp:382] loss -> loss
I0428 20:12:36.521267 32279 layer_factory.hpp:77] Creating layer loss
I0428 20:12:36.521522 32279 net.cpp:124] Setting up loss
I0428 20:12:36.521533 32279 net.cpp:131] Top shape: (1)
I0428 20:12:36.521535 32279 net.cpp:134]     with loss weight 1
I0428 20:12:36.521550 32279 net.cpp:139] Memory required for data: 6872324
I0428 20:12:36.521554 32279 net.cpp:200] loss needs backward computation.
I0428 20:12:36.521559 32279 net.cpp:200] relu3 needs backward computation.
I0428 20:12:36.521562 32279 net.cpp:200] ip3 needs backward computation.
I0428 20:12:36.521565 32279 net.cpp:200] relu2 needs backward computation.
I0428 20:12:36.521569 32279 net.cpp:200] ip2 needs backward computation.
I0428 20:12:36.521571 32279 net.cpp:200] relu1 needs backward computation.
I0428 20:12:36.521574 32279 net.cpp:200] ip1 needs backward computation.
I0428 20:12:36.521579 32279 net.cpp:200] pool1 needs backward computation.
I0428 20:12:36.521581 32279 net.cpp:200] conv1 needs backward computation.
I0428 20:12:36.521585 32279 net.cpp:200] pool0 needs backward computation.
I0428 20:12:36.521589 32279 net.cpp:200] conv0 needs backward computation.
I0428 20:12:36.521592 32279 net.cpp:202] mnist does not need backward computation.
I0428 20:12:36.521595 32279 net.cpp:244] This network produces output loss
I0428 20:12:36.521605 32279 net.cpp:257] Network initialization done.
I0428 20:12:36.521977 32279 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1183.prototxt
I0428 20:12:36.522007 32279 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:12:36.522110 32279 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:12:36.522199 32279 layer_factory.hpp:77] Creating layer mnist
I0428 20:12:36.522248 32279 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:12:36.522263 32279 net.cpp:86] Creating Layer mnist
I0428 20:12:36.522269 32279 net.cpp:382] mnist -> data
I0428 20:12:36.522276 32279 net.cpp:382] mnist -> label
I0428 20:12:36.522372 32279 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:12:36.524708 32279 net.cpp:124] Setting up mnist
I0428 20:12:36.524722 32279 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:12:36.524729 32279 net.cpp:131] Top shape: 100 (100)
I0428 20:12:36.524746 32279 net.cpp:139] Memory required for data: 314000
I0428 20:12:36.524750 32279 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:12:36.524758 32279 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:12:36.524761 32279 net.cpp:408] label_mnist_1_split <- label
I0428 20:12:36.524767 32279 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:12:36.524775 32279 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:12:36.524901 32279 net.cpp:124] Setting up label_mnist_1_split
I0428 20:12:36.524912 32279 net.cpp:131] Top shape: 100 (100)
I0428 20:12:36.524916 32279 net.cpp:131] Top shape: 100 (100)
I0428 20:12:36.524920 32279 net.cpp:139] Memory required for data: 314800
I0428 20:12:36.524924 32279 layer_factory.hpp:77] Creating layer conv0
I0428 20:12:36.524933 32279 net.cpp:86] Creating Layer conv0
I0428 20:12:36.524937 32279 net.cpp:408] conv0 <- data
I0428 20:12:36.524943 32279 net.cpp:382] conv0 -> conv0
I0428 20:12:36.526579 32279 net.cpp:124] Setting up conv0
I0428 20:12:36.526597 32279 net.cpp:131] Top shape: 100 25 24 24 (1440000)
I0428 20:12:36.526602 32279 net.cpp:139] Memory required for data: 6074800
I0428 20:12:36.526612 32279 layer_factory.hpp:77] Creating layer pool0
I0428 20:12:36.526619 32279 net.cpp:86] Creating Layer pool0
I0428 20:12:36.526624 32279 net.cpp:408] pool0 <- conv0
I0428 20:12:36.526630 32279 net.cpp:382] pool0 -> pool0
I0428 20:12:36.526671 32279 net.cpp:124] Setting up pool0
I0428 20:12:36.526679 32279 net.cpp:131] Top shape: 100 25 12 12 (360000)
I0428 20:12:36.526682 32279 net.cpp:139] Memory required for data: 7514800
I0428 20:12:36.526686 32279 layer_factory.hpp:77] Creating layer conv1
I0428 20:12:36.526696 32279 net.cpp:86] Creating Layer conv1
I0428 20:12:36.526700 32279 net.cpp:408] conv1 <- pool0
I0428 20:12:36.526715 32279 net.cpp:382] conv1 -> conv1
I0428 20:12:36.528785 32279 net.cpp:124] Setting up conv1
I0428 20:12:36.528802 32279 net.cpp:131] Top shape: 100 100 8 8 (640000)
I0428 20:12:36.528807 32279 net.cpp:139] Memory required for data: 10074800
I0428 20:12:36.528836 32279 layer_factory.hpp:77] Creating layer pool1
I0428 20:12:36.528856 32279 net.cpp:86] Creating Layer pool1
I0428 20:12:36.528861 32279 net.cpp:408] pool1 <- conv1
I0428 20:12:36.528867 32279 net.cpp:382] pool1 -> pool1
I0428 20:12:36.528944 32279 net.cpp:124] Setting up pool1
I0428 20:12:36.528951 32279 net.cpp:131] Top shape: 100 100 4 4 (160000)
I0428 20:12:36.528955 32279 net.cpp:139] Memory required for data: 10714800
I0428 20:12:36.528959 32279 layer_factory.hpp:77] Creating layer ip1
I0428 20:12:36.528965 32279 net.cpp:86] Creating Layer ip1
I0428 20:12:36.528969 32279 net.cpp:408] ip1 <- pool1
I0428 20:12:36.528975 32279 net.cpp:382] ip1 -> ip1
I0428 20:12:36.529199 32279 net.cpp:124] Setting up ip1
I0428 20:12:36.529209 32279 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:12:36.529234 32279 net.cpp:139] Memory required for data: 10718800
I0428 20:12:36.529242 32279 layer_factory.hpp:77] Creating layer relu1
I0428 20:12:36.529248 32279 net.cpp:86] Creating Layer relu1
I0428 20:12:36.529253 32279 net.cpp:408] relu1 <- ip1
I0428 20:12:36.529263 32279 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:12:36.529459 32279 net.cpp:124] Setting up relu1
I0428 20:12:36.529469 32279 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:12:36.529472 32279 net.cpp:139] Memory required for data: 10722800
I0428 20:12:36.529476 32279 layer_factory.hpp:77] Creating layer ip2
I0428 20:12:36.529484 32279 net.cpp:86] Creating Layer ip2
I0428 20:12:36.529489 32279 net.cpp:408] ip2 <- ip1
I0428 20:12:36.529494 32279 net.cpp:382] ip2 -> ip2
I0428 20:12:36.529602 32279 net.cpp:124] Setting up ip2
I0428 20:12:36.529609 32279 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:12:36.529613 32279 net.cpp:139] Memory required for data: 10726800
I0428 20:12:36.529618 32279 layer_factory.hpp:77] Creating layer relu2
I0428 20:12:36.529631 32279 net.cpp:86] Creating Layer relu2
I0428 20:12:36.529634 32279 net.cpp:408] relu2 <- ip2
I0428 20:12:36.529640 32279 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:12:36.529839 32279 net.cpp:124] Setting up relu2
I0428 20:12:36.529850 32279 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:12:36.529855 32279 net.cpp:139] Memory required for data: 10730800
I0428 20:12:36.529858 32279 layer_factory.hpp:77] Creating layer ip3
I0428 20:12:36.529865 32279 net.cpp:86] Creating Layer ip3
I0428 20:12:36.529867 32279 net.cpp:408] ip3 <- ip2
I0428 20:12:36.529872 32279 net.cpp:382] ip3 -> ip3
I0428 20:12:36.529980 32279 net.cpp:124] Setting up ip3
I0428 20:12:36.529989 32279 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:12:36.529992 32279 net.cpp:139] Memory required for data: 10734800
I0428 20:12:36.530001 32279 layer_factory.hpp:77] Creating layer relu3
I0428 20:12:36.530006 32279 net.cpp:86] Creating Layer relu3
I0428 20:12:36.530009 32279 net.cpp:408] relu3 <- ip3
I0428 20:12:36.530014 32279 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:12:36.530851 32279 net.cpp:124] Setting up relu3
I0428 20:12:36.530866 32279 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:12:36.530870 32279 net.cpp:139] Memory required for data: 10738800
I0428 20:12:36.530874 32279 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:12:36.530880 32279 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:12:36.530884 32279 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:12:36.530890 32279 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:12:36.530897 32279 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:12:36.530938 32279 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:12:36.530946 32279 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:12:36.530951 32279 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:12:36.530953 32279 net.cpp:139] Memory required for data: 10746800
I0428 20:12:36.530957 32279 layer_factory.hpp:77] Creating layer accuracy
I0428 20:12:36.530962 32279 net.cpp:86] Creating Layer accuracy
I0428 20:12:36.530966 32279 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:12:36.530971 32279 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:12:36.530975 32279 net.cpp:382] accuracy -> accuracy
I0428 20:12:36.530983 32279 net.cpp:124] Setting up accuracy
I0428 20:12:36.530987 32279 net.cpp:131] Top shape: (1)
I0428 20:12:36.530990 32279 net.cpp:139] Memory required for data: 10746804
I0428 20:12:36.530993 32279 layer_factory.hpp:77] Creating layer loss
I0428 20:12:36.530998 32279 net.cpp:86] Creating Layer loss
I0428 20:12:36.531002 32279 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:12:36.531007 32279 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:12:36.531010 32279 net.cpp:382] loss -> loss
I0428 20:12:36.531018 32279 layer_factory.hpp:77] Creating layer loss
I0428 20:12:36.531271 32279 net.cpp:124] Setting up loss
I0428 20:12:36.531285 32279 net.cpp:131] Top shape: (1)
I0428 20:12:36.531288 32279 net.cpp:134]     with loss weight 1
I0428 20:12:36.531306 32279 net.cpp:139] Memory required for data: 10746808
I0428 20:12:36.531311 32279 net.cpp:200] loss needs backward computation.
I0428 20:12:36.531316 32279 net.cpp:202] accuracy does not need backward computation.
I0428 20:12:36.531319 32279 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:12:36.531322 32279 net.cpp:200] relu3 needs backward computation.
I0428 20:12:36.531327 32279 net.cpp:200] ip3 needs backward computation.
I0428 20:12:36.531330 32279 net.cpp:200] relu2 needs backward computation.
I0428 20:12:36.531333 32279 net.cpp:200] ip2 needs backward computation.
I0428 20:12:36.531337 32279 net.cpp:200] relu1 needs backward computation.
I0428 20:12:36.531339 32279 net.cpp:200] ip1 needs backward computation.
I0428 20:12:36.531343 32279 net.cpp:200] pool1 needs backward computation.
I0428 20:12:36.531347 32279 net.cpp:200] conv1 needs backward computation.
I0428 20:12:36.531350 32279 net.cpp:200] pool0 needs backward computation.
I0428 20:12:36.531354 32279 net.cpp:200] conv0 needs backward computation.
I0428 20:12:36.531358 32279 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:12:36.531363 32279 net.cpp:202] mnist does not need backward computation.
I0428 20:12:36.531365 32279 net.cpp:244] This network produces output accuracy
I0428 20:12:36.531369 32279 net.cpp:244] This network produces output loss
I0428 20:12:36.531381 32279 net.cpp:257] Network initialization done.
I0428 20:12:36.531427 32279 solver.cpp:56] Solver scaffolding done.
I0428 20:12:36.531796 32279 caffe.cpp:248] Starting Optimization
I0428 20:12:36.531805 32279 solver.cpp:273] Solving LeNet
I0428 20:12:36.531808 32279 solver.cpp:274] Learning Rate Policy: inv
I0428 20:12:36.532632 32279 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:12:36.604290 32286 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:12:36.606199 32279 solver.cpp:398]     Test net output #0: accuracy = 0.0877
I0428 20:12:36.606235 32279 solver.cpp:398]     Test net output #1: loss = 2.32488 (* 1 = 2.32488 loss)
I0428 20:12:36.610833 32279 solver.cpp:219] Iteration 0 (0 iter/s, 0.0790006s/100 iters), loss = 2.30695
I0428 20:12:36.610857 32279 solver.cpp:238]     Train net output #0: loss = 2.30695 (* 1 = 2.30695 loss)
I0428 20:12:36.610885 32279 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:12:36.769335 32279 solver.cpp:219] Iteration 100 (631.054 iter/s, 0.158465s/100 iters), loss = 0.877644
I0428 20:12:36.769361 32279 solver.cpp:238]     Train net output #0: loss = 0.877644 (* 1 = 0.877644 loss)
I0428 20:12:36.769367 32279 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:12:36.930780 32279 solver.cpp:219] Iteration 200 (619.552 iter/s, 0.161407s/100 iters), loss = 0.453942
I0428 20:12:36.930805 32279 solver.cpp:238]     Train net output #0: loss = 0.453942 (* 1 = 0.453942 loss)
I0428 20:12:36.930829 32279 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:12:37.095155 32279 solver.cpp:219] Iteration 300 (608.507 iter/s, 0.164337s/100 iters), loss = 0.420819
I0428 20:12:37.095181 32279 solver.cpp:238]     Train net output #0: loss = 0.420819 (* 1 = 0.420819 loss)
I0428 20:12:37.095188 32279 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:12:37.265049 32279 solver.cpp:219] Iteration 400 (588.741 iter/s, 0.169854s/100 iters), loss = 0.125574
I0428 20:12:37.265077 32279 solver.cpp:238]     Train net output #0: loss = 0.125574 (* 1 = 0.125574 loss)
I0428 20:12:37.265084 32279 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:12:37.422615 32279 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:12:37.489892 32286 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:12:37.492265 32279 solver.cpp:398]     Test net output #0: accuracy = 0.9419
I0428 20:12:37.492286 32279 solver.cpp:398]     Test net output #1: loss = 0.188519 (* 1 = 0.188519 loss)
I0428 20:12:37.494143 32279 solver.cpp:219] Iteration 500 (436.587 iter/s, 0.229049s/100 iters), loss = 0.242942
I0428 20:12:37.494181 32279 solver.cpp:238]     Train net output #0: loss = 0.242942 (* 1 = 0.242942 loss)
I0428 20:12:37.494233 32279 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:12:37.654904 32279 solver.cpp:219] Iteration 600 (622.181 iter/s, 0.160725s/100 iters), loss = 0.142749
I0428 20:12:37.654929 32279 solver.cpp:238]     Train net output #0: loss = 0.142749 (* 1 = 0.142749 loss)
I0428 20:12:37.654935 32279 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:12:37.817996 32279 solver.cpp:219] Iteration 700 (613.295 iter/s, 0.163054s/100 iters), loss = 0.152596
I0428 20:12:37.818022 32279 solver.cpp:238]     Train net output #0: loss = 0.152596 (* 1 = 0.152596 loss)
I0428 20:12:37.818044 32279 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:12:37.994422 32279 solver.cpp:219] Iteration 800 (566.942 iter/s, 0.176385s/100 iters), loss = 0.3065
I0428 20:12:37.994452 32279 solver.cpp:238]     Train net output #0: loss = 0.3065 (* 1 = 0.3065 loss)
I0428 20:12:37.994460 32279 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:12:38.168207 32279 solver.cpp:219] Iteration 900 (575.566 iter/s, 0.173742s/100 iters), loss = 0.19089
I0428 20:12:38.168232 32279 solver.cpp:238]     Train net output #0: loss = 0.190889 (* 1 = 0.190889 loss)
I0428 20:12:38.168238 32279 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:12:38.222174 32285 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:12:38.324546 32279 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:12:38.326697 32279 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:12:38.327996 32279 solver.cpp:311] Iteration 1000, loss = 0.225791
I0428 20:12:38.328012 32279 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:12:38.394482 32286 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:12:38.396885 32279 solver.cpp:398]     Test net output #0: accuracy = 0.9626
I0428 20:12:38.396904 32279 solver.cpp:398]     Test net output #1: loss = 0.123866 (* 1 = 0.123866 loss)
I0428 20:12:38.396909 32279 solver.cpp:316] Optimization Done.
I0428 20:12:38.396929 32279 caffe.cpp:259] Optimization Done.
