I0428 19:56:34.538581 28467 caffe.cpp:218] Using GPUs 0
I0428 19:56:34.571110 28467 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:56:35.082005 28467 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test763.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:56:35.082149 28467 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test763.prototxt
I0428 19:56:35.082563 28467 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:56:35.082582 28467 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:56:35.082685 28467 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:56:35.082767 28467 layer_factory.hpp:77] Creating layer mnist
I0428 19:56:35.082866 28467 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:56:35.082893 28467 net.cpp:86] Creating Layer mnist
I0428 19:56:35.082902 28467 net.cpp:382] mnist -> data
I0428 19:56:35.082926 28467 net.cpp:382] mnist -> label
I0428 19:56:35.084017 28467 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:56:35.086465 28467 net.cpp:124] Setting up mnist
I0428 19:56:35.086483 28467 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:56:35.086519 28467 net.cpp:131] Top shape: 64 (64)
I0428 19:56:35.086524 28467 net.cpp:139] Memory required for data: 200960
I0428 19:56:35.086531 28467 layer_factory.hpp:77] Creating layer conv0
I0428 19:56:35.086546 28467 net.cpp:86] Creating Layer conv0
I0428 19:56:35.086567 28467 net.cpp:408] conv0 <- data
I0428 19:56:35.086581 28467 net.cpp:382] conv0 -> conv0
I0428 19:56:35.323738 28467 net.cpp:124] Setting up conv0
I0428 19:56:35.323781 28467 net.cpp:131] Top shape: 64 10 24 24 (368640)
I0428 19:56:35.323786 28467 net.cpp:139] Memory required for data: 1675520
I0428 19:56:35.323799 28467 layer_factory.hpp:77] Creating layer pool0
I0428 19:56:35.323812 28467 net.cpp:86] Creating Layer pool0
I0428 19:56:35.323817 28467 net.cpp:408] pool0 <- conv0
I0428 19:56:35.323822 28467 net.cpp:382] pool0 -> pool0
I0428 19:56:35.323931 28467 net.cpp:124] Setting up pool0
I0428 19:56:35.323938 28467 net.cpp:131] Top shape: 64 10 12 12 (92160)
I0428 19:56:35.323941 28467 net.cpp:139] Memory required for data: 2044160
I0428 19:56:35.323945 28467 layer_factory.hpp:77] Creating layer conv1
I0428 19:56:35.323954 28467 net.cpp:86] Creating Layer conv1
I0428 19:56:35.323957 28467 net.cpp:408] conv1 <- pool0
I0428 19:56:35.323962 28467 net.cpp:382] conv1 -> conv1
I0428 19:56:35.325775 28467 net.cpp:124] Setting up conv1
I0428 19:56:35.325821 28467 net.cpp:131] Top shape: 64 2 8 8 (8192)
I0428 19:56:35.325825 28467 net.cpp:139] Memory required for data: 2076928
I0428 19:56:35.325848 28467 layer_factory.hpp:77] Creating layer pool1
I0428 19:56:35.325856 28467 net.cpp:86] Creating Layer pool1
I0428 19:56:35.325860 28467 net.cpp:408] pool1 <- conv1
I0428 19:56:35.325865 28467 net.cpp:382] pool1 -> pool1
I0428 19:56:35.325917 28467 net.cpp:124] Setting up pool1
I0428 19:56:35.325922 28467 net.cpp:131] Top shape: 64 2 4 4 (2048)
I0428 19:56:35.325925 28467 net.cpp:139] Memory required for data: 2085120
I0428 19:56:35.325928 28467 layer_factory.hpp:77] Creating layer ip1
I0428 19:56:35.325939 28467 net.cpp:86] Creating Layer ip1
I0428 19:56:35.325943 28467 net.cpp:408] ip1 <- pool1
I0428 19:56:35.325947 28467 net.cpp:382] ip1 -> ip1
I0428 19:56:35.326046 28467 net.cpp:124] Setting up ip1
I0428 19:56:35.326055 28467 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:56:35.326057 28467 net.cpp:139] Memory required for data: 2091520
I0428 19:56:35.326064 28467 layer_factory.hpp:77] Creating layer relu1
I0428 19:56:35.326071 28467 net.cpp:86] Creating Layer relu1
I0428 19:56:35.326074 28467 net.cpp:408] relu1 <- ip1
I0428 19:56:35.326078 28467 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:56:35.326269 28467 net.cpp:124] Setting up relu1
I0428 19:56:35.326278 28467 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:56:35.326282 28467 net.cpp:139] Memory required for data: 2097920
I0428 19:56:35.326284 28467 layer_factory.hpp:77] Creating layer ip2
I0428 19:56:35.326290 28467 net.cpp:86] Creating Layer ip2
I0428 19:56:35.326293 28467 net.cpp:408] ip2 <- ip1
I0428 19:56:35.326297 28467 net.cpp:382] ip2 -> ip2
I0428 19:56:35.326385 28467 net.cpp:124] Setting up ip2
I0428 19:56:35.326390 28467 net.cpp:131] Top shape: 64 10 (640)
I0428 19:56:35.326395 28467 net.cpp:139] Memory required for data: 2100480
I0428 19:56:35.326400 28467 layer_factory.hpp:77] Creating layer relu2
I0428 19:56:35.326405 28467 net.cpp:86] Creating Layer relu2
I0428 19:56:35.326407 28467 net.cpp:408] relu2 <- ip2
I0428 19:56:35.326411 28467 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:56:35.327206 28467 net.cpp:124] Setting up relu2
I0428 19:56:35.327219 28467 net.cpp:131] Top shape: 64 10 (640)
I0428 19:56:35.327222 28467 net.cpp:139] Memory required for data: 2103040
I0428 19:56:35.327225 28467 layer_factory.hpp:77] Creating layer ip3
I0428 19:56:35.327234 28467 net.cpp:86] Creating Layer ip3
I0428 19:56:35.327236 28467 net.cpp:408] ip3 <- ip2
I0428 19:56:35.327242 28467 net.cpp:382] ip3 -> ip3
I0428 19:56:35.327338 28467 net.cpp:124] Setting up ip3
I0428 19:56:35.327347 28467 net.cpp:131] Top shape: 64 10 (640)
I0428 19:56:35.327349 28467 net.cpp:139] Memory required for data: 2105600
I0428 19:56:35.327358 28467 layer_factory.hpp:77] Creating layer relu3
I0428 19:56:35.327363 28467 net.cpp:86] Creating Layer relu3
I0428 19:56:35.327366 28467 net.cpp:408] relu3 <- ip3
I0428 19:56:35.327370 28467 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:56:35.327531 28467 net.cpp:124] Setting up relu3
I0428 19:56:35.327539 28467 net.cpp:131] Top shape: 64 10 (640)
I0428 19:56:35.327543 28467 net.cpp:139] Memory required for data: 2108160
I0428 19:56:35.327545 28467 layer_factory.hpp:77] Creating layer loss
I0428 19:56:35.327551 28467 net.cpp:86] Creating Layer loss
I0428 19:56:35.327555 28467 net.cpp:408] loss <- ip3
I0428 19:56:35.327559 28467 net.cpp:408] loss <- label
I0428 19:56:35.327564 28467 net.cpp:382] loss -> loss
I0428 19:56:35.327582 28467 layer_factory.hpp:77] Creating layer loss
I0428 19:56:35.327836 28467 net.cpp:124] Setting up loss
I0428 19:56:35.327847 28467 net.cpp:131] Top shape: (1)
I0428 19:56:35.327849 28467 net.cpp:134]     with loss weight 1
I0428 19:56:35.327862 28467 net.cpp:139] Memory required for data: 2108164
I0428 19:56:35.327865 28467 net.cpp:200] loss needs backward computation.
I0428 19:56:35.327869 28467 net.cpp:200] relu3 needs backward computation.
I0428 19:56:35.327872 28467 net.cpp:200] ip3 needs backward computation.
I0428 19:56:35.327874 28467 net.cpp:200] relu2 needs backward computation.
I0428 19:56:35.327877 28467 net.cpp:200] ip2 needs backward computation.
I0428 19:56:35.327879 28467 net.cpp:200] relu1 needs backward computation.
I0428 19:56:35.327883 28467 net.cpp:200] ip1 needs backward computation.
I0428 19:56:35.327885 28467 net.cpp:200] pool1 needs backward computation.
I0428 19:56:35.327888 28467 net.cpp:200] conv1 needs backward computation.
I0428 19:56:35.327891 28467 net.cpp:200] pool0 needs backward computation.
I0428 19:56:35.327893 28467 net.cpp:200] conv0 needs backward computation.
I0428 19:56:35.327896 28467 net.cpp:202] mnist does not need backward computation.
I0428 19:56:35.327899 28467 net.cpp:244] This network produces output loss
I0428 19:56:35.327909 28467 net.cpp:257] Network initialization done.
I0428 19:56:35.328269 28467 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test763.prototxt
I0428 19:56:35.328311 28467 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:56:35.328402 28467 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:56:35.328482 28467 layer_factory.hpp:77] Creating layer mnist
I0428 19:56:35.328524 28467 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:56:35.328537 28467 net.cpp:86] Creating Layer mnist
I0428 19:56:35.328546 28467 net.cpp:382] mnist -> data
I0428 19:56:35.328552 28467 net.cpp:382] mnist -> label
I0428 19:56:35.328637 28467 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:56:35.330814 28467 net.cpp:124] Setting up mnist
I0428 19:56:35.330843 28467 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:56:35.330848 28467 net.cpp:131] Top shape: 100 (100)
I0428 19:56:35.330852 28467 net.cpp:139] Memory required for data: 314000
I0428 19:56:35.330855 28467 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:56:35.330899 28467 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:56:35.330904 28467 net.cpp:408] label_mnist_1_split <- label
I0428 19:56:35.330929 28467 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:56:35.330940 28467 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:56:35.331001 28467 net.cpp:124] Setting up label_mnist_1_split
I0428 19:56:35.331007 28467 net.cpp:131] Top shape: 100 (100)
I0428 19:56:35.331012 28467 net.cpp:131] Top shape: 100 (100)
I0428 19:56:35.331014 28467 net.cpp:139] Memory required for data: 314800
I0428 19:56:35.331017 28467 layer_factory.hpp:77] Creating layer conv0
I0428 19:56:35.331027 28467 net.cpp:86] Creating Layer conv0
I0428 19:56:35.331032 28467 net.cpp:408] conv0 <- data
I0428 19:56:35.331037 28467 net.cpp:382] conv0 -> conv0
I0428 19:56:35.332900 28467 net.cpp:124] Setting up conv0
I0428 19:56:35.332913 28467 net.cpp:131] Top shape: 100 10 24 24 (576000)
I0428 19:56:35.332917 28467 net.cpp:139] Memory required for data: 2618800
I0428 19:56:35.332926 28467 layer_factory.hpp:77] Creating layer pool0
I0428 19:56:35.332933 28467 net.cpp:86] Creating Layer pool0
I0428 19:56:35.332937 28467 net.cpp:408] pool0 <- conv0
I0428 19:56:35.332942 28467 net.cpp:382] pool0 -> pool0
I0428 19:56:35.332978 28467 net.cpp:124] Setting up pool0
I0428 19:56:35.332983 28467 net.cpp:131] Top shape: 100 10 12 12 (144000)
I0428 19:56:35.332986 28467 net.cpp:139] Memory required for data: 3194800
I0428 19:56:35.332989 28467 layer_factory.hpp:77] Creating layer conv1
I0428 19:56:35.332998 28467 net.cpp:86] Creating Layer conv1
I0428 19:56:35.333001 28467 net.cpp:408] conv1 <- pool0
I0428 19:56:35.333008 28467 net.cpp:382] conv1 -> conv1
I0428 19:56:35.335278 28467 net.cpp:124] Setting up conv1
I0428 19:56:35.335292 28467 net.cpp:131] Top shape: 100 2 8 8 (12800)
I0428 19:56:35.335296 28467 net.cpp:139] Memory required for data: 3246000
I0428 19:56:35.335304 28467 layer_factory.hpp:77] Creating layer pool1
I0428 19:56:35.335326 28467 net.cpp:86] Creating Layer pool1
I0428 19:56:35.335352 28467 net.cpp:408] pool1 <- conv1
I0428 19:56:35.335357 28467 net.cpp:382] pool1 -> pool1
I0428 19:56:35.335398 28467 net.cpp:124] Setting up pool1
I0428 19:56:35.335404 28467 net.cpp:131] Top shape: 100 2 4 4 (3200)
I0428 19:56:35.335407 28467 net.cpp:139] Memory required for data: 3258800
I0428 19:56:35.335410 28467 layer_factory.hpp:77] Creating layer ip1
I0428 19:56:35.335419 28467 net.cpp:86] Creating Layer ip1
I0428 19:56:35.335428 28467 net.cpp:408] ip1 <- pool1
I0428 19:56:35.335433 28467 net.cpp:382] ip1 -> ip1
I0428 19:56:35.335561 28467 net.cpp:124] Setting up ip1
I0428 19:56:35.335569 28467 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:56:35.335582 28467 net.cpp:139] Memory required for data: 3268800
I0428 19:56:35.335590 28467 layer_factory.hpp:77] Creating layer relu1
I0428 19:56:35.335597 28467 net.cpp:86] Creating Layer relu1
I0428 19:56:35.335600 28467 net.cpp:408] relu1 <- ip1
I0428 19:56:35.335605 28467 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:56:35.335855 28467 net.cpp:124] Setting up relu1
I0428 19:56:35.335868 28467 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:56:35.335871 28467 net.cpp:139] Memory required for data: 3278800
I0428 19:56:35.335875 28467 layer_factory.hpp:77] Creating layer ip2
I0428 19:56:35.335881 28467 net.cpp:86] Creating Layer ip2
I0428 19:56:35.335885 28467 net.cpp:408] ip2 <- ip1
I0428 19:56:35.335889 28467 net.cpp:382] ip2 -> ip2
I0428 19:56:35.336038 28467 net.cpp:124] Setting up ip2
I0428 19:56:35.336046 28467 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:56:35.336050 28467 net.cpp:139] Memory required for data: 3282800
I0428 19:56:35.336055 28467 layer_factory.hpp:77] Creating layer relu2
I0428 19:56:35.336066 28467 net.cpp:86] Creating Layer relu2
I0428 19:56:35.336069 28467 net.cpp:408] relu2 <- ip2
I0428 19:56:35.336072 28467 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:56:35.336217 28467 net.cpp:124] Setting up relu2
I0428 19:56:35.336226 28467 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:56:35.336228 28467 net.cpp:139] Memory required for data: 3286800
I0428 19:56:35.336231 28467 layer_factory.hpp:77] Creating layer ip3
I0428 19:56:35.336237 28467 net.cpp:86] Creating Layer ip3
I0428 19:56:35.336246 28467 net.cpp:408] ip3 <- ip2
I0428 19:56:35.336251 28467 net.cpp:382] ip3 -> ip3
I0428 19:56:35.336359 28467 net.cpp:124] Setting up ip3
I0428 19:56:35.336381 28467 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:56:35.336385 28467 net.cpp:139] Memory required for data: 3290800
I0428 19:56:35.336392 28467 layer_factory.hpp:77] Creating layer relu3
I0428 19:56:35.336396 28467 net.cpp:86] Creating Layer relu3
I0428 19:56:35.336400 28467 net.cpp:408] relu3 <- ip3
I0428 19:56:35.336405 28467 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:56:35.337332 28467 net.cpp:124] Setting up relu3
I0428 19:56:35.337343 28467 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:56:35.337347 28467 net.cpp:139] Memory required for data: 3294800
I0428 19:56:35.337349 28467 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:56:35.337354 28467 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:56:35.337358 28467 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:56:35.337364 28467 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:56:35.337370 28467 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:56:35.337412 28467 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:56:35.337419 28467 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:56:35.337421 28467 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:56:35.337424 28467 net.cpp:139] Memory required for data: 3302800
I0428 19:56:35.337427 28467 layer_factory.hpp:77] Creating layer accuracy
I0428 19:56:35.337442 28467 net.cpp:86] Creating Layer accuracy
I0428 19:56:35.337445 28467 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:56:35.337450 28467 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:56:35.337453 28467 net.cpp:382] accuracy -> accuracy
I0428 19:56:35.337474 28467 net.cpp:124] Setting up accuracy
I0428 19:56:35.337478 28467 net.cpp:131] Top shape: (1)
I0428 19:56:35.337481 28467 net.cpp:139] Memory required for data: 3302804
I0428 19:56:35.337483 28467 layer_factory.hpp:77] Creating layer loss
I0428 19:56:35.337488 28467 net.cpp:86] Creating Layer loss
I0428 19:56:35.337491 28467 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:56:35.337496 28467 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:56:35.337498 28467 net.cpp:382] loss -> loss
I0428 19:56:35.337504 28467 layer_factory.hpp:77] Creating layer loss
I0428 19:56:35.337757 28467 net.cpp:124] Setting up loss
I0428 19:56:35.337767 28467 net.cpp:131] Top shape: (1)
I0428 19:56:35.337769 28467 net.cpp:134]     with loss weight 1
I0428 19:56:35.337775 28467 net.cpp:139] Memory required for data: 3302808
I0428 19:56:35.337790 28467 net.cpp:200] loss needs backward computation.
I0428 19:56:35.337793 28467 net.cpp:202] accuracy does not need backward computation.
I0428 19:56:35.337796 28467 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:56:35.337800 28467 net.cpp:200] relu3 needs backward computation.
I0428 19:56:35.337802 28467 net.cpp:200] ip3 needs backward computation.
I0428 19:56:35.337819 28467 net.cpp:200] relu2 needs backward computation.
I0428 19:56:35.337822 28467 net.cpp:200] ip2 needs backward computation.
I0428 19:56:35.337826 28467 net.cpp:200] relu1 needs backward computation.
I0428 19:56:35.337828 28467 net.cpp:200] ip1 needs backward computation.
I0428 19:56:35.337831 28467 net.cpp:200] pool1 needs backward computation.
I0428 19:56:35.337851 28467 net.cpp:200] conv1 needs backward computation.
I0428 19:56:35.337859 28467 net.cpp:200] pool0 needs backward computation.
I0428 19:56:35.337863 28467 net.cpp:200] conv0 needs backward computation.
I0428 19:56:35.337867 28467 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:56:35.337872 28467 net.cpp:202] mnist does not need backward computation.
I0428 19:56:35.337873 28467 net.cpp:244] This network produces output accuracy
I0428 19:56:35.337877 28467 net.cpp:244] This network produces output loss
I0428 19:56:35.337903 28467 net.cpp:257] Network initialization done.
I0428 19:56:35.337944 28467 solver.cpp:56] Solver scaffolding done.
I0428 19:56:35.338340 28467 caffe.cpp:248] Starting Optimization
I0428 19:56:35.338346 28467 solver.cpp:273] Solving LeNet
I0428 19:56:35.338349 28467 solver.cpp:274] Learning Rate Policy: inv
I0428 19:56:35.339202 28467 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:56:35.342798 28467 blocking_queue.cpp:49] Waiting for data
I0428 19:56:35.413415 28474 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:56:35.413947 28467 solver.cpp:398]     Test net output #0: accuracy = 0.0999
I0428 19:56:35.413971 28467 solver.cpp:398]     Test net output #1: loss = 2.30301 (* 1 = 2.30301 loss)
I0428 19:56:35.416473 28467 solver.cpp:219] Iteration 0 (0 iter/s, 0.0780975s/100 iters), loss = 2.29438
I0428 19:56:35.416496 28467 solver.cpp:238]     Train net output #0: loss = 2.29438 (* 1 = 2.29438 loss)
I0428 19:56:35.416522 28467 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:56:35.490255 28467 solver.cpp:219] Iteration 100 (1355.96 iter/s, 0.0737485s/100 iters), loss = 1.57659
I0428 19:56:35.490280 28467 solver.cpp:238]     Train net output #0: loss = 1.57659 (* 1 = 1.57659 loss)
I0428 19:56:35.490303 28467 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:56:35.563537 28467 solver.cpp:219] Iteration 200 (1365.19 iter/s, 0.0732498s/100 iters), loss = 1.63964
I0428 19:56:35.563562 28467 solver.cpp:238]     Train net output #0: loss = 1.63964 (* 1 = 1.63964 loss)
I0428 19:56:35.563585 28467 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:56:35.636008 28467 solver.cpp:219] Iteration 300 (1380.52 iter/s, 0.0724367s/100 iters), loss = 1.49649
I0428 19:56:35.636031 28467 solver.cpp:238]     Train net output #0: loss = 1.49649 (* 1 = 1.49649 loss)
I0428 19:56:35.636036 28467 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:56:35.710122 28467 solver.cpp:219] Iteration 400 (1349.85 iter/s, 0.0740823s/100 iters), loss = 1.2601
I0428 19:56:35.710145 28467 solver.cpp:238]     Train net output #0: loss = 1.2601 (* 1 = 1.2601 loss)
I0428 19:56:35.710151 28467 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:56:35.783006 28467 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:56:35.836493 28474 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:56:35.837023 28467 solver.cpp:398]     Test net output #0: accuracy = 0.4549
I0428 19:56:35.837044 28467 solver.cpp:398]     Test net output #1: loss = 1.36291 (* 1 = 1.36291 loss)
I0428 19:56:35.837844 28467 solver.cpp:219] Iteration 500 (783.162 iter/s, 0.127688s/100 iters), loss = 1.24713
I0428 19:56:35.837868 28467 solver.cpp:238]     Train net output #0: loss = 1.24713 (* 1 = 1.24713 loss)
I0428 19:56:35.837894 28467 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:56:35.911355 28467 solver.cpp:219] Iteration 600 (1360.95 iter/s, 0.0734781s/100 iters), loss = 1.55483
I0428 19:56:35.911378 28467 solver.cpp:238]     Train net output #0: loss = 1.55483 (* 1 = 1.55483 loss)
I0428 19:56:35.911399 28467 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:56:35.985414 28467 solver.cpp:219] Iteration 700 (1350.88 iter/s, 0.074026s/100 iters), loss = 1.26927
I0428 19:56:35.985436 28467 solver.cpp:238]     Train net output #0: loss = 1.26927 (* 1 = 1.26927 loss)
I0428 19:56:35.985457 28467 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:56:36.057950 28467 solver.cpp:219] Iteration 800 (1379.25 iter/s, 0.0725033s/100 iters), loss = 1.25511
I0428 19:56:36.057988 28467 solver.cpp:238]     Train net output #0: loss = 1.25511 (* 1 = 1.25511 loss)
I0428 19:56:36.057994 28467 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:56:36.138710 28467 solver.cpp:219] Iteration 900 (1238.73 iter/s, 0.0807279s/100 iters), loss = 1.21624
I0428 19:56:36.138733 28467 solver.cpp:238]     Train net output #0: loss = 1.21624 (* 1 = 1.21624 loss)
I0428 19:56:36.138738 28467 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:56:36.163784 28473 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:56:36.211741 28467 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:56:36.212461 28467 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:56:36.212930 28467 solver.cpp:311] Iteration 1000, loss = 1.02904
I0428 19:56:36.212944 28467 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:56:36.288700 28474 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:56:36.289238 28467 solver.cpp:398]     Test net output #0: accuracy = 0.6443
I0428 19:56:36.289259 28467 solver.cpp:398]     Test net output #1: loss = 0.926876 (* 1 = 0.926876 loss)
I0428 19:56:36.289264 28467 solver.cpp:316] Optimization Done.
I0428 19:56:36.289268 28467 caffe.cpp:259] Optimization Done.
