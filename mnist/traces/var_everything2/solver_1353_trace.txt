I0428 20:19:59.000372  1460 caffe.cpp:218] Using GPUs 0
I0428 20:19:59.038544  1460 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:19:59.562062  1460 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1353.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:19:59.562222  1460 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1353.prototxt
I0428 20:19:59.562875  1460 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:19:59.562906  1460 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:19:59.563066  1460 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:19:59.563180  1460 layer_factory.hpp:77] Creating layer mnist
I0428 20:19:59.563308  1460 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:19:59.563343  1460 net.cpp:86] Creating Layer mnist
I0428 20:19:59.563357  1460 net.cpp:382] mnist -> data
I0428 20:19:59.563387  1460 net.cpp:382] mnist -> label
I0428 20:19:59.564955  1460 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:19:59.567690  1460 net.cpp:124] Setting up mnist
I0428 20:19:59.567708  1460 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:19:59.567715  1460 net.cpp:131] Top shape: 64 (64)
I0428 20:19:59.567719  1460 net.cpp:139] Memory required for data: 200960
I0428 20:19:59.567726  1460 layer_factory.hpp:77] Creating layer conv0
I0428 20:19:59.567741  1460 net.cpp:86] Creating Layer conv0
I0428 20:19:59.567769  1460 net.cpp:408] conv0 <- data
I0428 20:19:59.567790  1460 net.cpp:382] conv0 -> conv0
I0428 20:19:59.859483  1460 net.cpp:124] Setting up conv0
I0428 20:19:59.859524  1460 net.cpp:131] Top shape: 64 50 24 24 (1843200)
I0428 20:19:59.859534  1460 net.cpp:139] Memory required for data: 7573760
I0428 20:19:59.859551  1460 layer_factory.hpp:77] Creating layer pool0
I0428 20:19:59.859568  1460 net.cpp:86] Creating Layer pool0
I0428 20:19:59.859573  1460 net.cpp:408] pool0 <- conv0
I0428 20:19:59.859581  1460 net.cpp:382] pool0 -> pool0
I0428 20:19:59.859658  1460 net.cpp:124] Setting up pool0
I0428 20:19:59.859673  1460 net.cpp:131] Top shape: 64 50 12 12 (460800)
I0428 20:19:59.859679  1460 net.cpp:139] Memory required for data: 9416960
I0428 20:19:59.859685  1460 layer_factory.hpp:77] Creating layer conv1
I0428 20:19:59.859704  1460 net.cpp:86] Creating Layer conv1
I0428 20:19:59.859715  1460 net.cpp:408] conv1 <- pool0
I0428 20:19:59.859726  1460 net.cpp:382] conv1 -> conv1
I0428 20:19:59.862781  1460 net.cpp:124] Setting up conv1
I0428 20:19:59.862799  1460 net.cpp:131] Top shape: 64 25 8 8 (102400)
I0428 20:19:59.862803  1460 net.cpp:139] Memory required for data: 9826560
I0428 20:19:59.862815  1460 layer_factory.hpp:77] Creating layer pool1
I0428 20:19:59.862824  1460 net.cpp:86] Creating Layer pool1
I0428 20:19:59.862828  1460 net.cpp:408] pool1 <- conv1
I0428 20:19:59.862838  1460 net.cpp:382] pool1 -> pool1
I0428 20:19:59.862900  1460 net.cpp:124] Setting up pool1
I0428 20:19:59.862915  1460 net.cpp:131] Top shape: 64 25 4 4 (25600)
I0428 20:19:59.862924  1460 net.cpp:139] Memory required for data: 9928960
I0428 20:19:59.862931  1460 layer_factory.hpp:77] Creating layer ip1
I0428 20:19:59.862946  1460 net.cpp:86] Creating Layer ip1
I0428 20:19:59.862953  1460 net.cpp:408] ip1 <- pool1
I0428 20:19:59.862963  1460 net.cpp:382] ip1 -> ip1
I0428 20:19:59.864691  1460 net.cpp:124] Setting up ip1
I0428 20:19:59.864713  1460 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:19:59.864718  1460 net.cpp:139] Memory required for data: 9935360
I0428 20:19:59.864729  1460 layer_factory.hpp:77] Creating layer relu1
I0428 20:19:59.864737  1460 net.cpp:86] Creating Layer relu1
I0428 20:19:59.864742  1460 net.cpp:408] relu1 <- ip1
I0428 20:19:59.864750  1460 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:19:59.865063  1460 net.cpp:124] Setting up relu1
I0428 20:19:59.865082  1460 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:19:59.865087  1460 net.cpp:139] Memory required for data: 9941760
I0428 20:19:59.865093  1460 layer_factory.hpp:77] Creating layer ip2
I0428 20:19:59.865108  1460 net.cpp:86] Creating Layer ip2
I0428 20:19:59.865113  1460 net.cpp:408] ip2 <- ip1
I0428 20:19:59.865125  1460 net.cpp:382] ip2 -> ip2
I0428 20:19:59.865314  1460 net.cpp:124] Setting up ip2
I0428 20:19:59.865327  1460 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:19:59.865334  1460 net.cpp:139] Memory required for data: 9954560
I0428 20:19:59.865345  1460 layer_factory.hpp:77] Creating layer relu2
I0428 20:19:59.865355  1460 net.cpp:86] Creating Layer relu2
I0428 20:19:59.865361  1460 net.cpp:408] relu2 <- ip2
I0428 20:19:59.865370  1460 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:19:59.866611  1460 net.cpp:124] Setting up relu2
I0428 20:19:59.866633  1460 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:19:59.866641  1460 net.cpp:139] Memory required for data: 9967360
I0428 20:19:59.866647  1460 layer_factory.hpp:77] Creating layer ip3
I0428 20:19:59.866659  1460 net.cpp:86] Creating Layer ip3
I0428 20:19:59.866665  1460 net.cpp:408] ip3 <- ip2
I0428 20:19:59.866679  1460 net.cpp:382] ip3 -> ip3
I0428 20:19:59.866858  1460 net.cpp:124] Setting up ip3
I0428 20:19:59.866874  1460 net.cpp:131] Top shape: 64 10 (640)
I0428 20:19:59.866880  1460 net.cpp:139] Memory required for data: 9969920
I0428 20:19:59.866895  1460 layer_factory.hpp:77] Creating layer relu3
I0428 20:19:59.866905  1460 net.cpp:86] Creating Layer relu3
I0428 20:19:59.866911  1460 net.cpp:408] relu3 <- ip3
I0428 20:19:59.866921  1460 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:19:59.867233  1460 net.cpp:124] Setting up relu3
I0428 20:19:59.867249  1460 net.cpp:131] Top shape: 64 10 (640)
I0428 20:19:59.867255  1460 net.cpp:139] Memory required for data: 9972480
I0428 20:19:59.867262  1460 layer_factory.hpp:77] Creating layer loss
I0428 20:19:59.867274  1460 net.cpp:86] Creating Layer loss
I0428 20:19:59.867280  1460 net.cpp:408] loss <- ip3
I0428 20:19:59.867288  1460 net.cpp:408] loss <- label
I0428 20:19:59.867297  1460 net.cpp:382] loss -> loss
I0428 20:19:59.867326  1460 layer_factory.hpp:77] Creating layer loss
I0428 20:19:59.867736  1460 net.cpp:124] Setting up loss
I0428 20:19:59.867750  1460 net.cpp:131] Top shape: (1)
I0428 20:19:59.867756  1460 net.cpp:134]     with loss weight 1
I0428 20:19:59.867777  1460 net.cpp:139] Memory required for data: 9972484
I0428 20:19:59.867784  1460 net.cpp:200] loss needs backward computation.
I0428 20:19:59.867791  1460 net.cpp:200] relu3 needs backward computation.
I0428 20:19:59.867797  1460 net.cpp:200] ip3 needs backward computation.
I0428 20:19:59.867804  1460 net.cpp:200] relu2 needs backward computation.
I0428 20:19:59.867810  1460 net.cpp:200] ip2 needs backward computation.
I0428 20:19:59.867815  1460 net.cpp:200] relu1 needs backward computation.
I0428 20:19:59.867820  1460 net.cpp:200] ip1 needs backward computation.
I0428 20:19:59.867827  1460 net.cpp:200] pool1 needs backward computation.
I0428 20:19:59.867833  1460 net.cpp:200] conv1 needs backward computation.
I0428 20:19:59.867839  1460 net.cpp:200] pool0 needs backward computation.
I0428 20:19:59.867846  1460 net.cpp:200] conv0 needs backward computation.
I0428 20:19:59.867852  1460 net.cpp:202] mnist does not need backward computation.
I0428 20:19:59.867857  1460 net.cpp:244] This network produces output loss
I0428 20:19:59.867877  1460 net.cpp:257] Network initialization done.
I0428 20:19:59.868492  1460 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1353.prototxt
I0428 20:19:59.868543  1460 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:19:59.868722  1460 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:19:59.868885  1460 layer_factory.hpp:77] Creating layer mnist
I0428 20:19:59.868966  1460 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:19:59.868988  1460 net.cpp:86] Creating Layer mnist
I0428 20:19:59.868995  1460 net.cpp:382] mnist -> data
I0428 20:19:59.869011  1460 net.cpp:382] mnist -> label
I0428 20:19:59.869153  1460 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:19:59.872217  1460 net.cpp:124] Setting up mnist
I0428 20:19:59.872236  1460 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:19:59.872246  1460 net.cpp:131] Top shape: 100 (100)
I0428 20:19:59.872252  1460 net.cpp:139] Memory required for data: 314000
I0428 20:19:59.872259  1460 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:19:59.872272  1460 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:19:59.872277  1460 net.cpp:408] label_mnist_1_split <- label
I0428 20:19:59.872287  1460 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:19:59.872300  1460 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:19:59.872434  1460 net.cpp:124] Setting up label_mnist_1_split
I0428 20:19:59.872448  1460 net.cpp:131] Top shape: 100 (100)
I0428 20:19:59.872455  1460 net.cpp:131] Top shape: 100 (100)
I0428 20:19:59.872462  1460 net.cpp:139] Memory required for data: 314800
I0428 20:19:59.872467  1460 layer_factory.hpp:77] Creating layer conv0
I0428 20:19:59.872488  1460 net.cpp:86] Creating Layer conv0
I0428 20:19:59.872501  1460 net.cpp:408] conv0 <- data
I0428 20:19:59.872511  1460 net.cpp:382] conv0 -> conv0
I0428 20:19:59.874815  1460 net.cpp:124] Setting up conv0
I0428 20:19:59.874832  1460 net.cpp:131] Top shape: 100 50 24 24 (2880000)
I0428 20:19:59.874837  1460 net.cpp:139] Memory required for data: 11834800
I0428 20:19:59.874847  1460 layer_factory.hpp:77] Creating layer pool0
I0428 20:19:59.874857  1460 net.cpp:86] Creating Layer pool0
I0428 20:19:59.874861  1460 net.cpp:408] pool0 <- conv0
I0428 20:19:59.874866  1460 net.cpp:382] pool0 -> pool0
I0428 20:19:59.874920  1460 net.cpp:124] Setting up pool0
I0428 20:19:59.874933  1460 net.cpp:131] Top shape: 100 50 12 12 (720000)
I0428 20:19:59.874939  1460 net.cpp:139] Memory required for data: 14714800
I0428 20:19:59.874950  1460 layer_factory.hpp:77] Creating layer conv1
I0428 20:19:59.874967  1460 net.cpp:86] Creating Layer conv1
I0428 20:19:59.874974  1460 net.cpp:408] conv1 <- pool0
I0428 20:19:59.874987  1460 net.cpp:382] conv1 -> conv1
I0428 20:19:59.877677  1460 net.cpp:124] Setting up conv1
I0428 20:19:59.877694  1460 net.cpp:131] Top shape: 100 25 8 8 (160000)
I0428 20:19:59.877698  1460 net.cpp:139] Memory required for data: 15354800
I0428 20:19:59.877709  1460 layer_factory.hpp:77] Creating layer pool1
I0428 20:19:59.877717  1460 net.cpp:86] Creating Layer pool1
I0428 20:19:59.877722  1460 net.cpp:408] pool1 <- conv1
I0428 20:19:59.877729  1460 net.cpp:382] pool1 -> pool1
I0428 20:19:59.877789  1460 net.cpp:124] Setting up pool1
I0428 20:19:59.877802  1460 net.cpp:131] Top shape: 100 25 4 4 (40000)
I0428 20:19:59.877822  1460 net.cpp:139] Memory required for data: 15514800
I0428 20:19:59.877830  1460 layer_factory.hpp:77] Creating layer ip1
I0428 20:19:59.877842  1460 net.cpp:86] Creating Layer ip1
I0428 20:19:59.877851  1460 net.cpp:408] ip1 <- pool1
I0428 20:19:59.877862  1460 net.cpp:382] ip1 -> ip1
I0428 20:19:59.878154  1460 net.cpp:124] Setting up ip1
I0428 20:19:59.878168  1460 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:19:59.878190  1460 net.cpp:139] Memory required for data: 15524800
I0428 20:19:59.878204  1460 layer_factory.hpp:77] Creating layer relu1
I0428 20:19:59.878214  1460 net.cpp:86] Creating Layer relu1
I0428 20:19:59.878221  1460 net.cpp:408] relu1 <- ip1
I0428 20:19:59.878233  1460 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:19:59.878624  1460 net.cpp:124] Setting up relu1
I0428 20:19:59.878640  1460 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:19:59.878648  1460 net.cpp:139] Memory required for data: 15534800
I0428 20:19:59.878654  1460 layer_factory.hpp:77] Creating layer ip2
I0428 20:19:59.878670  1460 net.cpp:86] Creating Layer ip2
I0428 20:19:59.878679  1460 net.cpp:408] ip2 <- ip1
I0428 20:19:59.878690  1460 net.cpp:382] ip2 -> ip2
I0428 20:19:59.878887  1460 net.cpp:124] Setting up ip2
I0428 20:19:59.878906  1460 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:19:59.878912  1460 net.cpp:139] Memory required for data: 15554800
I0428 20:19:59.878923  1460 layer_factory.hpp:77] Creating layer relu2
I0428 20:19:59.878932  1460 net.cpp:86] Creating Layer relu2
I0428 20:19:59.878938  1460 net.cpp:408] relu2 <- ip2
I0428 20:19:59.878948  1460 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:19:59.879230  1460 net.cpp:124] Setting up relu2
I0428 20:19:59.879245  1460 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:19:59.879251  1460 net.cpp:139] Memory required for data: 15574800
I0428 20:19:59.879256  1460 layer_factory.hpp:77] Creating layer ip3
I0428 20:19:59.879269  1460 net.cpp:86] Creating Layer ip3
I0428 20:19:59.879276  1460 net.cpp:408] ip3 <- ip2
I0428 20:19:59.879284  1460 net.cpp:382] ip3 -> ip3
I0428 20:19:59.879464  1460 net.cpp:124] Setting up ip3
I0428 20:19:59.879477  1460 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:19:59.879482  1460 net.cpp:139] Memory required for data: 15578800
I0428 20:19:59.879498  1460 layer_factory.hpp:77] Creating layer relu3
I0428 20:19:59.879509  1460 net.cpp:86] Creating Layer relu3
I0428 20:19:59.879515  1460 net.cpp:408] relu3 <- ip3
I0428 20:19:59.879523  1460 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:19:59.880784  1460 net.cpp:124] Setting up relu3
I0428 20:19:59.880807  1460 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:19:59.880822  1460 net.cpp:139] Memory required for data: 15582800
I0428 20:19:59.880830  1460 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:19:59.880839  1460 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:19:59.880846  1460 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:19:59.880854  1460 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:19:59.880867  1460 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:19:59.880931  1460 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:19:59.880944  1460 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:19:59.880951  1460 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:19:59.880957  1460 net.cpp:139] Memory required for data: 15590800
I0428 20:19:59.880964  1460 layer_factory.hpp:77] Creating layer accuracy
I0428 20:19:59.880977  1460 net.cpp:86] Creating Layer accuracy
I0428 20:19:59.880983  1460 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:19:59.880992  1460 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:19:59.881002  1460 net.cpp:382] accuracy -> accuracy
I0428 20:19:59.881014  1460 net.cpp:124] Setting up accuracy
I0428 20:19:59.881023  1460 net.cpp:131] Top shape: (1)
I0428 20:19:59.881029  1460 net.cpp:139] Memory required for data: 15590804
I0428 20:19:59.881034  1460 layer_factory.hpp:77] Creating layer loss
I0428 20:19:59.881045  1460 net.cpp:86] Creating Layer loss
I0428 20:19:59.881052  1460 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:19:59.881058  1460 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:19:59.881067  1460 net.cpp:382] loss -> loss
I0428 20:19:59.881078  1460 layer_factory.hpp:77] Creating layer loss
I0428 20:19:59.881491  1460 net.cpp:124] Setting up loss
I0428 20:19:59.881508  1460 net.cpp:131] Top shape: (1)
I0428 20:19:59.881515  1460 net.cpp:134]     with loss weight 1
I0428 20:19:59.881541  1460 net.cpp:139] Memory required for data: 15590808
I0428 20:19:59.881547  1460 net.cpp:200] loss needs backward computation.
I0428 20:19:59.881556  1460 net.cpp:202] accuracy does not need backward computation.
I0428 20:19:59.881562  1460 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:19:59.881568  1460 net.cpp:200] relu3 needs backward computation.
I0428 20:19:59.881574  1460 net.cpp:200] ip3 needs backward computation.
I0428 20:19:59.881580  1460 net.cpp:200] relu2 needs backward computation.
I0428 20:19:59.881587  1460 net.cpp:200] ip2 needs backward computation.
I0428 20:19:59.881592  1460 net.cpp:200] relu1 needs backward computation.
I0428 20:19:59.881598  1460 net.cpp:200] ip1 needs backward computation.
I0428 20:19:59.881604  1460 net.cpp:200] pool1 needs backward computation.
I0428 20:19:59.881610  1460 net.cpp:200] conv1 needs backward computation.
I0428 20:19:59.881616  1460 net.cpp:200] pool0 needs backward computation.
I0428 20:19:59.881623  1460 net.cpp:200] conv0 needs backward computation.
I0428 20:19:59.881631  1460 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:19:59.881639  1460 net.cpp:202] mnist does not need backward computation.
I0428 20:19:59.881644  1460 net.cpp:244] This network produces output accuracy
I0428 20:19:59.881650  1460 net.cpp:244] This network produces output loss
I0428 20:19:59.881672  1460 net.cpp:257] Network initialization done.
I0428 20:19:59.881744  1460 solver.cpp:56] Solver scaffolding done.
I0428 20:19:59.882316  1460 caffe.cpp:248] Starting Optimization
I0428 20:19:59.882328  1460 solver.cpp:273] Solving LeNet
I0428 20:19:59.882333  1460 solver.cpp:274] Learning Rate Policy: inv
I0428 20:19:59.883610  1460 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:19:59.892529  1460 blocking_queue.cpp:49] Waiting for data
I0428 20:19:59.961076  1468 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:19:59.962389  1460 solver.cpp:398]     Test net output #0: accuracy = 0.1007
I0428 20:19:59.962414  1460 solver.cpp:398]     Test net output #1: loss = 2.34706 (* 1 = 2.34706 loss)
I0428 20:19:59.967736  1460 solver.cpp:219] Iteration 0 (-9.51482e-43 iter/s, 0.0853699s/100 iters), loss = 2.38826
I0428 20:19:59.967767  1460 solver.cpp:238]     Train net output #0: loss = 2.38826 (* 1 = 2.38826 loss)
I0428 20:19:59.967780  1460 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:20:00.111029  1460 solver.cpp:219] Iteration 100 (698.113 iter/s, 0.143243s/100 iters), loss = 0.346656
I0428 20:20:00.111063  1460 solver.cpp:238]     Train net output #0: loss = 0.346656 (* 1 = 0.346656 loss)
I0428 20:20:00.111071  1460 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:20:00.254181  1460 solver.cpp:219] Iteration 200 (698.783 iter/s, 0.143106s/100 iters), loss = 0.190017
I0428 20:20:00.254216  1460 solver.cpp:238]     Train net output #0: loss = 0.190017 (* 1 = 0.190017 loss)
I0428 20:20:00.254225  1460 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:20:00.392565  1460 solver.cpp:219] Iteration 300 (722.87 iter/s, 0.138337s/100 iters), loss = 0.248343
I0428 20:20:00.392607  1460 solver.cpp:238]     Train net output #0: loss = 0.248343 (* 1 = 0.248343 loss)
I0428 20:20:00.392614  1460 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:20:00.538820  1460 solver.cpp:219] Iteration 400 (683.997 iter/s, 0.146199s/100 iters), loss = 0.0698929
I0428 20:20:00.538854  1460 solver.cpp:238]     Train net output #0: loss = 0.0698929 (* 1 = 0.0698929 loss)
I0428 20:20:00.538862  1460 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:20:00.677973  1460 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:20:00.747397  1468 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:20:00.749783  1460 solver.cpp:398]     Test net output #0: accuracy = 0.962
I0428 20:20:00.749804  1460 solver.cpp:398]     Test net output #1: loss = 0.119724 (* 1 = 0.119724 loss)
I0428 20:20:00.751191  1460 solver.cpp:219] Iteration 500 (470.986 iter/s, 0.21232s/100 iters), loss = 0.132496
I0428 20:20:00.751242  1460 solver.cpp:238]     Train net output #0: loss = 0.132496 (* 1 = 0.132496 loss)
I0428 20:20:00.751252  1460 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:20:00.902076  1460 solver.cpp:219] Iteration 600 (663.038 iter/s, 0.150821s/100 iters), loss = 0.166585
I0428 20:20:00.902117  1460 solver.cpp:238]     Train net output #0: loss = 0.166585 (* 1 = 0.166585 loss)
I0428 20:20:00.902127  1460 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:20:01.055254  1460 solver.cpp:219] Iteration 700 (653.057 iter/s, 0.153126s/100 iters), loss = 0.0900296
I0428 20:20:01.055296  1460 solver.cpp:238]     Train net output #0: loss = 0.0900296 (* 1 = 0.0900296 loss)
I0428 20:20:01.055306  1460 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:20:01.208725  1460 solver.cpp:219] Iteration 800 (651.824 iter/s, 0.153416s/100 iters), loss = 0.270822
I0428 20:20:01.208763  1460 solver.cpp:238]     Train net output #0: loss = 0.270822 (* 1 = 0.270822 loss)
I0428 20:20:01.208773  1460 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:20:01.357478  1460 solver.cpp:219] Iteration 900 (672.476 iter/s, 0.148704s/100 iters), loss = 0.123519
I0428 20:20:01.357518  1460 solver.cpp:238]     Train net output #0: loss = 0.123519 (* 1 = 0.123519 loss)
I0428 20:20:01.357527  1460 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:20:01.403725  1467 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:20:01.497143  1460 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:20:01.498862  1460 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:20:01.499908  1460 solver.cpp:311] Iteration 1000, loss = 0.124967
I0428 20:20:01.499927  1460 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:20:01.574828  1468 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:20:01.575973  1460 solver.cpp:398]     Test net output #0: accuracy = 0.9753
I0428 20:20:01.575996  1460 solver.cpp:398]     Test net output #1: loss = 0.0795319 (* 1 = 0.0795319 loss)
I0428 20:20:01.576004  1460 solver.cpp:316] Optimization Done.
I0428 20:20:01.576007  1460 caffe.cpp:259] Optimization Done.
