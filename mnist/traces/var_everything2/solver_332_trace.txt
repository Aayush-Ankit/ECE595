I0428 19:40:21.562961 24530 caffe.cpp:218] Using GPUs 0
I0428 19:40:21.600245 24530 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:40:22.110271 24530 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test332.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:40:22.110419 24530 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test332.prototxt
I0428 19:40:22.110828 24530 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:40:22.110852 24530 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:40:22.110954 24530 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:40:22.111033 24530 layer_factory.hpp:77] Creating layer mnist
I0428 19:40:22.111130 24530 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:40:22.111155 24530 net.cpp:86] Creating Layer mnist
I0428 19:40:22.111163 24530 net.cpp:382] mnist -> data
I0428 19:40:22.111186 24530 net.cpp:382] mnist -> label
I0428 19:40:22.112275 24530 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:40:22.114850 24530 net.cpp:124] Setting up mnist
I0428 19:40:22.114869 24530 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:40:22.114876 24530 net.cpp:131] Top shape: 64 (64)
I0428 19:40:22.114879 24530 net.cpp:139] Memory required for data: 200960
I0428 19:40:22.114886 24530 layer_factory.hpp:77] Creating layer conv0
I0428 19:40:22.114904 24530 net.cpp:86] Creating Layer conv0
I0428 19:40:22.114925 24530 net.cpp:408] conv0 <- data
I0428 19:40:22.114939 24530 net.cpp:382] conv0 -> conv0
I0428 19:40:22.398224 24530 net.cpp:124] Setting up conv0
I0428 19:40:22.398252 24530 net.cpp:131] Top shape: 64 2 24 24 (73728)
I0428 19:40:22.398257 24530 net.cpp:139] Memory required for data: 495872
I0428 19:40:22.398272 24530 layer_factory.hpp:77] Creating layer pool0
I0428 19:40:22.398284 24530 net.cpp:86] Creating Layer pool0
I0428 19:40:22.398288 24530 net.cpp:408] pool0 <- conv0
I0428 19:40:22.398294 24530 net.cpp:382] pool0 -> pool0
I0428 19:40:22.398339 24530 net.cpp:124] Setting up pool0
I0428 19:40:22.398345 24530 net.cpp:131] Top shape: 64 2 12 12 (18432)
I0428 19:40:22.398349 24530 net.cpp:139] Memory required for data: 569600
I0428 19:40:22.398351 24530 layer_factory.hpp:77] Creating layer conv1
I0428 19:40:22.398363 24530 net.cpp:86] Creating Layer conv1
I0428 19:40:22.398366 24530 net.cpp:408] conv1 <- pool0
I0428 19:40:22.398371 24530 net.cpp:382] conv1 -> conv1
I0428 19:40:22.400197 24530 net.cpp:124] Setting up conv1
I0428 19:40:22.400213 24530 net.cpp:131] Top shape: 64 5 8 8 (20480)
I0428 19:40:22.400218 24530 net.cpp:139] Memory required for data: 651520
I0428 19:40:22.400243 24530 layer_factory.hpp:77] Creating layer pool1
I0428 19:40:22.400251 24530 net.cpp:86] Creating Layer pool1
I0428 19:40:22.400255 24530 net.cpp:408] pool1 <- conv1
I0428 19:40:22.400260 24530 net.cpp:382] pool1 -> pool1
I0428 19:40:22.400300 24530 net.cpp:124] Setting up pool1
I0428 19:40:22.400305 24530 net.cpp:131] Top shape: 64 5 4 4 (5120)
I0428 19:40:22.400308 24530 net.cpp:139] Memory required for data: 672000
I0428 19:40:22.400311 24530 layer_factory.hpp:77] Creating layer ip1
I0428 19:40:22.400319 24530 net.cpp:86] Creating Layer ip1
I0428 19:40:22.400322 24530 net.cpp:408] ip1 <- pool1
I0428 19:40:22.400327 24530 net.cpp:382] ip1 -> ip1
I0428 19:40:22.400426 24530 net.cpp:124] Setting up ip1
I0428 19:40:22.400434 24530 net.cpp:131] Top shape: 64 10 (640)
I0428 19:40:22.400452 24530 net.cpp:139] Memory required for data: 674560
I0428 19:40:22.400460 24530 layer_factory.hpp:77] Creating layer relu1
I0428 19:40:22.400465 24530 net.cpp:86] Creating Layer relu1
I0428 19:40:22.400467 24530 net.cpp:408] relu1 <- ip1
I0428 19:40:22.400472 24530 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:40:22.400636 24530 net.cpp:124] Setting up relu1
I0428 19:40:22.400645 24530 net.cpp:131] Top shape: 64 10 (640)
I0428 19:40:22.400648 24530 net.cpp:139] Memory required for data: 677120
I0428 19:40:22.400652 24530 layer_factory.hpp:77] Creating layer ip2
I0428 19:40:22.400658 24530 net.cpp:86] Creating Layer ip2
I0428 19:40:22.400661 24530 net.cpp:408] ip2 <- ip1
I0428 19:40:22.400666 24530 net.cpp:382] ip2 -> ip2
I0428 19:40:22.400769 24530 net.cpp:124] Setting up ip2
I0428 19:40:22.400777 24530 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:40:22.400780 24530 net.cpp:139] Memory required for data: 689920
I0428 19:40:22.400786 24530 layer_factory.hpp:77] Creating layer relu2
I0428 19:40:22.400792 24530 net.cpp:86] Creating Layer relu2
I0428 19:40:22.400795 24530 net.cpp:408] relu2 <- ip2
I0428 19:40:22.400800 24530 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:40:22.401638 24530 net.cpp:124] Setting up relu2
I0428 19:40:22.401653 24530 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:40:22.401656 24530 net.cpp:139] Memory required for data: 702720
I0428 19:40:22.401660 24530 layer_factory.hpp:77] Creating layer ip3
I0428 19:40:22.401667 24530 net.cpp:86] Creating Layer ip3
I0428 19:40:22.401671 24530 net.cpp:408] ip3 <- ip2
I0428 19:40:22.401676 24530 net.cpp:382] ip3 -> ip3
I0428 19:40:22.401782 24530 net.cpp:124] Setting up ip3
I0428 19:40:22.401790 24530 net.cpp:131] Top shape: 64 10 (640)
I0428 19:40:22.401794 24530 net.cpp:139] Memory required for data: 705280
I0428 19:40:22.401803 24530 layer_factory.hpp:77] Creating layer relu3
I0428 19:40:22.401808 24530 net.cpp:86] Creating Layer relu3
I0428 19:40:22.401810 24530 net.cpp:408] relu3 <- ip3
I0428 19:40:22.401814 24530 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:40:22.401985 24530 net.cpp:124] Setting up relu3
I0428 19:40:22.401995 24530 net.cpp:131] Top shape: 64 10 (640)
I0428 19:40:22.401998 24530 net.cpp:139] Memory required for data: 707840
I0428 19:40:22.402003 24530 layer_factory.hpp:77] Creating layer loss
I0428 19:40:22.402009 24530 net.cpp:86] Creating Layer loss
I0428 19:40:22.402011 24530 net.cpp:408] loss <- ip3
I0428 19:40:22.402015 24530 net.cpp:408] loss <- label
I0428 19:40:22.402021 24530 net.cpp:382] loss -> loss
I0428 19:40:22.402035 24530 layer_factory.hpp:77] Creating layer loss
I0428 19:40:22.402268 24530 net.cpp:124] Setting up loss
I0428 19:40:22.402278 24530 net.cpp:131] Top shape: (1)
I0428 19:40:22.402282 24530 net.cpp:134]     with loss weight 1
I0428 19:40:22.402297 24530 net.cpp:139] Memory required for data: 707844
I0428 19:40:22.402300 24530 net.cpp:200] loss needs backward computation.
I0428 19:40:22.402303 24530 net.cpp:200] relu3 needs backward computation.
I0428 19:40:22.402307 24530 net.cpp:200] ip3 needs backward computation.
I0428 19:40:22.402310 24530 net.cpp:200] relu2 needs backward computation.
I0428 19:40:22.402313 24530 net.cpp:200] ip2 needs backward computation.
I0428 19:40:22.402317 24530 net.cpp:200] relu1 needs backward computation.
I0428 19:40:22.402318 24530 net.cpp:200] ip1 needs backward computation.
I0428 19:40:22.402321 24530 net.cpp:200] pool1 needs backward computation.
I0428 19:40:22.402324 24530 net.cpp:200] conv1 needs backward computation.
I0428 19:40:22.402328 24530 net.cpp:200] pool0 needs backward computation.
I0428 19:40:22.402331 24530 net.cpp:200] conv0 needs backward computation.
I0428 19:40:22.402334 24530 net.cpp:202] mnist does not need backward computation.
I0428 19:40:22.402338 24530 net.cpp:244] This network produces output loss
I0428 19:40:22.402348 24530 net.cpp:257] Network initialization done.
I0428 19:40:22.402696 24530 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test332.prototxt
I0428 19:40:22.402725 24530 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:40:22.402812 24530 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:40:22.402889 24530 layer_factory.hpp:77] Creating layer mnist
I0428 19:40:22.402935 24530 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:40:22.402946 24530 net.cpp:86] Creating Layer mnist
I0428 19:40:22.402951 24530 net.cpp:382] mnist -> data
I0428 19:40:22.402959 24530 net.cpp:382] mnist -> label
I0428 19:40:22.403041 24530 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:40:22.405138 24530 net.cpp:124] Setting up mnist
I0428 19:40:22.405151 24530 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:40:22.405158 24530 net.cpp:131] Top shape: 100 (100)
I0428 19:40:22.405160 24530 net.cpp:139] Memory required for data: 314000
I0428 19:40:22.405164 24530 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:40:22.405170 24530 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:40:22.405174 24530 net.cpp:408] label_mnist_1_split <- label
I0428 19:40:22.405179 24530 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:40:22.405185 24530 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:40:22.405222 24530 net.cpp:124] Setting up label_mnist_1_split
I0428 19:40:22.405227 24530 net.cpp:131] Top shape: 100 (100)
I0428 19:40:22.405231 24530 net.cpp:131] Top shape: 100 (100)
I0428 19:40:22.405234 24530 net.cpp:139] Memory required for data: 314800
I0428 19:40:22.405237 24530 layer_factory.hpp:77] Creating layer conv0
I0428 19:40:22.405246 24530 net.cpp:86] Creating Layer conv0
I0428 19:40:22.405248 24530 net.cpp:408] conv0 <- data
I0428 19:40:22.405253 24530 net.cpp:382] conv0 -> conv0
I0428 19:40:22.406919 24530 net.cpp:124] Setting up conv0
I0428 19:40:22.406932 24530 net.cpp:131] Top shape: 100 2 24 24 (115200)
I0428 19:40:22.406936 24530 net.cpp:139] Memory required for data: 775600
I0428 19:40:22.406946 24530 layer_factory.hpp:77] Creating layer pool0
I0428 19:40:22.406952 24530 net.cpp:86] Creating Layer pool0
I0428 19:40:22.406956 24530 net.cpp:408] pool0 <- conv0
I0428 19:40:22.406961 24530 net.cpp:382] pool0 -> pool0
I0428 19:40:22.406996 24530 net.cpp:124] Setting up pool0
I0428 19:40:22.407002 24530 net.cpp:131] Top shape: 100 2 12 12 (28800)
I0428 19:40:22.407006 24530 net.cpp:139] Memory required for data: 890800
I0428 19:40:22.407008 24530 layer_factory.hpp:77] Creating layer conv1
I0428 19:40:22.407016 24530 net.cpp:86] Creating Layer conv1
I0428 19:40:22.407021 24530 net.cpp:408] conv1 <- pool0
I0428 19:40:22.407024 24530 net.cpp:382] conv1 -> conv1
I0428 19:40:22.408452 24530 net.cpp:124] Setting up conv1
I0428 19:40:22.408466 24530 net.cpp:131] Top shape: 100 5 8 8 (32000)
I0428 19:40:22.408470 24530 net.cpp:139] Memory required for data: 1018800
I0428 19:40:22.408478 24530 layer_factory.hpp:77] Creating layer pool1
I0428 19:40:22.408498 24530 net.cpp:86] Creating Layer pool1
I0428 19:40:22.408501 24530 net.cpp:408] pool1 <- conv1
I0428 19:40:22.408506 24530 net.cpp:382] pool1 -> pool1
I0428 19:40:22.408543 24530 net.cpp:124] Setting up pool1
I0428 19:40:22.408550 24530 net.cpp:131] Top shape: 100 5 4 4 (8000)
I0428 19:40:22.408552 24530 net.cpp:139] Memory required for data: 1050800
I0428 19:40:22.408555 24530 layer_factory.hpp:77] Creating layer ip1
I0428 19:40:22.408561 24530 net.cpp:86] Creating Layer ip1
I0428 19:40:22.408565 24530 net.cpp:408] ip1 <- pool1
I0428 19:40:22.408570 24530 net.cpp:382] ip1 -> ip1
I0428 19:40:22.408665 24530 net.cpp:124] Setting up ip1
I0428 19:40:22.408674 24530 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:40:22.408694 24530 net.cpp:139] Memory required for data: 1054800
I0428 19:40:22.408701 24530 layer_factory.hpp:77] Creating layer relu1
I0428 19:40:22.408706 24530 net.cpp:86] Creating Layer relu1
I0428 19:40:22.408715 24530 net.cpp:408] relu1 <- ip1
I0428 19:40:22.408720 24530 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:40:22.408954 24530 net.cpp:124] Setting up relu1
I0428 19:40:22.408967 24530 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:40:22.408972 24530 net.cpp:139] Memory required for data: 1058800
I0428 19:40:22.408978 24530 layer_factory.hpp:77] Creating layer ip2
I0428 19:40:22.408991 24530 net.cpp:86] Creating Layer ip2
I0428 19:40:22.409011 24530 net.cpp:408] ip2 <- ip1
I0428 19:40:22.409018 24530 net.cpp:382] ip2 -> ip2
I0428 19:40:22.409137 24530 net.cpp:124] Setting up ip2
I0428 19:40:22.409160 24530 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:40:22.409164 24530 net.cpp:139] Memory required for data: 1078800
I0428 19:40:22.409170 24530 layer_factory.hpp:77] Creating layer relu2
I0428 19:40:22.409190 24530 net.cpp:86] Creating Layer relu2
I0428 19:40:22.409194 24530 net.cpp:408] relu2 <- ip2
I0428 19:40:22.409198 24530 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:40:22.409364 24530 net.cpp:124] Setting up relu2
I0428 19:40:22.409373 24530 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:40:22.409376 24530 net.cpp:139] Memory required for data: 1098800
I0428 19:40:22.409379 24530 layer_factory.hpp:77] Creating layer ip3
I0428 19:40:22.409385 24530 net.cpp:86] Creating Layer ip3
I0428 19:40:22.409389 24530 net.cpp:408] ip3 <- ip2
I0428 19:40:22.409394 24530 net.cpp:382] ip3 -> ip3
I0428 19:40:22.409520 24530 net.cpp:124] Setting up ip3
I0428 19:40:22.409528 24530 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:40:22.409531 24530 net.cpp:139] Memory required for data: 1102800
I0428 19:40:22.409539 24530 layer_factory.hpp:77] Creating layer relu3
I0428 19:40:22.409543 24530 net.cpp:86] Creating Layer relu3
I0428 19:40:22.409548 24530 net.cpp:408] relu3 <- ip3
I0428 19:40:22.409554 24530 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:40:22.410342 24530 net.cpp:124] Setting up relu3
I0428 19:40:22.410356 24530 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:40:22.410359 24530 net.cpp:139] Memory required for data: 1106800
I0428 19:40:22.410363 24530 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:40:22.410368 24530 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:40:22.410372 24530 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:40:22.410378 24530 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:40:22.410384 24530 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:40:22.410426 24530 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:40:22.410431 24530 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:40:22.410435 24530 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:40:22.410439 24530 net.cpp:139] Memory required for data: 1114800
I0428 19:40:22.410440 24530 layer_factory.hpp:77] Creating layer accuracy
I0428 19:40:22.410446 24530 net.cpp:86] Creating Layer accuracy
I0428 19:40:22.410450 24530 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:40:22.410454 24530 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:40:22.410459 24530 net.cpp:382] accuracy -> accuracy
I0428 19:40:22.410466 24530 net.cpp:124] Setting up accuracy
I0428 19:40:22.410470 24530 net.cpp:131] Top shape: (1)
I0428 19:40:22.410473 24530 net.cpp:139] Memory required for data: 1114804
I0428 19:40:22.410476 24530 layer_factory.hpp:77] Creating layer loss
I0428 19:40:22.410480 24530 net.cpp:86] Creating Layer loss
I0428 19:40:22.410485 24530 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:40:22.410488 24530 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:40:22.410492 24530 net.cpp:382] loss -> loss
I0428 19:40:22.410498 24530 layer_factory.hpp:77] Creating layer loss
I0428 19:40:22.410730 24530 net.cpp:124] Setting up loss
I0428 19:40:22.410739 24530 net.cpp:131] Top shape: (1)
I0428 19:40:22.410742 24530 net.cpp:134]     with loss weight 1
I0428 19:40:22.410749 24530 net.cpp:139] Memory required for data: 1114808
I0428 19:40:22.410763 24530 net.cpp:200] loss needs backward computation.
I0428 19:40:22.410768 24530 net.cpp:202] accuracy does not need backward computation.
I0428 19:40:22.410771 24530 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:40:22.410774 24530 net.cpp:200] relu3 needs backward computation.
I0428 19:40:22.410778 24530 net.cpp:200] ip3 needs backward computation.
I0428 19:40:22.410780 24530 net.cpp:200] relu2 needs backward computation.
I0428 19:40:22.410784 24530 net.cpp:200] ip2 needs backward computation.
I0428 19:40:22.410786 24530 net.cpp:200] relu1 needs backward computation.
I0428 19:40:22.410789 24530 net.cpp:200] ip1 needs backward computation.
I0428 19:40:22.410792 24530 net.cpp:200] pool1 needs backward computation.
I0428 19:40:22.410795 24530 net.cpp:200] conv1 needs backward computation.
I0428 19:40:22.410799 24530 net.cpp:200] pool0 needs backward computation.
I0428 19:40:22.410804 24530 net.cpp:200] conv0 needs backward computation.
I0428 19:40:22.410809 24530 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:40:22.410815 24530 net.cpp:202] mnist does not need backward computation.
I0428 19:40:22.410818 24530 net.cpp:244] This network produces output accuracy
I0428 19:40:22.410821 24530 net.cpp:244] This network produces output loss
I0428 19:40:22.410833 24530 net.cpp:257] Network initialization done.
I0428 19:40:22.410873 24530 solver.cpp:56] Solver scaffolding done.
I0428 19:40:22.411203 24530 caffe.cpp:248] Starting Optimization
I0428 19:40:22.411211 24530 solver.cpp:273] Solving LeNet
I0428 19:40:22.411214 24530 solver.cpp:274] Learning Rate Policy: inv
I0428 19:40:22.412070 24530 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:40:22.413974 24530 blocking_queue.cpp:49] Waiting for data
I0428 19:40:22.487388 24538 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:40:22.487856 24530 solver.cpp:398]     Test net output #0: accuracy = 0.1036
I0428 19:40:22.487876 24530 solver.cpp:398]     Test net output #1: loss = 2.29988 (* 1 = 2.29988 loss)
I0428 19:40:22.489766 24530 solver.cpp:219] Iteration 0 (0 iter/s, 0.0785243s/100 iters), loss = 2.29663
I0428 19:40:22.489799 24530 solver.cpp:238]     Train net output #0: loss = 2.29663 (* 1 = 2.29663 loss)
I0428 19:40:22.489811 24530 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:40:22.561839 24530 solver.cpp:219] Iteration 100 (1388.32 iter/s, 0.0720294s/100 iters), loss = 1.68449
I0428 19:40:22.561866 24530 solver.cpp:238]     Train net output #0: loss = 1.68449 (* 1 = 1.68449 loss)
I0428 19:40:22.561887 24530 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:40:22.631680 24530 solver.cpp:219] Iteration 200 (1432.56 iter/s, 0.0698051s/100 iters), loss = 1.07094
I0428 19:40:22.631721 24530 solver.cpp:238]     Train net output #0: loss = 1.07094 (* 1 = 1.07094 loss)
I0428 19:40:22.631727 24530 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:40:22.700464 24530 solver.cpp:219] Iteration 300 (1454.69 iter/s, 0.0687432s/100 iters), loss = 1.04635
I0428 19:40:22.700502 24530 solver.cpp:238]     Train net output #0: loss = 1.04635 (* 1 = 1.04635 loss)
I0428 19:40:22.700508 24530 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:40:22.770684 24530 solver.cpp:219] Iteration 400 (1424.81 iter/s, 0.070185s/100 iters), loss = 0.415087
I0428 19:40:22.770709 24530 solver.cpp:238]     Train net output #0: loss = 0.415087 (* 1 = 0.415087 loss)
I0428 19:40:22.770715 24530 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:40:22.839378 24530 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:40:22.914448 24538 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:40:22.914911 24530 solver.cpp:398]     Test net output #0: accuracy = 0.908
I0428 19:40:22.914932 24530 solver.cpp:398]     Test net output #1: loss = 0.299481 (* 1 = 0.299481 loss)
I0428 19:40:22.915783 24530 solver.cpp:219] Iteration 500 (689.366 iter/s, 0.145061s/100 iters), loss = 0.321772
I0428 19:40:22.915809 24530 solver.cpp:238]     Train net output #0: loss = 0.321772 (* 1 = 0.321772 loss)
I0428 19:40:22.915832 24530 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:40:22.989442 24530 solver.cpp:219] Iteration 600 (1358.42 iter/s, 0.0736152s/100 iters), loss = 0.225769
I0428 19:40:22.989481 24530 solver.cpp:238]     Train net output #0: loss = 0.225769 (* 1 = 0.225769 loss)
I0428 19:40:22.989488 24530 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:40:23.060305 24530 solver.cpp:219] Iteration 700 (1411.95 iter/s, 0.0708239s/100 iters), loss = 0.359776
I0428 19:40:23.060348 24530 solver.cpp:238]     Train net output #0: loss = 0.359776 (* 1 = 0.359776 loss)
I0428 19:40:23.060354 24530 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:40:23.130291 24530 solver.cpp:219] Iteration 800 (1429.94 iter/s, 0.069933s/100 iters), loss = 0.335849
I0428 19:40:23.130319 24530 solver.cpp:238]     Train net output #0: loss = 0.335849 (* 1 = 0.335849 loss)
I0428 19:40:23.130326 24530 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:40:23.200356 24530 solver.cpp:219] Iteration 900 (1428.37 iter/s, 0.0700099s/100 iters), loss = 0.235012
I0428 19:40:23.200397 24530 solver.cpp:238]     Train net output #0: loss = 0.235012 (* 1 = 0.235012 loss)
I0428 19:40:23.200403 24530 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:40:23.223825 24536 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:40:23.269734 24530 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:40:23.270421 24530 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:40:23.270912 24530 solver.cpp:311] Iteration 1000, loss = 0.117202
I0428 19:40:23.270927 24530 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:40:23.346020 24538 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:40:23.346489 24530 solver.cpp:398]     Test net output #0: accuracy = 0.9511
I0428 19:40:23.346511 24530 solver.cpp:398]     Test net output #1: loss = 0.16224 (* 1 = 0.16224 loss)
I0428 19:40:23.346516 24530 solver.cpp:316] Optimization Done.
I0428 19:40:23.346519 24530 caffe.cpp:259] Optimization Done.
