I0428 20:32:34.815796  4122 caffe.cpp:218] Using GPUs 0
I0428 20:32:34.853005  4122 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:32:35.366246  4122 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1574.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:32:35.366385  4122 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1574.prototxt
I0428 20:32:35.366801  4122 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:32:35.366819  4122 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:32:35.366924  4122 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:32:35.367004  4122 layer_factory.hpp:77] Creating layer mnist
I0428 20:32:35.367106  4122 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:32:35.367130  4122 net.cpp:86] Creating Layer mnist
I0428 20:32:35.367136  4122 net.cpp:382] mnist -> data
I0428 20:32:35.367159  4122 net.cpp:382] mnist -> label
I0428 20:32:35.368248  4122 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:32:35.370718  4122 net.cpp:124] Setting up mnist
I0428 20:32:35.370738  4122 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:32:35.370743  4122 net.cpp:131] Top shape: 64 (64)
I0428 20:32:35.370748  4122 net.cpp:139] Memory required for data: 200960
I0428 20:32:35.370754  4122 layer_factory.hpp:77] Creating layer conv0
I0428 20:32:35.370782  4122 net.cpp:86] Creating Layer conv0
I0428 20:32:35.370803  4122 net.cpp:408] conv0 <- data
I0428 20:32:35.370816  4122 net.cpp:382] conv0 -> conv0
I0428 20:32:35.662302  4122 net.cpp:124] Setting up conv0
I0428 20:32:35.662329  4122 net.cpp:131] Top shape: 64 100 24 24 (3686400)
I0428 20:32:35.662334  4122 net.cpp:139] Memory required for data: 14946560
I0428 20:32:35.662351  4122 layer_factory.hpp:77] Creating layer pool0
I0428 20:32:35.662365  4122 net.cpp:86] Creating Layer pool0
I0428 20:32:35.662370  4122 net.cpp:408] pool0 <- conv0
I0428 20:32:35.662376  4122 net.cpp:382] pool0 -> pool0
I0428 20:32:35.662430  4122 net.cpp:124] Setting up pool0
I0428 20:32:35.662437  4122 net.cpp:131] Top shape: 64 100 12 12 (921600)
I0428 20:32:35.662441  4122 net.cpp:139] Memory required for data: 18632960
I0428 20:32:35.662444  4122 layer_factory.hpp:77] Creating layer conv1
I0428 20:32:35.662456  4122 net.cpp:86] Creating Layer conv1
I0428 20:32:35.662461  4122 net.cpp:408] conv1 <- pool0
I0428 20:32:35.662466  4122 net.cpp:382] conv1 -> conv1
I0428 20:32:35.664834  4122 net.cpp:124] Setting up conv1
I0428 20:32:35.664849  4122 net.cpp:131] Top shape: 64 25 8 8 (102400)
I0428 20:32:35.664854  4122 net.cpp:139] Memory required for data: 19042560
I0428 20:32:35.664862  4122 layer_factory.hpp:77] Creating layer pool1
I0428 20:32:35.664870  4122 net.cpp:86] Creating Layer pool1
I0428 20:32:35.664875  4122 net.cpp:408] pool1 <- conv1
I0428 20:32:35.664881  4122 net.cpp:382] pool1 -> pool1
I0428 20:32:35.664921  4122 net.cpp:124] Setting up pool1
I0428 20:32:35.664927  4122 net.cpp:131] Top shape: 64 25 4 4 (25600)
I0428 20:32:35.664930  4122 net.cpp:139] Memory required for data: 19144960
I0428 20:32:35.664933  4122 layer_factory.hpp:77] Creating layer ip1
I0428 20:32:35.664940  4122 net.cpp:86] Creating Layer ip1
I0428 20:32:35.664944  4122 net.cpp:408] ip1 <- pool1
I0428 20:32:35.664949  4122 net.cpp:382] ip1 -> ip1
I0428 20:32:35.665078  4122 net.cpp:124] Setting up ip1
I0428 20:32:35.665087  4122 net.cpp:131] Top shape: 64 10 (640)
I0428 20:32:35.665091  4122 net.cpp:139] Memory required for data: 19147520
I0428 20:32:35.665098  4122 layer_factory.hpp:77] Creating layer relu1
I0428 20:32:35.665103  4122 net.cpp:86] Creating Layer relu1
I0428 20:32:35.665107  4122 net.cpp:408] relu1 <- ip1
I0428 20:32:35.665112  4122 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:32:35.665292  4122 net.cpp:124] Setting up relu1
I0428 20:32:35.665300  4122 net.cpp:131] Top shape: 64 10 (640)
I0428 20:32:35.665304  4122 net.cpp:139] Memory required for data: 19150080
I0428 20:32:35.665307  4122 layer_factory.hpp:77] Creating layer ip2
I0428 20:32:35.665314  4122 net.cpp:86] Creating Layer ip2
I0428 20:32:35.665318  4122 net.cpp:408] ip2 <- ip1
I0428 20:32:35.665333  4122 net.cpp:382] ip2 -> ip2
I0428 20:32:35.665429  4122 net.cpp:124] Setting up ip2
I0428 20:32:35.665436  4122 net.cpp:131] Top shape: 64 10 (640)
I0428 20:32:35.665439  4122 net.cpp:139] Memory required for data: 19152640
I0428 20:32:35.665446  4122 layer_factory.hpp:77] Creating layer relu2
I0428 20:32:35.665452  4122 net.cpp:86] Creating Layer relu2
I0428 20:32:35.665454  4122 net.cpp:408] relu2 <- ip2
I0428 20:32:35.665458  4122 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:32:35.666213  4122 net.cpp:124] Setting up relu2
I0428 20:32:35.666225  4122 net.cpp:131] Top shape: 64 10 (640)
I0428 20:32:35.666229  4122 net.cpp:139] Memory required for data: 19155200
I0428 20:32:35.666232  4122 layer_factory.hpp:77] Creating layer ip3
I0428 20:32:35.666240  4122 net.cpp:86] Creating Layer ip3
I0428 20:32:35.666244  4122 net.cpp:408] ip3 <- ip2
I0428 20:32:35.666249  4122 net.cpp:382] ip3 -> ip3
I0428 20:32:35.666352  4122 net.cpp:124] Setting up ip3
I0428 20:32:35.666359  4122 net.cpp:131] Top shape: 64 10 (640)
I0428 20:32:35.666363  4122 net.cpp:139] Memory required for data: 19157760
I0428 20:32:35.666370  4122 layer_factory.hpp:77] Creating layer relu3
I0428 20:32:35.666376  4122 net.cpp:86] Creating Layer relu3
I0428 20:32:35.666379  4122 net.cpp:408] relu3 <- ip3
I0428 20:32:35.666383  4122 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:32:35.666558  4122 net.cpp:124] Setting up relu3
I0428 20:32:35.666568  4122 net.cpp:131] Top shape: 64 10 (640)
I0428 20:32:35.666571  4122 net.cpp:139] Memory required for data: 19160320
I0428 20:32:35.666574  4122 layer_factory.hpp:77] Creating layer loss
I0428 20:32:35.666580  4122 net.cpp:86] Creating Layer loss
I0428 20:32:35.666584  4122 net.cpp:408] loss <- ip3
I0428 20:32:35.666587  4122 net.cpp:408] loss <- label
I0428 20:32:35.666594  4122 net.cpp:382] loss -> loss
I0428 20:32:35.666609  4122 layer_factory.hpp:77] Creating layer loss
I0428 20:32:35.666838  4122 net.cpp:124] Setting up loss
I0428 20:32:35.666848  4122 net.cpp:131] Top shape: (1)
I0428 20:32:35.666852  4122 net.cpp:134]     with loss weight 1
I0428 20:32:35.666867  4122 net.cpp:139] Memory required for data: 19160324
I0428 20:32:35.666872  4122 net.cpp:200] loss needs backward computation.
I0428 20:32:35.666874  4122 net.cpp:200] relu3 needs backward computation.
I0428 20:32:35.666878  4122 net.cpp:200] ip3 needs backward computation.
I0428 20:32:35.666882  4122 net.cpp:200] relu2 needs backward computation.
I0428 20:32:35.666884  4122 net.cpp:200] ip2 needs backward computation.
I0428 20:32:35.666887  4122 net.cpp:200] relu1 needs backward computation.
I0428 20:32:35.666890  4122 net.cpp:200] ip1 needs backward computation.
I0428 20:32:35.666893  4122 net.cpp:200] pool1 needs backward computation.
I0428 20:32:35.666896  4122 net.cpp:200] conv1 needs backward computation.
I0428 20:32:35.666900  4122 net.cpp:200] pool0 needs backward computation.
I0428 20:32:35.666903  4122 net.cpp:200] conv0 needs backward computation.
I0428 20:32:35.666906  4122 net.cpp:202] mnist does not need backward computation.
I0428 20:32:35.666909  4122 net.cpp:244] This network produces output loss
I0428 20:32:35.666919  4122 net.cpp:257] Network initialization done.
I0428 20:32:35.667259  4122 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1574.prototxt
I0428 20:32:35.667287  4122 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:32:35.667383  4122 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:32:35.667467  4122 layer_factory.hpp:77] Creating layer mnist
I0428 20:32:35.667517  4122 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:32:35.667531  4122 net.cpp:86] Creating Layer mnist
I0428 20:32:35.667536  4122 net.cpp:382] mnist -> data
I0428 20:32:35.667543  4122 net.cpp:382] mnist -> label
I0428 20:32:35.667634  4122 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:32:35.669888  4122 net.cpp:124] Setting up mnist
I0428 20:32:35.669903  4122 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:32:35.669909  4122 net.cpp:131] Top shape: 100 (100)
I0428 20:32:35.669911  4122 net.cpp:139] Memory required for data: 314000
I0428 20:32:35.669915  4122 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:32:35.669922  4122 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:32:35.669926  4122 net.cpp:408] label_mnist_1_split <- label
I0428 20:32:35.669931  4122 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:32:35.669939  4122 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:32:35.669978  4122 net.cpp:124] Setting up label_mnist_1_split
I0428 20:32:35.669984  4122 net.cpp:131] Top shape: 100 (100)
I0428 20:32:35.669988  4122 net.cpp:131] Top shape: 100 (100)
I0428 20:32:35.669991  4122 net.cpp:139] Memory required for data: 314800
I0428 20:32:35.669994  4122 layer_factory.hpp:77] Creating layer conv0
I0428 20:32:35.670002  4122 net.cpp:86] Creating Layer conv0
I0428 20:32:35.670006  4122 net.cpp:408] conv0 <- data
I0428 20:32:35.670011  4122 net.cpp:382] conv0 -> conv0
I0428 20:32:35.671763  4122 net.cpp:124] Setting up conv0
I0428 20:32:35.671778  4122 net.cpp:131] Top shape: 100 100 24 24 (5760000)
I0428 20:32:35.671782  4122 net.cpp:139] Memory required for data: 23354800
I0428 20:32:35.671792  4122 layer_factory.hpp:77] Creating layer pool0
I0428 20:32:35.671798  4122 net.cpp:86] Creating Layer pool0
I0428 20:32:35.671802  4122 net.cpp:408] pool0 <- conv0
I0428 20:32:35.671808  4122 net.cpp:382] pool0 -> pool0
I0428 20:32:35.671847  4122 net.cpp:124] Setting up pool0
I0428 20:32:35.671852  4122 net.cpp:131] Top shape: 100 100 12 12 (1440000)
I0428 20:32:35.671855  4122 net.cpp:139] Memory required for data: 29114800
I0428 20:32:35.671859  4122 layer_factory.hpp:77] Creating layer conv1
I0428 20:32:35.671869  4122 net.cpp:86] Creating Layer conv1
I0428 20:32:35.671872  4122 net.cpp:408] conv1 <- pool0
I0428 20:32:35.671878  4122 net.cpp:382] conv1 -> conv1
I0428 20:32:35.673866  4122 net.cpp:124] Setting up conv1
I0428 20:32:35.673882  4122 net.cpp:131] Top shape: 100 25 8 8 (160000)
I0428 20:32:35.673885  4122 net.cpp:139] Memory required for data: 29754800
I0428 20:32:35.673894  4122 layer_factory.hpp:77] Creating layer pool1
I0428 20:32:35.673902  4122 net.cpp:86] Creating Layer pool1
I0428 20:32:35.673907  4122 net.cpp:408] pool1 <- conv1
I0428 20:32:35.673912  4122 net.cpp:382] pool1 -> pool1
I0428 20:32:35.673952  4122 net.cpp:124] Setting up pool1
I0428 20:32:35.673960  4122 net.cpp:131] Top shape: 100 25 4 4 (40000)
I0428 20:32:35.673964  4122 net.cpp:139] Memory required for data: 29914800
I0428 20:32:35.673966  4122 layer_factory.hpp:77] Creating layer ip1
I0428 20:32:35.673979  4122 net.cpp:86] Creating Layer ip1
I0428 20:32:35.673982  4122 net.cpp:408] ip1 <- pool1
I0428 20:32:35.673988  4122 net.cpp:382] ip1 -> ip1
I0428 20:32:35.674121  4122 net.cpp:124] Setting up ip1
I0428 20:32:35.674141  4122 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:32:35.674145  4122 net.cpp:139] Memory required for data: 29918800
I0428 20:32:35.674154  4122 layer_factory.hpp:77] Creating layer relu1
I0428 20:32:35.674165  4122 net.cpp:86] Creating Layer relu1
I0428 20:32:35.674168  4122 net.cpp:408] relu1 <- ip1
I0428 20:32:35.674175  4122 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:32:35.674353  4122 net.cpp:124] Setting up relu1
I0428 20:32:35.674363  4122 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:32:35.674366  4122 net.cpp:139] Memory required for data: 29922800
I0428 20:32:35.674370  4122 layer_factory.hpp:77] Creating layer ip2
I0428 20:32:35.674378  4122 net.cpp:86] Creating Layer ip2
I0428 20:32:35.674381  4122 net.cpp:408] ip2 <- ip1
I0428 20:32:35.674386  4122 net.cpp:382] ip2 -> ip2
I0428 20:32:35.674491  4122 net.cpp:124] Setting up ip2
I0428 20:32:35.674499  4122 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:32:35.674501  4122 net.cpp:139] Memory required for data: 29926800
I0428 20:32:35.674507  4122 layer_factory.hpp:77] Creating layer relu2
I0428 20:32:35.674511  4122 net.cpp:86] Creating Layer relu2
I0428 20:32:35.674515  4122 net.cpp:408] relu2 <- ip2
I0428 20:32:35.674528  4122 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:32:35.674743  4122 net.cpp:124] Setting up relu2
I0428 20:32:35.674752  4122 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:32:35.674756  4122 net.cpp:139] Memory required for data: 29930800
I0428 20:32:35.674759  4122 layer_factory.hpp:77] Creating layer ip3
I0428 20:32:35.674765  4122 net.cpp:86] Creating Layer ip3
I0428 20:32:35.674769  4122 net.cpp:408] ip3 <- ip2
I0428 20:32:35.674775  4122 net.cpp:382] ip3 -> ip3
I0428 20:32:35.674937  4122 net.cpp:124] Setting up ip3
I0428 20:32:35.674947  4122 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:32:35.674950  4122 net.cpp:139] Memory required for data: 29934800
I0428 20:32:35.674958  4122 layer_factory.hpp:77] Creating layer relu3
I0428 20:32:35.674963  4122 net.cpp:86] Creating Layer relu3
I0428 20:32:35.674967  4122 net.cpp:408] relu3 <- ip3
I0428 20:32:35.674970  4122 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:32:35.675745  4122 net.cpp:124] Setting up relu3
I0428 20:32:35.675760  4122 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:32:35.675763  4122 net.cpp:139] Memory required for data: 29938800
I0428 20:32:35.675767  4122 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:32:35.675775  4122 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:32:35.675778  4122 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:32:35.675783  4122 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:32:35.675791  4122 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:32:35.675837  4122 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:32:35.675843  4122 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:32:35.675846  4122 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:32:35.675849  4122 net.cpp:139] Memory required for data: 29946800
I0428 20:32:35.675853  4122 layer_factory.hpp:77] Creating layer accuracy
I0428 20:32:35.675858  4122 net.cpp:86] Creating Layer accuracy
I0428 20:32:35.675863  4122 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:32:35.675866  4122 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:32:35.675871  4122 net.cpp:382] accuracy -> accuracy
I0428 20:32:35.675879  4122 net.cpp:124] Setting up accuracy
I0428 20:32:35.675882  4122 net.cpp:131] Top shape: (1)
I0428 20:32:35.675885  4122 net.cpp:139] Memory required for data: 29946804
I0428 20:32:35.675889  4122 layer_factory.hpp:77] Creating layer loss
I0428 20:32:35.675892  4122 net.cpp:86] Creating Layer loss
I0428 20:32:35.675896  4122 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:32:35.675900  4122 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:32:35.675906  4122 net.cpp:382] loss -> loss
I0428 20:32:35.675912  4122 layer_factory.hpp:77] Creating layer loss
I0428 20:32:35.676239  4122 net.cpp:124] Setting up loss
I0428 20:32:35.676249  4122 net.cpp:131] Top shape: (1)
I0428 20:32:35.676252  4122 net.cpp:134]     with loss weight 1
I0428 20:32:35.676268  4122 net.cpp:139] Memory required for data: 29946808
I0428 20:32:35.676272  4122 net.cpp:200] loss needs backward computation.
I0428 20:32:35.676276  4122 net.cpp:202] accuracy does not need backward computation.
I0428 20:32:35.676287  4122 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:32:35.676290  4122 net.cpp:200] relu3 needs backward computation.
I0428 20:32:35.676295  4122 net.cpp:200] ip3 needs backward computation.
I0428 20:32:35.676297  4122 net.cpp:200] relu2 needs backward computation.
I0428 20:32:35.676301  4122 net.cpp:200] ip2 needs backward computation.
I0428 20:32:35.676303  4122 net.cpp:200] relu1 needs backward computation.
I0428 20:32:35.676306  4122 net.cpp:200] ip1 needs backward computation.
I0428 20:32:35.676309  4122 net.cpp:200] pool1 needs backward computation.
I0428 20:32:35.676312  4122 net.cpp:200] conv1 needs backward computation.
I0428 20:32:35.676316  4122 net.cpp:200] pool0 needs backward computation.
I0428 20:32:35.676318  4122 net.cpp:200] conv0 needs backward computation.
I0428 20:32:35.676322  4122 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:32:35.676332  4122 net.cpp:202] mnist does not need backward computation.
I0428 20:32:35.676334  4122 net.cpp:244] This network produces output accuracy
I0428 20:32:35.676338  4122 net.cpp:244] This network produces output loss
I0428 20:32:35.676350  4122 net.cpp:257] Network initialization done.
I0428 20:32:35.676394  4122 solver.cpp:56] Solver scaffolding done.
I0428 20:32:35.676746  4122 caffe.cpp:248] Starting Optimization
I0428 20:32:35.676753  4122 solver.cpp:273] Solving LeNet
I0428 20:32:35.676764  4122 solver.cpp:274] Learning Rate Policy: inv
I0428 20:32:35.676977  4122 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:32:35.774130  4129 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:32:35.776757  4122 solver.cpp:398]     Test net output #0: accuracy = 0.1344
I0428 20:32:35.776777  4122 solver.cpp:398]     Test net output #1: loss = 2.33088 (* 1 = 2.33088 loss)
I0428 20:32:35.781177  4122 solver.cpp:219] Iteration 0 (-7.79122e-43 iter/s, 0.104369s/100 iters), loss = 2.33176
I0428 20:32:35.781208  4122 solver.cpp:238]     Train net output #0: loss = 2.33176 (* 1 = 2.33176 loss)
I0428 20:32:35.781225  4122 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:32:36.001595  4122 solver.cpp:219] Iteration 100 (453.795 iter/s, 0.220364s/100 iters), loss = 0.729459
I0428 20:32:36.001641  4122 solver.cpp:238]     Train net output #0: loss = 0.729459 (* 1 = 0.729459 loss)
I0428 20:32:36.001653  4122 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:32:36.231756  4122 solver.cpp:219] Iteration 200 (434.607 iter/s, 0.230093s/100 iters), loss = 0.553355
I0428 20:32:36.231807  4122 solver.cpp:238]     Train net output #0: loss = 0.553355 (* 1 = 0.553355 loss)
I0428 20:32:36.231822  4122 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:32:36.460574  4122 solver.cpp:219] Iteration 300 (437.158 iter/s, 0.22875s/100 iters), loss = 0.332355
I0428 20:32:36.460623  4122 solver.cpp:238]     Train net output #0: loss = 0.332355 (* 1 = 0.332355 loss)
I0428 20:32:36.460634  4122 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:32:36.689458  4122 solver.cpp:219] Iteration 400 (437.033 iter/s, 0.228816s/100 iters), loss = 0.273427
I0428 20:32:36.689513  4122 solver.cpp:238]     Train net output #0: loss = 0.273427 (* 1 = 0.273427 loss)
I0428 20:32:36.689524  4122 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:32:36.913717  4122 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:32:37.017163  4129 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:32:37.020741  4122 solver.cpp:398]     Test net output #0: accuracy = 0.9498
I0428 20:32:37.020768  4122 solver.cpp:398]     Test net output #1: loss = 0.168674 (* 1 = 0.168674 loss)
I0428 20:32:37.022761  4122 solver.cpp:219] Iteration 500 (300.092 iter/s, 0.333231s/100 iters), loss = 0.174844
I0428 20:32:37.022790  4122 solver.cpp:238]     Train net output #0: loss = 0.174844 (* 1 = 0.174844 loss)
I0428 20:32:37.022819  4122 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:32:37.245177  4122 solver.cpp:219] Iteration 600 (449.706 iter/s, 0.222368s/100 iters), loss = 0.085564
I0428 20:32:37.245220  4122 solver.cpp:238]     Train net output #0: loss = 0.0855641 (* 1 = 0.0855641 loss)
I0428 20:32:37.245230  4122 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:32:37.464054  4122 solver.cpp:219] Iteration 700 (456.999 iter/s, 0.218819s/100 iters), loss = 0.239001
I0428 20:32:37.464092  4122 solver.cpp:238]     Train net output #0: loss = 0.239001 (* 1 = 0.239001 loss)
I0428 20:32:37.464102  4122 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:32:37.681457  4122 solver.cpp:219] Iteration 800 (460.098 iter/s, 0.217345s/100 iters), loss = 0.323188
I0428 20:32:37.681514  4122 solver.cpp:238]     Train net output #0: loss = 0.323188 (* 1 = 0.323188 loss)
I0428 20:32:37.681529  4122 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:32:37.919059  4122 solver.cpp:219] Iteration 900 (421.001 iter/s, 0.237529s/100 iters), loss = 0.211182
I0428 20:32:37.919111  4122 solver.cpp:238]     Train net output #0: loss = 0.211182 (* 1 = 0.211182 loss)
I0428 20:32:37.919124  4122 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:32:37.998451  4128 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:32:38.150120  4122 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:32:38.153614  4122 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:32:38.156152  4122 solver.cpp:311] Iteration 1000, loss = 0.137758
I0428 20:32:38.156180  4122 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:32:38.259387  4129 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:32:38.263180  4122 solver.cpp:398]     Test net output #0: accuracy = 0.9672
I0428 20:32:38.263205  4122 solver.cpp:398]     Test net output #1: loss = 0.108784 (* 1 = 0.108784 loss)
I0428 20:32:38.263211  4122 solver.cpp:316] Optimization Done.
I0428 20:32:38.263217  4122 caffe.cpp:259] Optimization Done.
