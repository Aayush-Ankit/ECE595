I0428 20:17:12.494436   821 caffe.cpp:218] Using GPUs 0
I0428 20:17:12.531740   821 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:17:13.046010   821 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1289.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:17:13.046149   821 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1289.prototxt
I0428 20:17:13.046494   821 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:17:13.046510   821 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:17:13.046593   821 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0428 20:17:13.046664   821 layer_factory.hpp:77] Creating layer mnist
I0428 20:17:13.046766   821 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:17:13.046788   821 net.cpp:86] Creating Layer mnist
I0428 20:17:13.046797   821 net.cpp:382] mnist -> data
I0428 20:17:13.046820   821 net.cpp:382] mnist -> label
I0428 20:17:13.047933   821 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:17:13.050401   821 net.cpp:124] Setting up mnist
I0428 20:17:13.050420   821 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:17:13.050426   821 net.cpp:131] Top shape: 64 (64)
I0428 20:17:13.050428   821 net.cpp:139] Memory required for data: 200960
I0428 20:17:13.050436   821 layer_factory.hpp:77] Creating layer conv0
I0428 20:17:13.050473   821 net.cpp:86] Creating Layer conv0
I0428 20:17:13.050480   821 net.cpp:408] conv0 <- data
I0428 20:17:13.050493   821 net.cpp:382] conv0 -> conv0
I0428 20:17:13.339876   821 net.cpp:124] Setting up conv0
I0428 20:17:13.339905   821 net.cpp:131] Top shape: 64 50 24 24 (1843200)
I0428 20:17:13.339908   821 net.cpp:139] Memory required for data: 7573760
I0428 20:17:13.339925   821 layer_factory.hpp:77] Creating layer pool0
I0428 20:17:13.339941   821 net.cpp:86] Creating Layer pool0
I0428 20:17:13.339946   821 net.cpp:408] pool0 <- conv0
I0428 20:17:13.339951   821 net.cpp:382] pool0 -> pool0
I0428 20:17:13.339999   821 net.cpp:124] Setting up pool0
I0428 20:17:13.340005   821 net.cpp:131] Top shape: 64 50 12 12 (460800)
I0428 20:17:13.340025   821 net.cpp:139] Memory required for data: 9416960
I0428 20:17:13.340029   821 layer_factory.hpp:77] Creating layer conv1
I0428 20:17:13.340040   821 net.cpp:86] Creating Layer conv1
I0428 20:17:13.340044   821 net.cpp:408] conv1 <- pool0
I0428 20:17:13.340049   821 net.cpp:382] conv1 -> conv1
I0428 20:17:13.342959   821 net.cpp:124] Setting up conv1
I0428 20:17:13.342975   821 net.cpp:131] Top shape: 64 10 8 8 (40960)
I0428 20:17:13.342979   821 net.cpp:139] Memory required for data: 9580800
I0428 20:17:13.342988   821 layer_factory.hpp:77] Creating layer pool1
I0428 20:17:13.342998   821 net.cpp:86] Creating Layer pool1
I0428 20:17:13.343001   821 net.cpp:408] pool1 <- conv1
I0428 20:17:13.343008   821 net.cpp:382] pool1 -> pool1
I0428 20:17:13.343063   821 net.cpp:124] Setting up pool1
I0428 20:17:13.343070   821 net.cpp:131] Top shape: 64 10 4 4 (10240)
I0428 20:17:13.343073   821 net.cpp:139] Memory required for data: 9621760
I0428 20:17:13.343076   821 layer_factory.hpp:77] Creating layer ip1
I0428 20:17:13.343085   821 net.cpp:86] Creating Layer ip1
I0428 20:17:13.343087   821 net.cpp:408] ip1 <- pool1
I0428 20:17:13.343092   821 net.cpp:382] ip1 -> ip1
I0428 20:17:13.343224   821 net.cpp:124] Setting up ip1
I0428 20:17:13.343232   821 net.cpp:131] Top shape: 64 10 (640)
I0428 20:17:13.343235   821 net.cpp:139] Memory required for data: 9624320
I0428 20:17:13.343242   821 layer_factory.hpp:77] Creating layer relu1
I0428 20:17:13.343248   821 net.cpp:86] Creating Layer relu1
I0428 20:17:13.343251   821 net.cpp:408] relu1 <- ip1
I0428 20:17:13.343255   821 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:17:13.343426   821 net.cpp:124] Setting up relu1
I0428 20:17:13.343436   821 net.cpp:131] Top shape: 64 10 (640)
I0428 20:17:13.343439   821 net.cpp:139] Memory required for data: 9626880
I0428 20:17:13.343442   821 layer_factory.hpp:77] Creating layer loss
I0428 20:17:13.343448   821 net.cpp:86] Creating Layer loss
I0428 20:17:13.343451   821 net.cpp:408] loss <- ip1
I0428 20:17:13.343456   821 net.cpp:408] loss <- label
I0428 20:17:13.343461   821 net.cpp:382] loss -> loss
I0428 20:17:13.343477   821 layer_factory.hpp:77] Creating layer loss
I0428 20:17:13.344310   821 net.cpp:124] Setting up loss
I0428 20:17:13.344323   821 net.cpp:131] Top shape: (1)
I0428 20:17:13.344343   821 net.cpp:134]     with loss weight 1
I0428 20:17:13.344374   821 net.cpp:139] Memory required for data: 9626884
I0428 20:17:13.344378   821 net.cpp:200] loss needs backward computation.
I0428 20:17:13.344383   821 net.cpp:200] relu1 needs backward computation.
I0428 20:17:13.344385   821 net.cpp:200] ip1 needs backward computation.
I0428 20:17:13.344388   821 net.cpp:200] pool1 needs backward computation.
I0428 20:17:13.344391   821 net.cpp:200] conv1 needs backward computation.
I0428 20:17:13.344394   821 net.cpp:200] pool0 needs backward computation.
I0428 20:17:13.344398   821 net.cpp:200] conv0 needs backward computation.
I0428 20:17:13.344401   821 net.cpp:202] mnist does not need backward computation.
I0428 20:17:13.344405   821 net.cpp:244] This network produces output loss
I0428 20:17:13.344413   821 net.cpp:257] Network initialization done.
I0428 20:17:13.344686   821 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1289.prototxt
I0428 20:17:13.344710   821 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:17:13.344805   821 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0428 20:17:13.344895   821 layer_factory.hpp:77] Creating layer mnist
I0428 20:17:13.344941   821 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:17:13.344957   821 net.cpp:86] Creating Layer mnist
I0428 20:17:13.344962   821 net.cpp:382] mnist -> data
I0428 20:17:13.344971   821 net.cpp:382] mnist -> label
I0428 20:17:13.345062   821 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:17:13.347103   821 net.cpp:124] Setting up mnist
I0428 20:17:13.347137   821 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:17:13.347143   821 net.cpp:131] Top shape: 100 (100)
I0428 20:17:13.347146   821 net.cpp:139] Memory required for data: 314000
I0428 20:17:13.347151   821 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:17:13.347172   821 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:17:13.347177   821 net.cpp:408] label_mnist_1_split <- label
I0428 20:17:13.347182   821 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:17:13.347190   821 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:17:13.347280   821 net.cpp:124] Setting up label_mnist_1_split
I0428 20:17:13.347287   821 net.cpp:131] Top shape: 100 (100)
I0428 20:17:13.347291   821 net.cpp:131] Top shape: 100 (100)
I0428 20:17:13.347295   821 net.cpp:139] Memory required for data: 314800
I0428 20:17:13.347297   821 layer_factory.hpp:77] Creating layer conv0
I0428 20:17:13.347306   821 net.cpp:86] Creating Layer conv0
I0428 20:17:13.347311   821 net.cpp:408] conv0 <- data
I0428 20:17:13.347316   821 net.cpp:382] conv0 -> conv0
I0428 20:17:13.348306   821 net.cpp:124] Setting up conv0
I0428 20:17:13.348318   821 net.cpp:131] Top shape: 100 50 24 24 (2880000)
I0428 20:17:13.348338   821 net.cpp:139] Memory required for data: 11834800
I0428 20:17:13.348358   821 layer_factory.hpp:77] Creating layer pool0
I0428 20:17:13.348366   821 net.cpp:86] Creating Layer pool0
I0428 20:17:13.348369   821 net.cpp:408] pool0 <- conv0
I0428 20:17:13.348374   821 net.cpp:382] pool0 -> pool0
I0428 20:17:13.348423   821 net.cpp:124] Setting up pool0
I0428 20:17:13.348428   821 net.cpp:131] Top shape: 100 50 12 12 (720000)
I0428 20:17:13.348431   821 net.cpp:139] Memory required for data: 14714800
I0428 20:17:13.348434   821 layer_factory.hpp:77] Creating layer conv1
I0428 20:17:13.348443   821 net.cpp:86] Creating Layer conv1
I0428 20:17:13.348446   821 net.cpp:408] conv1 <- pool0
I0428 20:17:13.348453   821 net.cpp:382] conv1 -> conv1
I0428 20:17:13.350244   821 net.cpp:124] Setting up conv1
I0428 20:17:13.350260   821 net.cpp:131] Top shape: 100 10 8 8 (64000)
I0428 20:17:13.350263   821 net.cpp:139] Memory required for data: 14970800
I0428 20:17:13.350273   821 layer_factory.hpp:77] Creating layer pool1
I0428 20:17:13.350281   821 net.cpp:86] Creating Layer pool1
I0428 20:17:13.350297   821 net.cpp:408] pool1 <- conv1
I0428 20:17:13.350304   821 net.cpp:382] pool1 -> pool1
I0428 20:17:13.350345   821 net.cpp:124] Setting up pool1
I0428 20:17:13.350353   821 net.cpp:131] Top shape: 100 10 4 4 (16000)
I0428 20:17:13.350358   821 net.cpp:139] Memory required for data: 15034800
I0428 20:17:13.350360   821 layer_factory.hpp:77] Creating layer ip1
I0428 20:17:13.350368   821 net.cpp:86] Creating Layer ip1
I0428 20:17:13.350371   821 net.cpp:408] ip1 <- pool1
I0428 20:17:13.350378   821 net.cpp:382] ip1 -> ip1
I0428 20:17:13.350497   821 net.cpp:124] Setting up ip1
I0428 20:17:13.350504   821 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:17:13.350507   821 net.cpp:139] Memory required for data: 15038800
I0428 20:17:13.350514   821 layer_factory.hpp:77] Creating layer relu1
I0428 20:17:13.350520   821 net.cpp:86] Creating Layer relu1
I0428 20:17:13.350523   821 net.cpp:408] relu1 <- ip1
I0428 20:17:13.350529   821 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:17:13.351414   821 net.cpp:124] Setting up relu1
I0428 20:17:13.351426   821 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:17:13.351430   821 net.cpp:139] Memory required for data: 15042800
I0428 20:17:13.351434   821 layer_factory.hpp:77] Creating layer ip1_relu1_0_split
I0428 20:17:13.351442   821 net.cpp:86] Creating Layer ip1_relu1_0_split
I0428 20:17:13.351446   821 net.cpp:408] ip1_relu1_0_split <- ip1
I0428 20:17:13.351454   821 net.cpp:382] ip1_relu1_0_split -> ip1_relu1_0_split_0
I0428 20:17:13.351461   821 net.cpp:382] ip1_relu1_0_split -> ip1_relu1_0_split_1
I0428 20:17:13.351506   821 net.cpp:124] Setting up ip1_relu1_0_split
I0428 20:17:13.351529   821 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:17:13.351533   821 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:17:13.351536   821 net.cpp:139] Memory required for data: 15050800
I0428 20:17:13.351539   821 layer_factory.hpp:77] Creating layer accuracy
I0428 20:17:13.351546   821 net.cpp:86] Creating Layer accuracy
I0428 20:17:13.351549   821 net.cpp:408] accuracy <- ip1_relu1_0_split_0
I0428 20:17:13.351553   821 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:17:13.351557   821 net.cpp:382] accuracy -> accuracy
I0428 20:17:13.351564   821 net.cpp:124] Setting up accuracy
I0428 20:17:13.351568   821 net.cpp:131] Top shape: (1)
I0428 20:17:13.351572   821 net.cpp:139] Memory required for data: 15050804
I0428 20:17:13.351574   821 layer_factory.hpp:77] Creating layer loss
I0428 20:17:13.351580   821 net.cpp:86] Creating Layer loss
I0428 20:17:13.351584   821 net.cpp:408] loss <- ip1_relu1_0_split_1
I0428 20:17:13.351588   821 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:17:13.351593   821 net.cpp:382] loss -> loss
I0428 20:17:13.351599   821 layer_factory.hpp:77] Creating layer loss
I0428 20:17:13.351861   821 net.cpp:124] Setting up loss
I0428 20:17:13.351871   821 net.cpp:131] Top shape: (1)
I0428 20:17:13.351874   821 net.cpp:134]     with loss weight 1
I0428 20:17:13.351881   821 net.cpp:139] Memory required for data: 15050808
I0428 20:17:13.351886   821 net.cpp:200] loss needs backward computation.
I0428 20:17:13.351889   821 net.cpp:202] accuracy does not need backward computation.
I0428 20:17:13.351893   821 net.cpp:200] ip1_relu1_0_split needs backward computation.
I0428 20:17:13.351897   821 net.cpp:200] relu1 needs backward computation.
I0428 20:17:13.351899   821 net.cpp:200] ip1 needs backward computation.
I0428 20:17:13.351902   821 net.cpp:200] pool1 needs backward computation.
I0428 20:17:13.351905   821 net.cpp:200] conv1 needs backward computation.
I0428 20:17:13.351909   821 net.cpp:200] pool0 needs backward computation.
I0428 20:17:13.351912   821 net.cpp:200] conv0 needs backward computation.
I0428 20:17:13.351917   821 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:17:13.351920   821 net.cpp:202] mnist does not need backward computation.
I0428 20:17:13.351924   821 net.cpp:244] This network produces output accuracy
I0428 20:17:13.351927   821 net.cpp:244] This network produces output loss
I0428 20:17:13.351948   821 net.cpp:257] Network initialization done.
I0428 20:17:13.351985   821 solver.cpp:56] Solver scaffolding done.
I0428 20:17:13.352208   821 caffe.cpp:248] Starting Optimization
I0428 20:17:13.352216   821 solver.cpp:273] Solving LeNet
I0428 20:17:13.352219   821 solver.cpp:274] Learning Rate Policy: inv
I0428 20:17:13.352334   821 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:17:13.359122   821 blocking_queue.cpp:49] Waiting for data
I0428 20:17:13.428169   828 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:17:13.428946   821 solver.cpp:398]     Test net output #0: accuracy = 0.0806
I0428 20:17:13.428964   821 solver.cpp:398]     Test net output #1: loss = 2.36017 (* 1 = 2.36017 loss)
I0428 20:17:13.433198   821 solver.cpp:219] Iteration 0 (-1.14246e-30 iter/s, 0.080939s/100 iters), loss = 2.36484
I0428 20:17:13.433253   821 solver.cpp:238]     Train net output #0: loss = 2.36484 (* 1 = 2.36484 loss)
I0428 20:17:13.433264   821 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:17:13.550644   821 solver.cpp:219] Iteration 100 (851.829 iter/s, 0.117394s/100 iters), loss = 1.02934
I0428 20:17:13.550685   821 solver.cpp:238]     Train net output #0: loss = 1.02934 (* 1 = 1.02934 loss)
I0428 20:17:13.550693   821 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:17:13.659095   821 solver.cpp:219] Iteration 200 (922.39 iter/s, 0.108414s/100 iters), loss = 0.584012
I0428 20:17:13.659122   821 solver.cpp:238]     Train net output #0: loss = 0.584012 (* 1 = 0.584012 loss)
I0428 20:17:13.659131   821 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:17:13.768611   821 solver.cpp:219] Iteration 300 (913.435 iter/s, 0.109477s/100 iters), loss = 0.611103
I0428 20:17:13.768643   821 solver.cpp:238]     Train net output #0: loss = 0.611103 (* 1 = 0.611103 loss)
I0428 20:17:13.768652   821 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:17:13.878868   821 solver.cpp:219] Iteration 400 (907.316 iter/s, 0.110215s/100 iters), loss = 0.349502
I0428 20:17:13.878896   821 solver.cpp:238]     Train net output #0: loss = 0.349502 (* 1 = 0.349502 loss)
I0428 20:17:13.878903   821 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:17:13.986618   821 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:17:14.060499   828 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:17:14.061353   821 solver.cpp:398]     Test net output #0: accuracy = 0.87
I0428 20:17:14.061394   821 solver.cpp:398]     Test net output #1: loss = 0.359217 (* 1 = 0.359217 loss)
I0428 20:17:14.062525   821 solver.cpp:219] Iteration 500 (544.616 iter/s, 0.183615s/100 iters), loss = 0.398737
I0428 20:17:14.062566   821 solver.cpp:238]     Train net output #0: loss = 0.398737 (* 1 = 0.398737 loss)
I0428 20:17:14.062573   821 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:17:14.171069   821 solver.cpp:219] Iteration 600 (921.72 iter/s, 0.108493s/100 iters), loss = 0.416458
I0428 20:17:14.171113   821 solver.cpp:238]     Train net output #0: loss = 0.416458 (* 1 = 0.416458 loss)
I0428 20:17:14.171120   821 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:17:14.277053   821 solver.cpp:219] Iteration 700 (944.022 iter/s, 0.10593s/100 iters), loss = 0.161734
I0428 20:17:14.277078   821 solver.cpp:238]     Train net output #0: loss = 0.161734 (* 1 = 0.161734 loss)
I0428 20:17:14.277086   821 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:17:14.384222   821 solver.cpp:219] Iteration 800 (933.428 iter/s, 0.107132s/100 iters), loss = 0.268019
I0428 20:17:14.384248   821 solver.cpp:238]     Train net output #0: loss = 0.268019 (* 1 = 0.268019 loss)
I0428 20:17:14.384255   821 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:17:14.493536   821 solver.cpp:219] Iteration 900 (915.104 iter/s, 0.109277s/100 iters), loss = 0.177134
I0428 20:17:14.493563   821 solver.cpp:238]     Train net output #0: loss = 0.177133 (* 1 = 0.177133 loss)
I0428 20:17:14.493571   821 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:17:14.528561   827 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:17:14.597977   821 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:17:14.598963   821 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:17:14.599678   821 solver.cpp:311] Iteration 1000, loss = 0.118307
I0428 20:17:14.599694   821 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:17:14.674901   828 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:17:14.675578   821 solver.cpp:398]     Test net output #0: accuracy = 0.9706
I0428 20:17:14.675598   821 solver.cpp:398]     Test net output #1: loss = 0.093356 (* 1 = 0.093356 loss)
I0428 20:17:14.675604   821 solver.cpp:316] Optimization Done.
I0428 20:17:14.675608   821 caffe.cpp:259] Optimization Done.
