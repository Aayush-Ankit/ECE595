I0428 19:50:39.775463 27067 caffe.cpp:218] Using GPUs 0
I0428 19:50:39.809725 27067 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:50:40.318095 27067 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test611.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:50:40.318239 27067 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test611.prototxt
I0428 19:50:40.318662 27067 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:50:40.318682 27067 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:50:40.318785 27067 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:50:40.318867 27067 layer_factory.hpp:77] Creating layer mnist
I0428 19:50:40.318966 27067 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:50:40.318994 27067 net.cpp:86] Creating Layer mnist
I0428 19:50:40.319002 27067 net.cpp:382] mnist -> data
I0428 19:50:40.319025 27067 net.cpp:382] mnist -> label
I0428 19:50:40.320116 27067 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:50:40.322582 27067 net.cpp:124] Setting up mnist
I0428 19:50:40.322599 27067 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:50:40.322605 27067 net.cpp:131] Top shape: 64 (64)
I0428 19:50:40.322608 27067 net.cpp:139] Memory required for data: 200960
I0428 19:50:40.322616 27067 layer_factory.hpp:77] Creating layer conv0
I0428 19:50:40.322666 27067 net.cpp:86] Creating Layer conv0
I0428 19:50:40.322687 27067 net.cpp:408] conv0 <- data
I0428 19:50:40.322701 27067 net.cpp:382] conv0 -> conv0
I0428 19:50:40.605113 27067 net.cpp:124] Setting up conv0
I0428 19:50:40.605139 27067 net.cpp:131] Top shape: 64 5 24 24 (184320)
I0428 19:50:40.605144 27067 net.cpp:139] Memory required for data: 938240
I0428 19:50:40.605159 27067 layer_factory.hpp:77] Creating layer pool0
I0428 19:50:40.605170 27067 net.cpp:86] Creating Layer pool0
I0428 19:50:40.605178 27067 net.cpp:408] pool0 <- conv0
I0428 19:50:40.605183 27067 net.cpp:382] pool0 -> pool0
I0428 19:50:40.605244 27067 net.cpp:124] Setting up pool0
I0428 19:50:40.605250 27067 net.cpp:131] Top shape: 64 5 12 12 (46080)
I0428 19:50:40.605253 27067 net.cpp:139] Memory required for data: 1122560
I0428 19:50:40.605257 27067 layer_factory.hpp:77] Creating layer conv1
I0428 19:50:40.605268 27067 net.cpp:86] Creating Layer conv1
I0428 19:50:40.605270 27067 net.cpp:408] conv1 <- pool0
I0428 19:50:40.605275 27067 net.cpp:382] conv1 -> conv1
I0428 19:50:40.607851 27067 net.cpp:124] Setting up conv1
I0428 19:50:40.607864 27067 net.cpp:131] Top shape: 64 10 8 8 (40960)
I0428 19:50:40.607867 27067 net.cpp:139] Memory required for data: 1286400
I0428 19:50:40.607892 27067 layer_factory.hpp:77] Creating layer pool1
I0428 19:50:40.607899 27067 net.cpp:86] Creating Layer pool1
I0428 19:50:40.607903 27067 net.cpp:408] pool1 <- conv1
I0428 19:50:40.607908 27067 net.cpp:382] pool1 -> pool1
I0428 19:50:40.607945 27067 net.cpp:124] Setting up pool1
I0428 19:50:40.607951 27067 net.cpp:131] Top shape: 64 10 4 4 (10240)
I0428 19:50:40.607954 27067 net.cpp:139] Memory required for data: 1327360
I0428 19:50:40.607956 27067 layer_factory.hpp:77] Creating layer ip1
I0428 19:50:40.607964 27067 net.cpp:86] Creating Layer ip1
I0428 19:50:40.607966 27067 net.cpp:408] ip1 <- pool1
I0428 19:50:40.607971 27067 net.cpp:382] ip1 -> ip1
I0428 19:50:40.608085 27067 net.cpp:124] Setting up ip1
I0428 19:50:40.608093 27067 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:50:40.608096 27067 net.cpp:139] Memory required for data: 1333760
I0428 19:50:40.608103 27067 layer_factory.hpp:77] Creating layer relu1
I0428 19:50:40.608108 27067 net.cpp:86] Creating Layer relu1
I0428 19:50:40.608111 27067 net.cpp:408] relu1 <- ip1
I0428 19:50:40.608115 27067 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:50:40.608280 27067 net.cpp:124] Setting up relu1
I0428 19:50:40.608289 27067 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:50:40.608292 27067 net.cpp:139] Memory required for data: 1340160
I0428 19:50:40.608296 27067 layer_factory.hpp:77] Creating layer ip2
I0428 19:50:40.608302 27067 net.cpp:86] Creating Layer ip2
I0428 19:50:40.608305 27067 net.cpp:408] ip2 <- ip1
I0428 19:50:40.608309 27067 net.cpp:382] ip2 -> ip2
I0428 19:50:40.608402 27067 net.cpp:124] Setting up ip2
I0428 19:50:40.608409 27067 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:50:40.608412 27067 net.cpp:139] Memory required for data: 1346560
I0428 19:50:40.608417 27067 layer_factory.hpp:77] Creating layer relu2
I0428 19:50:40.608438 27067 net.cpp:86] Creating Layer relu2
I0428 19:50:40.608441 27067 net.cpp:408] relu2 <- ip2
I0428 19:50:40.608446 27067 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:50:40.609253 27067 net.cpp:124] Setting up relu2
I0428 19:50:40.609266 27067 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:50:40.609268 27067 net.cpp:139] Memory required for data: 1352960
I0428 19:50:40.609272 27067 layer_factory.hpp:77] Creating layer ip3
I0428 19:50:40.609278 27067 net.cpp:86] Creating Layer ip3
I0428 19:50:40.609282 27067 net.cpp:408] ip3 <- ip2
I0428 19:50:40.609287 27067 net.cpp:382] ip3 -> ip3
I0428 19:50:40.609377 27067 net.cpp:124] Setting up ip3
I0428 19:50:40.609385 27067 net.cpp:131] Top shape: 64 10 (640)
I0428 19:50:40.609387 27067 net.cpp:139] Memory required for data: 1355520
I0428 19:50:40.609395 27067 layer_factory.hpp:77] Creating layer relu3
I0428 19:50:40.609400 27067 net.cpp:86] Creating Layer relu3
I0428 19:50:40.609402 27067 net.cpp:408] relu3 <- ip3
I0428 19:50:40.609406 27067 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:50:40.609577 27067 net.cpp:124] Setting up relu3
I0428 19:50:40.609586 27067 net.cpp:131] Top shape: 64 10 (640)
I0428 19:50:40.609589 27067 net.cpp:139] Memory required for data: 1358080
I0428 19:50:40.609592 27067 layer_factory.hpp:77] Creating layer loss
I0428 19:50:40.609598 27067 net.cpp:86] Creating Layer loss
I0428 19:50:40.609601 27067 net.cpp:408] loss <- ip3
I0428 19:50:40.609606 27067 net.cpp:408] loss <- label
I0428 19:50:40.609611 27067 net.cpp:382] loss -> loss
I0428 19:50:40.609623 27067 layer_factory.hpp:77] Creating layer loss
I0428 19:50:40.609835 27067 net.cpp:124] Setting up loss
I0428 19:50:40.609858 27067 net.cpp:131] Top shape: (1)
I0428 19:50:40.609861 27067 net.cpp:134]     with loss weight 1
I0428 19:50:40.609875 27067 net.cpp:139] Memory required for data: 1358084
I0428 19:50:40.609879 27067 net.cpp:200] loss needs backward computation.
I0428 19:50:40.609882 27067 net.cpp:200] relu3 needs backward computation.
I0428 19:50:40.609884 27067 net.cpp:200] ip3 needs backward computation.
I0428 19:50:40.609887 27067 net.cpp:200] relu2 needs backward computation.
I0428 19:50:40.609890 27067 net.cpp:200] ip2 needs backward computation.
I0428 19:50:40.609892 27067 net.cpp:200] relu1 needs backward computation.
I0428 19:50:40.609895 27067 net.cpp:200] ip1 needs backward computation.
I0428 19:50:40.609899 27067 net.cpp:200] pool1 needs backward computation.
I0428 19:50:40.609901 27067 net.cpp:200] conv1 needs backward computation.
I0428 19:50:40.609905 27067 net.cpp:200] pool0 needs backward computation.
I0428 19:50:40.609907 27067 net.cpp:200] conv0 needs backward computation.
I0428 19:50:40.609910 27067 net.cpp:202] mnist does not need backward computation.
I0428 19:50:40.609912 27067 net.cpp:244] This network produces output loss
I0428 19:50:40.609921 27067 net.cpp:257] Network initialization done.
I0428 19:50:40.610229 27067 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test611.prototxt
I0428 19:50:40.610255 27067 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:50:40.610340 27067 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:50:40.610414 27067 layer_factory.hpp:77] Creating layer mnist
I0428 19:50:40.610457 27067 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:50:40.610469 27067 net.cpp:86] Creating Layer mnist
I0428 19:50:40.610473 27067 net.cpp:382] mnist -> data
I0428 19:50:40.610481 27067 net.cpp:382] mnist -> label
I0428 19:50:40.610558 27067 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:50:40.612386 27067 net.cpp:124] Setting up mnist
I0428 19:50:40.612398 27067 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:50:40.612403 27067 net.cpp:131] Top shape: 100 (100)
I0428 19:50:40.612406 27067 net.cpp:139] Memory required for data: 314000
I0428 19:50:40.612411 27067 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:50:40.612416 27067 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:50:40.612419 27067 net.cpp:408] label_mnist_1_split <- label
I0428 19:50:40.612424 27067 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:50:40.612431 27067 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:50:40.612517 27067 net.cpp:124] Setting up label_mnist_1_split
I0428 19:50:40.612524 27067 net.cpp:131] Top shape: 100 (100)
I0428 19:50:40.612529 27067 net.cpp:131] Top shape: 100 (100)
I0428 19:50:40.612531 27067 net.cpp:139] Memory required for data: 314800
I0428 19:50:40.612534 27067 layer_factory.hpp:77] Creating layer conv0
I0428 19:50:40.612541 27067 net.cpp:86] Creating Layer conv0
I0428 19:50:40.612545 27067 net.cpp:408] conv0 <- data
I0428 19:50:40.612550 27067 net.cpp:382] conv0 -> conv0
I0428 19:50:40.614351 27067 net.cpp:124] Setting up conv0
I0428 19:50:40.614365 27067 net.cpp:131] Top shape: 100 5 24 24 (288000)
I0428 19:50:40.614369 27067 net.cpp:139] Memory required for data: 1466800
I0428 19:50:40.614377 27067 layer_factory.hpp:77] Creating layer pool0
I0428 19:50:40.614383 27067 net.cpp:86] Creating Layer pool0
I0428 19:50:40.614387 27067 net.cpp:408] pool0 <- conv0
I0428 19:50:40.614392 27067 net.cpp:382] pool0 -> pool0
I0428 19:50:40.614426 27067 net.cpp:124] Setting up pool0
I0428 19:50:40.614431 27067 net.cpp:131] Top shape: 100 5 12 12 (72000)
I0428 19:50:40.614434 27067 net.cpp:139] Memory required for data: 1754800
I0428 19:50:40.614436 27067 layer_factory.hpp:77] Creating layer conv1
I0428 19:50:40.614444 27067 net.cpp:86] Creating Layer conv1
I0428 19:50:40.614447 27067 net.cpp:408] conv1 <- pool0
I0428 19:50:40.614452 27067 net.cpp:382] conv1 -> conv1
I0428 19:50:40.616523 27067 net.cpp:124] Setting up conv1
I0428 19:50:40.616552 27067 net.cpp:131] Top shape: 100 10 8 8 (64000)
I0428 19:50:40.616556 27067 net.cpp:139] Memory required for data: 2010800
I0428 19:50:40.616564 27067 layer_factory.hpp:77] Creating layer pool1
I0428 19:50:40.616574 27067 net.cpp:86] Creating Layer pool1
I0428 19:50:40.616576 27067 net.cpp:408] pool1 <- conv1
I0428 19:50:40.616581 27067 net.cpp:382] pool1 -> pool1
I0428 19:50:40.616618 27067 net.cpp:124] Setting up pool1
I0428 19:50:40.616624 27067 net.cpp:131] Top shape: 100 10 4 4 (16000)
I0428 19:50:40.616627 27067 net.cpp:139] Memory required for data: 2074800
I0428 19:50:40.616629 27067 layer_factory.hpp:77] Creating layer ip1
I0428 19:50:40.616636 27067 net.cpp:86] Creating Layer ip1
I0428 19:50:40.616638 27067 net.cpp:408] ip1 <- pool1
I0428 19:50:40.616643 27067 net.cpp:382] ip1 -> ip1
I0428 19:50:40.616767 27067 net.cpp:124] Setting up ip1
I0428 19:50:40.616775 27067 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:50:40.616788 27067 net.cpp:139] Memory required for data: 2084800
I0428 19:50:40.616796 27067 layer_factory.hpp:77] Creating layer relu1
I0428 19:50:40.616801 27067 net.cpp:86] Creating Layer relu1
I0428 19:50:40.616803 27067 net.cpp:408] relu1 <- ip1
I0428 19:50:40.616807 27067 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:50:40.617049 27067 net.cpp:124] Setting up relu1
I0428 19:50:40.617058 27067 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:50:40.617061 27067 net.cpp:139] Memory required for data: 2094800
I0428 19:50:40.617065 27067 layer_factory.hpp:77] Creating layer ip2
I0428 19:50:40.617076 27067 net.cpp:86] Creating Layer ip2
I0428 19:50:40.617079 27067 net.cpp:408] ip2 <- ip1
I0428 19:50:40.617084 27067 net.cpp:382] ip2 -> ip2
I0428 19:50:40.617182 27067 net.cpp:124] Setting up ip2
I0428 19:50:40.617190 27067 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:50:40.617208 27067 net.cpp:139] Memory required for data: 2104800
I0428 19:50:40.617214 27067 layer_factory.hpp:77] Creating layer relu2
I0428 19:50:40.617219 27067 net.cpp:86] Creating Layer relu2
I0428 19:50:40.617223 27067 net.cpp:408] relu2 <- ip2
I0428 19:50:40.617231 27067 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:50:40.617382 27067 net.cpp:124] Setting up relu2
I0428 19:50:40.617390 27067 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:50:40.617394 27067 net.cpp:139] Memory required for data: 2114800
I0428 19:50:40.617398 27067 layer_factory.hpp:77] Creating layer ip3
I0428 19:50:40.617403 27067 net.cpp:86] Creating Layer ip3
I0428 19:50:40.617405 27067 net.cpp:408] ip3 <- ip2
I0428 19:50:40.617410 27067 net.cpp:382] ip3 -> ip3
I0428 19:50:40.617506 27067 net.cpp:124] Setting up ip3
I0428 19:50:40.617513 27067 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:50:40.617530 27067 net.cpp:139] Memory required for data: 2118800
I0428 19:50:40.617537 27067 layer_factory.hpp:77] Creating layer relu3
I0428 19:50:40.617542 27067 net.cpp:86] Creating Layer relu3
I0428 19:50:40.617545 27067 net.cpp:408] relu3 <- ip3
I0428 19:50:40.617549 27067 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:50:40.618306 27067 net.cpp:124] Setting up relu3
I0428 19:50:40.618317 27067 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:50:40.618336 27067 net.cpp:139] Memory required for data: 2122800
I0428 19:50:40.618340 27067 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:50:40.618345 27067 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:50:40.618350 27067 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:50:40.618355 27067 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:50:40.618361 27067 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:50:40.618432 27067 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:50:40.618438 27067 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:50:40.618443 27067 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:50:40.618445 27067 net.cpp:139] Memory required for data: 2130800
I0428 19:50:40.618448 27067 layer_factory.hpp:77] Creating layer accuracy
I0428 19:50:40.618453 27067 net.cpp:86] Creating Layer accuracy
I0428 19:50:40.618458 27067 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:50:40.618461 27067 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:50:40.618465 27067 net.cpp:382] accuracy -> accuracy
I0428 19:50:40.618472 27067 net.cpp:124] Setting up accuracy
I0428 19:50:40.618476 27067 net.cpp:131] Top shape: (1)
I0428 19:50:40.618479 27067 net.cpp:139] Memory required for data: 2130804
I0428 19:50:40.618484 27067 layer_factory.hpp:77] Creating layer loss
I0428 19:50:40.618487 27067 net.cpp:86] Creating Layer loss
I0428 19:50:40.618490 27067 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:50:40.618494 27067 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:50:40.618499 27067 net.cpp:382] loss -> loss
I0428 19:50:40.618505 27067 layer_factory.hpp:77] Creating layer loss
I0428 19:50:40.618744 27067 net.cpp:124] Setting up loss
I0428 19:50:40.618753 27067 net.cpp:131] Top shape: (1)
I0428 19:50:40.618757 27067 net.cpp:134]     with loss weight 1
I0428 19:50:40.618763 27067 net.cpp:139] Memory required for data: 2130808
I0428 19:50:40.618777 27067 net.cpp:200] loss needs backward computation.
I0428 19:50:40.618782 27067 net.cpp:202] accuracy does not need backward computation.
I0428 19:50:40.618785 27067 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:50:40.618788 27067 net.cpp:200] relu3 needs backward computation.
I0428 19:50:40.618791 27067 net.cpp:200] ip3 needs backward computation.
I0428 19:50:40.618794 27067 net.cpp:200] relu2 needs backward computation.
I0428 19:50:40.618798 27067 net.cpp:200] ip2 needs backward computation.
I0428 19:50:40.618800 27067 net.cpp:200] relu1 needs backward computation.
I0428 19:50:40.618803 27067 net.cpp:200] ip1 needs backward computation.
I0428 19:50:40.618806 27067 net.cpp:200] pool1 needs backward computation.
I0428 19:50:40.618809 27067 net.cpp:200] conv1 needs backward computation.
I0428 19:50:40.618813 27067 net.cpp:200] pool0 needs backward computation.
I0428 19:50:40.618816 27067 net.cpp:200] conv0 needs backward computation.
I0428 19:50:40.618820 27067 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:50:40.618824 27067 net.cpp:202] mnist does not need backward computation.
I0428 19:50:40.618826 27067 net.cpp:244] This network produces output accuracy
I0428 19:50:40.618830 27067 net.cpp:244] This network produces output loss
I0428 19:50:40.618840 27067 net.cpp:257] Network initialization done.
I0428 19:50:40.618880 27067 solver.cpp:56] Solver scaffolding done.
I0428 19:50:40.619246 27067 caffe.cpp:248] Starting Optimization
I0428 19:50:40.619252 27067 solver.cpp:273] Solving LeNet
I0428 19:50:40.619256 27067 solver.cpp:274] Learning Rate Policy: inv
I0428 19:50:40.620118 27067 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:50:40.623201 27067 blocking_queue.cpp:49] Waiting for data
I0428 19:50:40.694705 27074 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:50:40.695266 27067 solver.cpp:398]     Test net output #0: accuracy = 0.0977
I0428 19:50:40.695300 27067 solver.cpp:398]     Test net output #1: loss = 2.2992 (* 1 = 2.2992 loss)
I0428 19:50:40.697134 27067 solver.cpp:219] Iteration 0 (0 iter/s, 0.0778527s/100 iters), loss = 2.2953
I0428 19:50:40.697187 27067 solver.cpp:238]     Train net output #0: loss = 2.2953 (* 1 = 2.2953 loss)
I0428 19:50:40.697199 27067 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:50:40.778313 27067 solver.cpp:219] Iteration 100 (1232.88 iter/s, 0.081111s/100 iters), loss = 1.39524
I0428 19:50:40.778383 27067 solver.cpp:238]     Train net output #0: loss = 1.39524 (* 1 = 1.39524 loss)
I0428 19:50:40.778398 27067 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:50:40.870573 27067 solver.cpp:219] Iteration 200 (1084.82 iter/s, 0.0921815s/100 iters), loss = 0.787741
I0428 19:50:40.870604 27067 solver.cpp:238]     Train net output #0: loss = 0.787741 (* 1 = 0.787741 loss)
I0428 19:50:40.870618 27067 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:50:40.950130 27067 solver.cpp:219] Iteration 300 (1257.6 iter/s, 0.0795167s/100 iters), loss = 1.14291
I0428 19:50:40.950160 27067 solver.cpp:238]     Train net output #0: loss = 1.14291 (* 1 = 1.14291 loss)
I0428 19:50:40.950170 27067 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:50:41.028836 27067 solver.cpp:219] Iteration 400 (1271.2 iter/s, 0.0786659s/100 iters), loss = 0.780743
I0428 19:50:41.028864 27067 solver.cpp:238]     Train net output #0: loss = 0.780743 (* 1 = 0.780743 loss)
I0428 19:50:41.028872 27067 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:50:41.105972 27067 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:50:41.159143 27074 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:50:41.160349 27067 solver.cpp:398]     Test net output #0: accuracy = 0.6608
I0428 19:50:41.160392 27067 solver.cpp:398]     Test net output #1: loss = 0.845995 (* 1 = 0.845995 loss)
I0428 19:50:41.161593 27067 solver.cpp:219] Iteration 500 (753.515 iter/s, 0.132711s/100 iters), loss = 0.831002
I0428 19:50:41.161656 27067 solver.cpp:238]     Train net output #0: loss = 0.831002 (* 1 = 0.831002 loss)
I0428 19:50:41.161697 27067 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:50:41.254823 27067 solver.cpp:219] Iteration 600 (1073.33 iter/s, 0.0931677s/100 iters), loss = 0.974446
I0428 19:50:41.254854 27067 solver.cpp:238]     Train net output #0: loss = 0.974446 (* 1 = 0.974446 loss)
I0428 19:50:41.254863 27067 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:50:41.347123 27067 solver.cpp:219] Iteration 700 (1083.93 iter/s, 0.0922573s/100 iters), loss = 0.66277
I0428 19:50:41.347158 27067 solver.cpp:238]     Train net output #0: loss = 0.66277 (* 1 = 0.66277 loss)
I0428 19:50:41.347180 27067 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:50:41.428356 27067 solver.cpp:219] Iteration 800 (1231.71 iter/s, 0.081188s/100 iters), loss = 0.981679
I0428 19:50:41.428385 27067 solver.cpp:238]     Train net output #0: loss = 0.981679 (* 1 = 0.981679 loss)
I0428 19:50:41.428406 27067 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:50:41.506839 27067 solver.cpp:219] Iteration 900 (1274.79 iter/s, 0.0784445s/100 iters), loss = 0.980175
I0428 19:50:41.506868 27067 solver.cpp:238]     Train net output #0: loss = 0.980175 (* 1 = 0.980175 loss)
I0428 19:50:41.506876 27067 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:50:41.532806 27073 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:50:41.583423 27067 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:50:41.584194 27067 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:50:41.584676 27067 solver.cpp:311] Iteration 1000, loss = 0.898465
I0428 19:50:41.584693 27067 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:50:41.637248 27074 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:50:41.637770 27067 solver.cpp:398]     Test net output #0: accuracy = 0.6685
I0428 19:50:41.637794 27067 solver.cpp:398]     Test net output #1: loss = 0.809745 (* 1 = 0.809745 loss)
I0428 19:50:41.637800 27067 solver.cpp:316] Optimization Done.
I0428 19:50:41.637804 27067 caffe.cpp:259] Optimization Done.
