I0428 19:48:11.143148 26420 caffe.cpp:218] Using GPUs 0
I0428 19:48:11.176312 26420 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:48:11.675410 26420 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test537.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:48:11.675534 26420 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test537.prototxt
I0428 19:48:11.675915 26420 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:48:11.675931 26420 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:48:11.676013 26420 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:48:11.676080 26420 layer_factory.hpp:77] Creating layer mnist
I0428 19:48:11.676162 26420 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:48:11.676182 26420 net.cpp:86] Creating Layer mnist
I0428 19:48:11.676188 26420 net.cpp:382] mnist -> data
I0428 19:48:11.676208 26420 net.cpp:382] mnist -> label
I0428 19:48:11.677127 26420 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:48:11.679256 26420 net.cpp:124] Setting up mnist
I0428 19:48:11.679272 26420 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:48:11.679278 26420 net.cpp:131] Top shape: 64 (64)
I0428 19:48:11.679280 26420 net.cpp:139] Memory required for data: 200960
I0428 19:48:11.679302 26420 layer_factory.hpp:77] Creating layer conv0
I0428 19:48:11.679327 26420 net.cpp:86] Creating Layer conv0
I0428 19:48:11.679347 26420 net.cpp:408] conv0 <- data
I0428 19:48:11.679358 26420 net.cpp:382] conv0 -> conv0
I0428 19:48:11.914006 26420 net.cpp:124] Setting up conv0
I0428 19:48:11.914032 26420 net.cpp:131] Top shape: 64 5 24 24 (184320)
I0428 19:48:11.914036 26420 net.cpp:139] Memory required for data: 938240
I0428 19:48:11.914065 26420 layer_factory.hpp:77] Creating layer pool0
I0428 19:48:11.914078 26420 net.cpp:86] Creating Layer pool0
I0428 19:48:11.914083 26420 net.cpp:408] pool0 <- conv0
I0428 19:48:11.914088 26420 net.cpp:382] pool0 -> pool0
I0428 19:48:11.914149 26420 net.cpp:124] Setting up pool0
I0428 19:48:11.914163 26420 net.cpp:131] Top shape: 64 5 12 12 (46080)
I0428 19:48:11.914166 26420 net.cpp:139] Memory required for data: 1122560
I0428 19:48:11.914170 26420 layer_factory.hpp:77] Creating layer conv1
I0428 19:48:11.914180 26420 net.cpp:86] Creating Layer conv1
I0428 19:48:11.914183 26420 net.cpp:408] conv1 <- pool0
I0428 19:48:11.914187 26420 net.cpp:382] conv1 -> conv1
I0428 19:48:11.915953 26420 net.cpp:124] Setting up conv1
I0428 19:48:11.915967 26420 net.cpp:131] Top shape: 64 2 8 8 (8192)
I0428 19:48:11.915971 26420 net.cpp:139] Memory required for data: 1155328
I0428 19:48:11.915980 26420 layer_factory.hpp:77] Creating layer pool1
I0428 19:48:11.915987 26420 net.cpp:86] Creating Layer pool1
I0428 19:48:11.915992 26420 net.cpp:408] pool1 <- conv1
I0428 19:48:11.915997 26420 net.cpp:382] pool1 -> pool1
I0428 19:48:11.916046 26420 net.cpp:124] Setting up pool1
I0428 19:48:11.916060 26420 net.cpp:131] Top shape: 64 2 4 4 (2048)
I0428 19:48:11.916064 26420 net.cpp:139] Memory required for data: 1163520
I0428 19:48:11.916066 26420 layer_factory.hpp:77] Creating layer ip1
I0428 19:48:11.916077 26420 net.cpp:86] Creating Layer ip1
I0428 19:48:11.916081 26420 net.cpp:408] ip1 <- pool1
I0428 19:48:11.916086 26420 net.cpp:382] ip1 -> ip1
I0428 19:48:11.917119 26420 net.cpp:124] Setting up ip1
I0428 19:48:11.917131 26420 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:48:11.917135 26420 net.cpp:139] Memory required for data: 1176320
I0428 19:48:11.917143 26420 layer_factory.hpp:77] Creating layer relu1
I0428 19:48:11.917150 26420 net.cpp:86] Creating Layer relu1
I0428 19:48:11.917155 26420 net.cpp:408] relu1 <- ip1
I0428 19:48:11.917160 26420 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:48:11.917340 26420 net.cpp:124] Setting up relu1
I0428 19:48:11.917347 26420 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:48:11.917351 26420 net.cpp:139] Memory required for data: 1189120
I0428 19:48:11.917354 26420 layer_factory.hpp:77] Creating layer ip2
I0428 19:48:11.917361 26420 net.cpp:86] Creating Layer ip2
I0428 19:48:11.917363 26420 net.cpp:408] ip2 <- ip1
I0428 19:48:11.917368 26420 net.cpp:382] ip2 -> ip2
I0428 19:48:11.917480 26420 net.cpp:124] Setting up ip2
I0428 19:48:11.917486 26420 net.cpp:131] Top shape: 64 10 (640)
I0428 19:48:11.917490 26420 net.cpp:139] Memory required for data: 1191680
I0428 19:48:11.917495 26420 layer_factory.hpp:77] Creating layer relu2
I0428 19:48:11.917501 26420 net.cpp:86] Creating Layer relu2
I0428 19:48:11.917505 26420 net.cpp:408] relu2 <- ip2
I0428 19:48:11.917508 26420 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:48:11.918231 26420 net.cpp:124] Setting up relu2
I0428 19:48:11.918243 26420 net.cpp:131] Top shape: 64 10 (640)
I0428 19:48:11.918246 26420 net.cpp:139] Memory required for data: 1194240
I0428 19:48:11.918249 26420 layer_factory.hpp:77] Creating layer ip3
I0428 19:48:11.918256 26420 net.cpp:86] Creating Layer ip3
I0428 19:48:11.918259 26420 net.cpp:408] ip3 <- ip2
I0428 19:48:11.918264 26420 net.cpp:382] ip3 -> ip3
I0428 19:48:11.918356 26420 net.cpp:124] Setting up ip3
I0428 19:48:11.918365 26420 net.cpp:131] Top shape: 64 10 (640)
I0428 19:48:11.918367 26420 net.cpp:139] Memory required for data: 1196800
I0428 19:48:11.918375 26420 layer_factory.hpp:77] Creating layer relu3
I0428 19:48:11.918380 26420 net.cpp:86] Creating Layer relu3
I0428 19:48:11.918381 26420 net.cpp:408] relu3 <- ip3
I0428 19:48:11.918385 26420 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:48:11.918560 26420 net.cpp:124] Setting up relu3
I0428 19:48:11.918567 26420 net.cpp:131] Top shape: 64 10 (640)
I0428 19:48:11.918570 26420 net.cpp:139] Memory required for data: 1199360
I0428 19:48:11.918573 26420 layer_factory.hpp:77] Creating layer loss
I0428 19:48:11.918578 26420 net.cpp:86] Creating Layer loss
I0428 19:48:11.918581 26420 net.cpp:408] loss <- ip3
I0428 19:48:11.918586 26420 net.cpp:408] loss <- label
I0428 19:48:11.918591 26420 net.cpp:382] loss -> loss
I0428 19:48:11.918606 26420 layer_factory.hpp:77] Creating layer loss
I0428 19:48:11.918833 26420 net.cpp:124] Setting up loss
I0428 19:48:11.918841 26420 net.cpp:131] Top shape: (1)
I0428 19:48:11.918844 26420 net.cpp:134]     with loss weight 1
I0428 19:48:11.918858 26420 net.cpp:139] Memory required for data: 1199364
I0428 19:48:11.918860 26420 net.cpp:200] loss needs backward computation.
I0428 19:48:11.918864 26420 net.cpp:200] relu3 needs backward computation.
I0428 19:48:11.918866 26420 net.cpp:200] ip3 needs backward computation.
I0428 19:48:11.918870 26420 net.cpp:200] relu2 needs backward computation.
I0428 19:48:11.918874 26420 net.cpp:200] ip2 needs backward computation.
I0428 19:48:11.918875 26420 net.cpp:200] relu1 needs backward computation.
I0428 19:48:11.918879 26420 net.cpp:200] ip1 needs backward computation.
I0428 19:48:11.918881 26420 net.cpp:200] pool1 needs backward computation.
I0428 19:48:11.918884 26420 net.cpp:200] conv1 needs backward computation.
I0428 19:48:11.918887 26420 net.cpp:200] pool0 needs backward computation.
I0428 19:48:11.918890 26420 net.cpp:200] conv0 needs backward computation.
I0428 19:48:11.918893 26420 net.cpp:202] mnist does not need backward computation.
I0428 19:48:11.918896 26420 net.cpp:244] This network produces output loss
I0428 19:48:11.918905 26420 net.cpp:257] Network initialization done.
I0428 19:48:11.919201 26420 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test537.prototxt
I0428 19:48:11.919226 26420 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:48:11.919312 26420 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:48:11.919383 26420 layer_factory.hpp:77] Creating layer mnist
I0428 19:48:11.919425 26420 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:48:11.919436 26420 net.cpp:86] Creating Layer mnist
I0428 19:48:11.919440 26420 net.cpp:382] mnist -> data
I0428 19:48:11.919447 26420 net.cpp:382] mnist -> label
I0428 19:48:11.919523 26420 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:48:11.921478 26420 net.cpp:124] Setting up mnist
I0428 19:48:11.921506 26420 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:48:11.921511 26420 net.cpp:131] Top shape: 100 (100)
I0428 19:48:11.921515 26420 net.cpp:139] Memory required for data: 314000
I0428 19:48:11.921519 26420 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:48:11.921546 26420 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:48:11.921550 26420 net.cpp:408] label_mnist_1_split <- label
I0428 19:48:11.921555 26420 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:48:11.921561 26420 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:48:11.921605 26420 net.cpp:124] Setting up label_mnist_1_split
I0428 19:48:11.921612 26420 net.cpp:131] Top shape: 100 (100)
I0428 19:48:11.921617 26420 net.cpp:131] Top shape: 100 (100)
I0428 19:48:11.921618 26420 net.cpp:139] Memory required for data: 314800
I0428 19:48:11.921622 26420 layer_factory.hpp:77] Creating layer conv0
I0428 19:48:11.921629 26420 net.cpp:86] Creating Layer conv0
I0428 19:48:11.921633 26420 net.cpp:408] conv0 <- data
I0428 19:48:11.921638 26420 net.cpp:382] conv0 -> conv0
I0428 19:48:11.923408 26420 net.cpp:124] Setting up conv0
I0428 19:48:11.923424 26420 net.cpp:131] Top shape: 100 5 24 24 (288000)
I0428 19:48:11.923429 26420 net.cpp:139] Memory required for data: 1466800
I0428 19:48:11.923437 26420 layer_factory.hpp:77] Creating layer pool0
I0428 19:48:11.923444 26420 net.cpp:86] Creating Layer pool0
I0428 19:48:11.923447 26420 net.cpp:408] pool0 <- conv0
I0428 19:48:11.923452 26420 net.cpp:382] pool0 -> pool0
I0428 19:48:11.923488 26420 net.cpp:124] Setting up pool0
I0428 19:48:11.923494 26420 net.cpp:131] Top shape: 100 5 12 12 (72000)
I0428 19:48:11.923497 26420 net.cpp:139] Memory required for data: 1754800
I0428 19:48:11.923501 26420 layer_factory.hpp:77] Creating layer conv1
I0428 19:48:11.923508 26420 net.cpp:86] Creating Layer conv1
I0428 19:48:11.923512 26420 net.cpp:408] conv1 <- pool0
I0428 19:48:11.923517 26420 net.cpp:382] conv1 -> conv1
I0428 19:48:11.925695 26420 net.cpp:124] Setting up conv1
I0428 19:48:11.925709 26420 net.cpp:131] Top shape: 100 2 8 8 (12800)
I0428 19:48:11.925714 26420 net.cpp:139] Memory required for data: 1806000
I0428 19:48:11.925740 26420 layer_factory.hpp:77] Creating layer pool1
I0428 19:48:11.925746 26420 net.cpp:86] Creating Layer pool1
I0428 19:48:11.925750 26420 net.cpp:408] pool1 <- conv1
I0428 19:48:11.925762 26420 net.cpp:382] pool1 -> pool1
I0428 19:48:11.925806 26420 net.cpp:124] Setting up pool1
I0428 19:48:11.925813 26420 net.cpp:131] Top shape: 100 2 4 4 (3200)
I0428 19:48:11.925817 26420 net.cpp:139] Memory required for data: 1818800
I0428 19:48:11.925819 26420 layer_factory.hpp:77] Creating layer ip1
I0428 19:48:11.925825 26420 net.cpp:86] Creating Layer ip1
I0428 19:48:11.925829 26420 net.cpp:408] ip1 <- pool1
I0428 19:48:11.925833 26420 net.cpp:382] ip1 -> ip1
I0428 19:48:11.926036 26420 net.cpp:124] Setting up ip1
I0428 19:48:11.926045 26420 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:48:11.926059 26420 net.cpp:139] Memory required for data: 1838800
I0428 19:48:11.926069 26420 layer_factory.hpp:77] Creating layer relu1
I0428 19:48:11.926074 26420 net.cpp:86] Creating Layer relu1
I0428 19:48:11.926077 26420 net.cpp:408] relu1 <- ip1
I0428 19:48:11.926082 26420 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:48:11.926275 26420 net.cpp:124] Setting up relu1
I0428 19:48:11.926282 26420 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:48:11.926286 26420 net.cpp:139] Memory required for data: 1858800
I0428 19:48:11.926290 26420 layer_factory.hpp:77] Creating layer ip2
I0428 19:48:11.926296 26420 net.cpp:86] Creating Layer ip2
I0428 19:48:11.926300 26420 net.cpp:408] ip2 <- ip1
I0428 19:48:11.926306 26420 net.cpp:382] ip2 -> ip2
I0428 19:48:11.926409 26420 net.cpp:124] Setting up ip2
I0428 19:48:11.926417 26420 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:48:11.926420 26420 net.cpp:139] Memory required for data: 1862800
I0428 19:48:11.926425 26420 layer_factory.hpp:77] Creating layer relu2
I0428 19:48:11.926430 26420 net.cpp:86] Creating Layer relu2
I0428 19:48:11.926434 26420 net.cpp:408] relu2 <- ip2
I0428 19:48:11.926437 26420 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:48:11.926599 26420 net.cpp:124] Setting up relu2
I0428 19:48:11.926609 26420 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:48:11.926612 26420 net.cpp:139] Memory required for data: 1866800
I0428 19:48:11.926615 26420 layer_factory.hpp:77] Creating layer ip3
I0428 19:48:11.926627 26420 net.cpp:86] Creating Layer ip3
I0428 19:48:11.926630 26420 net.cpp:408] ip3 <- ip2
I0428 19:48:11.926635 26420 net.cpp:382] ip3 -> ip3
I0428 19:48:11.926739 26420 net.cpp:124] Setting up ip3
I0428 19:48:11.926746 26420 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:48:11.926749 26420 net.cpp:139] Memory required for data: 1870800
I0428 19:48:11.926758 26420 layer_factory.hpp:77] Creating layer relu3
I0428 19:48:11.926762 26420 net.cpp:86] Creating Layer relu3
I0428 19:48:11.926765 26420 net.cpp:408] relu3 <- ip3
I0428 19:48:11.926769 26420 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:48:11.927609 26420 net.cpp:124] Setting up relu3
I0428 19:48:11.927621 26420 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:48:11.927624 26420 net.cpp:139] Memory required for data: 1874800
I0428 19:48:11.927628 26420 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:48:11.927634 26420 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:48:11.927637 26420 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:48:11.927642 26420 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:48:11.927649 26420 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:48:11.927700 26420 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:48:11.927706 26420 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:48:11.927709 26420 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:48:11.927712 26420 net.cpp:139] Memory required for data: 1882800
I0428 19:48:11.927716 26420 layer_factory.hpp:77] Creating layer accuracy
I0428 19:48:11.927721 26420 net.cpp:86] Creating Layer accuracy
I0428 19:48:11.927723 26420 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:48:11.927727 26420 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:48:11.927732 26420 net.cpp:382] accuracy -> accuracy
I0428 19:48:11.927738 26420 net.cpp:124] Setting up accuracy
I0428 19:48:11.927742 26420 net.cpp:131] Top shape: (1)
I0428 19:48:11.927745 26420 net.cpp:139] Memory required for data: 1882804
I0428 19:48:11.927748 26420 layer_factory.hpp:77] Creating layer loss
I0428 19:48:11.927752 26420 net.cpp:86] Creating Layer loss
I0428 19:48:11.927755 26420 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:48:11.927759 26420 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:48:11.927763 26420 net.cpp:382] loss -> loss
I0428 19:48:11.927769 26420 layer_factory.hpp:77] Creating layer loss
I0428 19:48:11.928005 26420 net.cpp:124] Setting up loss
I0428 19:48:11.928014 26420 net.cpp:131] Top shape: (1)
I0428 19:48:11.928017 26420 net.cpp:134]     with loss weight 1
I0428 19:48:11.928025 26420 net.cpp:139] Memory required for data: 1882808
I0428 19:48:11.928037 26420 net.cpp:200] loss needs backward computation.
I0428 19:48:11.928041 26420 net.cpp:202] accuracy does not need backward computation.
I0428 19:48:11.928045 26420 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:48:11.928050 26420 net.cpp:200] relu3 needs backward computation.
I0428 19:48:11.928053 26420 net.cpp:200] ip3 needs backward computation.
I0428 19:48:11.928056 26420 net.cpp:200] relu2 needs backward computation.
I0428 19:48:11.928059 26420 net.cpp:200] ip2 needs backward computation.
I0428 19:48:11.928062 26420 net.cpp:200] relu1 needs backward computation.
I0428 19:48:11.928066 26420 net.cpp:200] ip1 needs backward computation.
I0428 19:48:11.928069 26420 net.cpp:200] pool1 needs backward computation.
I0428 19:48:11.928073 26420 net.cpp:200] conv1 needs backward computation.
I0428 19:48:11.928076 26420 net.cpp:200] pool0 needs backward computation.
I0428 19:48:11.928079 26420 net.cpp:200] conv0 needs backward computation.
I0428 19:48:11.928083 26420 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:48:11.928088 26420 net.cpp:202] mnist does not need backward computation.
I0428 19:48:11.928092 26420 net.cpp:244] This network produces output accuracy
I0428 19:48:11.928094 26420 net.cpp:244] This network produces output loss
I0428 19:48:11.928105 26420 net.cpp:257] Network initialization done.
I0428 19:48:11.928146 26420 solver.cpp:56] Solver scaffolding done.
I0428 19:48:11.928514 26420 caffe.cpp:248] Starting Optimization
I0428 19:48:11.928520 26420 solver.cpp:273] Solving LeNet
I0428 19:48:11.928524 26420 solver.cpp:274] Learning Rate Policy: inv
I0428 19:48:11.929471 26420 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:48:11.933159 26420 blocking_queue.cpp:49] Waiting for data
I0428 19:48:12.000659 26427 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:48:12.001783 26420 solver.cpp:398]     Test net output #0: accuracy = 0.0905
I0428 19:48:12.001829 26420 solver.cpp:398]     Test net output #1: loss = 2.35676 (* 1 = 2.35676 loss)
I0428 19:48:12.004832 26420 solver.cpp:219] Iteration 0 (-1.06639e-42 iter/s, 0.0762604s/100 iters), loss = 2.31254
I0428 19:48:12.004883 26420 solver.cpp:238]     Train net output #0: loss = 2.31254 (* 1 = 2.31254 loss)
I0428 19:48:12.004904 26420 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:48:12.099794 26420 solver.cpp:219] Iteration 100 (1053.7 iter/s, 0.0949034s/100 iters), loss = 0.744338
I0428 19:48:12.099829 26420 solver.cpp:238]     Train net output #0: loss = 0.744338 (* 1 = 0.744338 loss)
I0428 19:48:12.099838 26420 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:48:12.174904 26420 solver.cpp:219] Iteration 200 (1332.15 iter/s, 0.0750664s/100 iters), loss = 0.357014
I0428 19:48:12.174934 26420 solver.cpp:238]     Train net output #0: loss = 0.357014 (* 1 = 0.357014 loss)
I0428 19:48:12.174942 26420 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:48:12.274966 26420 solver.cpp:219] Iteration 300 (999.808 iter/s, 0.100019s/100 iters), loss = 0.301556
I0428 19:48:12.275007 26420 solver.cpp:238]     Train net output #0: loss = 0.301556 (* 1 = 0.301556 loss)
I0428 19:48:12.275017 26420 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:48:12.352246 26420 solver.cpp:219] Iteration 400 (1294.85 iter/s, 0.0772291s/100 iters), loss = 0.315939
I0428 19:48:12.352280 26420 solver.cpp:238]     Train net output #0: loss = 0.315939 (* 1 = 0.315939 loss)
I0428 19:48:12.352288 26420 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:48:12.425585 26420 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:48:12.478212 26427 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:48:12.479677 26420 solver.cpp:398]     Test net output #0: accuracy = 0.9213
I0428 19:48:12.479722 26420 solver.cpp:398]     Test net output #1: loss = 0.264342 (* 1 = 0.264342 loss)
I0428 19:48:12.480909 26420 solver.cpp:219] Iteration 500 (777.525 iter/s, 0.128613s/100 iters), loss = 0.284183
I0428 19:48:12.480957 26420 solver.cpp:238]     Train net output #0: loss = 0.284183 (* 1 = 0.284183 loss)
I0428 19:48:12.480999 26420 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:48:12.589499 26420 solver.cpp:219] Iteration 600 (921.364 iter/s, 0.108535s/100 iters), loss = 0.150869
I0428 19:48:12.589543 26420 solver.cpp:238]     Train net output #0: loss = 0.150869 (* 1 = 0.150869 loss)
I0428 19:48:12.589560 26420 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:48:12.671494 26420 solver.cpp:219] Iteration 700 (1220.35 iter/s, 0.0819435s/100 iters), loss = 0.313223
I0428 19:48:12.671525 26420 solver.cpp:238]     Train net output #0: loss = 0.313223 (* 1 = 0.313223 loss)
I0428 19:48:12.671532 26420 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:48:12.747117 26420 solver.cpp:219] Iteration 800 (1323.04 iter/s, 0.0755834s/100 iters), loss = 0.315148
I0428 19:48:12.747148 26420 solver.cpp:238]     Train net output #0: loss = 0.315148 (* 1 = 0.315148 loss)
I0428 19:48:12.747155 26420 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:48:12.825659 26420 solver.cpp:219] Iteration 900 (1273.87 iter/s, 0.0785011s/100 iters), loss = 0.350991
I0428 19:48:12.825688 26420 solver.cpp:238]     Train net output #0: loss = 0.350991 (* 1 = 0.350991 loss)
I0428 19:48:12.825696 26420 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:48:12.852582 26426 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:48:12.901177 26420 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:48:12.901901 26420 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:48:12.902351 26420 solver.cpp:311] Iteration 1000, loss = 0.285001
I0428 19:48:12.902370 26420 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:48:12.955061 26427 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:48:12.955579 26420 solver.cpp:398]     Test net output #0: accuracy = 0.9453
I0428 19:48:12.955603 26420 solver.cpp:398]     Test net output #1: loss = 0.178321 (* 1 = 0.178321 loss)
I0428 19:48:12.955620 26420 solver.cpp:316] Optimization Done.
I0428 19:48:12.955623 26420 caffe.cpp:259] Optimization Done.
