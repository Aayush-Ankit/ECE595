I0428 19:47:18.421393 26160 caffe.cpp:218] Using GPUs 0
I0428 19:47:18.458555 26160 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:47:18.967752 26160 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test511.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:47:18.967896 26160 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test511.prototxt
I0428 19:47:18.968273 26160 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:47:18.968289 26160 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:47:18.968379 26160 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0428 19:47:18.968451 26160 layer_factory.hpp:77] Creating layer mnist
I0428 19:47:18.968547 26160 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:47:18.968570 26160 net.cpp:86] Creating Layer mnist
I0428 19:47:18.968580 26160 net.cpp:382] mnist -> data
I0428 19:47:18.968602 26160 net.cpp:382] mnist -> label
I0428 19:47:18.969702 26160 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:47:18.972160 26160 net.cpp:124] Setting up mnist
I0428 19:47:18.972180 26160 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:47:18.972187 26160 net.cpp:131] Top shape: 64 (64)
I0428 19:47:18.972189 26160 net.cpp:139] Memory required for data: 200960
I0428 19:47:18.972198 26160 layer_factory.hpp:77] Creating layer conv0
I0428 19:47:18.972229 26160 net.cpp:86] Creating Layer conv0
I0428 19:47:18.972234 26160 net.cpp:408] conv0 <- data
I0428 19:47:18.972246 26160 net.cpp:382] conv0 -> conv0
I0428 19:47:19.258895 26160 net.cpp:124] Setting up conv0
I0428 19:47:19.258939 26160 net.cpp:131] Top shape: 64 5 24 24 (184320)
I0428 19:47:19.258942 26160 net.cpp:139] Memory required for data: 938240
I0428 19:47:19.258993 26160 layer_factory.hpp:77] Creating layer pool0
I0428 19:47:19.259007 26160 net.cpp:86] Creating Layer pool0
I0428 19:47:19.259011 26160 net.cpp:408] pool0 <- conv0
I0428 19:47:19.259017 26160 net.cpp:382] pool0 -> pool0
I0428 19:47:19.259068 26160 net.cpp:124] Setting up pool0
I0428 19:47:19.259075 26160 net.cpp:131] Top shape: 64 5 12 12 (46080)
I0428 19:47:19.259078 26160 net.cpp:139] Memory required for data: 1122560
I0428 19:47:19.259083 26160 layer_factory.hpp:77] Creating layer conv1
I0428 19:47:19.259093 26160 net.cpp:86] Creating Layer conv1
I0428 19:47:19.259095 26160 net.cpp:408] conv1 <- pool0
I0428 19:47:19.259100 26160 net.cpp:382] conv1 -> conv1
I0428 19:47:19.261137 26160 net.cpp:124] Setting up conv1
I0428 19:47:19.261153 26160 net.cpp:131] Top shape: 64 2 8 8 (8192)
I0428 19:47:19.261157 26160 net.cpp:139] Memory required for data: 1155328
I0428 19:47:19.261168 26160 layer_factory.hpp:77] Creating layer pool1
I0428 19:47:19.261175 26160 net.cpp:86] Creating Layer pool1
I0428 19:47:19.261178 26160 net.cpp:408] pool1 <- conv1
I0428 19:47:19.261198 26160 net.cpp:382] pool1 -> pool1
I0428 19:47:19.261265 26160 net.cpp:124] Setting up pool1
I0428 19:47:19.261270 26160 net.cpp:131] Top shape: 64 2 4 4 (2048)
I0428 19:47:19.261272 26160 net.cpp:139] Memory required for data: 1163520
I0428 19:47:19.261276 26160 layer_factory.hpp:77] Creating layer ip1
I0428 19:47:19.261282 26160 net.cpp:86] Creating Layer ip1
I0428 19:47:19.261286 26160 net.cpp:408] ip1 <- pool1
I0428 19:47:19.261291 26160 net.cpp:382] ip1 -> ip1
I0428 19:47:19.261399 26160 net.cpp:124] Setting up ip1
I0428 19:47:19.261407 26160 net.cpp:131] Top shape: 64 10 (640)
I0428 19:47:19.261410 26160 net.cpp:139] Memory required for data: 1166080
I0428 19:47:19.261417 26160 layer_factory.hpp:77] Creating layer relu1
I0428 19:47:19.261423 26160 net.cpp:86] Creating Layer relu1
I0428 19:47:19.261426 26160 net.cpp:408] relu1 <- ip1
I0428 19:47:19.261430 26160 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:47:19.261633 26160 net.cpp:124] Setting up relu1
I0428 19:47:19.261642 26160 net.cpp:131] Top shape: 64 10 (640)
I0428 19:47:19.261646 26160 net.cpp:139] Memory required for data: 1168640
I0428 19:47:19.261649 26160 layer_factory.hpp:77] Creating layer ip2
I0428 19:47:19.261656 26160 net.cpp:86] Creating Layer ip2
I0428 19:47:19.261659 26160 net.cpp:408] ip2 <- ip1
I0428 19:47:19.261665 26160 net.cpp:382] ip2 -> ip2
I0428 19:47:19.261765 26160 net.cpp:124] Setting up ip2
I0428 19:47:19.261787 26160 net.cpp:131] Top shape: 64 10 (640)
I0428 19:47:19.261790 26160 net.cpp:139] Memory required for data: 1171200
I0428 19:47:19.261796 26160 layer_factory.hpp:77] Creating layer relu2
I0428 19:47:19.261802 26160 net.cpp:86] Creating Layer relu2
I0428 19:47:19.261806 26160 net.cpp:408] relu2 <- ip2
I0428 19:47:19.261811 26160 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:47:19.262586 26160 net.cpp:124] Setting up relu2
I0428 19:47:19.262615 26160 net.cpp:131] Top shape: 64 10 (640)
I0428 19:47:19.262619 26160 net.cpp:139] Memory required for data: 1173760
I0428 19:47:19.262622 26160 layer_factory.hpp:77] Creating layer loss
I0428 19:47:19.262629 26160 net.cpp:86] Creating Layer loss
I0428 19:47:19.262634 26160 net.cpp:408] loss <- ip2
I0428 19:47:19.262639 26160 net.cpp:408] loss <- label
I0428 19:47:19.262645 26160 net.cpp:382] loss -> loss
I0428 19:47:19.262661 26160 layer_factory.hpp:77] Creating layer loss
I0428 19:47:19.262915 26160 net.cpp:124] Setting up loss
I0428 19:47:19.262925 26160 net.cpp:131] Top shape: (1)
I0428 19:47:19.262929 26160 net.cpp:134]     with loss weight 1
I0428 19:47:19.262944 26160 net.cpp:139] Memory required for data: 1173764
I0428 19:47:19.262948 26160 net.cpp:200] loss needs backward computation.
I0428 19:47:19.262953 26160 net.cpp:200] relu2 needs backward computation.
I0428 19:47:19.262955 26160 net.cpp:200] ip2 needs backward computation.
I0428 19:47:19.262959 26160 net.cpp:200] relu1 needs backward computation.
I0428 19:47:19.262962 26160 net.cpp:200] ip1 needs backward computation.
I0428 19:47:19.262965 26160 net.cpp:200] pool1 needs backward computation.
I0428 19:47:19.262980 26160 net.cpp:200] conv1 needs backward computation.
I0428 19:47:19.262984 26160 net.cpp:200] pool0 needs backward computation.
I0428 19:47:19.262987 26160 net.cpp:200] conv0 needs backward computation.
I0428 19:47:19.262991 26160 net.cpp:202] mnist does not need backward computation.
I0428 19:47:19.262995 26160 net.cpp:244] This network produces output loss
I0428 19:47:19.263003 26160 net.cpp:257] Network initialization done.
I0428 19:47:19.263352 26160 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test511.prototxt
I0428 19:47:19.263392 26160 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:47:19.263476 26160 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0428 19:47:19.263540 26160 layer_factory.hpp:77] Creating layer mnist
I0428 19:47:19.263583 26160 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:47:19.263595 26160 net.cpp:86] Creating Layer mnist
I0428 19:47:19.263599 26160 net.cpp:382] mnist -> data
I0428 19:47:19.263607 26160 net.cpp:382] mnist -> label
I0428 19:47:19.263726 26160 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:47:19.266053 26160 net.cpp:124] Setting up mnist
I0428 19:47:19.266083 26160 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:47:19.266089 26160 net.cpp:131] Top shape: 100 (100)
I0428 19:47:19.266093 26160 net.cpp:139] Memory required for data: 314000
I0428 19:47:19.266096 26160 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:47:19.266104 26160 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:47:19.266108 26160 net.cpp:408] label_mnist_1_split <- label
I0428 19:47:19.266113 26160 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:47:19.266120 26160 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:47:19.266207 26160 net.cpp:124] Setting up label_mnist_1_split
I0428 19:47:19.266227 26160 net.cpp:131] Top shape: 100 (100)
I0428 19:47:19.266232 26160 net.cpp:131] Top shape: 100 (100)
I0428 19:47:19.266234 26160 net.cpp:139] Memory required for data: 314800
I0428 19:47:19.266237 26160 layer_factory.hpp:77] Creating layer conv0
I0428 19:47:19.266247 26160 net.cpp:86] Creating Layer conv0
I0428 19:47:19.266250 26160 net.cpp:408] conv0 <- data
I0428 19:47:19.266255 26160 net.cpp:382] conv0 -> conv0
I0428 19:47:19.267910 26160 net.cpp:124] Setting up conv0
I0428 19:47:19.267940 26160 net.cpp:131] Top shape: 100 5 24 24 (288000)
I0428 19:47:19.267945 26160 net.cpp:139] Memory required for data: 1466800
I0428 19:47:19.267954 26160 layer_factory.hpp:77] Creating layer pool0
I0428 19:47:19.267961 26160 net.cpp:86] Creating Layer pool0
I0428 19:47:19.267964 26160 net.cpp:408] pool0 <- conv0
I0428 19:47:19.267968 26160 net.cpp:382] pool0 -> pool0
I0428 19:47:19.268021 26160 net.cpp:124] Setting up pool0
I0428 19:47:19.268043 26160 net.cpp:131] Top shape: 100 5 12 12 (72000)
I0428 19:47:19.268045 26160 net.cpp:139] Memory required for data: 1754800
I0428 19:47:19.268049 26160 layer_factory.hpp:77] Creating layer conv1
I0428 19:47:19.268057 26160 net.cpp:86] Creating Layer conv1
I0428 19:47:19.268061 26160 net.cpp:408] conv1 <- pool0
I0428 19:47:19.268066 26160 net.cpp:382] conv1 -> conv1
I0428 19:47:19.270324 26160 net.cpp:124] Setting up conv1
I0428 19:47:19.270354 26160 net.cpp:131] Top shape: 100 2 8 8 (12800)
I0428 19:47:19.270359 26160 net.cpp:139] Memory required for data: 1806000
I0428 19:47:19.270368 26160 layer_factory.hpp:77] Creating layer pool1
I0428 19:47:19.270375 26160 net.cpp:86] Creating Layer pool1
I0428 19:47:19.270380 26160 net.cpp:408] pool1 <- conv1
I0428 19:47:19.270385 26160 net.cpp:382] pool1 -> pool1
I0428 19:47:19.270457 26160 net.cpp:124] Setting up pool1
I0428 19:47:19.270463 26160 net.cpp:131] Top shape: 100 2 4 4 (3200)
I0428 19:47:19.270467 26160 net.cpp:139] Memory required for data: 1818800
I0428 19:47:19.270470 26160 layer_factory.hpp:77] Creating layer ip1
I0428 19:47:19.270476 26160 net.cpp:86] Creating Layer ip1
I0428 19:47:19.270480 26160 net.cpp:408] ip1 <- pool1
I0428 19:47:19.270485 26160 net.cpp:382] ip1 -> ip1
I0428 19:47:19.270629 26160 net.cpp:124] Setting up ip1
I0428 19:47:19.270638 26160 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:47:19.270642 26160 net.cpp:139] Memory required for data: 1822800
I0428 19:47:19.270649 26160 layer_factory.hpp:77] Creating layer relu1
I0428 19:47:19.270654 26160 net.cpp:86] Creating Layer relu1
I0428 19:47:19.270658 26160 net.cpp:408] relu1 <- ip1
I0428 19:47:19.270663 26160 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:47:19.270877 26160 net.cpp:124] Setting up relu1
I0428 19:47:19.270887 26160 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:47:19.270891 26160 net.cpp:139] Memory required for data: 1826800
I0428 19:47:19.270895 26160 layer_factory.hpp:77] Creating layer ip2
I0428 19:47:19.270902 26160 net.cpp:86] Creating Layer ip2
I0428 19:47:19.270906 26160 net.cpp:408] ip2 <- ip1
I0428 19:47:19.270911 26160 net.cpp:382] ip2 -> ip2
I0428 19:47:19.271013 26160 net.cpp:124] Setting up ip2
I0428 19:47:19.271021 26160 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:47:19.271025 26160 net.cpp:139] Memory required for data: 1830800
I0428 19:47:19.271031 26160 layer_factory.hpp:77] Creating layer relu2
I0428 19:47:19.271036 26160 net.cpp:86] Creating Layer relu2
I0428 19:47:19.271039 26160 net.cpp:408] relu2 <- ip2
I0428 19:47:19.271044 26160 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:47:19.271214 26160 net.cpp:124] Setting up relu2
I0428 19:47:19.271222 26160 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:47:19.271226 26160 net.cpp:139] Memory required for data: 1834800
I0428 19:47:19.271229 26160 layer_factory.hpp:77] Creating layer ip2_relu2_0_split
I0428 19:47:19.271234 26160 net.cpp:86] Creating Layer ip2_relu2_0_split
I0428 19:47:19.271236 26160 net.cpp:408] ip2_relu2_0_split <- ip2
I0428 19:47:19.271241 26160 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_0
I0428 19:47:19.271260 26160 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_1
I0428 19:47:19.271297 26160 net.cpp:124] Setting up ip2_relu2_0_split
I0428 19:47:19.271304 26160 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:47:19.271308 26160 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:47:19.271311 26160 net.cpp:139] Memory required for data: 1842800
I0428 19:47:19.271314 26160 layer_factory.hpp:77] Creating layer accuracy
I0428 19:47:19.271337 26160 net.cpp:86] Creating Layer accuracy
I0428 19:47:19.271340 26160 net.cpp:408] accuracy <- ip2_relu2_0_split_0
I0428 19:47:19.271345 26160 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:47:19.271349 26160 net.cpp:382] accuracy -> accuracy
I0428 19:47:19.271358 26160 net.cpp:124] Setting up accuracy
I0428 19:47:19.271361 26160 net.cpp:131] Top shape: (1)
I0428 19:47:19.271364 26160 net.cpp:139] Memory required for data: 1842804
I0428 19:47:19.271374 26160 layer_factory.hpp:77] Creating layer loss
I0428 19:47:19.271381 26160 net.cpp:86] Creating Layer loss
I0428 19:47:19.271384 26160 net.cpp:408] loss <- ip2_relu2_0_split_1
I0428 19:47:19.271389 26160 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:47:19.271394 26160 net.cpp:382] loss -> loss
I0428 19:47:19.271400 26160 layer_factory.hpp:77] Creating layer loss
I0428 19:47:19.271672 26160 net.cpp:124] Setting up loss
I0428 19:47:19.271680 26160 net.cpp:131] Top shape: (1)
I0428 19:47:19.271684 26160 net.cpp:134]     with loss weight 1
I0428 19:47:19.271690 26160 net.cpp:139] Memory required for data: 1842808
I0428 19:47:19.271693 26160 net.cpp:200] loss needs backward computation.
I0428 19:47:19.271697 26160 net.cpp:202] accuracy does not need backward computation.
I0428 19:47:19.271702 26160 net.cpp:200] ip2_relu2_0_split needs backward computation.
I0428 19:47:19.271704 26160 net.cpp:200] relu2 needs backward computation.
I0428 19:47:19.271723 26160 net.cpp:200] ip2 needs backward computation.
I0428 19:47:19.271725 26160 net.cpp:200] relu1 needs backward computation.
I0428 19:47:19.271728 26160 net.cpp:200] ip1 needs backward computation.
I0428 19:47:19.271731 26160 net.cpp:200] pool1 needs backward computation.
I0428 19:47:19.271734 26160 net.cpp:200] conv1 needs backward computation.
I0428 19:47:19.271737 26160 net.cpp:200] pool0 needs backward computation.
I0428 19:47:19.271740 26160 net.cpp:200] conv0 needs backward computation.
I0428 19:47:19.271744 26160 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:47:19.271747 26160 net.cpp:202] mnist does not need backward computation.
I0428 19:47:19.271751 26160 net.cpp:244] This network produces output accuracy
I0428 19:47:19.271754 26160 net.cpp:244] This network produces output loss
I0428 19:47:19.271764 26160 net.cpp:257] Network initialization done.
I0428 19:47:19.271801 26160 solver.cpp:56] Solver scaffolding done.
I0428 19:47:19.272068 26160 caffe.cpp:248] Starting Optimization
I0428 19:47:19.272074 26160 solver.cpp:273] Solving LeNet
I0428 19:47:19.272078 26160 solver.cpp:274] Learning Rate Policy: inv
I0428 19:47:19.272948 26160 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:47:19.276417 26160 blocking_queue.cpp:49] Waiting for data
I0428 19:47:19.350075 26167 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:47:19.350543 26160 solver.cpp:398]     Test net output #0: accuracy = 0.109
I0428 19:47:19.350575 26160 solver.cpp:398]     Test net output #1: loss = 2.31082 (* 1 = 2.31082 loss)
I0428 19:47:19.352461 26160 solver.cpp:219] Iteration 0 (-1.02855e-42 iter/s, 0.0803581s/100 iters), loss = 2.29097
I0428 19:47:19.352500 26160 solver.cpp:238]     Train net output #0: loss = 2.29097 (* 1 = 2.29097 loss)
I0428 19:47:19.352514 26160 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:47:19.430580 26160 solver.cpp:219] Iteration 100 (1280.72 iter/s, 0.078081s/100 iters), loss = 1.42136
I0428 19:47:19.430621 26160 solver.cpp:238]     Train net output #0: loss = 1.42136 (* 1 = 1.42136 loss)
I0428 19:47:19.430644 26160 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:47:19.497529 26160 solver.cpp:219] Iteration 200 (1494.43 iter/s, 0.0669154s/100 iters), loss = 1.3259
I0428 19:47:19.497583 26160 solver.cpp:238]     Train net output #0: loss = 1.3259 (* 1 = 1.3259 loss)
I0428 19:47:19.497591 26160 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:47:19.560978 26160 solver.cpp:219] Iteration 300 (1577.59 iter/s, 0.0633876s/100 iters), loss = 1.36254
I0428 19:47:19.561019 26160 solver.cpp:238]     Train net output #0: loss = 1.36254 (* 1 = 1.36254 loss)
I0428 19:47:19.561027 26160 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:47:19.624383 26160 solver.cpp:219] Iteration 400 (1577.98 iter/s, 0.0633722s/100 iters), loss = 1.26466
I0428 19:47:19.624423 26160 solver.cpp:238]     Train net output #0: loss = 1.26466 (* 1 = 1.26466 loss)
I0428 19:47:19.624428 26160 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:47:19.686655 26160 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:47:19.763331 26167 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:47:19.763828 26160 solver.cpp:398]     Test net output #0: accuracy = 0.698
I0428 19:47:19.763862 26160 solver.cpp:398]     Test net output #1: loss = 1.03615 (* 1 = 1.03615 loss)
I0428 19:47:19.764581 26160 solver.cpp:219] Iteration 500 (713.46 iter/s, 0.140162s/100 iters), loss = 1.06863
I0428 19:47:19.764618 26160 solver.cpp:238]     Train net output #0: loss = 1.06863 (* 1 = 1.06863 loss)
I0428 19:47:19.764626 26160 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:47:19.837049 26160 solver.cpp:219] Iteration 600 (1380.5 iter/s, 0.0724377s/100 iters), loss = 0.937056
I0428 19:47:19.837088 26160 solver.cpp:238]     Train net output #0: loss = 0.937056 (* 1 = 0.937056 loss)
I0428 19:47:19.837095 26160 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:47:19.899732 26160 solver.cpp:219] Iteration 700 (1596.53 iter/s, 0.062636s/100 iters), loss = 0.729311
I0428 19:47:19.899771 26160 solver.cpp:238]     Train net output #0: loss = 0.729311 (* 1 = 0.729311 loss)
I0428 19:47:19.899778 26160 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:47:19.960840 26160 solver.cpp:219] Iteration 800 (1637.71 iter/s, 0.061061s/100 iters), loss = 1.17366
I0428 19:47:19.960880 26160 solver.cpp:238]     Train net output #0: loss = 1.17366 (* 1 = 1.17366 loss)
I0428 19:47:19.960886 26160 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:47:20.022450 26160 solver.cpp:219] Iteration 900 (1623.95 iter/s, 0.0615782s/100 iters), loss = 1.16952
I0428 19:47:20.022490 26160 solver.cpp:238]     Train net output #0: loss = 1.16952 (* 1 = 1.16952 loss)
I0428 19:47:20.022495 26160 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:47:20.043166 26166 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:47:20.084619 26160 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:47:20.085249 26160 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:47:20.085621 26160 solver.cpp:311] Iteration 1000, loss = 0.825865
I0428 19:47:20.085638 26160 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:47:20.160465 26167 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:47:20.161048 26160 solver.cpp:398]     Test net output #0: accuracy = 0.7253
I0428 19:47:20.161072 26160 solver.cpp:398]     Test net output #1: loss = 0.919222 (* 1 = 0.919222 loss)
I0428 19:47:20.161079 26160 solver.cpp:316] Optimization Done.
I0428 19:47:20.161083 26160 caffe.cpp:259] Optimization Done.
