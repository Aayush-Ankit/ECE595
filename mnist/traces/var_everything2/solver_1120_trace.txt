I0428 20:09:51.818707 31718 caffe.cpp:218] Using GPUs 0
I0428 20:09:51.853606 31718 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:09:52.359112 31718 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1120.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:09:52.359252 31718 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1120.prototxt
I0428 20:09:52.359645 31718 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:09:52.359663 31718 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:09:52.359757 31718 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:09:52.359829 31718 layer_factory.hpp:77] Creating layer mnist
I0428 20:09:52.359923 31718 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:09:52.359946 31718 net.cpp:86] Creating Layer mnist
I0428 20:09:52.359953 31718 net.cpp:382] mnist -> data
I0428 20:09:52.359975 31718 net.cpp:382] mnist -> label
I0428 20:09:52.361073 31718 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:09:52.363512 31718 net.cpp:124] Setting up mnist
I0428 20:09:52.363530 31718 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:09:52.363536 31718 net.cpp:131] Top shape: 64 (64)
I0428 20:09:52.363540 31718 net.cpp:139] Memory required for data: 200960
I0428 20:09:52.363548 31718 layer_factory.hpp:77] Creating layer conv0
I0428 20:09:52.363562 31718 net.cpp:86] Creating Layer conv0
I0428 20:09:52.363582 31718 net.cpp:408] conv0 <- data
I0428 20:09:52.363595 31718 net.cpp:382] conv0 -> conv0
I0428 20:09:52.626648 31718 net.cpp:124] Setting up conv0
I0428 20:09:52.626690 31718 net.cpp:131] Top shape: 64 25 24 24 (921600)
I0428 20:09:52.626694 31718 net.cpp:139] Memory required for data: 3887360
I0428 20:09:52.626708 31718 layer_factory.hpp:77] Creating layer pool0
I0428 20:09:52.626720 31718 net.cpp:86] Creating Layer pool0
I0428 20:09:52.626724 31718 net.cpp:408] pool0 <- conv0
I0428 20:09:52.626729 31718 net.cpp:382] pool0 -> pool0
I0428 20:09:52.626790 31718 net.cpp:124] Setting up pool0
I0428 20:09:52.626794 31718 net.cpp:131] Top shape: 64 25 12 12 (230400)
I0428 20:09:52.626797 31718 net.cpp:139] Memory required for data: 4808960
I0428 20:09:52.626801 31718 layer_factory.hpp:77] Creating layer conv1
I0428 20:09:52.626811 31718 net.cpp:86] Creating Layer conv1
I0428 20:09:52.626813 31718 net.cpp:408] conv1 <- pool0
I0428 20:09:52.626818 31718 net.cpp:382] conv1 -> conv1
I0428 20:09:52.629739 31718 net.cpp:124] Setting up conv1
I0428 20:09:52.629770 31718 net.cpp:131] Top shape: 64 25 8 8 (102400)
I0428 20:09:52.629772 31718 net.cpp:139] Memory required for data: 5218560
I0428 20:09:52.629781 31718 layer_factory.hpp:77] Creating layer pool1
I0428 20:09:52.629788 31718 net.cpp:86] Creating Layer pool1
I0428 20:09:52.629791 31718 net.cpp:408] pool1 <- conv1
I0428 20:09:52.629796 31718 net.cpp:382] pool1 -> pool1
I0428 20:09:52.629847 31718 net.cpp:124] Setting up pool1
I0428 20:09:52.629851 31718 net.cpp:131] Top shape: 64 25 4 4 (25600)
I0428 20:09:52.629854 31718 net.cpp:139] Memory required for data: 5320960
I0428 20:09:52.629858 31718 layer_factory.hpp:77] Creating layer ip1
I0428 20:09:52.629864 31718 net.cpp:86] Creating Layer ip1
I0428 20:09:52.629868 31718 net.cpp:408] ip1 <- pool1
I0428 20:09:52.629871 31718 net.cpp:382] ip1 -> ip1
I0428 20:09:52.630012 31718 net.cpp:124] Setting up ip1
I0428 20:09:52.630019 31718 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:09:52.630023 31718 net.cpp:139] Memory required for data: 5327360
I0428 20:09:52.630029 31718 layer_factory.hpp:77] Creating layer relu1
I0428 20:09:52.630035 31718 net.cpp:86] Creating Layer relu1
I0428 20:09:52.630038 31718 net.cpp:408] relu1 <- ip1
I0428 20:09:52.630043 31718 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:09:52.630198 31718 net.cpp:124] Setting up relu1
I0428 20:09:52.630206 31718 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:09:52.630209 31718 net.cpp:139] Memory required for data: 5333760
I0428 20:09:52.630211 31718 layer_factory.hpp:77] Creating layer ip2
I0428 20:09:52.630218 31718 net.cpp:86] Creating Layer ip2
I0428 20:09:52.630220 31718 net.cpp:408] ip2 <- ip1
I0428 20:09:52.630225 31718 net.cpp:382] ip2 -> ip2
I0428 20:09:52.631167 31718 net.cpp:124] Setting up ip2
I0428 20:09:52.631180 31718 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:09:52.631199 31718 net.cpp:139] Memory required for data: 5346560
I0428 20:09:52.631206 31718 layer_factory.hpp:77] Creating layer relu2
I0428 20:09:52.631211 31718 net.cpp:86] Creating Layer relu2
I0428 20:09:52.631216 31718 net.cpp:408] relu2 <- ip2
I0428 20:09:52.631219 31718 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:09:52.631937 31718 net.cpp:124] Setting up relu2
I0428 20:09:52.631949 31718 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:09:52.631968 31718 net.cpp:139] Memory required for data: 5359360
I0428 20:09:52.631971 31718 layer_factory.hpp:77] Creating layer ip3
I0428 20:09:52.631978 31718 net.cpp:86] Creating Layer ip3
I0428 20:09:52.631981 31718 net.cpp:408] ip3 <- ip2
I0428 20:09:52.631988 31718 net.cpp:382] ip3 -> ip3
I0428 20:09:52.632086 31718 net.cpp:124] Setting up ip3
I0428 20:09:52.632094 31718 net.cpp:131] Top shape: 64 10 (640)
I0428 20:09:52.632097 31718 net.cpp:139] Memory required for data: 5361920
I0428 20:09:52.632105 31718 layer_factory.hpp:77] Creating layer relu3
I0428 20:09:52.632110 31718 net.cpp:86] Creating Layer relu3
I0428 20:09:52.632113 31718 net.cpp:408] relu3 <- ip3
I0428 20:09:52.632117 31718 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:09:52.632278 31718 net.cpp:124] Setting up relu3
I0428 20:09:52.632287 31718 net.cpp:131] Top shape: 64 10 (640)
I0428 20:09:52.632289 31718 net.cpp:139] Memory required for data: 5364480
I0428 20:09:52.632292 31718 layer_factory.hpp:77] Creating layer loss
I0428 20:09:52.632298 31718 net.cpp:86] Creating Layer loss
I0428 20:09:52.632302 31718 net.cpp:408] loss <- ip3
I0428 20:09:52.632305 31718 net.cpp:408] loss <- label
I0428 20:09:52.632310 31718 net.cpp:382] loss -> loss
I0428 20:09:52.632328 31718 layer_factory.hpp:77] Creating layer loss
I0428 20:09:52.632575 31718 net.cpp:124] Setting up loss
I0428 20:09:52.632585 31718 net.cpp:131] Top shape: (1)
I0428 20:09:52.632588 31718 net.cpp:134]     with loss weight 1
I0428 20:09:52.632601 31718 net.cpp:139] Memory required for data: 5364484
I0428 20:09:52.632604 31718 net.cpp:200] loss needs backward computation.
I0428 20:09:52.632608 31718 net.cpp:200] relu3 needs backward computation.
I0428 20:09:52.632611 31718 net.cpp:200] ip3 needs backward computation.
I0428 20:09:52.632614 31718 net.cpp:200] relu2 needs backward computation.
I0428 20:09:52.632617 31718 net.cpp:200] ip2 needs backward computation.
I0428 20:09:52.632621 31718 net.cpp:200] relu1 needs backward computation.
I0428 20:09:52.632622 31718 net.cpp:200] ip1 needs backward computation.
I0428 20:09:52.632625 31718 net.cpp:200] pool1 needs backward computation.
I0428 20:09:52.632628 31718 net.cpp:200] conv1 needs backward computation.
I0428 20:09:52.632632 31718 net.cpp:200] pool0 needs backward computation.
I0428 20:09:52.632634 31718 net.cpp:200] conv0 needs backward computation.
I0428 20:09:52.632638 31718 net.cpp:202] mnist does not need backward computation.
I0428 20:09:52.632642 31718 net.cpp:244] This network produces output loss
I0428 20:09:52.632649 31718 net.cpp:257] Network initialization done.
I0428 20:09:52.633046 31718 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1120.prototxt
I0428 20:09:52.633091 31718 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:09:52.633215 31718 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:09:52.633294 31718 layer_factory.hpp:77] Creating layer mnist
I0428 20:09:52.633335 31718 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:09:52.633350 31718 net.cpp:86] Creating Layer mnist
I0428 20:09:52.633355 31718 net.cpp:382] mnist -> data
I0428 20:09:52.633363 31718 net.cpp:382] mnist -> label
I0428 20:09:52.633446 31718 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:09:52.635377 31718 net.cpp:124] Setting up mnist
I0428 20:09:52.635421 31718 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:09:52.635426 31718 net.cpp:131] Top shape: 100 (100)
I0428 20:09:52.635444 31718 net.cpp:139] Memory required for data: 314000
I0428 20:09:52.635448 31718 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:09:52.635458 31718 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:09:52.635462 31718 net.cpp:408] label_mnist_1_split <- label
I0428 20:09:52.635468 31718 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:09:52.635473 31718 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:09:52.635570 31718 net.cpp:124] Setting up label_mnist_1_split
I0428 20:09:52.635577 31718 net.cpp:131] Top shape: 100 (100)
I0428 20:09:52.635581 31718 net.cpp:131] Top shape: 100 (100)
I0428 20:09:52.635584 31718 net.cpp:139] Memory required for data: 314800
I0428 20:09:52.635587 31718 layer_factory.hpp:77] Creating layer conv0
I0428 20:09:52.635596 31718 net.cpp:86] Creating Layer conv0
I0428 20:09:52.635599 31718 net.cpp:408] conv0 <- data
I0428 20:09:52.635604 31718 net.cpp:382] conv0 -> conv0
I0428 20:09:52.637217 31718 net.cpp:124] Setting up conv0
I0428 20:09:52.637245 31718 net.cpp:131] Top shape: 100 25 24 24 (1440000)
I0428 20:09:52.637249 31718 net.cpp:139] Memory required for data: 6074800
I0428 20:09:52.637259 31718 layer_factory.hpp:77] Creating layer pool0
I0428 20:09:52.637264 31718 net.cpp:86] Creating Layer pool0
I0428 20:09:52.637267 31718 net.cpp:408] pool0 <- conv0
I0428 20:09:52.637272 31718 net.cpp:382] pool0 -> pool0
I0428 20:09:52.637307 31718 net.cpp:124] Setting up pool0
I0428 20:09:52.637313 31718 net.cpp:131] Top shape: 100 25 12 12 (360000)
I0428 20:09:52.637316 31718 net.cpp:139] Memory required for data: 7514800
I0428 20:09:52.637320 31718 layer_factory.hpp:77] Creating layer conv1
I0428 20:09:52.637327 31718 net.cpp:86] Creating Layer conv1
I0428 20:09:52.637331 31718 net.cpp:408] conv1 <- pool0
I0428 20:09:52.637336 31718 net.cpp:382] conv1 -> conv1
I0428 20:09:52.639530 31718 net.cpp:124] Setting up conv1
I0428 20:09:52.639559 31718 net.cpp:131] Top shape: 100 25 8 8 (160000)
I0428 20:09:52.639562 31718 net.cpp:139] Memory required for data: 8154800
I0428 20:09:52.639571 31718 layer_factory.hpp:77] Creating layer pool1
I0428 20:09:52.639577 31718 net.cpp:86] Creating Layer pool1
I0428 20:09:52.639580 31718 net.cpp:408] pool1 <- conv1
I0428 20:09:52.639585 31718 net.cpp:382] pool1 -> pool1
I0428 20:09:52.639644 31718 net.cpp:124] Setting up pool1
I0428 20:09:52.639667 31718 net.cpp:131] Top shape: 100 25 4 4 (40000)
I0428 20:09:52.639669 31718 net.cpp:139] Memory required for data: 8314800
I0428 20:09:52.639672 31718 layer_factory.hpp:77] Creating layer ip1
I0428 20:09:52.639678 31718 net.cpp:86] Creating Layer ip1
I0428 20:09:52.639681 31718 net.cpp:408] ip1 <- pool1
I0428 20:09:52.639686 31718 net.cpp:382] ip1 -> ip1
I0428 20:09:52.639883 31718 net.cpp:124] Setting up ip1
I0428 20:09:52.639891 31718 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:09:52.639905 31718 net.cpp:139] Memory required for data: 8324800
I0428 20:09:52.639912 31718 layer_factory.hpp:77] Creating layer relu1
I0428 20:09:52.639919 31718 net.cpp:86] Creating Layer relu1
I0428 20:09:52.639921 31718 net.cpp:408] relu1 <- ip1
I0428 20:09:52.639926 31718 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:09:52.640101 31718 net.cpp:124] Setting up relu1
I0428 20:09:52.640110 31718 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:09:52.640115 31718 net.cpp:139] Memory required for data: 8334800
I0428 20:09:52.640117 31718 layer_factory.hpp:77] Creating layer ip2
I0428 20:09:52.640125 31718 net.cpp:86] Creating Layer ip2
I0428 20:09:52.640127 31718 net.cpp:408] ip2 <- ip1
I0428 20:09:52.640132 31718 net.cpp:382] ip2 -> ip2
I0428 20:09:52.640236 31718 net.cpp:124] Setting up ip2
I0428 20:09:52.640244 31718 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:09:52.640247 31718 net.cpp:139] Memory required for data: 8354800
I0428 20:09:52.640261 31718 layer_factory.hpp:77] Creating layer relu2
I0428 20:09:52.640265 31718 net.cpp:86] Creating Layer relu2
I0428 20:09:52.640269 31718 net.cpp:408] relu2 <- ip2
I0428 20:09:52.640274 31718 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:09:52.640449 31718 net.cpp:124] Setting up relu2
I0428 20:09:52.640457 31718 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:09:52.640461 31718 net.cpp:139] Memory required for data: 8374800
I0428 20:09:52.640465 31718 layer_factory.hpp:77] Creating layer ip3
I0428 20:09:52.640471 31718 net.cpp:86] Creating Layer ip3
I0428 20:09:52.640475 31718 net.cpp:408] ip3 <- ip2
I0428 20:09:52.640480 31718 net.cpp:382] ip3 -> ip3
I0428 20:09:52.640578 31718 net.cpp:124] Setting up ip3
I0428 20:09:52.640585 31718 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:09:52.640588 31718 net.cpp:139] Memory required for data: 8378800
I0428 20:09:52.640595 31718 layer_factory.hpp:77] Creating layer relu3
I0428 20:09:52.640600 31718 net.cpp:86] Creating Layer relu3
I0428 20:09:52.640604 31718 net.cpp:408] relu3 <- ip3
I0428 20:09:52.640607 31718 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:09:52.641535 31718 net.cpp:124] Setting up relu3
I0428 20:09:52.641561 31718 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:09:52.641566 31718 net.cpp:139] Memory required for data: 8382800
I0428 20:09:52.641568 31718 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:09:52.641574 31718 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:09:52.641578 31718 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:09:52.641583 31718 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:09:52.641589 31718 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:09:52.641626 31718 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:09:52.641631 31718 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:09:52.641635 31718 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:09:52.641638 31718 net.cpp:139] Memory required for data: 8390800
I0428 20:09:52.641641 31718 layer_factory.hpp:77] Creating layer accuracy
I0428 20:09:52.641646 31718 net.cpp:86] Creating Layer accuracy
I0428 20:09:52.641650 31718 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:09:52.641654 31718 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:09:52.641665 31718 net.cpp:382] accuracy -> accuracy
I0428 20:09:52.641674 31718 net.cpp:124] Setting up accuracy
I0428 20:09:52.641677 31718 net.cpp:131] Top shape: (1)
I0428 20:09:52.641680 31718 net.cpp:139] Memory required for data: 8390804
I0428 20:09:52.641683 31718 layer_factory.hpp:77] Creating layer loss
I0428 20:09:52.641687 31718 net.cpp:86] Creating Layer loss
I0428 20:09:52.641690 31718 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:09:52.641700 31718 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:09:52.641705 31718 net.cpp:382] loss -> loss
I0428 20:09:52.641710 31718 layer_factory.hpp:77] Creating layer loss
I0428 20:09:52.641969 31718 net.cpp:124] Setting up loss
I0428 20:09:52.641979 31718 net.cpp:131] Top shape: (1)
I0428 20:09:52.641988 31718 net.cpp:134]     with loss weight 1
I0428 20:09:52.642004 31718 net.cpp:139] Memory required for data: 8390808
I0428 20:09:52.642009 31718 net.cpp:200] loss needs backward computation.
I0428 20:09:52.642014 31718 net.cpp:202] accuracy does not need backward computation.
I0428 20:09:52.642017 31718 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:09:52.642020 31718 net.cpp:200] relu3 needs backward computation.
I0428 20:09:52.642024 31718 net.cpp:200] ip3 needs backward computation.
I0428 20:09:52.642026 31718 net.cpp:200] relu2 needs backward computation.
I0428 20:09:52.642030 31718 net.cpp:200] ip2 needs backward computation.
I0428 20:09:52.642032 31718 net.cpp:200] relu1 needs backward computation.
I0428 20:09:52.642035 31718 net.cpp:200] ip1 needs backward computation.
I0428 20:09:52.642038 31718 net.cpp:200] pool1 needs backward computation.
I0428 20:09:52.642042 31718 net.cpp:200] conv1 needs backward computation.
I0428 20:09:52.642045 31718 net.cpp:200] pool0 needs backward computation.
I0428 20:09:52.642048 31718 net.cpp:200] conv0 needs backward computation.
I0428 20:09:52.642052 31718 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:09:52.642057 31718 net.cpp:202] mnist does not need backward computation.
I0428 20:09:52.642060 31718 net.cpp:244] This network produces output accuracy
I0428 20:09:52.642063 31718 net.cpp:244] This network produces output loss
I0428 20:09:52.642073 31718 net.cpp:257] Network initialization done.
I0428 20:09:52.642117 31718 solver.cpp:56] Solver scaffolding done.
I0428 20:09:52.642457 31718 caffe.cpp:248] Starting Optimization
I0428 20:09:52.642463 31718 solver.cpp:273] Solving LeNet
I0428 20:09:52.642467 31718 solver.cpp:274] Learning Rate Policy: inv
I0428 20:09:52.643290 31718 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:09:52.649150 31718 blocking_queue.cpp:49] Waiting for data
I0428 20:09:52.710400 31725 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:09:52.711031 31718 solver.cpp:398]     Test net output #0: accuracy = 0.0937
I0428 20:09:52.711066 31718 solver.cpp:398]     Test net output #1: loss = 2.30322 (* 1 = 2.30322 loss)
I0428 20:09:52.714660 31718 solver.cpp:219] Iteration 0 (-1.24623e-30 iter/s, 0.0721469s/100 iters), loss = 2.30343
I0428 20:09:52.714699 31718 solver.cpp:238]     Train net output #0: loss = 2.30343 (* 1 = 2.30343 loss)
I0428 20:09:52.714710 31718 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:09:52.822104 31718 solver.cpp:219] Iteration 100 (931.035 iter/s, 0.107407s/100 iters), loss = 0.297381
I0428 20:09:52.822139 31718 solver.cpp:238]     Train net output #0: loss = 0.297381 (* 1 = 0.297381 loss)
I0428 20:09:52.822149 31718 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:09:52.924212 31718 solver.cpp:219] Iteration 200 (979.776 iter/s, 0.102064s/100 iters), loss = 0.173343
I0428 20:09:52.924237 31718 solver.cpp:238]     Train net output #0: loss = 0.173343 (* 1 = 0.173343 loss)
I0428 20:09:52.924242 31718 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:09:53.031070 31718 solver.cpp:219] Iteration 300 (936.129 iter/s, 0.106823s/100 iters), loss = 0.276318
I0428 20:09:53.031111 31718 solver.cpp:238]     Train net output #0: loss = 0.276318 (* 1 = 0.276318 loss)
I0428 20:09:53.031118 31718 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:09:53.138470 31718 solver.cpp:219] Iteration 400 (931.408 iter/s, 0.107364s/100 iters), loss = 0.0990307
I0428 20:09:53.138510 31718 solver.cpp:238]     Train net output #0: loss = 0.0990307 (* 1 = 0.0990307 loss)
I0428 20:09:53.138517 31718 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:09:53.241248 31718 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:09:53.298127 31725 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:09:53.298781 31718 solver.cpp:398]     Test net output #0: accuracy = 0.959
I0428 20:09:53.298815 31718 solver.cpp:398]     Test net output #1: loss = 0.122266 (* 1 = 0.122266 loss)
I0428 20:09:53.299871 31718 solver.cpp:219] Iteration 500 (619.781 iter/s, 0.161347s/100 iters), loss = 0.166446
I0428 20:09:53.299962 31718 solver.cpp:238]     Train net output #0: loss = 0.166446 (* 1 = 0.166446 loss)
I0428 20:09:53.299970 31718 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:09:53.409323 31718 solver.cpp:219] Iteration 600 (914.339 iter/s, 0.109369s/100 iters), loss = 0.0864291
I0428 20:09:53.409364 31718 solver.cpp:238]     Train net output #0: loss = 0.0864291 (* 1 = 0.0864291 loss)
I0428 20:09:53.409371 31718 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:09:53.509328 31718 solver.cpp:219] Iteration 700 (1000.47 iter/s, 0.0999526s/100 iters), loss = 0.147419
I0428 20:09:53.509354 31718 solver.cpp:238]     Train net output #0: loss = 0.147419 (* 1 = 0.147419 loss)
I0428 20:09:53.509361 31718 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:09:53.607256 31718 solver.cpp:219] Iteration 800 (1021.53 iter/s, 0.0978924s/100 iters), loss = 0.321712
I0428 20:09:53.607297 31718 solver.cpp:238]     Train net output #0: loss = 0.321712 (* 1 = 0.321712 loss)
I0428 20:09:53.607303 31718 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:09:53.710165 31718 solver.cpp:219] Iteration 900 (972.062 iter/s, 0.102874s/100 iters), loss = 0.154894
I0428 20:09:53.710206 31718 solver.cpp:238]     Train net output #0: loss = 0.154894 (* 1 = 0.154894 loss)
I0428 20:09:53.710211 31718 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:09:53.743712 31724 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:09:53.809195 31718 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:09:53.810436 31718 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:09:53.811188 31718 solver.cpp:311] Iteration 1000, loss = 0.042918
I0428 20:09:53.811203 31718 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:09:53.884480 31725 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:09:53.885090 31718 solver.cpp:398]     Test net output #0: accuracy = 0.9769
I0428 20:09:53.885109 31718 solver.cpp:398]     Test net output #1: loss = 0.0688005 (* 1 = 0.0688005 loss)
I0428 20:09:53.885115 31718 solver.cpp:316] Optimization Done.
I0428 20:09:53.885118 31718 caffe.cpp:259] Optimization Done.
