I0428 20:09:30.984388 31641 caffe.cpp:218] Using GPUs 0
I0428 20:09:31.021610 31641 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:09:31.543583 31641 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1111.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:09:31.543720 31641 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1111.prototxt
I0428 20:09:31.544142 31641 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:09:31.544162 31641 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:09:31.544271 31641 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:09:31.544353 31641 layer_factory.hpp:77] Creating layer mnist
I0428 20:09:31.544452 31641 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:09:31.544476 31641 net.cpp:86] Creating Layer mnist
I0428 20:09:31.544486 31641 net.cpp:382] mnist -> data
I0428 20:09:31.544507 31641 net.cpp:382] mnist -> label
I0428 20:09:31.545608 31641 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:09:31.548082 31641 net.cpp:124] Setting up mnist
I0428 20:09:31.548099 31641 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:09:31.548105 31641 net.cpp:131] Top shape: 64 (64)
I0428 20:09:31.548108 31641 net.cpp:139] Memory required for data: 200960
I0428 20:09:31.548115 31641 layer_factory.hpp:77] Creating layer conv0
I0428 20:09:31.548147 31641 net.cpp:86] Creating Layer conv0
I0428 20:09:31.548169 31641 net.cpp:408] conv0 <- data
I0428 20:09:31.548183 31641 net.cpp:382] conv0 -> conv0
I0428 20:09:31.836576 31641 net.cpp:124] Setting up conv0
I0428 20:09:31.836607 31641 net.cpp:131] Top shape: 64 25 24 24 (921600)
I0428 20:09:31.836612 31641 net.cpp:139] Memory required for data: 3887360
I0428 20:09:31.836632 31641 layer_factory.hpp:77] Creating layer pool0
I0428 20:09:31.836647 31641 net.cpp:86] Creating Layer pool0
I0428 20:09:31.836652 31641 net.cpp:408] pool0 <- conv0
I0428 20:09:31.836658 31641 net.cpp:382] pool0 -> pool0
I0428 20:09:31.836714 31641 net.cpp:124] Setting up pool0
I0428 20:09:31.836731 31641 net.cpp:131] Top shape: 64 25 12 12 (230400)
I0428 20:09:31.836735 31641 net.cpp:139] Memory required for data: 4808960
I0428 20:09:31.836738 31641 layer_factory.hpp:77] Creating layer conv1
I0428 20:09:31.836751 31641 net.cpp:86] Creating Layer conv1
I0428 20:09:31.836756 31641 net.cpp:408] conv1 <- pool0
I0428 20:09:31.836761 31641 net.cpp:382] conv1 -> conv1
I0428 20:09:31.840026 31641 net.cpp:124] Setting up conv1
I0428 20:09:31.840044 31641 net.cpp:131] Top shape: 64 25 8 8 (102400)
I0428 20:09:31.840049 31641 net.cpp:139] Memory required for data: 5218560
I0428 20:09:31.840060 31641 layer_factory.hpp:77] Creating layer pool1
I0428 20:09:31.840070 31641 net.cpp:86] Creating Layer pool1
I0428 20:09:31.840075 31641 net.cpp:408] pool1 <- conv1
I0428 20:09:31.840080 31641 net.cpp:382] pool1 -> pool1
I0428 20:09:31.840126 31641 net.cpp:124] Setting up pool1
I0428 20:09:31.840132 31641 net.cpp:131] Top shape: 64 25 4 4 (25600)
I0428 20:09:31.840137 31641 net.cpp:139] Memory required for data: 5320960
I0428 20:09:31.840139 31641 layer_factory.hpp:77] Creating layer ip1
I0428 20:09:31.840147 31641 net.cpp:86] Creating Layer ip1
I0428 20:09:31.840152 31641 net.cpp:408] ip1 <- pool1
I0428 20:09:31.840157 31641 net.cpp:382] ip1 -> ip1
I0428 20:09:31.841334 31641 net.cpp:124] Setting up ip1
I0428 20:09:31.841351 31641 net.cpp:131] Top shape: 64 10 (640)
I0428 20:09:31.841354 31641 net.cpp:139] Memory required for data: 5323520
I0428 20:09:31.841364 31641 layer_factory.hpp:77] Creating layer relu1
I0428 20:09:31.841372 31641 net.cpp:86] Creating Layer relu1
I0428 20:09:31.841377 31641 net.cpp:408] relu1 <- ip1
I0428 20:09:31.841382 31641 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:09:31.841583 31641 net.cpp:124] Setting up relu1
I0428 20:09:31.841594 31641 net.cpp:131] Top shape: 64 10 (640)
I0428 20:09:31.841611 31641 net.cpp:139] Memory required for data: 5326080
I0428 20:09:31.841615 31641 layer_factory.hpp:77] Creating layer ip2
I0428 20:09:31.841624 31641 net.cpp:86] Creating Layer ip2
I0428 20:09:31.841626 31641 net.cpp:408] ip2 <- ip1
I0428 20:09:31.841632 31641 net.cpp:382] ip2 -> ip2
I0428 20:09:31.841751 31641 net.cpp:124] Setting up ip2
I0428 20:09:31.841760 31641 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:09:31.841764 31641 net.cpp:139] Memory required for data: 5338880
I0428 20:09:31.841770 31641 layer_factory.hpp:77] Creating layer relu2
I0428 20:09:31.841778 31641 net.cpp:86] Creating Layer relu2
I0428 20:09:31.841781 31641 net.cpp:408] relu2 <- ip2
I0428 20:09:31.841787 31641 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:09:31.842636 31641 net.cpp:124] Setting up relu2
I0428 20:09:31.842651 31641 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:09:31.842667 31641 net.cpp:139] Memory required for data: 5351680
I0428 20:09:31.842672 31641 layer_factory.hpp:77] Creating layer ip3
I0428 20:09:31.842680 31641 net.cpp:86] Creating Layer ip3
I0428 20:09:31.842684 31641 net.cpp:408] ip3 <- ip2
I0428 20:09:31.842691 31641 net.cpp:382] ip3 -> ip3
I0428 20:09:31.842814 31641 net.cpp:124] Setting up ip3
I0428 20:09:31.842824 31641 net.cpp:131] Top shape: 64 10 (640)
I0428 20:09:31.842828 31641 net.cpp:139] Memory required for data: 5354240
I0428 20:09:31.842838 31641 layer_factory.hpp:77] Creating layer relu3
I0428 20:09:31.842844 31641 net.cpp:86] Creating Layer relu3
I0428 20:09:31.842847 31641 net.cpp:408] relu3 <- ip3
I0428 20:09:31.842852 31641 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:09:31.843049 31641 net.cpp:124] Setting up relu3
I0428 20:09:31.843060 31641 net.cpp:131] Top shape: 64 10 (640)
I0428 20:09:31.843063 31641 net.cpp:139] Memory required for data: 5356800
I0428 20:09:31.843067 31641 layer_factory.hpp:77] Creating layer loss
I0428 20:09:31.843075 31641 net.cpp:86] Creating Layer loss
I0428 20:09:31.843077 31641 net.cpp:408] loss <- ip3
I0428 20:09:31.843082 31641 net.cpp:408] loss <- label
I0428 20:09:31.843088 31641 net.cpp:382] loss -> loss
I0428 20:09:31.843109 31641 layer_factory.hpp:77] Creating layer loss
I0428 20:09:31.843374 31641 net.cpp:124] Setting up loss
I0428 20:09:31.843385 31641 net.cpp:131] Top shape: (1)
I0428 20:09:31.843389 31641 net.cpp:134]     with loss weight 1
I0428 20:09:31.843405 31641 net.cpp:139] Memory required for data: 5356804
I0428 20:09:31.843410 31641 net.cpp:200] loss needs backward computation.
I0428 20:09:31.843413 31641 net.cpp:200] relu3 needs backward computation.
I0428 20:09:31.843417 31641 net.cpp:200] ip3 needs backward computation.
I0428 20:09:31.843420 31641 net.cpp:200] relu2 needs backward computation.
I0428 20:09:31.843425 31641 net.cpp:200] ip2 needs backward computation.
I0428 20:09:31.843427 31641 net.cpp:200] relu1 needs backward computation.
I0428 20:09:31.843430 31641 net.cpp:200] ip1 needs backward computation.
I0428 20:09:31.843435 31641 net.cpp:200] pool1 needs backward computation.
I0428 20:09:31.843438 31641 net.cpp:200] conv1 needs backward computation.
I0428 20:09:31.843441 31641 net.cpp:200] pool0 needs backward computation.
I0428 20:09:31.843444 31641 net.cpp:200] conv0 needs backward computation.
I0428 20:09:31.843449 31641 net.cpp:202] mnist does not need backward computation.
I0428 20:09:31.843452 31641 net.cpp:244] This network produces output loss
I0428 20:09:31.843462 31641 net.cpp:257] Network initialization done.
I0428 20:09:31.843845 31641 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1111.prototxt
I0428 20:09:31.843878 31641 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:09:31.843984 31641 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:09:31.844079 31641 layer_factory.hpp:77] Creating layer mnist
I0428 20:09:31.844135 31641 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:09:31.844151 31641 net.cpp:86] Creating Layer mnist
I0428 20:09:31.844156 31641 net.cpp:382] mnist -> data
I0428 20:09:31.844166 31641 net.cpp:382] mnist -> label
I0428 20:09:31.844267 31641 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:09:31.846499 31641 net.cpp:124] Setting up mnist
I0428 20:09:31.846515 31641 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:09:31.846523 31641 net.cpp:131] Top shape: 100 (100)
I0428 20:09:31.846525 31641 net.cpp:139] Memory required for data: 314000
I0428 20:09:31.846530 31641 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:09:31.846539 31641 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:09:31.846542 31641 net.cpp:408] label_mnist_1_split <- label
I0428 20:09:31.846549 31641 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:09:31.846559 31641 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:09:31.846607 31641 net.cpp:124] Setting up label_mnist_1_split
I0428 20:09:31.846614 31641 net.cpp:131] Top shape: 100 (100)
I0428 20:09:31.846618 31641 net.cpp:131] Top shape: 100 (100)
I0428 20:09:31.846622 31641 net.cpp:139] Memory required for data: 314800
I0428 20:09:31.846626 31641 layer_factory.hpp:77] Creating layer conv0
I0428 20:09:31.846635 31641 net.cpp:86] Creating Layer conv0
I0428 20:09:31.846639 31641 net.cpp:408] conv0 <- data
I0428 20:09:31.846644 31641 net.cpp:382] conv0 -> conv0
I0428 20:09:31.848522 31641 net.cpp:124] Setting up conv0
I0428 20:09:31.848539 31641 net.cpp:131] Top shape: 100 25 24 24 (1440000)
I0428 20:09:31.848544 31641 net.cpp:139] Memory required for data: 6074800
I0428 20:09:31.848556 31641 layer_factory.hpp:77] Creating layer pool0
I0428 20:09:31.848563 31641 net.cpp:86] Creating Layer pool0
I0428 20:09:31.848567 31641 net.cpp:408] pool0 <- conv0
I0428 20:09:31.848573 31641 net.cpp:382] pool0 -> pool0
I0428 20:09:31.848618 31641 net.cpp:124] Setting up pool0
I0428 20:09:31.848624 31641 net.cpp:131] Top shape: 100 25 12 12 (360000)
I0428 20:09:31.848628 31641 net.cpp:139] Memory required for data: 7514800
I0428 20:09:31.848631 31641 layer_factory.hpp:77] Creating layer conv1
I0428 20:09:31.848641 31641 net.cpp:86] Creating Layer conv1
I0428 20:09:31.848644 31641 net.cpp:408] conv1 <- pool0
I0428 20:09:31.848650 31641 net.cpp:382] conv1 -> conv1
I0428 20:09:31.851171 31641 net.cpp:124] Setting up conv1
I0428 20:09:31.851187 31641 net.cpp:131] Top shape: 100 25 8 8 (160000)
I0428 20:09:31.851192 31641 net.cpp:139] Memory required for data: 8154800
I0428 20:09:31.851203 31641 layer_factory.hpp:77] Creating layer pool1
I0428 20:09:31.851210 31641 net.cpp:86] Creating Layer pool1
I0428 20:09:31.851215 31641 net.cpp:408] pool1 <- conv1
I0428 20:09:31.851222 31641 net.cpp:382] pool1 -> pool1
I0428 20:09:31.851272 31641 net.cpp:124] Setting up pool1
I0428 20:09:31.851280 31641 net.cpp:131] Top shape: 100 25 4 4 (40000)
I0428 20:09:31.851284 31641 net.cpp:139] Memory required for data: 8314800
I0428 20:09:31.851299 31641 layer_factory.hpp:77] Creating layer ip1
I0428 20:09:31.851305 31641 net.cpp:86] Creating Layer ip1
I0428 20:09:31.851310 31641 net.cpp:408] ip1 <- pool1
I0428 20:09:31.851320 31641 net.cpp:382] ip1 -> ip1
I0428 20:09:31.851466 31641 net.cpp:124] Setting up ip1
I0428 20:09:31.851475 31641 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:09:31.851492 31641 net.cpp:139] Memory required for data: 8318800
I0428 20:09:31.851502 31641 layer_factory.hpp:77] Creating layer relu1
I0428 20:09:31.851510 31641 net.cpp:86] Creating Layer relu1
I0428 20:09:31.851513 31641 net.cpp:408] relu1 <- ip1
I0428 20:09:31.851518 31641 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:09:31.851714 31641 net.cpp:124] Setting up relu1
I0428 20:09:31.851725 31641 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:09:31.851728 31641 net.cpp:139] Memory required for data: 8322800
I0428 20:09:31.851738 31641 layer_factory.hpp:77] Creating layer ip2
I0428 20:09:31.851747 31641 net.cpp:86] Creating Layer ip2
I0428 20:09:31.851750 31641 net.cpp:408] ip2 <- ip1
I0428 20:09:31.851757 31641 net.cpp:382] ip2 -> ip2
I0428 20:09:31.851892 31641 net.cpp:124] Setting up ip2
I0428 20:09:31.851902 31641 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:09:31.851905 31641 net.cpp:139] Memory required for data: 8342800
I0428 20:09:31.851912 31641 layer_factory.hpp:77] Creating layer relu2
I0428 20:09:31.851917 31641 net.cpp:86] Creating Layer relu2
I0428 20:09:31.851922 31641 net.cpp:408] relu2 <- ip2
I0428 20:09:31.851927 31641 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:09:31.852118 31641 net.cpp:124] Setting up relu2
I0428 20:09:31.852128 31641 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:09:31.852133 31641 net.cpp:139] Memory required for data: 8362800
I0428 20:09:31.852136 31641 layer_factory.hpp:77] Creating layer ip3
I0428 20:09:31.852143 31641 net.cpp:86] Creating Layer ip3
I0428 20:09:31.852146 31641 net.cpp:408] ip3 <- ip2
I0428 20:09:31.852152 31641 net.cpp:382] ip3 -> ip3
I0428 20:09:31.852284 31641 net.cpp:124] Setting up ip3
I0428 20:09:31.852293 31641 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:09:31.852296 31641 net.cpp:139] Memory required for data: 8366800
I0428 20:09:31.852305 31641 layer_factory.hpp:77] Creating layer relu3
I0428 20:09:31.852310 31641 net.cpp:86] Creating Layer relu3
I0428 20:09:31.852314 31641 net.cpp:408] relu3 <- ip3
I0428 20:09:31.852319 31641 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:09:31.853240 31641 net.cpp:124] Setting up relu3
I0428 20:09:31.853255 31641 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:09:31.853271 31641 net.cpp:139] Memory required for data: 8370800
I0428 20:09:31.853274 31641 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:09:31.853282 31641 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:09:31.853286 31641 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:09:31.853293 31641 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:09:31.853301 31641 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:09:31.853345 31641 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:09:31.853351 31641 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:09:31.853358 31641 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:09:31.853360 31641 net.cpp:139] Memory required for data: 8378800
I0428 20:09:31.853364 31641 layer_factory.hpp:77] Creating layer accuracy
I0428 20:09:31.853369 31641 net.cpp:86] Creating Layer accuracy
I0428 20:09:31.853374 31641 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:09:31.853379 31641 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:09:31.853384 31641 net.cpp:382] accuracy -> accuracy
I0428 20:09:31.853391 31641 net.cpp:124] Setting up accuracy
I0428 20:09:31.853396 31641 net.cpp:131] Top shape: (1)
I0428 20:09:31.853400 31641 net.cpp:139] Memory required for data: 8378804
I0428 20:09:31.853404 31641 layer_factory.hpp:77] Creating layer loss
I0428 20:09:31.853408 31641 net.cpp:86] Creating Layer loss
I0428 20:09:31.853412 31641 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:09:31.853416 31641 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:09:31.853421 31641 net.cpp:382] loss -> loss
I0428 20:09:31.853428 31641 layer_factory.hpp:77] Creating layer loss
I0428 20:09:31.853714 31641 net.cpp:124] Setting up loss
I0428 20:09:31.853725 31641 net.cpp:131] Top shape: (1)
I0428 20:09:31.853730 31641 net.cpp:134]     with loss weight 1
I0428 20:09:31.853756 31641 net.cpp:139] Memory required for data: 8378808
I0428 20:09:31.853760 31641 net.cpp:200] loss needs backward computation.
I0428 20:09:31.853766 31641 net.cpp:202] accuracy does not need backward computation.
I0428 20:09:31.853771 31641 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:09:31.853775 31641 net.cpp:200] relu3 needs backward computation.
I0428 20:09:31.853780 31641 net.cpp:200] ip3 needs backward computation.
I0428 20:09:31.853793 31641 net.cpp:200] relu2 needs backward computation.
I0428 20:09:31.853797 31641 net.cpp:200] ip2 needs backward computation.
I0428 20:09:31.853801 31641 net.cpp:200] relu1 needs backward computation.
I0428 20:09:31.853804 31641 net.cpp:200] ip1 needs backward computation.
I0428 20:09:31.853808 31641 net.cpp:200] pool1 needs backward computation.
I0428 20:09:31.853812 31641 net.cpp:200] conv1 needs backward computation.
I0428 20:09:31.853816 31641 net.cpp:200] pool0 needs backward computation.
I0428 20:09:31.853821 31641 net.cpp:200] conv0 needs backward computation.
I0428 20:09:31.853824 31641 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:09:31.853829 31641 net.cpp:202] mnist does not need backward computation.
I0428 20:09:31.853833 31641 net.cpp:244] This network produces output accuracy
I0428 20:09:31.853837 31641 net.cpp:244] This network produces output loss
I0428 20:09:31.853849 31641 net.cpp:257] Network initialization done.
I0428 20:09:31.853900 31641 solver.cpp:56] Solver scaffolding done.
I0428 20:09:31.854351 31641 caffe.cpp:248] Starting Optimization
I0428 20:09:31.854358 31641 solver.cpp:273] Solving LeNet
I0428 20:09:31.854362 31641 solver.cpp:274] Learning Rate Policy: inv
I0428 20:09:31.855232 31641 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:09:31.862160 31641 blocking_queue.cpp:49] Waiting for data
I0428 20:09:31.927278 31648 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:09:31.927938 31641 solver.cpp:398]     Test net output #0: accuracy = 0.0894
I0428 20:09:31.927968 31641 solver.cpp:398]     Test net output #1: loss = 2.3359 (* 1 = 2.3359 loss)
I0428 20:09:31.931990 31641 solver.cpp:219] Iteration 0 (0 iter/s, 0.0775864s/100 iters), loss = 2.325
I0428 20:09:31.932021 31641 solver.cpp:238]     Train net output #0: loss = 2.325 (* 1 = 2.325 loss)
I0428 20:09:31.932034 31641 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:09:32.036315 31641 solver.cpp:219] Iteration 100 (958.918 iter/s, 0.104284s/100 iters), loss = 1.05032
I0428 20:09:32.036341 31641 solver.cpp:238]     Train net output #0: loss = 1.05032 (* 1 = 1.05032 loss)
I0428 20:09:32.036348 31641 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:09:32.142341 31641 solver.cpp:219] Iteration 200 (943.493 iter/s, 0.105989s/100 iters), loss = 0.841834
I0428 20:09:32.142379 31641 solver.cpp:238]     Train net output #0: loss = 0.841834 (* 1 = 0.841834 loss)
I0428 20:09:32.142385 31641 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:09:32.242225 31641 solver.cpp:219] Iteration 300 (1001.49 iter/s, 0.0998511s/100 iters), loss = 0.388246
I0428 20:09:32.242265 31641 solver.cpp:238]     Train net output #0: loss = 0.388246 (* 1 = 0.388246 loss)
I0428 20:09:32.242271 31641 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:09:32.351249 31641 solver.cpp:219] Iteration 400 (917.649 iter/s, 0.108974s/100 iters), loss = 0.477257
I0428 20:09:32.351302 31641 solver.cpp:238]     Train net output #0: loss = 0.477257 (* 1 = 0.477257 loss)
I0428 20:09:32.351308 31641 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:09:32.456547 31641 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:09:32.518071 31648 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:09:32.518733 31641 solver.cpp:398]     Test net output #0: accuracy = 0.7856
I0428 20:09:32.518766 31641 solver.cpp:398]     Test net output #1: loss = 0.559193 (* 1 = 0.559193 loss)
I0428 20:09:32.519778 31641 solver.cpp:219] Iteration 500 (593.553 iter/s, 0.168477s/100 iters), loss = 0.815997
I0428 20:09:32.519860 31641 solver.cpp:238]     Train net output #0: loss = 0.815997 (* 1 = 0.815997 loss)
I0428 20:09:32.519896 31641 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:09:32.627514 31641 solver.cpp:219] Iteration 600 (928.836 iter/s, 0.107662s/100 iters), loss = 0.495687
I0428 20:09:32.627554 31641 solver.cpp:238]     Train net output #0: loss = 0.495687 (* 1 = 0.495687 loss)
I0428 20:09:32.627562 31641 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:09:32.726073 31641 solver.cpp:219] Iteration 700 (1015.14 iter/s, 0.0985086s/100 iters), loss = 0.791295
I0428 20:09:32.726112 31641 solver.cpp:238]     Train net output #0: loss = 0.791295 (* 1 = 0.791295 loss)
I0428 20:09:32.726119 31641 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:09:32.824856 31641 solver.cpp:219] Iteration 800 (1012.82 iter/s, 0.0987341s/100 iters), loss = 0.71103
I0428 20:09:32.824898 31641 solver.cpp:238]     Train net output #0: loss = 0.71103 (* 1 = 0.71103 loss)
I0428 20:09:32.824905 31641 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:09:32.932646 31641 solver.cpp:219] Iteration 900 (928.194 iter/s, 0.107736s/100 iters), loss = 0.314123
I0428 20:09:32.932674 31641 solver.cpp:238]     Train net output #0: loss = 0.314123 (* 1 = 0.314123 loss)
I0428 20:09:32.932683 31641 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:09:32.966338 31647 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:09:33.037879 31641 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:09:33.038923 31641 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:09:33.039608 31641 solver.cpp:311] Iteration 1000, loss = 0.567697
I0428 20:09:33.039624 31641 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:09:33.095278 31648 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:09:33.095924 31641 solver.cpp:398]     Test net output #0: accuracy = 0.7959
I0428 20:09:33.095958 31641 solver.cpp:398]     Test net output #1: loss = 0.507845 (* 1 = 0.507845 loss)
I0428 20:09:33.095964 31641 solver.cpp:316] Optimization Done.
I0428 20:09:33.095968 31641 caffe.cpp:259] Optimization Done.
