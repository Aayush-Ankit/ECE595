I0428 20:05:22.717638 30602 caffe.cpp:218] Using GPUs 0
I0428 20:05:22.751314 30602 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:05:23.277596 30602 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test998.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:05:23.277788 30602 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test998.prototxt
I0428 20:05:23.278370 30602 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:05:23.278406 30602 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:05:23.278520 30602 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:05:23.278607 30602 layer_factory.hpp:77] Creating layer mnist
I0428 20:05:23.278707 30602 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:05:23.278730 30602 net.cpp:86] Creating Layer mnist
I0428 20:05:23.278740 30602 net.cpp:382] mnist -> data
I0428 20:05:23.278762 30602 net.cpp:382] mnist -> label
I0428 20:05:23.280040 30602 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:05:23.282980 30602 net.cpp:124] Setting up mnist
I0428 20:05:23.283025 30602 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:05:23.283033 30602 net.cpp:131] Top shape: 64 (64)
I0428 20:05:23.283038 30602 net.cpp:139] Memory required for data: 200960
I0428 20:05:23.283047 30602 layer_factory.hpp:77] Creating layer conv0
I0428 20:05:23.283066 30602 net.cpp:86] Creating Layer conv0
I0428 20:05:23.283087 30602 net.cpp:408] conv0 <- data
I0428 20:05:23.283099 30602 net.cpp:382] conv0 -> conv0
I0428 20:05:23.570312 30602 net.cpp:124] Setting up conv0
I0428 20:05:23.570344 30602 net.cpp:131] Top shape: 64 25 24 24 (921600)
I0428 20:05:23.570350 30602 net.cpp:139] Memory required for data: 3887360
I0428 20:05:23.570384 30602 layer_factory.hpp:77] Creating layer pool0
I0428 20:05:23.570400 30602 net.cpp:86] Creating Layer pool0
I0428 20:05:23.570406 30602 net.cpp:408] pool0 <- conv0
I0428 20:05:23.570415 30602 net.cpp:382] pool0 -> pool0
I0428 20:05:23.570467 30602 net.cpp:124] Setting up pool0
I0428 20:05:23.570477 30602 net.cpp:131] Top shape: 64 25 12 12 (230400)
I0428 20:05:23.570482 30602 net.cpp:139] Memory required for data: 4808960
I0428 20:05:23.570485 30602 layer_factory.hpp:77] Creating layer conv1
I0428 20:05:23.570499 30602 net.cpp:86] Creating Layer conv1
I0428 20:05:23.570504 30602 net.cpp:408] conv1 <- pool0
I0428 20:05:23.570510 30602 net.cpp:382] conv1 -> conv1
I0428 20:05:23.574168 30602 net.cpp:124] Setting up conv1
I0428 20:05:23.574184 30602 net.cpp:131] Top shape: 64 2 8 8 (8192)
I0428 20:05:23.574189 30602 net.cpp:139] Memory required for data: 4841728
I0428 20:05:23.574200 30602 layer_factory.hpp:77] Creating layer pool1
I0428 20:05:23.574209 30602 net.cpp:86] Creating Layer pool1
I0428 20:05:23.574214 30602 net.cpp:408] pool1 <- conv1
I0428 20:05:23.574223 30602 net.cpp:382] pool1 -> pool1
I0428 20:05:23.574280 30602 net.cpp:124] Setting up pool1
I0428 20:05:23.574287 30602 net.cpp:131] Top shape: 64 2 4 4 (2048)
I0428 20:05:23.574291 30602 net.cpp:139] Memory required for data: 4849920
I0428 20:05:23.574295 30602 layer_factory.hpp:77] Creating layer ip1
I0428 20:05:23.574306 30602 net.cpp:86] Creating Layer ip1
I0428 20:05:23.574311 30602 net.cpp:408] ip1 <- pool1
I0428 20:05:23.574318 30602 net.cpp:382] ip1 -> ip1
I0428 20:05:23.574445 30602 net.cpp:124] Setting up ip1
I0428 20:05:23.574455 30602 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:05:23.574458 30602 net.cpp:139] Memory required for data: 4856320
I0428 20:05:23.574468 30602 layer_factory.hpp:77] Creating layer relu1
I0428 20:05:23.574476 30602 net.cpp:86] Creating Layer relu1
I0428 20:05:23.574481 30602 net.cpp:408] relu1 <- ip1
I0428 20:05:23.574486 30602 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:05:23.574695 30602 net.cpp:124] Setting up relu1
I0428 20:05:23.574707 30602 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:05:23.574712 30602 net.cpp:139] Memory required for data: 4862720
I0428 20:05:23.574715 30602 layer_factory.hpp:77] Creating layer ip2
I0428 20:05:23.574723 30602 net.cpp:86] Creating Layer ip2
I0428 20:05:23.574729 30602 net.cpp:408] ip2 <- ip1
I0428 20:05:23.574736 30602 net.cpp:382] ip2 -> ip2
I0428 20:05:23.574854 30602 net.cpp:124] Setting up ip2
I0428 20:05:23.574864 30602 net.cpp:131] Top shape: 64 10 (640)
I0428 20:05:23.574868 30602 net.cpp:139] Memory required for data: 4865280
I0428 20:05:23.574875 30602 layer_factory.hpp:77] Creating layer relu2
I0428 20:05:23.574883 30602 net.cpp:86] Creating Layer relu2
I0428 20:05:23.574887 30602 net.cpp:408] relu2 <- ip2
I0428 20:05:23.574894 30602 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:05:23.575932 30602 net.cpp:124] Setting up relu2
I0428 20:05:23.575963 30602 net.cpp:131] Top shape: 64 10 (640)
I0428 20:05:23.575968 30602 net.cpp:139] Memory required for data: 4867840
I0428 20:05:23.575973 30602 layer_factory.hpp:77] Creating layer ip3
I0428 20:05:23.575983 30602 net.cpp:86] Creating Layer ip3
I0428 20:05:23.576002 30602 net.cpp:408] ip3 <- ip2
I0428 20:05:23.576009 30602 net.cpp:382] ip3 -> ip3
I0428 20:05:23.576130 30602 net.cpp:124] Setting up ip3
I0428 20:05:23.576140 30602 net.cpp:131] Top shape: 64 10 (640)
I0428 20:05:23.576144 30602 net.cpp:139] Memory required for data: 4870400
I0428 20:05:23.576154 30602 layer_factory.hpp:77] Creating layer relu3
I0428 20:05:23.576162 30602 net.cpp:86] Creating Layer relu3
I0428 20:05:23.576167 30602 net.cpp:408] relu3 <- ip3
I0428 20:05:23.576172 30602 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:05:23.576385 30602 net.cpp:124] Setting up relu3
I0428 20:05:23.576396 30602 net.cpp:131] Top shape: 64 10 (640)
I0428 20:05:23.576400 30602 net.cpp:139] Memory required for data: 4872960
I0428 20:05:23.576406 30602 layer_factory.hpp:77] Creating layer loss
I0428 20:05:23.576413 30602 net.cpp:86] Creating Layer loss
I0428 20:05:23.576418 30602 net.cpp:408] loss <- ip3
I0428 20:05:23.576423 30602 net.cpp:408] loss <- label
I0428 20:05:23.576431 30602 net.cpp:382] loss -> loss
I0428 20:05:23.576452 30602 layer_factory.hpp:77] Creating layer loss
I0428 20:05:23.576792 30602 net.cpp:124] Setting up loss
I0428 20:05:23.576805 30602 net.cpp:131] Top shape: (1)
I0428 20:05:23.576833 30602 net.cpp:134]     with loss weight 1
I0428 20:05:23.576853 30602 net.cpp:139] Memory required for data: 4872964
I0428 20:05:23.576858 30602 net.cpp:200] loss needs backward computation.
I0428 20:05:23.576864 30602 net.cpp:200] relu3 needs backward computation.
I0428 20:05:23.576884 30602 net.cpp:200] ip3 needs backward computation.
I0428 20:05:23.576889 30602 net.cpp:200] relu2 needs backward computation.
I0428 20:05:23.576894 30602 net.cpp:200] ip2 needs backward computation.
I0428 20:05:23.576898 30602 net.cpp:200] relu1 needs backward computation.
I0428 20:05:23.576903 30602 net.cpp:200] ip1 needs backward computation.
I0428 20:05:23.576908 30602 net.cpp:200] pool1 needs backward computation.
I0428 20:05:23.576913 30602 net.cpp:200] conv1 needs backward computation.
I0428 20:05:23.576918 30602 net.cpp:200] pool0 needs backward computation.
I0428 20:05:23.576925 30602 net.cpp:200] conv0 needs backward computation.
I0428 20:05:23.576930 30602 net.cpp:202] mnist does not need backward computation.
I0428 20:05:23.576934 30602 net.cpp:244] This network produces output loss
I0428 20:05:23.576948 30602 net.cpp:257] Network initialization done.
I0428 20:05:23.577464 30602 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test998.prototxt
I0428 20:05:23.577543 30602 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:05:23.577679 30602 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:05:23.577798 30602 layer_factory.hpp:77] Creating layer mnist
I0428 20:05:23.577868 30602 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:05:23.577884 30602 net.cpp:86] Creating Layer mnist
I0428 20:05:23.577893 30602 net.cpp:382] mnist -> data
I0428 20:05:23.577903 30602 net.cpp:382] mnist -> label
I0428 20:05:23.578008 30602 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:05:23.580838 30602 net.cpp:124] Setting up mnist
I0428 20:05:23.580883 30602 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:05:23.580904 30602 net.cpp:131] Top shape: 100 (100)
I0428 20:05:23.580909 30602 net.cpp:139] Memory required for data: 314000
I0428 20:05:23.580914 30602 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:05:23.580922 30602 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:05:23.580927 30602 net.cpp:408] label_mnist_1_split <- label
I0428 20:05:23.580951 30602 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:05:23.580960 30602 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:05:23.581032 30602 net.cpp:124] Setting up label_mnist_1_split
I0428 20:05:23.581042 30602 net.cpp:131] Top shape: 100 (100)
I0428 20:05:23.581048 30602 net.cpp:131] Top shape: 100 (100)
I0428 20:05:23.581051 30602 net.cpp:139] Memory required for data: 314800
I0428 20:05:23.581055 30602 layer_factory.hpp:77] Creating layer conv0
I0428 20:05:23.581069 30602 net.cpp:86] Creating Layer conv0
I0428 20:05:23.581074 30602 net.cpp:408] conv0 <- data
I0428 20:05:23.581082 30602 net.cpp:382] conv0 -> conv0
I0428 20:05:23.583000 30602 net.cpp:124] Setting up conv0
I0428 20:05:23.583031 30602 net.cpp:131] Top shape: 100 25 24 24 (1440000)
I0428 20:05:23.583036 30602 net.cpp:139] Memory required for data: 6074800
I0428 20:05:23.583050 30602 layer_factory.hpp:77] Creating layer pool0
I0428 20:05:23.583060 30602 net.cpp:86] Creating Layer pool0
I0428 20:05:23.583065 30602 net.cpp:408] pool0 <- conv0
I0428 20:05:23.583071 30602 net.cpp:382] pool0 -> pool0
I0428 20:05:23.583118 30602 net.cpp:124] Setting up pool0
I0428 20:05:23.583129 30602 net.cpp:131] Top shape: 100 25 12 12 (360000)
I0428 20:05:23.583134 30602 net.cpp:139] Memory required for data: 7514800
I0428 20:05:23.583138 30602 layer_factory.hpp:77] Creating layer conv1
I0428 20:05:23.583151 30602 net.cpp:86] Creating Layer conv1
I0428 20:05:23.583156 30602 net.cpp:408] conv1 <- pool0
I0428 20:05:23.583165 30602 net.cpp:382] conv1 -> conv1
I0428 20:05:23.585973 30602 net.cpp:124] Setting up conv1
I0428 20:05:23.586002 30602 net.cpp:131] Top shape: 100 2 8 8 (12800)
I0428 20:05:23.586006 30602 net.cpp:139] Memory required for data: 7566000
I0428 20:05:23.586019 30602 layer_factory.hpp:77] Creating layer pool1
I0428 20:05:23.586028 30602 net.cpp:86] Creating Layer pool1
I0428 20:05:23.586032 30602 net.cpp:408] pool1 <- conv1
I0428 20:05:23.586040 30602 net.cpp:382] pool1 -> pool1
I0428 20:05:23.586115 30602 net.cpp:124] Setting up pool1
I0428 20:05:23.586125 30602 net.cpp:131] Top shape: 100 2 4 4 (3200)
I0428 20:05:23.586130 30602 net.cpp:139] Memory required for data: 7578800
I0428 20:05:23.586134 30602 layer_factory.hpp:77] Creating layer ip1
I0428 20:05:23.586143 30602 net.cpp:86] Creating Layer ip1
I0428 20:05:23.586149 30602 net.cpp:408] ip1 <- pool1
I0428 20:05:23.586158 30602 net.cpp:382] ip1 -> ip1
I0428 20:05:23.586290 30602 net.cpp:124] Setting up ip1
I0428 20:05:23.586302 30602 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:05:23.586318 30602 net.cpp:139] Memory required for data: 7588800
I0428 20:05:23.586331 30602 layer_factory.hpp:77] Creating layer relu1
I0428 20:05:23.586340 30602 net.cpp:86] Creating Layer relu1
I0428 20:05:23.586345 30602 net.cpp:408] relu1 <- ip1
I0428 20:05:23.586351 30602 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:05:23.586637 30602 net.cpp:124] Setting up relu1
I0428 20:05:23.586647 30602 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:05:23.586652 30602 net.cpp:139] Memory required for data: 7598800
I0428 20:05:23.586658 30602 layer_factory.hpp:77] Creating layer ip2
I0428 20:05:23.586668 30602 net.cpp:86] Creating Layer ip2
I0428 20:05:23.586673 30602 net.cpp:408] ip2 <- ip1
I0428 20:05:23.586680 30602 net.cpp:382] ip2 -> ip2
I0428 20:05:23.586808 30602 net.cpp:124] Setting up ip2
I0428 20:05:23.586833 30602 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:05:23.586838 30602 net.cpp:139] Memory required for data: 7602800
I0428 20:05:23.586844 30602 layer_factory.hpp:77] Creating layer relu2
I0428 20:05:23.586853 30602 net.cpp:86] Creating Layer relu2
I0428 20:05:23.586858 30602 net.cpp:408] relu2 <- ip2
I0428 20:05:23.586863 30602 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:05:23.587115 30602 net.cpp:124] Setting up relu2
I0428 20:05:23.587126 30602 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:05:23.587131 30602 net.cpp:139] Memory required for data: 7606800
I0428 20:05:23.587136 30602 layer_factory.hpp:77] Creating layer ip3
I0428 20:05:23.587146 30602 net.cpp:86] Creating Layer ip3
I0428 20:05:23.587151 30602 net.cpp:408] ip3 <- ip2
I0428 20:05:23.587159 30602 net.cpp:382] ip3 -> ip3
I0428 20:05:23.587285 30602 net.cpp:124] Setting up ip3
I0428 20:05:23.587297 30602 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:05:23.587301 30602 net.cpp:139] Memory required for data: 7610800
I0428 20:05:23.587312 30602 layer_factory.hpp:77] Creating layer relu3
I0428 20:05:23.587319 30602 net.cpp:86] Creating Layer relu3
I0428 20:05:23.587323 30602 net.cpp:408] relu3 <- ip3
I0428 20:05:23.587329 30602 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:05:23.588629 30602 net.cpp:124] Setting up relu3
I0428 20:05:23.588660 30602 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:05:23.588665 30602 net.cpp:139] Memory required for data: 7614800
I0428 20:05:23.588670 30602 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:05:23.588678 30602 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:05:23.588683 30602 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:05:23.588691 30602 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:05:23.588716 30602 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:05:23.588779 30602 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:05:23.588789 30602 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:05:23.588795 30602 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:05:23.588799 30602 net.cpp:139] Memory required for data: 7622800
I0428 20:05:23.588804 30602 layer_factory.hpp:77] Creating layer accuracy
I0428 20:05:23.588851 30602 net.cpp:86] Creating Layer accuracy
I0428 20:05:23.588856 30602 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:05:23.588862 30602 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:05:23.588868 30602 net.cpp:382] accuracy -> accuracy
I0428 20:05:23.588878 30602 net.cpp:124] Setting up accuracy
I0428 20:05:23.588884 30602 net.cpp:131] Top shape: (1)
I0428 20:05:23.588889 30602 net.cpp:139] Memory required for data: 7622804
I0428 20:05:23.588893 30602 layer_factory.hpp:77] Creating layer loss
I0428 20:05:23.588901 30602 net.cpp:86] Creating Layer loss
I0428 20:05:23.588907 30602 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:05:23.588912 30602 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:05:23.588917 30602 net.cpp:382] loss -> loss
I0428 20:05:23.588925 30602 layer_factory.hpp:77] Creating layer loss
I0428 20:05:23.589267 30602 net.cpp:124] Setting up loss
I0428 20:05:23.589278 30602 net.cpp:131] Top shape: (1)
I0428 20:05:23.589298 30602 net.cpp:134]     with loss weight 1
I0428 20:05:23.589305 30602 net.cpp:139] Memory required for data: 7622808
I0428 20:05:23.589323 30602 net.cpp:200] loss needs backward computation.
I0428 20:05:23.589329 30602 net.cpp:202] accuracy does not need backward computation.
I0428 20:05:23.589334 30602 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:05:23.589337 30602 net.cpp:200] relu3 needs backward computation.
I0428 20:05:23.589341 30602 net.cpp:200] ip3 needs backward computation.
I0428 20:05:23.589345 30602 net.cpp:200] relu2 needs backward computation.
I0428 20:05:23.589349 30602 net.cpp:200] ip2 needs backward computation.
I0428 20:05:23.589354 30602 net.cpp:200] relu1 needs backward computation.
I0428 20:05:23.589357 30602 net.cpp:200] ip1 needs backward computation.
I0428 20:05:23.589361 30602 net.cpp:200] pool1 needs backward computation.
I0428 20:05:23.589365 30602 net.cpp:200] conv1 needs backward computation.
I0428 20:05:23.589370 30602 net.cpp:200] pool0 needs backward computation.
I0428 20:05:23.589375 30602 net.cpp:200] conv0 needs backward computation.
I0428 20:05:23.589378 30602 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:05:23.589383 30602 net.cpp:202] mnist does not need backward computation.
I0428 20:05:23.589387 30602 net.cpp:244] This network produces output accuracy
I0428 20:05:23.589392 30602 net.cpp:244] This network produces output loss
I0428 20:05:23.589407 30602 net.cpp:257] Network initialization done.
I0428 20:05:23.589460 30602 solver.cpp:56] Solver scaffolding done.
I0428 20:05:23.589974 30602 caffe.cpp:248] Starting Optimization
I0428 20:05:23.589982 30602 solver.cpp:273] Solving LeNet
I0428 20:05:23.590003 30602 solver.cpp:274] Learning Rate Policy: inv
I0428 20:05:23.590967 30602 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:05:23.651569 30609 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:05:23.653535 30602 solver.cpp:398]     Test net output #0: accuracy = 0.1053
I0428 20:05:23.653573 30602 solver.cpp:398]     Test net output #1: loss = 2.30283 (* 1 = 2.30283 loss)
I0428 20:05:23.657037 30602 solver.cpp:219] Iteration 0 (0 iter/s, 0.0670076s/100 iters), loss = 2.31069
I0428 20:05:23.657081 30602 solver.cpp:238]     Train net output #0: loss = 2.31069 (* 1 = 2.31069 loss)
I0428 20:05:23.657095 30602 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:05:23.742911 30602 solver.cpp:219] Iteration 100 (1165.24 iter/s, 0.0858194s/100 iters), loss = 1.82773
I0428 20:05:23.742951 30602 solver.cpp:238]     Train net output #0: loss = 1.82773 (* 1 = 1.82773 loss)
I0428 20:05:23.742957 30602 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:05:23.827520 30602 solver.cpp:219] Iteration 200 (1182.38 iter/s, 0.0845754s/100 iters), loss = 1.3737
I0428 20:05:23.827559 30602 solver.cpp:238]     Train net output #0: loss = 1.3737 (* 1 = 1.3737 loss)
I0428 20:05:23.827565 30602 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:05:23.928201 30602 solver.cpp:219] Iteration 300 (993.593 iter/s, 0.100645s/100 iters), loss = 1.73866
I0428 20:05:23.928248 30602 solver.cpp:238]     Train net output #0: loss = 1.73866 (* 1 = 1.73866 loss)
I0428 20:05:23.928257 30602 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:05:24.018432 30602 solver.cpp:219] Iteration 400 (1108.92 iter/s, 0.0901775s/100 iters), loss = 1.28036
I0428 20:05:24.018473 30602 solver.cpp:238]     Train net output #0: loss = 1.28036 (* 1 = 1.28036 loss)
I0428 20:05:24.018481 30602 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:05:24.106951 30602 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:05:24.160285 30609 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:05:24.162163 30602 solver.cpp:398]     Test net output #0: accuracy = 0.4518
I0428 20:05:24.162199 30602 solver.cpp:398]     Test net output #1: loss = 1.39071 (* 1 = 1.39071 loss)
I0428 20:05:24.163061 30602 solver.cpp:219] Iteration 500 (691.631 iter/s, 0.144586s/100 iters), loss = 1.2759
I0428 20:05:24.163086 30602 solver.cpp:238]     Train net output #0: loss = 1.2759 (* 1 = 1.2759 loss)
I0428 20:05:24.163118 30602 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:05:24.252444 30602 solver.cpp:219] Iteration 600 (1119.24 iter/s, 0.0893461s/100 iters), loss = 1.44275
I0428 20:05:24.252472 30602 solver.cpp:238]     Train net output #0: loss = 1.44275 (* 1 = 1.44275 loss)
I0428 20:05:24.252480 30602 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:05:24.341346 30602 solver.cpp:219] Iteration 700 (1125.31 iter/s, 0.0888648s/100 iters), loss = 1.25981
I0428 20:05:24.341387 30602 solver.cpp:238]     Train net output #0: loss = 1.25981 (* 1 = 1.25981 loss)
I0428 20:05:24.341393 30602 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:05:24.431135 30602 solver.cpp:219] Iteration 800 (1114.35 iter/s, 0.0897382s/100 iters), loss = 1.25593
I0428 20:05:24.431177 30602 solver.cpp:238]     Train net output #0: loss = 1.25593 (* 1 = 1.25593 loss)
I0428 20:05:24.431185 30602 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:05:24.520781 30602 solver.cpp:219] Iteration 900 (1116.02 iter/s, 0.0896041s/100 iters), loss = 1.02111
I0428 20:05:24.520828 30602 solver.cpp:238]     Train net output #0: loss = 1.02111 (* 1 = 1.02111 loss)
I0428 20:05:24.520836 30602 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:05:24.551012 30608 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:05:24.609800 30602 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:05:24.610604 30602 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:05:24.611192 30602 solver.cpp:311] Iteration 1000, loss = 1.07374
I0428 20:05:24.611210 30602 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:05:24.662597 30609 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:05:24.664099 30602 solver.cpp:398]     Test net output #0: accuracy = 0.551
I0428 20:05:24.664119 30602 solver.cpp:398]     Test net output #1: loss = 1.12209 (* 1 = 1.12209 loss)
I0428 20:05:24.664144 30602 solver.cpp:316] Optimization Done.
I0428 20:05:24.664162 30602 caffe.cpp:259] Optimization Done.
