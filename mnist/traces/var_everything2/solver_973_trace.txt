I0428 20:04:28.838323 30392 caffe.cpp:218] Using GPUs 0
I0428 20:04:28.867467 30392 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:04:29.315565 30392 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test973.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:04:29.315712 30392 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test973.prototxt
I0428 20:04:29.316038 30392 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:04:29.316051 30392 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:04:29.316129 30392 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:04:29.316189 30392 layer_factory.hpp:77] Creating layer mnist
I0428 20:04:29.316272 30392 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:04:29.316292 30392 net.cpp:86] Creating Layer mnist
I0428 20:04:29.316298 30392 net.cpp:382] mnist -> data
I0428 20:04:29.316316 30392 net.cpp:382] mnist -> label
I0428 20:04:29.317211 30392 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:04:29.319463 30392 net.cpp:124] Setting up mnist
I0428 20:04:29.319492 30392 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:04:29.319499 30392 net.cpp:131] Top shape: 64 (64)
I0428 20:04:29.319501 30392 net.cpp:139] Memory required for data: 200960
I0428 20:04:29.319507 30392 layer_factory.hpp:77] Creating layer conv0
I0428 20:04:29.319520 30392 net.cpp:86] Creating Layer conv0
I0428 20:04:29.319538 30392 net.cpp:408] conv0 <- data
I0428 20:04:29.319548 30392 net.cpp:382] conv0 -> conv0
I0428 20:04:29.552969 30392 net.cpp:124] Setting up conv0
I0428 20:04:29.553014 30392 net.cpp:131] Top shape: 64 10 24 24 (368640)
I0428 20:04:29.553017 30392 net.cpp:139] Memory required for data: 1675520
I0428 20:04:29.553032 30392 layer_factory.hpp:77] Creating layer pool0
I0428 20:04:29.553045 30392 net.cpp:86] Creating Layer pool0
I0428 20:04:29.553050 30392 net.cpp:408] pool0 <- conv0
I0428 20:04:29.553055 30392 net.cpp:382] pool0 -> pool0
I0428 20:04:29.553133 30392 net.cpp:124] Setting up pool0
I0428 20:04:29.553155 30392 net.cpp:131] Top shape: 64 10 12 12 (92160)
I0428 20:04:29.553158 30392 net.cpp:139] Memory required for data: 2044160
I0428 20:04:29.553161 30392 layer_factory.hpp:77] Creating layer conv1
I0428 20:04:29.553172 30392 net.cpp:86] Creating Layer conv1
I0428 20:04:29.553175 30392 net.cpp:408] conv1 <- pool0
I0428 20:04:29.553180 30392 net.cpp:382] conv1 -> conv1
I0428 20:04:29.555172 30392 net.cpp:124] Setting up conv1
I0428 20:04:29.555187 30392 net.cpp:131] Top shape: 64 100 8 8 (409600)
I0428 20:04:29.555191 30392 net.cpp:139] Memory required for data: 3682560
I0428 20:04:29.555199 30392 layer_factory.hpp:77] Creating layer pool1
I0428 20:04:29.555207 30392 net.cpp:86] Creating Layer pool1
I0428 20:04:29.555210 30392 net.cpp:408] pool1 <- conv1
I0428 20:04:29.555215 30392 net.cpp:382] pool1 -> pool1
I0428 20:04:29.555251 30392 net.cpp:124] Setting up pool1
I0428 20:04:29.555258 30392 net.cpp:131] Top shape: 64 100 4 4 (102400)
I0428 20:04:29.555260 30392 net.cpp:139] Memory required for data: 4092160
I0428 20:04:29.555263 30392 layer_factory.hpp:77] Creating layer ip1
I0428 20:04:29.555270 30392 net.cpp:86] Creating Layer ip1
I0428 20:04:29.555274 30392 net.cpp:408] ip1 <- pool1
I0428 20:04:29.555279 30392 net.cpp:382] ip1 -> ip1
I0428 20:04:29.556709 30392 net.cpp:124] Setting up ip1
I0428 20:04:29.556720 30392 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:04:29.556740 30392 net.cpp:139] Memory required for data: 4104960
I0428 20:04:29.556747 30392 layer_factory.hpp:77] Creating layer relu1
I0428 20:04:29.556753 30392 net.cpp:86] Creating Layer relu1
I0428 20:04:29.556756 30392 net.cpp:408] relu1 <- ip1
I0428 20:04:29.556761 30392 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:04:29.556988 30392 net.cpp:124] Setting up relu1
I0428 20:04:29.556998 30392 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:04:29.557001 30392 net.cpp:139] Memory required for data: 4117760
I0428 20:04:29.557004 30392 layer_factory.hpp:77] Creating layer ip2
I0428 20:04:29.557011 30392 net.cpp:86] Creating Layer ip2
I0428 20:04:29.557015 30392 net.cpp:408] ip2 <- ip1
I0428 20:04:29.557020 30392 net.cpp:382] ip2 -> ip2
I0428 20:04:29.558027 30392 net.cpp:124] Setting up ip2
I0428 20:04:29.558039 30392 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:04:29.558058 30392 net.cpp:139] Memory required for data: 4130560
I0428 20:04:29.558064 30392 layer_factory.hpp:77] Creating layer relu2
I0428 20:04:29.558070 30392 net.cpp:86] Creating Layer relu2
I0428 20:04:29.558074 30392 net.cpp:408] relu2 <- ip2
I0428 20:04:29.558079 30392 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:04:29.558900 30392 net.cpp:124] Setting up relu2
I0428 20:04:29.558913 30392 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:04:29.558933 30392 net.cpp:139] Memory required for data: 4143360
I0428 20:04:29.558936 30392 layer_factory.hpp:77] Creating layer ip3
I0428 20:04:29.558959 30392 net.cpp:86] Creating Layer ip3
I0428 20:04:29.558961 30392 net.cpp:408] ip3 <- ip2
I0428 20:04:29.558967 30392 net.cpp:382] ip3 -> ip3
I0428 20:04:29.559070 30392 net.cpp:124] Setting up ip3
I0428 20:04:29.559078 30392 net.cpp:131] Top shape: 64 10 (640)
I0428 20:04:29.559082 30392 net.cpp:139] Memory required for data: 4145920
I0428 20:04:29.559089 30392 layer_factory.hpp:77] Creating layer relu3
I0428 20:04:29.559094 30392 net.cpp:86] Creating Layer relu3
I0428 20:04:29.559098 30392 net.cpp:408] relu3 <- ip3
I0428 20:04:29.559103 30392 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:04:29.559273 30392 net.cpp:124] Setting up relu3
I0428 20:04:29.559283 30392 net.cpp:131] Top shape: 64 10 (640)
I0428 20:04:29.559285 30392 net.cpp:139] Memory required for data: 4148480
I0428 20:04:29.559288 30392 layer_factory.hpp:77] Creating layer loss
I0428 20:04:29.559309 30392 net.cpp:86] Creating Layer loss
I0428 20:04:29.559314 30392 net.cpp:408] loss <- ip3
I0428 20:04:29.559317 30392 net.cpp:408] loss <- label
I0428 20:04:29.559322 30392 net.cpp:382] loss -> loss
I0428 20:04:29.559337 30392 layer_factory.hpp:77] Creating layer loss
I0428 20:04:29.559562 30392 net.cpp:124] Setting up loss
I0428 20:04:29.559572 30392 net.cpp:131] Top shape: (1)
I0428 20:04:29.559576 30392 net.cpp:134]     with loss weight 1
I0428 20:04:29.559590 30392 net.cpp:139] Memory required for data: 4148484
I0428 20:04:29.559593 30392 net.cpp:200] loss needs backward computation.
I0428 20:04:29.559597 30392 net.cpp:200] relu3 needs backward computation.
I0428 20:04:29.559600 30392 net.cpp:200] ip3 needs backward computation.
I0428 20:04:29.559604 30392 net.cpp:200] relu2 needs backward computation.
I0428 20:04:29.559607 30392 net.cpp:200] ip2 needs backward computation.
I0428 20:04:29.559609 30392 net.cpp:200] relu1 needs backward computation.
I0428 20:04:29.559612 30392 net.cpp:200] ip1 needs backward computation.
I0428 20:04:29.559615 30392 net.cpp:200] pool1 needs backward computation.
I0428 20:04:29.559618 30392 net.cpp:200] conv1 needs backward computation.
I0428 20:04:29.559623 30392 net.cpp:200] pool0 needs backward computation.
I0428 20:04:29.559625 30392 net.cpp:200] conv0 needs backward computation.
I0428 20:04:29.559629 30392 net.cpp:202] mnist does not need backward computation.
I0428 20:04:29.559631 30392 net.cpp:244] This network produces output loss
I0428 20:04:29.559641 30392 net.cpp:257] Network initialization done.
I0428 20:04:29.559986 30392 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test973.prototxt
I0428 20:04:29.560029 30392 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:04:29.560117 30392 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:04:29.560201 30392 layer_factory.hpp:77] Creating layer mnist
I0428 20:04:29.560243 30392 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:04:29.560256 30392 net.cpp:86] Creating Layer mnist
I0428 20:04:29.560261 30392 net.cpp:382] mnist -> data
I0428 20:04:29.560268 30392 net.cpp:382] mnist -> label
I0428 20:04:29.560353 30392 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:04:29.561549 30392 net.cpp:124] Setting up mnist
I0428 20:04:29.561589 30392 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:04:29.561592 30392 net.cpp:131] Top shape: 100 (100)
I0428 20:04:29.561595 30392 net.cpp:139] Memory required for data: 314000
I0428 20:04:29.561599 30392 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:04:29.561636 30392 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:04:29.561640 30392 net.cpp:408] label_mnist_1_split <- label
I0428 20:04:29.561646 30392 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:04:29.561652 30392 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:04:29.561712 30392 net.cpp:124] Setting up label_mnist_1_split
I0428 20:04:29.561717 30392 net.cpp:131] Top shape: 100 (100)
I0428 20:04:29.561720 30392 net.cpp:131] Top shape: 100 (100)
I0428 20:04:29.561723 30392 net.cpp:139] Memory required for data: 314800
I0428 20:04:29.561725 30392 layer_factory.hpp:77] Creating layer conv0
I0428 20:04:29.561734 30392 net.cpp:86] Creating Layer conv0
I0428 20:04:29.561738 30392 net.cpp:408] conv0 <- data
I0428 20:04:29.561743 30392 net.cpp:382] conv0 -> conv0
I0428 20:04:29.563448 30392 net.cpp:124] Setting up conv0
I0428 20:04:29.563463 30392 net.cpp:131] Top shape: 100 10 24 24 (576000)
I0428 20:04:29.563467 30392 net.cpp:139] Memory required for data: 2618800
I0428 20:04:29.563475 30392 layer_factory.hpp:77] Creating layer pool0
I0428 20:04:29.563482 30392 net.cpp:86] Creating Layer pool0
I0428 20:04:29.563485 30392 net.cpp:408] pool0 <- conv0
I0428 20:04:29.563491 30392 net.cpp:382] pool0 -> pool0
I0428 20:04:29.563525 30392 net.cpp:124] Setting up pool0
I0428 20:04:29.563532 30392 net.cpp:131] Top shape: 100 10 12 12 (144000)
I0428 20:04:29.563535 30392 net.cpp:139] Memory required for data: 3194800
I0428 20:04:29.563539 30392 layer_factory.hpp:77] Creating layer conv1
I0428 20:04:29.563549 30392 net.cpp:86] Creating Layer conv1
I0428 20:04:29.563552 30392 net.cpp:408] conv1 <- pool0
I0428 20:04:29.563557 30392 net.cpp:382] conv1 -> conv1
I0428 20:04:29.565426 30392 net.cpp:124] Setting up conv1
I0428 20:04:29.565454 30392 net.cpp:131] Top shape: 100 100 8 8 (640000)
I0428 20:04:29.565459 30392 net.cpp:139] Memory required for data: 5754800
I0428 20:04:29.565466 30392 layer_factory.hpp:77] Creating layer pool1
I0428 20:04:29.565474 30392 net.cpp:86] Creating Layer pool1
I0428 20:04:29.565492 30392 net.cpp:408] pool1 <- conv1
I0428 20:04:29.565498 30392 net.cpp:382] pool1 -> pool1
I0428 20:04:29.565548 30392 net.cpp:124] Setting up pool1
I0428 20:04:29.565556 30392 net.cpp:131] Top shape: 100 100 4 4 (160000)
I0428 20:04:29.565564 30392 net.cpp:139] Memory required for data: 6394800
I0428 20:04:29.565567 30392 layer_factory.hpp:77] Creating layer ip1
I0428 20:04:29.565573 30392 net.cpp:86] Creating Layer ip1
I0428 20:04:29.565577 30392 net.cpp:408] ip1 <- pool1
I0428 20:04:29.565582 30392 net.cpp:382] ip1 -> ip1
I0428 20:04:29.566156 30392 net.cpp:124] Setting up ip1
I0428 20:04:29.566164 30392 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:04:29.566193 30392 net.cpp:139] Memory required for data: 6414800
I0428 20:04:29.566201 30392 layer_factory.hpp:77] Creating layer relu1
I0428 20:04:29.566206 30392 net.cpp:86] Creating Layer relu1
I0428 20:04:29.566210 30392 net.cpp:408] relu1 <- ip1
I0428 20:04:29.566215 30392 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:04:29.566458 30392 net.cpp:124] Setting up relu1
I0428 20:04:29.566468 30392 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:04:29.566470 30392 net.cpp:139] Memory required for data: 6434800
I0428 20:04:29.566473 30392 layer_factory.hpp:77] Creating layer ip2
I0428 20:04:29.566488 30392 net.cpp:86] Creating Layer ip2
I0428 20:04:29.566491 30392 net.cpp:408] ip2 <- ip1
I0428 20:04:29.566500 30392 net.cpp:382] ip2 -> ip2
I0428 20:04:29.566639 30392 net.cpp:124] Setting up ip2
I0428 20:04:29.566648 30392 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:04:29.566650 30392 net.cpp:139] Memory required for data: 6454800
I0428 20:04:29.566655 30392 layer_factory.hpp:77] Creating layer relu2
I0428 20:04:29.566660 30392 net.cpp:86] Creating Layer relu2
I0428 20:04:29.566664 30392 net.cpp:408] relu2 <- ip2
I0428 20:04:29.566669 30392 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:04:29.566829 30392 net.cpp:124] Setting up relu2
I0428 20:04:29.566838 30392 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:04:29.566841 30392 net.cpp:139] Memory required for data: 6474800
I0428 20:04:29.566845 30392 layer_factory.hpp:77] Creating layer ip3
I0428 20:04:29.566857 30392 net.cpp:86] Creating Layer ip3
I0428 20:04:29.566865 30392 net.cpp:408] ip3 <- ip2
I0428 20:04:29.566871 30392 net.cpp:382] ip3 -> ip3
I0428 20:04:29.566988 30392 net.cpp:124] Setting up ip3
I0428 20:04:29.566997 30392 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:04:29.566999 30392 net.cpp:139] Memory required for data: 6478800
I0428 20:04:29.567008 30392 layer_factory.hpp:77] Creating layer relu3
I0428 20:04:29.567013 30392 net.cpp:86] Creating Layer relu3
I0428 20:04:29.567031 30392 net.cpp:408] relu3 <- ip3
I0428 20:04:29.567035 30392 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:04:29.567858 30392 net.cpp:124] Setting up relu3
I0428 20:04:29.567870 30392 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:04:29.567889 30392 net.cpp:139] Memory required for data: 6482800
I0428 20:04:29.567893 30392 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:04:29.567899 30392 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:04:29.567903 30392 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:04:29.567908 30392 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:04:29.567914 30392 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:04:29.567956 30392 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:04:29.567962 30392 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:04:29.567966 30392 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:04:29.567968 30392 net.cpp:139] Memory required for data: 6490800
I0428 20:04:29.567971 30392 layer_factory.hpp:77] Creating layer accuracy
I0428 20:04:29.567977 30392 net.cpp:86] Creating Layer accuracy
I0428 20:04:29.567981 30392 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:04:29.567984 30392 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:04:29.567989 30392 net.cpp:382] accuracy -> accuracy
I0428 20:04:29.567996 30392 net.cpp:124] Setting up accuracy
I0428 20:04:29.567999 30392 net.cpp:131] Top shape: (1)
I0428 20:04:29.568002 30392 net.cpp:139] Memory required for data: 6490804
I0428 20:04:29.568006 30392 layer_factory.hpp:77] Creating layer loss
I0428 20:04:29.568009 30392 net.cpp:86] Creating Layer loss
I0428 20:04:29.568019 30392 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:04:29.568023 30392 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:04:29.568028 30392 net.cpp:382] loss -> loss
I0428 20:04:29.568035 30392 layer_factory.hpp:77] Creating layer loss
I0428 20:04:29.568312 30392 net.cpp:124] Setting up loss
I0428 20:04:29.568321 30392 net.cpp:131] Top shape: (1)
I0428 20:04:29.568325 30392 net.cpp:134]     with loss weight 1
I0428 20:04:29.568339 30392 net.cpp:139] Memory required for data: 6490808
I0428 20:04:29.568343 30392 net.cpp:200] loss needs backward computation.
I0428 20:04:29.568347 30392 net.cpp:202] accuracy does not need backward computation.
I0428 20:04:29.568351 30392 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:04:29.568354 30392 net.cpp:200] relu3 needs backward computation.
I0428 20:04:29.568357 30392 net.cpp:200] ip3 needs backward computation.
I0428 20:04:29.568361 30392 net.cpp:200] relu2 needs backward computation.
I0428 20:04:29.568363 30392 net.cpp:200] ip2 needs backward computation.
I0428 20:04:29.568367 30392 net.cpp:200] relu1 needs backward computation.
I0428 20:04:29.568369 30392 net.cpp:200] ip1 needs backward computation.
I0428 20:04:29.568372 30392 net.cpp:200] pool1 needs backward computation.
I0428 20:04:29.568375 30392 net.cpp:200] conv1 needs backward computation.
I0428 20:04:29.568378 30392 net.cpp:200] pool0 needs backward computation.
I0428 20:04:29.568382 30392 net.cpp:200] conv0 needs backward computation.
I0428 20:04:29.568387 30392 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:04:29.568389 30392 net.cpp:202] mnist does not need backward computation.
I0428 20:04:29.568392 30392 net.cpp:244] This network produces output accuracy
I0428 20:04:29.568397 30392 net.cpp:244] This network produces output loss
I0428 20:04:29.568408 30392 net.cpp:257] Network initialization done.
I0428 20:04:29.568462 30392 solver.cpp:56] Solver scaffolding done.
I0428 20:04:29.568800 30392 caffe.cpp:248] Starting Optimization
I0428 20:04:29.568806 30392 solver.cpp:273] Solving LeNet
I0428 20:04:29.568831 30392 solver.cpp:274] Learning Rate Policy: inv
I0428 20:04:29.569780 30392 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:04:29.575228 30392 blocking_queue.cpp:49] Waiting for data
I0428 20:04:29.646366 30399 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:04:29.647047 30392 solver.cpp:398]     Test net output #0: accuracy = 0.1105
I0428 20:04:29.647080 30392 solver.cpp:398]     Test net output #1: loss = 2.3108 (* 1 = 2.3108 loss)
I0428 20:04:29.650856 30392 solver.cpp:219] Iteration 0 (-5.54079e-31 iter/s, 0.0819825s/100 iters), loss = 2.30644
I0428 20:04:29.650895 30392 solver.cpp:238]     Train net output #0: loss = 2.30644 (* 1 = 2.30644 loss)
I0428 20:04:29.650907 30392 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:04:29.812569 30392 solver.cpp:219] Iteration 100 (618.536 iter/s, 0.161672s/100 iters), loss = 0.781656
I0428 20:04:29.812608 30392 solver.cpp:238]     Train net output #0: loss = 0.781656 (* 1 = 0.781656 loss)
I0428 20:04:29.812614 30392 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:04:29.967618 30392 solver.cpp:219] Iteration 200 (645.179 iter/s, 0.154996s/100 iters), loss = 0.242523
I0428 20:04:29.967644 30392 solver.cpp:238]     Train net output #0: loss = 0.242523 (* 1 = 0.242523 loss)
I0428 20:04:29.967667 30392 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:04:30.124541 30392 solver.cpp:219] Iteration 300 (637.417 iter/s, 0.156883s/100 iters), loss = 0.181549
I0428 20:04:30.124567 30392 solver.cpp:238]     Train net output #0: loss = 0.181549 (* 1 = 0.181549 loss)
I0428 20:04:30.124588 30392 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:04:30.281939 30392 solver.cpp:219] Iteration 400 (635.493 iter/s, 0.157358s/100 iters), loss = 0.121125
I0428 20:04:30.281963 30392 solver.cpp:238]     Train net output #0: loss = 0.121125 (* 1 = 0.121125 loss)
I0428 20:04:30.281970 30392 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:04:30.437131 30392 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:04:30.513545 30399 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:04:30.514195 30392 solver.cpp:398]     Test net output #0: accuracy = 0.9603
I0428 20:04:30.514235 30392 solver.cpp:398]     Test net output #1: loss = 0.123393 (* 1 = 0.123393 loss)
I0428 20:04:30.516000 30392 solver.cpp:219] Iteration 500 (427.316 iter/s, 0.234019s/100 iters), loss = 0.121902
I0428 20:04:30.516041 30392 solver.cpp:238]     Train net output #0: loss = 0.121903 (* 1 = 0.121903 loss)
I0428 20:04:30.516048 30392 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:04:30.671816 30392 solver.cpp:219] Iteration 600 (641.994 iter/s, 0.155765s/100 iters), loss = 0.100085
I0428 20:04:30.671841 30392 solver.cpp:238]     Train net output #0: loss = 0.100085 (* 1 = 0.100085 loss)
I0428 20:04:30.671864 30392 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:04:30.827512 30392 solver.cpp:219] Iteration 700 (642.433 iter/s, 0.155658s/100 iters), loss = 0.12583
I0428 20:04:30.827570 30392 solver.cpp:238]     Train net output #0: loss = 0.125831 (* 1 = 0.125831 loss)
I0428 20:04:30.827592 30392 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:04:30.981778 30392 solver.cpp:219] Iteration 800 (648.525 iter/s, 0.154196s/100 iters), loss = 0.20753
I0428 20:04:30.981806 30392 solver.cpp:238]     Train net output #0: loss = 0.20753 (* 1 = 0.20753 loss)
I0428 20:04:30.981812 30392 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:04:31.141427 30392 solver.cpp:219] Iteration 900 (626.532 iter/s, 0.159609s/100 iters), loss = 0.15736
I0428 20:04:31.141466 30392 solver.cpp:238]     Train net output #0: loss = 0.15736 (* 1 = 0.15736 loss)
I0428 20:04:31.141472 30392 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:04:31.194358 30398 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:04:31.294052 30392 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:04:31.297062 30392 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:04:31.298445 30392 solver.cpp:311] Iteration 1000, loss = 0.116292
I0428 20:04:31.298461 30392 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:04:31.367681 30399 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:04:31.368331 30392 solver.cpp:398]     Test net output #0: accuracy = 0.9785
I0428 20:04:31.368366 30392 solver.cpp:398]     Test net output #1: loss = 0.0661259 (* 1 = 0.0661259 loss)
I0428 20:04:31.368371 30392 solver.cpp:316] Optimization Done.
I0428 20:04:31.368374 30392 caffe.cpp:259] Optimization Done.
