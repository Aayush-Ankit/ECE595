I0428 19:48:45.659967 26589 caffe.cpp:218] Using GPUs 0
I0428 19:48:45.701968 26589 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:48:46.152721 26589 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test554.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:48:46.152928 26589 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test554.prototxt
I0428 19:48:46.153273 26589 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:48:46.153291 26589 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:48:46.153369 26589 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0428 19:48:46.153456 26589 layer_factory.hpp:77] Creating layer mnist
I0428 19:48:46.153563 26589 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:48:46.153589 26589 net.cpp:86] Creating Layer mnist
I0428 19:48:46.153599 26589 net.cpp:382] mnist -> data
I0428 19:48:46.153625 26589 net.cpp:382] mnist -> label
I0428 19:48:46.154774 26589 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:48:46.157078 26589 net.cpp:124] Setting up mnist
I0428 19:48:46.157129 26589 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:48:46.157138 26589 net.cpp:131] Top shape: 64 (64)
I0428 19:48:46.157143 26589 net.cpp:139] Memory required for data: 200960
I0428 19:48:46.157151 26589 layer_factory.hpp:77] Creating layer conv0
I0428 19:48:46.157172 26589 net.cpp:86] Creating Layer conv0
I0428 19:48:46.157179 26589 net.cpp:408] conv0 <- data
I0428 19:48:46.157193 26589 net.cpp:382] conv0 -> conv0
I0428 19:48:46.395056 26589 net.cpp:124] Setting up conv0
I0428 19:48:46.395086 26589 net.cpp:131] Top shape: 64 5 24 24 (184320)
I0428 19:48:46.395092 26589 net.cpp:139] Memory required for data: 938240
I0428 19:48:46.395164 26589 layer_factory.hpp:77] Creating layer pool0
I0428 19:48:46.395182 26589 net.cpp:86] Creating Layer pool0
I0428 19:48:46.395190 26589 net.cpp:408] pool0 <- conv0
I0428 19:48:46.395200 26589 net.cpp:382] pool0 -> pool0
I0428 19:48:46.395259 26589 net.cpp:124] Setting up pool0
I0428 19:48:46.395269 26589 net.cpp:131] Top shape: 64 5 12 12 (46080)
I0428 19:48:46.395273 26589 net.cpp:139] Memory required for data: 1122560
I0428 19:48:46.395278 26589 layer_factory.hpp:77] Creating layer conv1
I0428 19:48:46.395295 26589 net.cpp:86] Creating Layer conv1
I0428 19:48:46.395301 26589 net.cpp:408] conv1 <- pool0
I0428 19:48:46.395309 26589 net.cpp:382] conv1 -> conv1
I0428 19:48:46.397166 26589 net.cpp:124] Setting up conv1
I0428 19:48:46.397182 26589 net.cpp:131] Top shape: 64 5 8 8 (20480)
I0428 19:48:46.397188 26589 net.cpp:139] Memory required for data: 1204480
I0428 19:48:46.397200 26589 layer_factory.hpp:77] Creating layer pool1
I0428 19:48:46.397212 26589 net.cpp:86] Creating Layer pool1
I0428 19:48:46.397217 26589 net.cpp:408] pool1 <- conv1
I0428 19:48:46.397225 26589 net.cpp:382] pool1 -> pool1
I0428 19:48:46.397279 26589 net.cpp:124] Setting up pool1
I0428 19:48:46.397289 26589 net.cpp:131] Top shape: 64 5 4 4 (5120)
I0428 19:48:46.397294 26589 net.cpp:139] Memory required for data: 1224960
I0428 19:48:46.397299 26589 layer_factory.hpp:77] Creating layer ip1
I0428 19:48:46.397310 26589 net.cpp:86] Creating Layer ip1
I0428 19:48:46.397315 26589 net.cpp:408] ip1 <- pool1
I0428 19:48:46.397322 26589 net.cpp:382] ip1 -> ip1
I0428 19:48:46.398358 26589 net.cpp:124] Setting up ip1
I0428 19:48:46.398372 26589 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:48:46.398378 26589 net.cpp:139] Memory required for data: 1231360
I0428 19:48:46.398392 26589 layer_factory.hpp:77] Creating layer relu1
I0428 19:48:46.398401 26589 net.cpp:86] Creating Layer relu1
I0428 19:48:46.398412 26589 net.cpp:408] relu1 <- ip1
I0428 19:48:46.398421 26589 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:48:46.398602 26589 net.cpp:124] Setting up relu1
I0428 19:48:46.398612 26589 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:48:46.398617 26589 net.cpp:139] Memory required for data: 1237760
I0428 19:48:46.398636 26589 layer_factory.hpp:77] Creating layer ip2
I0428 19:48:46.398646 26589 net.cpp:86] Creating Layer ip2
I0428 19:48:46.398651 26589 net.cpp:408] ip2 <- ip1
I0428 19:48:46.398659 26589 net.cpp:382] ip2 -> ip2
I0428 19:48:46.398753 26589 net.cpp:124] Setting up ip2
I0428 19:48:46.398761 26589 net.cpp:131] Top shape: 64 10 (640)
I0428 19:48:46.398766 26589 net.cpp:139] Memory required for data: 1240320
I0428 19:48:46.398775 26589 layer_factory.hpp:77] Creating layer relu2
I0428 19:48:46.398784 26589 net.cpp:86] Creating Layer relu2
I0428 19:48:46.398790 26589 net.cpp:408] relu2 <- ip2
I0428 19:48:46.398797 26589 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:48:46.399588 26589 net.cpp:124] Setting up relu2
I0428 19:48:46.399602 26589 net.cpp:131] Top shape: 64 10 (640)
I0428 19:48:46.399607 26589 net.cpp:139] Memory required for data: 1242880
I0428 19:48:46.399613 26589 layer_factory.hpp:77] Creating layer loss
I0428 19:48:46.399628 26589 net.cpp:86] Creating Layer loss
I0428 19:48:46.399636 26589 net.cpp:408] loss <- ip2
I0428 19:48:46.399643 26589 net.cpp:408] loss <- label
I0428 19:48:46.399652 26589 net.cpp:382] loss -> loss
I0428 19:48:46.399672 26589 layer_factory.hpp:77] Creating layer loss
I0428 19:48:46.399935 26589 net.cpp:124] Setting up loss
I0428 19:48:46.399945 26589 net.cpp:131] Top shape: (1)
I0428 19:48:46.399950 26589 net.cpp:134]     with loss weight 1
I0428 19:48:46.399969 26589 net.cpp:139] Memory required for data: 1242884
I0428 19:48:46.399976 26589 net.cpp:200] loss needs backward computation.
I0428 19:48:46.399982 26589 net.cpp:200] relu2 needs backward computation.
I0428 19:48:46.399987 26589 net.cpp:200] ip2 needs backward computation.
I0428 19:48:46.399992 26589 net.cpp:200] relu1 needs backward computation.
I0428 19:48:46.399997 26589 net.cpp:200] ip1 needs backward computation.
I0428 19:48:46.400002 26589 net.cpp:200] pool1 needs backward computation.
I0428 19:48:46.400018 26589 net.cpp:200] conv1 needs backward computation.
I0428 19:48:46.400024 26589 net.cpp:200] pool0 needs backward computation.
I0428 19:48:46.400029 26589 net.cpp:200] conv0 needs backward computation.
I0428 19:48:46.400035 26589 net.cpp:202] mnist does not need backward computation.
I0428 19:48:46.400040 26589 net.cpp:244] This network produces output loss
I0428 19:48:46.400053 26589 net.cpp:257] Network initialization done.
I0428 19:48:46.400342 26589 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test554.prototxt
I0428 19:48:46.400374 26589 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:48:46.400465 26589 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0428 19:48:46.400581 26589 layer_factory.hpp:77] Creating layer mnist
I0428 19:48:46.400640 26589 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:48:46.400655 26589 net.cpp:86] Creating Layer mnist
I0428 19:48:46.400665 26589 net.cpp:382] mnist -> data
I0428 19:48:46.400676 26589 net.cpp:382] mnist -> label
I0428 19:48:46.400868 26589 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:48:46.403300 26589 net.cpp:124] Setting up mnist
I0428 19:48:46.403314 26589 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:48:46.403323 26589 net.cpp:131] Top shape: 100 (100)
I0428 19:48:46.403328 26589 net.cpp:139] Memory required for data: 314000
I0428 19:48:46.403334 26589 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:48:46.403344 26589 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:48:46.403350 26589 net.cpp:408] label_mnist_1_split <- label
I0428 19:48:46.403362 26589 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:48:46.403373 26589 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:48:46.403463 26589 net.cpp:124] Setting up label_mnist_1_split
I0428 19:48:46.403482 26589 net.cpp:131] Top shape: 100 (100)
I0428 19:48:46.403491 26589 net.cpp:131] Top shape: 100 (100)
I0428 19:48:46.403496 26589 net.cpp:139] Memory required for data: 314800
I0428 19:48:46.403501 26589 layer_factory.hpp:77] Creating layer conv0
I0428 19:48:46.403543 26589 net.cpp:86] Creating Layer conv0
I0428 19:48:46.403549 26589 net.cpp:408] conv0 <- data
I0428 19:48:46.403558 26589 net.cpp:382] conv0 -> conv0
I0428 19:48:46.405027 26589 net.cpp:124] Setting up conv0
I0428 19:48:46.405043 26589 net.cpp:131] Top shape: 100 5 24 24 (288000)
I0428 19:48:46.405050 26589 net.cpp:139] Memory required for data: 1466800
I0428 19:48:46.405064 26589 layer_factory.hpp:77] Creating layer pool0
I0428 19:48:46.405081 26589 net.cpp:86] Creating Layer pool0
I0428 19:48:46.405087 26589 net.cpp:408] pool0 <- conv0
I0428 19:48:46.405095 26589 net.cpp:382] pool0 -> pool0
I0428 19:48:46.405164 26589 net.cpp:124] Setting up pool0
I0428 19:48:46.405172 26589 net.cpp:131] Top shape: 100 5 12 12 (72000)
I0428 19:48:46.405177 26589 net.cpp:139] Memory required for data: 1754800
I0428 19:48:46.405184 26589 layer_factory.hpp:77] Creating layer conv1
I0428 19:48:46.405213 26589 net.cpp:86] Creating Layer conv1
I0428 19:48:46.405220 26589 net.cpp:408] conv1 <- pool0
I0428 19:48:46.405230 26589 net.cpp:382] conv1 -> conv1
I0428 19:48:46.407166 26589 net.cpp:124] Setting up conv1
I0428 19:48:46.407179 26589 net.cpp:131] Top shape: 100 5 8 8 (32000)
I0428 19:48:46.407186 26589 net.cpp:139] Memory required for data: 1882800
I0428 19:48:46.407198 26589 layer_factory.hpp:77] Creating layer pool1
I0428 19:48:46.407207 26589 net.cpp:86] Creating Layer pool1
I0428 19:48:46.407213 26589 net.cpp:408] pool1 <- conv1
I0428 19:48:46.407222 26589 net.cpp:382] pool1 -> pool1
I0428 19:48:46.407265 26589 net.cpp:124] Setting up pool1
I0428 19:48:46.407274 26589 net.cpp:131] Top shape: 100 5 4 4 (8000)
I0428 19:48:46.407280 26589 net.cpp:139] Memory required for data: 1914800
I0428 19:48:46.407286 26589 layer_factory.hpp:77] Creating layer ip1
I0428 19:48:46.407295 26589 net.cpp:86] Creating Layer ip1
I0428 19:48:46.407300 26589 net.cpp:408] ip1 <- pool1
I0428 19:48:46.407308 26589 net.cpp:382] ip1 -> ip1
I0428 19:48:46.407433 26589 net.cpp:124] Setting up ip1
I0428 19:48:46.407443 26589 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:48:46.407446 26589 net.cpp:139] Memory required for data: 1924800
I0428 19:48:46.407459 26589 layer_factory.hpp:77] Creating layer relu1
I0428 19:48:46.407466 26589 net.cpp:86] Creating Layer relu1
I0428 19:48:46.407474 26589 net.cpp:408] relu1 <- ip1
I0428 19:48:46.407480 26589 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:48:46.407656 26589 net.cpp:124] Setting up relu1
I0428 19:48:46.407666 26589 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:48:46.407672 26589 net.cpp:139] Memory required for data: 1934800
I0428 19:48:46.407677 26589 layer_factory.hpp:77] Creating layer ip2
I0428 19:48:46.407688 26589 net.cpp:86] Creating Layer ip2
I0428 19:48:46.407693 26589 net.cpp:408] ip2 <- ip1
I0428 19:48:46.407701 26589 net.cpp:382] ip2 -> ip2
I0428 19:48:46.407800 26589 net.cpp:124] Setting up ip2
I0428 19:48:46.407809 26589 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:48:46.407814 26589 net.cpp:139] Memory required for data: 1938800
I0428 19:48:46.407824 26589 layer_factory.hpp:77] Creating layer relu2
I0428 19:48:46.407831 26589 net.cpp:86] Creating Layer relu2
I0428 19:48:46.407837 26589 net.cpp:408] relu2 <- ip2
I0428 19:48:46.407845 26589 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:48:46.407999 26589 net.cpp:124] Setting up relu2
I0428 19:48:46.408008 26589 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:48:46.408015 26589 net.cpp:139] Memory required for data: 1942800
I0428 19:48:46.408020 26589 layer_factory.hpp:77] Creating layer ip2_relu2_0_split
I0428 19:48:46.408041 26589 net.cpp:86] Creating Layer ip2_relu2_0_split
I0428 19:48:46.408047 26589 net.cpp:408] ip2_relu2_0_split <- ip2
I0428 19:48:46.408054 26589 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_0
I0428 19:48:46.408077 26589 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_1
I0428 19:48:46.408124 26589 net.cpp:124] Setting up ip2_relu2_0_split
I0428 19:48:46.408133 26589 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:48:46.408139 26589 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:48:46.408144 26589 net.cpp:139] Memory required for data: 1950800
I0428 19:48:46.408149 26589 layer_factory.hpp:77] Creating layer accuracy
I0428 19:48:46.408157 26589 net.cpp:86] Creating Layer accuracy
I0428 19:48:46.408164 26589 net.cpp:408] accuracy <- ip2_relu2_0_split_0
I0428 19:48:46.408170 26589 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:48:46.408177 26589 net.cpp:382] accuracy -> accuracy
I0428 19:48:46.408188 26589 net.cpp:124] Setting up accuracy
I0428 19:48:46.408196 26589 net.cpp:131] Top shape: (1)
I0428 19:48:46.408201 26589 net.cpp:139] Memory required for data: 1950804
I0428 19:48:46.408206 26589 layer_factory.hpp:77] Creating layer loss
I0428 19:48:46.408215 26589 net.cpp:86] Creating Layer loss
I0428 19:48:46.408219 26589 net.cpp:408] loss <- ip2_relu2_0_split_1
I0428 19:48:46.408226 26589 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:48:46.408234 26589 net.cpp:382] loss -> loss
I0428 19:48:46.408247 26589 layer_factory.hpp:77] Creating layer loss
I0428 19:48:46.408501 26589 net.cpp:124] Setting up loss
I0428 19:48:46.408510 26589 net.cpp:131] Top shape: (1)
I0428 19:48:46.408516 26589 net.cpp:134]     with loss weight 1
I0428 19:48:46.408525 26589 net.cpp:139] Memory required for data: 1950808
I0428 19:48:46.408530 26589 net.cpp:200] loss needs backward computation.
I0428 19:48:46.408536 26589 net.cpp:202] accuracy does not need backward computation.
I0428 19:48:46.408543 26589 net.cpp:200] ip2_relu2_0_split needs backward computation.
I0428 19:48:46.408548 26589 net.cpp:200] relu2 needs backward computation.
I0428 19:48:46.408553 26589 net.cpp:200] ip2 needs backward computation.
I0428 19:48:46.408558 26589 net.cpp:200] relu1 needs backward computation.
I0428 19:48:46.408563 26589 net.cpp:200] ip1 needs backward computation.
I0428 19:48:46.408568 26589 net.cpp:200] pool1 needs backward computation.
I0428 19:48:46.408573 26589 net.cpp:200] conv1 needs backward computation.
I0428 19:48:46.408579 26589 net.cpp:200] pool0 needs backward computation.
I0428 19:48:46.408584 26589 net.cpp:200] conv0 needs backward computation.
I0428 19:48:46.408591 26589 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:48:46.408597 26589 net.cpp:202] mnist does not need backward computation.
I0428 19:48:46.408602 26589 net.cpp:244] This network produces output accuracy
I0428 19:48:46.408607 26589 net.cpp:244] This network produces output loss
I0428 19:48:46.408622 26589 net.cpp:257] Network initialization done.
I0428 19:48:46.408661 26589 solver.cpp:56] Solver scaffolding done.
I0428 19:48:46.409023 26589 caffe.cpp:248] Starting Optimization
I0428 19:48:46.409031 26589 solver.cpp:273] Solving LeNet
I0428 19:48:46.409037 26589 solver.cpp:274] Learning Rate Policy: inv
I0428 19:48:46.409914 26589 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:48:46.413337 26589 blocking_queue.cpp:49] Waiting for data
I0428 19:48:46.484455 26596 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:48:46.484931 26589 solver.cpp:398]     Test net output #0: accuracy = 0.0611
I0428 19:48:46.484953 26589 solver.cpp:398]     Test net output #1: loss = 2.29967 (* 1 = 2.29967 loss)
I0428 19:48:46.486704 26589 solver.cpp:219] Iteration 0 (-1.04677e-42 iter/s, 0.0776366s/100 iters), loss = 2.30409
I0428 19:48:46.486742 26589 solver.cpp:238]     Train net output #0: loss = 2.30409 (* 1 = 2.30409 loss)
I0428 19:48:46.486762 26589 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:48:46.551998 26589 solver.cpp:219] Iteration 100 (1532.48 iter/s, 0.0652537s/100 iters), loss = 0.475594
I0428 19:48:46.552026 26589 solver.cpp:238]     Train net output #0: loss = 0.475594 (* 1 = 0.475594 loss)
I0428 19:48:46.552050 26589 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:48:46.618005 26589 solver.cpp:219] Iteration 200 (1515.77 iter/s, 0.0659733s/100 iters), loss = 0.262413
I0428 19:48:46.618057 26589 solver.cpp:238]     Train net output #0: loss = 0.262413 (* 1 = 0.262413 loss)
I0428 19:48:46.618098 26589 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:48:46.683428 26589 solver.cpp:219] Iteration 300 (1529.85 iter/s, 0.065366s/100 iters), loss = 0.250934
I0428 19:48:46.683454 26589 solver.cpp:238]     Train net output #0: loss = 0.250934 (* 1 = 0.250934 loss)
I0428 19:48:46.683480 26589 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:48:46.748589 26589 solver.cpp:219] Iteration 400 (1535.45 iter/s, 0.0651274s/100 iters), loss = 0.161659
I0428 19:48:46.748616 26589 solver.cpp:238]     Train net output #0: loss = 0.161659 (* 1 = 0.161659 loss)
I0428 19:48:46.748641 26589 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:48:46.814059 26589 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:48:46.866964 26596 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:48:46.867405 26589 solver.cpp:398]     Test net output #0: accuracy = 0.9521
I0428 19:48:46.867427 26589 solver.cpp:398]     Test net output #1: loss = 0.159485 (* 1 = 0.159485 loss)
I0428 19:48:46.868151 26589 solver.cpp:219] Iteration 500 (836.635 iter/s, 0.119526s/100 iters), loss = 0.122135
I0428 19:48:46.868177 26589 solver.cpp:238]     Train net output #0: loss = 0.122135 (* 1 = 0.122135 loss)
I0428 19:48:46.868193 26589 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:48:46.931426 26589 solver.cpp:219] Iteration 600 (1581.22 iter/s, 0.0632422s/100 iters), loss = 0.11871
I0428 19:48:46.931452 26589 solver.cpp:238]     Train net output #0: loss = 0.11871 (* 1 = 0.11871 loss)
I0428 19:48:46.931478 26589 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:48:46.996688 26589 solver.cpp:219] Iteration 700 (1533.04 iter/s, 0.0652298s/100 iters), loss = 0.228886
I0428 19:48:46.996716 26589 solver.cpp:238]     Train net output #0: loss = 0.228886 (* 1 = 0.228886 loss)
I0428 19:48:46.996740 26589 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:48:47.062577 26589 solver.cpp:219] Iteration 800 (1518.5 iter/s, 0.0658546s/100 iters), loss = 0.363777
I0428 19:48:47.062616 26589 solver.cpp:238]     Train net output #0: loss = 0.363777 (* 1 = 0.363777 loss)
I0428 19:48:47.062641 26589 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:48:47.127480 26589 solver.cpp:219] Iteration 900 (1541.51 iter/s, 0.0648714s/100 iters), loss = 0.136854
I0428 19:48:47.127506 26589 solver.cpp:238]     Train net output #0: loss = 0.136854 (* 1 = 0.136854 loss)
I0428 19:48:47.127530 26589 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:48:47.148967 26595 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:48:47.190948 26589 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:48:47.191552 26589 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:48:47.191963 26589 solver.cpp:311] Iteration 1000, loss = 0.16665
I0428 19:48:47.191995 26589 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:48:47.235088 26596 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:48:47.235543 26589 solver.cpp:398]     Test net output #0: accuracy = 0.964
I0428 19:48:47.235563 26589 solver.cpp:398]     Test net output #1: loss = 0.11504 (* 1 = 0.11504 loss)
I0428 19:48:47.235570 26589 solver.cpp:316] Optimization Done.
I0428 19:48:47.235574 26589 caffe.cpp:259] Optimization Done.
