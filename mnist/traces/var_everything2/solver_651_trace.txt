I0428 19:52:03.273232 27425 caffe.cpp:218] Using GPUs 0
I0428 19:52:03.310967 27425 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:52:03.824246 27425 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test651.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:52:03.824390 27425 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test651.prototxt
I0428 19:52:03.824816 27425 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:52:03.824837 27425 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:52:03.824944 27425 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:52:03.825026 27425 layer_factory.hpp:77] Creating layer mnist
I0428 19:52:03.825129 27425 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:52:03.825151 27425 net.cpp:86] Creating Layer mnist
I0428 19:52:03.825160 27425 net.cpp:382] mnist -> data
I0428 19:52:03.825182 27425 net.cpp:382] mnist -> label
I0428 19:52:03.826386 27425 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:52:03.828830 27425 net.cpp:124] Setting up mnist
I0428 19:52:03.828848 27425 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:52:03.828855 27425 net.cpp:131] Top shape: 64 (64)
I0428 19:52:03.828858 27425 net.cpp:139] Memory required for data: 200960
I0428 19:52:03.828866 27425 layer_factory.hpp:77] Creating layer conv0
I0428 19:52:03.828889 27425 net.cpp:86] Creating Layer conv0
I0428 19:52:03.828910 27425 net.cpp:408] conv0 <- data
I0428 19:52:03.828923 27425 net.cpp:382] conv0 -> conv0
I0428 19:52:04.116199 27425 net.cpp:124] Setting up conv0
I0428 19:52:04.116226 27425 net.cpp:131] Top shape: 64 5 24 24 (184320)
I0428 19:52:04.116230 27425 net.cpp:139] Memory required for data: 938240
I0428 19:52:04.116245 27425 layer_factory.hpp:77] Creating layer pool0
I0428 19:52:04.116259 27425 net.cpp:86] Creating Layer pool0
I0428 19:52:04.116263 27425 net.cpp:408] pool0 <- conv0
I0428 19:52:04.116268 27425 net.cpp:382] pool0 -> pool0
I0428 19:52:04.116317 27425 net.cpp:124] Setting up pool0
I0428 19:52:04.116324 27425 net.cpp:131] Top shape: 64 5 12 12 (46080)
I0428 19:52:04.116328 27425 net.cpp:139] Memory required for data: 1122560
I0428 19:52:04.116330 27425 layer_factory.hpp:77] Creating layer conv1
I0428 19:52:04.116340 27425 net.cpp:86] Creating Layer conv1
I0428 19:52:04.116344 27425 net.cpp:408] conv1 <- pool0
I0428 19:52:04.116349 27425 net.cpp:382] conv1 -> conv1
I0428 19:52:04.119058 27425 net.cpp:124] Setting up conv1
I0428 19:52:04.119073 27425 net.cpp:131] Top shape: 64 25 8 8 (102400)
I0428 19:52:04.119077 27425 net.cpp:139] Memory required for data: 1532160
I0428 19:52:04.119086 27425 layer_factory.hpp:77] Creating layer pool1
I0428 19:52:04.119094 27425 net.cpp:86] Creating Layer pool1
I0428 19:52:04.119098 27425 net.cpp:408] pool1 <- conv1
I0428 19:52:04.119103 27425 net.cpp:382] pool1 -> pool1
I0428 19:52:04.119141 27425 net.cpp:124] Setting up pool1
I0428 19:52:04.119149 27425 net.cpp:131] Top shape: 64 25 4 4 (25600)
I0428 19:52:04.119153 27425 net.cpp:139] Memory required for data: 1634560
I0428 19:52:04.119155 27425 layer_factory.hpp:77] Creating layer ip1
I0428 19:52:04.119161 27425 net.cpp:86] Creating Layer ip1
I0428 19:52:04.119165 27425 net.cpp:408] ip1 <- pool1
I0428 19:52:04.119169 27425 net.cpp:382] ip1 -> ip1
I0428 19:52:04.120152 27425 net.cpp:124] Setting up ip1
I0428 19:52:04.120165 27425 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:52:04.120168 27425 net.cpp:139] Memory required for data: 1640960
I0428 19:52:04.120177 27425 layer_factory.hpp:77] Creating layer relu1
I0428 19:52:04.120183 27425 net.cpp:86] Creating Layer relu1
I0428 19:52:04.120187 27425 net.cpp:408] relu1 <- ip1
I0428 19:52:04.120192 27425 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:52:04.120364 27425 net.cpp:124] Setting up relu1
I0428 19:52:04.120373 27425 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:52:04.120378 27425 net.cpp:139] Memory required for data: 1647360
I0428 19:52:04.120381 27425 layer_factory.hpp:77] Creating layer ip2
I0428 19:52:04.120388 27425 net.cpp:86] Creating Layer ip2
I0428 19:52:04.120390 27425 net.cpp:408] ip2 <- ip1
I0428 19:52:04.120395 27425 net.cpp:382] ip2 -> ip2
I0428 19:52:04.120502 27425 net.cpp:124] Setting up ip2
I0428 19:52:04.120509 27425 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:52:04.120512 27425 net.cpp:139] Memory required for data: 1660160
I0428 19:52:04.120517 27425 layer_factory.hpp:77] Creating layer relu2
I0428 19:52:04.120523 27425 net.cpp:86] Creating Layer relu2
I0428 19:52:04.120527 27425 net.cpp:408] relu2 <- ip2
I0428 19:52:04.120532 27425 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:52:04.121281 27425 net.cpp:124] Setting up relu2
I0428 19:52:04.121294 27425 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:52:04.121297 27425 net.cpp:139] Memory required for data: 1672960
I0428 19:52:04.121301 27425 layer_factory.hpp:77] Creating layer ip3
I0428 19:52:04.121307 27425 net.cpp:86] Creating Layer ip3
I0428 19:52:04.121311 27425 net.cpp:408] ip3 <- ip2
I0428 19:52:04.121316 27425 net.cpp:382] ip3 -> ip3
I0428 19:52:04.121417 27425 net.cpp:124] Setting up ip3
I0428 19:52:04.121424 27425 net.cpp:131] Top shape: 64 10 (640)
I0428 19:52:04.121428 27425 net.cpp:139] Memory required for data: 1675520
I0428 19:52:04.121436 27425 layer_factory.hpp:77] Creating layer relu3
I0428 19:52:04.121441 27425 net.cpp:86] Creating Layer relu3
I0428 19:52:04.121444 27425 net.cpp:408] relu3 <- ip3
I0428 19:52:04.121448 27425 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:52:04.121614 27425 net.cpp:124] Setting up relu3
I0428 19:52:04.121623 27425 net.cpp:131] Top shape: 64 10 (640)
I0428 19:52:04.121625 27425 net.cpp:139] Memory required for data: 1678080
I0428 19:52:04.121629 27425 layer_factory.hpp:77] Creating layer loss
I0428 19:52:04.121634 27425 net.cpp:86] Creating Layer loss
I0428 19:52:04.121637 27425 net.cpp:408] loss <- ip3
I0428 19:52:04.121641 27425 net.cpp:408] loss <- label
I0428 19:52:04.121647 27425 net.cpp:382] loss -> loss
I0428 19:52:04.121665 27425 layer_factory.hpp:77] Creating layer loss
I0428 19:52:04.121892 27425 net.cpp:124] Setting up loss
I0428 19:52:04.121901 27425 net.cpp:131] Top shape: (1)
I0428 19:52:04.121904 27425 net.cpp:134]     with loss weight 1
I0428 19:52:04.121919 27425 net.cpp:139] Memory required for data: 1678084
I0428 19:52:04.121923 27425 net.cpp:200] loss needs backward computation.
I0428 19:52:04.121927 27425 net.cpp:200] relu3 needs backward computation.
I0428 19:52:04.121929 27425 net.cpp:200] ip3 needs backward computation.
I0428 19:52:04.121932 27425 net.cpp:200] relu2 needs backward computation.
I0428 19:52:04.121935 27425 net.cpp:200] ip2 needs backward computation.
I0428 19:52:04.121938 27425 net.cpp:200] relu1 needs backward computation.
I0428 19:52:04.121940 27425 net.cpp:200] ip1 needs backward computation.
I0428 19:52:04.121943 27425 net.cpp:200] pool1 needs backward computation.
I0428 19:52:04.121948 27425 net.cpp:200] conv1 needs backward computation.
I0428 19:52:04.121950 27425 net.cpp:200] pool0 needs backward computation.
I0428 19:52:04.121953 27425 net.cpp:200] conv0 needs backward computation.
I0428 19:52:04.121956 27425 net.cpp:202] mnist does not need backward computation.
I0428 19:52:04.121959 27425 net.cpp:244] This network produces output loss
I0428 19:52:04.121968 27425 net.cpp:257] Network initialization done.
I0428 19:52:04.122282 27425 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test651.prototxt
I0428 19:52:04.122308 27425 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:52:04.122396 27425 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:52:04.122475 27425 layer_factory.hpp:77] Creating layer mnist
I0428 19:52:04.122519 27425 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:52:04.122531 27425 net.cpp:86] Creating Layer mnist
I0428 19:52:04.122535 27425 net.cpp:382] mnist -> data
I0428 19:52:04.122544 27425 net.cpp:382] mnist -> label
I0428 19:52:04.122627 27425 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:52:04.124650 27425 net.cpp:124] Setting up mnist
I0428 19:52:04.124662 27425 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:52:04.124667 27425 net.cpp:131] Top shape: 100 (100)
I0428 19:52:04.124670 27425 net.cpp:139] Memory required for data: 314000
I0428 19:52:04.124675 27425 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:52:04.124681 27425 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:52:04.124685 27425 net.cpp:408] label_mnist_1_split <- label
I0428 19:52:04.124689 27425 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:52:04.124696 27425 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:52:04.124742 27425 net.cpp:124] Setting up label_mnist_1_split
I0428 19:52:04.124748 27425 net.cpp:131] Top shape: 100 (100)
I0428 19:52:04.124752 27425 net.cpp:131] Top shape: 100 (100)
I0428 19:52:04.124754 27425 net.cpp:139] Memory required for data: 314800
I0428 19:52:04.124758 27425 layer_factory.hpp:77] Creating layer conv0
I0428 19:52:04.124765 27425 net.cpp:86] Creating Layer conv0
I0428 19:52:04.124768 27425 net.cpp:408] conv0 <- data
I0428 19:52:04.124773 27425 net.cpp:382] conv0 -> conv0
I0428 19:52:04.126312 27425 net.cpp:124] Setting up conv0
I0428 19:52:04.126327 27425 net.cpp:131] Top shape: 100 5 24 24 (288000)
I0428 19:52:04.126330 27425 net.cpp:139] Memory required for data: 1466800
I0428 19:52:04.126339 27425 layer_factory.hpp:77] Creating layer pool0
I0428 19:52:04.126345 27425 net.cpp:86] Creating Layer pool0
I0428 19:52:04.126348 27425 net.cpp:408] pool0 <- conv0
I0428 19:52:04.126353 27425 net.cpp:382] pool0 -> pool0
I0428 19:52:04.126389 27425 net.cpp:124] Setting up pool0
I0428 19:52:04.126395 27425 net.cpp:131] Top shape: 100 5 12 12 (72000)
I0428 19:52:04.126399 27425 net.cpp:139] Memory required for data: 1754800
I0428 19:52:04.126401 27425 layer_factory.hpp:77] Creating layer conv1
I0428 19:52:04.126410 27425 net.cpp:86] Creating Layer conv1
I0428 19:52:04.126413 27425 net.cpp:408] conv1 <- pool0
I0428 19:52:04.126418 27425 net.cpp:382] conv1 -> conv1
I0428 19:52:04.128430 27425 net.cpp:124] Setting up conv1
I0428 19:52:04.128443 27425 net.cpp:131] Top shape: 100 25 8 8 (160000)
I0428 19:52:04.128446 27425 net.cpp:139] Memory required for data: 2394800
I0428 19:52:04.128455 27425 layer_factory.hpp:77] Creating layer pool1
I0428 19:52:04.128461 27425 net.cpp:86] Creating Layer pool1
I0428 19:52:04.128464 27425 net.cpp:408] pool1 <- conv1
I0428 19:52:04.128469 27425 net.cpp:382] pool1 -> pool1
I0428 19:52:04.128509 27425 net.cpp:124] Setting up pool1
I0428 19:52:04.128515 27425 net.cpp:131] Top shape: 100 25 4 4 (40000)
I0428 19:52:04.128518 27425 net.cpp:139] Memory required for data: 2554800
I0428 19:52:04.128521 27425 layer_factory.hpp:77] Creating layer ip1
I0428 19:52:04.128527 27425 net.cpp:86] Creating Layer ip1
I0428 19:52:04.128530 27425 net.cpp:408] ip1 <- pool1
I0428 19:52:04.128535 27425 net.cpp:382] ip1 -> ip1
I0428 19:52:04.128684 27425 net.cpp:124] Setting up ip1
I0428 19:52:04.128691 27425 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:52:04.128703 27425 net.cpp:139] Memory required for data: 2564800
I0428 19:52:04.128711 27425 layer_factory.hpp:77] Creating layer relu1
I0428 19:52:04.128717 27425 net.cpp:86] Creating Layer relu1
I0428 19:52:04.128720 27425 net.cpp:408] relu1 <- ip1
I0428 19:52:04.128731 27425 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:52:04.128895 27425 net.cpp:124] Setting up relu1
I0428 19:52:04.128903 27425 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:52:04.128907 27425 net.cpp:139] Memory required for data: 2574800
I0428 19:52:04.128911 27425 layer_factory.hpp:77] Creating layer ip2
I0428 19:52:04.128917 27425 net.cpp:86] Creating Layer ip2
I0428 19:52:04.128926 27425 net.cpp:408] ip2 <- ip1
I0428 19:52:04.128931 27425 net.cpp:382] ip2 -> ip2
I0428 19:52:04.129032 27425 net.cpp:124] Setting up ip2
I0428 19:52:04.129040 27425 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:52:04.129042 27425 net.cpp:139] Memory required for data: 2594800
I0428 19:52:04.129047 27425 layer_factory.hpp:77] Creating layer relu2
I0428 19:52:04.129052 27425 net.cpp:86] Creating Layer relu2
I0428 19:52:04.129055 27425 net.cpp:408] relu2 <- ip2
I0428 19:52:04.129060 27425 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:52:04.129246 27425 net.cpp:124] Setting up relu2
I0428 19:52:04.129254 27425 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:52:04.129257 27425 net.cpp:139] Memory required for data: 2614800
I0428 19:52:04.129261 27425 layer_factory.hpp:77] Creating layer ip3
I0428 19:52:04.129266 27425 net.cpp:86] Creating Layer ip3
I0428 19:52:04.129268 27425 net.cpp:408] ip3 <- ip2
I0428 19:52:04.129273 27425 net.cpp:382] ip3 -> ip3
I0428 19:52:04.129382 27425 net.cpp:124] Setting up ip3
I0428 19:52:04.129390 27425 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:52:04.129392 27425 net.cpp:139] Memory required for data: 2618800
I0428 19:52:04.129405 27425 layer_factory.hpp:77] Creating layer relu3
I0428 19:52:04.129410 27425 net.cpp:86] Creating Layer relu3
I0428 19:52:04.129416 27425 net.cpp:408] relu3 <- ip3
I0428 19:52:04.129421 27425 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:52:04.130316 27425 net.cpp:124] Setting up relu3
I0428 19:52:04.130329 27425 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:52:04.130332 27425 net.cpp:139] Memory required for data: 2622800
I0428 19:52:04.130336 27425 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:52:04.130342 27425 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:52:04.130344 27425 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:52:04.130349 27425 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:52:04.130355 27425 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:52:04.130391 27425 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:52:04.130398 27425 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:52:04.130401 27425 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:52:04.130404 27425 net.cpp:139] Memory required for data: 2630800
I0428 19:52:04.130408 27425 layer_factory.hpp:77] Creating layer accuracy
I0428 19:52:04.130412 27425 net.cpp:86] Creating Layer accuracy
I0428 19:52:04.130415 27425 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:52:04.130419 27425 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:52:04.130424 27425 net.cpp:382] accuracy -> accuracy
I0428 19:52:04.130430 27425 net.cpp:124] Setting up accuracy
I0428 19:52:04.130434 27425 net.cpp:131] Top shape: (1)
I0428 19:52:04.130436 27425 net.cpp:139] Memory required for data: 2630804
I0428 19:52:04.130439 27425 layer_factory.hpp:77] Creating layer loss
I0428 19:52:04.130444 27425 net.cpp:86] Creating Layer loss
I0428 19:52:04.130446 27425 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:52:04.130450 27425 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:52:04.130455 27425 net.cpp:382] loss -> loss
I0428 19:52:04.130460 27425 layer_factory.hpp:77] Creating layer loss
I0428 19:52:04.130784 27425 net.cpp:124] Setting up loss
I0428 19:52:04.130796 27425 net.cpp:131] Top shape: (1)
I0428 19:52:04.130800 27425 net.cpp:134]     with loss weight 1
I0428 19:52:04.130807 27425 net.cpp:139] Memory required for data: 2630808
I0428 19:52:04.130821 27425 net.cpp:200] loss needs backward computation.
I0428 19:52:04.130826 27425 net.cpp:202] accuracy does not need backward computation.
I0428 19:52:04.130831 27425 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:52:04.130833 27425 net.cpp:200] relu3 needs backward computation.
I0428 19:52:04.130836 27425 net.cpp:200] ip3 needs backward computation.
I0428 19:52:04.130841 27425 net.cpp:200] relu2 needs backward computation.
I0428 19:52:04.130843 27425 net.cpp:200] ip2 needs backward computation.
I0428 19:52:04.130846 27425 net.cpp:200] relu1 needs backward computation.
I0428 19:52:04.130851 27425 net.cpp:200] ip1 needs backward computation.
I0428 19:52:04.130857 27425 net.cpp:200] pool1 needs backward computation.
I0428 19:52:04.130863 27425 net.cpp:200] conv1 needs backward computation.
I0428 19:52:04.130867 27425 net.cpp:200] pool0 needs backward computation.
I0428 19:52:04.130872 27425 net.cpp:200] conv0 needs backward computation.
I0428 19:52:04.130875 27425 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:52:04.130880 27425 net.cpp:202] mnist does not need backward computation.
I0428 19:52:04.130883 27425 net.cpp:244] This network produces output accuracy
I0428 19:52:04.130887 27425 net.cpp:244] This network produces output loss
I0428 19:52:04.130898 27425 net.cpp:257] Network initialization done.
I0428 19:52:04.130952 27425 solver.cpp:56] Solver scaffolding done.
I0428 19:52:04.131340 27425 caffe.cpp:248] Starting Optimization
I0428 19:52:04.131345 27425 solver.cpp:273] Solving LeNet
I0428 19:52:04.131348 27425 solver.cpp:274] Learning Rate Policy: inv
I0428 19:52:04.132071 27425 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:52:04.136945 27425 blocking_queue.cpp:49] Waiting for data
I0428 19:52:04.186403 27432 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:52:04.186936 27425 solver.cpp:398]     Test net output #0: accuracy = 0.1184
I0428 19:52:04.186970 27425 solver.cpp:398]     Test net output #1: loss = 2.35223 (* 1 = 2.35223 loss)
I0428 19:52:04.188787 27425 solver.cpp:219] Iteration 0 (-6.80661e-31 iter/s, 0.0574167s/100 iters), loss = 2.30812
I0428 19:52:04.188817 27425 solver.cpp:238]     Train net output #0: loss = 2.30812 (* 1 = 2.30812 loss)
I0428 19:52:04.188839 27425 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:52:04.272290 27425 solver.cpp:219] Iteration 100 (1198.03 iter/s, 0.0834703s/100 iters), loss = 0.6131
I0428 19:52:04.272330 27425 solver.cpp:238]     Train net output #0: loss = 0.6131 (* 1 = 0.6131 loss)
I0428 19:52:04.272336 27425 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:52:04.355729 27425 solver.cpp:219] Iteration 200 (1198.96 iter/s, 0.0834058s/100 iters), loss = 0.45333
I0428 19:52:04.355767 27425 solver.cpp:238]     Train net output #0: loss = 0.45333 (* 1 = 0.45333 loss)
I0428 19:52:04.355772 27425 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:52:04.438405 27425 solver.cpp:219] Iteration 300 (1210.01 iter/s, 0.082644s/100 iters), loss = 0.471257
I0428 19:52:04.438444 27425 solver.cpp:238]     Train net output #0: loss = 0.471257 (* 1 = 0.471257 loss)
I0428 19:52:04.438452 27425 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:52:04.522609 27425 solver.cpp:219] Iteration 400 (1188.04 iter/s, 0.0841719s/100 iters), loss = 0.178271
I0428 19:52:04.522650 27425 solver.cpp:238]     Train net output #0: loss = 0.178271 (* 1 = 0.178271 loss)
I0428 19:52:04.522655 27425 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:52:04.605348 27425 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:52:04.651877 27432 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:52:04.652385 27425 solver.cpp:398]     Test net output #0: accuracy = 0.9478
I0428 19:52:04.652405 27425 solver.cpp:398]     Test net output #1: loss = 0.166361 (* 1 = 0.166361 loss)
I0428 19:52:04.653344 27425 solver.cpp:219] Iteration 500 (765.18 iter/s, 0.130688s/100 iters), loss = 0.121166
I0428 19:52:04.653398 27425 solver.cpp:238]     Train net output #0: loss = 0.121166 (* 1 = 0.121166 loss)
I0428 19:52:04.653419 27425 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:52:04.747265 27425 solver.cpp:219] Iteration 600 (1065.4 iter/s, 0.0938613s/100 iters), loss = 0.177149
I0428 19:52:04.747308 27425 solver.cpp:238]     Train net output #0: loss = 0.177149 (* 1 = 0.177149 loss)
I0428 19:52:04.747313 27425 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:52:04.831223 27425 solver.cpp:219] Iteration 700 (1191.57 iter/s, 0.0839228s/100 iters), loss = 0.120548
I0428 19:52:04.831261 27425 solver.cpp:238]     Train net output #0: loss = 0.120548 (* 1 = 0.120548 loss)
I0428 19:52:04.831267 27425 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:52:04.913493 27425 solver.cpp:219] Iteration 800 (1215.98 iter/s, 0.0822385s/100 iters), loss = 0.277871
I0428 19:52:04.913532 27425 solver.cpp:238]     Train net output #0: loss = 0.277871 (* 1 = 0.277871 loss)
I0428 19:52:04.913538 27425 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:52:04.997040 27425 solver.cpp:219] Iteration 900 (1197.4 iter/s, 0.0835144s/100 iters), loss = 0.207993
I0428 19:52:04.997081 27425 solver.cpp:238]     Train net output #0: loss = 0.207993 (* 1 = 0.207993 loss)
I0428 19:52:04.997087 27425 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:52:05.024822 27431 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:52:05.079509 27425 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:52:05.080478 27425 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:52:05.081038 27425 solver.cpp:311] Iteration 1000, loss = 0.138295
I0428 19:52:05.081071 27425 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:52:05.154726 27432 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:52:05.155273 27425 solver.cpp:398]     Test net output #0: accuracy = 0.9681
I0428 19:52:05.155308 27425 solver.cpp:398]     Test net output #1: loss = 0.0972763 (* 1 = 0.0972763 loss)
I0428 19:52:05.155313 27425 solver.cpp:316] Optimization Done.
I0428 19:52:05.155315 27425 caffe.cpp:259] Optimization Done.
