I0428 19:48:39.794626 26540 caffe.cpp:218] Using GPUs 0
I0428 19:48:39.835636 26540 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:48:40.298032 26540 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test551.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:48:40.298184 26540 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test551.prototxt
I0428 19:48:40.298502 26540 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:48:40.298521 26540 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:48:40.298593 26540 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0428 19:48:40.298676 26540 layer_factory.hpp:77] Creating layer mnist
I0428 19:48:40.298775 26540 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:48:40.298801 26540 net.cpp:86] Creating Layer mnist
I0428 19:48:40.298812 26540 net.cpp:382] mnist -> data
I0428 19:48:40.298836 26540 net.cpp:382] mnist -> label
I0428 19:48:40.299917 26540 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:48:40.302240 26540 net.cpp:124] Setting up mnist
I0428 19:48:40.302273 26540 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:48:40.302281 26540 net.cpp:131] Top shape: 64 (64)
I0428 19:48:40.302285 26540 net.cpp:139] Memory required for data: 200960
I0428 19:48:40.302294 26540 layer_factory.hpp:77] Creating layer conv0
I0428 19:48:40.302314 26540 net.cpp:86] Creating Layer conv0
I0428 19:48:40.302323 26540 net.cpp:408] conv0 <- data
I0428 19:48:40.302340 26540 net.cpp:382] conv0 -> conv0
I0428 19:48:40.541350 26540 net.cpp:124] Setting up conv0
I0428 19:48:40.541379 26540 net.cpp:131] Top shape: 64 5 24 24 (184320)
I0428 19:48:40.541384 26540 net.cpp:139] Memory required for data: 938240
I0428 19:48:40.541448 26540 layer_factory.hpp:77] Creating layer pool0
I0428 19:48:40.541467 26540 net.cpp:86] Creating Layer pool0
I0428 19:48:40.541474 26540 net.cpp:408] pool0 <- conv0
I0428 19:48:40.541483 26540 net.cpp:382] pool0 -> pool0
I0428 19:48:40.541537 26540 net.cpp:124] Setting up pool0
I0428 19:48:40.541545 26540 net.cpp:131] Top shape: 64 5 12 12 (46080)
I0428 19:48:40.541550 26540 net.cpp:139] Memory required for data: 1122560
I0428 19:48:40.541555 26540 layer_factory.hpp:77] Creating layer conv1
I0428 19:48:40.541570 26540 net.cpp:86] Creating Layer conv1
I0428 19:48:40.541576 26540 net.cpp:408] conv1 <- pool0
I0428 19:48:40.541585 26540 net.cpp:382] conv1 -> conv1
I0428 19:48:40.543599 26540 net.cpp:124] Setting up conv1
I0428 19:48:40.543615 26540 net.cpp:131] Top shape: 64 5 8 8 (20480)
I0428 19:48:40.543635 26540 net.cpp:139] Memory required for data: 1204480
I0428 19:48:40.543663 26540 layer_factory.hpp:77] Creating layer pool1
I0428 19:48:40.543680 26540 net.cpp:86] Creating Layer pool1
I0428 19:48:40.543689 26540 net.cpp:408] pool1 <- conv1
I0428 19:48:40.543696 26540 net.cpp:382] pool1 -> pool1
I0428 19:48:40.543743 26540 net.cpp:124] Setting up pool1
I0428 19:48:40.543752 26540 net.cpp:131] Top shape: 64 5 4 4 (5120)
I0428 19:48:40.543757 26540 net.cpp:139] Memory required for data: 1224960
I0428 19:48:40.543763 26540 layer_factory.hpp:77] Creating layer ip1
I0428 19:48:40.543777 26540 net.cpp:86] Creating Layer ip1
I0428 19:48:40.543784 26540 net.cpp:408] ip1 <- pool1
I0428 19:48:40.543792 26540 net.cpp:382] ip1 -> ip1
I0428 19:48:40.543906 26540 net.cpp:124] Setting up ip1
I0428 19:48:40.543915 26540 net.cpp:131] Top shape: 64 10 (640)
I0428 19:48:40.543920 26540 net.cpp:139] Memory required for data: 1227520
I0428 19:48:40.543931 26540 layer_factory.hpp:77] Creating layer relu1
I0428 19:48:40.543941 26540 net.cpp:86] Creating Layer relu1
I0428 19:48:40.543948 26540 net.cpp:408] relu1 <- ip1
I0428 19:48:40.543957 26540 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:48:40.544148 26540 net.cpp:124] Setting up relu1
I0428 19:48:40.544159 26540 net.cpp:131] Top shape: 64 10 (640)
I0428 19:48:40.544164 26540 net.cpp:139] Memory required for data: 1230080
I0428 19:48:40.544169 26540 layer_factory.hpp:77] Creating layer ip2
I0428 19:48:40.544181 26540 net.cpp:86] Creating Layer ip2
I0428 19:48:40.544188 26540 net.cpp:408] ip2 <- ip1
I0428 19:48:40.544195 26540 net.cpp:382] ip2 -> ip2
I0428 19:48:40.544307 26540 net.cpp:124] Setting up ip2
I0428 19:48:40.544317 26540 net.cpp:131] Top shape: 64 10 (640)
I0428 19:48:40.544322 26540 net.cpp:139] Memory required for data: 1232640
I0428 19:48:40.544330 26540 layer_factory.hpp:77] Creating layer relu2
I0428 19:48:40.544342 26540 net.cpp:86] Creating Layer relu2
I0428 19:48:40.544348 26540 net.cpp:408] relu2 <- ip2
I0428 19:48:40.544356 26540 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:48:40.545155 26540 net.cpp:124] Setting up relu2
I0428 19:48:40.545169 26540 net.cpp:131] Top shape: 64 10 (640)
I0428 19:48:40.545176 26540 net.cpp:139] Memory required for data: 1235200
I0428 19:48:40.545181 26540 layer_factory.hpp:77] Creating layer loss
I0428 19:48:40.545188 26540 net.cpp:86] Creating Layer loss
I0428 19:48:40.545209 26540 net.cpp:408] loss <- ip2
I0428 19:48:40.545231 26540 net.cpp:408] loss <- label
I0428 19:48:40.545240 26540 net.cpp:382] loss -> loss
I0428 19:48:40.545266 26540 layer_factory.hpp:77] Creating layer loss
I0428 19:48:40.545539 26540 net.cpp:124] Setting up loss
I0428 19:48:40.545552 26540 net.cpp:131] Top shape: (1)
I0428 19:48:40.545557 26540 net.cpp:134]     with loss weight 1
I0428 19:48:40.545575 26540 net.cpp:139] Memory required for data: 1235204
I0428 19:48:40.545581 26540 net.cpp:200] loss needs backward computation.
I0428 19:48:40.545588 26540 net.cpp:200] relu2 needs backward computation.
I0428 19:48:40.545593 26540 net.cpp:200] ip2 needs backward computation.
I0428 19:48:40.545598 26540 net.cpp:200] relu1 needs backward computation.
I0428 19:48:40.545603 26540 net.cpp:200] ip1 needs backward computation.
I0428 19:48:40.545608 26540 net.cpp:200] pool1 needs backward computation.
I0428 19:48:40.545622 26540 net.cpp:200] conv1 needs backward computation.
I0428 19:48:40.545629 26540 net.cpp:200] pool0 needs backward computation.
I0428 19:48:40.545634 26540 net.cpp:200] conv0 needs backward computation.
I0428 19:48:40.545639 26540 net.cpp:202] mnist does not need backward computation.
I0428 19:48:40.545644 26540 net.cpp:244] This network produces output loss
I0428 19:48:40.545656 26540 net.cpp:257] Network initialization done.
I0428 19:48:40.545948 26540 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test551.prototxt
I0428 19:48:40.545996 26540 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:48:40.546094 26540 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0428 19:48:40.546198 26540 layer_factory.hpp:77] Creating layer mnist
I0428 19:48:40.546262 26540 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:48:40.546296 26540 net.cpp:86] Creating Layer mnist
I0428 19:48:40.546321 26540 net.cpp:382] mnist -> data
I0428 19:48:40.546334 26540 net.cpp:382] mnist -> label
I0428 19:48:40.546474 26540 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:48:40.548588 26540 net.cpp:124] Setting up mnist
I0428 19:48:40.548602 26540 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:48:40.548610 26540 net.cpp:131] Top shape: 100 (100)
I0428 19:48:40.548615 26540 net.cpp:139] Memory required for data: 314000
I0428 19:48:40.548621 26540 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:48:40.548638 26540 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:48:40.548645 26540 net.cpp:408] label_mnist_1_split <- label
I0428 19:48:40.548655 26540 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:48:40.548665 26540 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:48:40.548773 26540 net.cpp:124] Setting up label_mnist_1_split
I0428 19:48:40.548789 26540 net.cpp:131] Top shape: 100 (100)
I0428 19:48:40.548796 26540 net.cpp:131] Top shape: 100 (100)
I0428 19:48:40.548800 26540 net.cpp:139] Memory required for data: 314800
I0428 19:48:40.548805 26540 layer_factory.hpp:77] Creating layer conv0
I0428 19:48:40.548844 26540 net.cpp:86] Creating Layer conv0
I0428 19:48:40.548851 26540 net.cpp:408] conv0 <- data
I0428 19:48:40.548861 26540 net.cpp:382] conv0 -> conv0
I0428 19:48:40.550447 26540 net.cpp:124] Setting up conv0
I0428 19:48:40.550462 26540 net.cpp:131] Top shape: 100 5 24 24 (288000)
I0428 19:48:40.550467 26540 net.cpp:139] Memory required for data: 1466800
I0428 19:48:40.550484 26540 layer_factory.hpp:77] Creating layer pool0
I0428 19:48:40.550509 26540 net.cpp:86] Creating Layer pool0
I0428 19:48:40.550515 26540 net.cpp:408] pool0 <- conv0
I0428 19:48:40.550523 26540 net.cpp:382] pool0 -> pool0
I0428 19:48:40.550565 26540 net.cpp:124] Setting up pool0
I0428 19:48:40.550575 26540 net.cpp:131] Top shape: 100 5 12 12 (72000)
I0428 19:48:40.550578 26540 net.cpp:139] Memory required for data: 1754800
I0428 19:48:40.550583 26540 layer_factory.hpp:77] Creating layer conv1
I0428 19:48:40.550597 26540 net.cpp:86] Creating Layer conv1
I0428 19:48:40.550604 26540 net.cpp:408] conv1 <- pool0
I0428 19:48:40.550616 26540 net.cpp:382] conv1 -> conv1
I0428 19:48:40.552850 26540 net.cpp:124] Setting up conv1
I0428 19:48:40.552881 26540 net.cpp:131] Top shape: 100 5 8 8 (32000)
I0428 19:48:40.552886 26540 net.cpp:139] Memory required for data: 1882800
I0428 19:48:40.552917 26540 layer_factory.hpp:77] Creating layer pool1
I0428 19:48:40.552927 26540 net.cpp:86] Creating Layer pool1
I0428 19:48:40.552934 26540 net.cpp:408] pool1 <- conv1
I0428 19:48:40.552947 26540 net.cpp:382] pool1 -> pool1
I0428 19:48:40.552997 26540 net.cpp:124] Setting up pool1
I0428 19:48:40.553007 26540 net.cpp:131] Top shape: 100 5 4 4 (8000)
I0428 19:48:40.553012 26540 net.cpp:139] Memory required for data: 1914800
I0428 19:48:40.553019 26540 layer_factory.hpp:77] Creating layer ip1
I0428 19:48:40.553030 26540 net.cpp:86] Creating Layer ip1
I0428 19:48:40.553036 26540 net.cpp:408] ip1 <- pool1
I0428 19:48:40.553048 26540 net.cpp:382] ip1 -> ip1
I0428 19:48:40.553200 26540 net.cpp:124] Setting up ip1
I0428 19:48:40.553210 26540 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:48:40.553215 26540 net.cpp:139] Memory required for data: 1918800
I0428 19:48:40.553226 26540 layer_factory.hpp:77] Creating layer relu1
I0428 19:48:40.553236 26540 net.cpp:86] Creating Layer relu1
I0428 19:48:40.553242 26540 net.cpp:408] relu1 <- ip1
I0428 19:48:40.553251 26540 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:48:40.553424 26540 net.cpp:124] Setting up relu1
I0428 19:48:40.553434 26540 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:48:40.553439 26540 net.cpp:139] Memory required for data: 1922800
I0428 19:48:40.553445 26540 layer_factory.hpp:77] Creating layer ip2
I0428 19:48:40.553455 26540 net.cpp:86] Creating Layer ip2
I0428 19:48:40.553462 26540 net.cpp:408] ip2 <- ip1
I0428 19:48:40.553472 26540 net.cpp:382] ip2 -> ip2
I0428 19:48:40.553578 26540 net.cpp:124] Setting up ip2
I0428 19:48:40.553587 26540 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:48:40.553601 26540 net.cpp:139] Memory required for data: 1926800
I0428 19:48:40.553611 26540 layer_factory.hpp:77] Creating layer relu2
I0428 19:48:40.553617 26540 net.cpp:86] Creating Layer relu2
I0428 19:48:40.553637 26540 net.cpp:408] relu2 <- ip2
I0428 19:48:40.553645 26540 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:48:40.553881 26540 net.cpp:124] Setting up relu2
I0428 19:48:40.553891 26540 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:48:40.553896 26540 net.cpp:139] Memory required for data: 1930800
I0428 19:48:40.553917 26540 layer_factory.hpp:77] Creating layer ip2_relu2_0_split
I0428 19:48:40.553941 26540 net.cpp:86] Creating Layer ip2_relu2_0_split
I0428 19:48:40.553946 26540 net.cpp:408] ip2_relu2_0_split <- ip2
I0428 19:48:40.553956 26540 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_0
I0428 19:48:40.553978 26540 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_1
I0428 19:48:40.554026 26540 net.cpp:124] Setting up ip2_relu2_0_split
I0428 19:48:40.554036 26540 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:48:40.554044 26540 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:48:40.554050 26540 net.cpp:139] Memory required for data: 1938800
I0428 19:48:40.554055 26540 layer_factory.hpp:77] Creating layer accuracy
I0428 19:48:40.554067 26540 net.cpp:86] Creating Layer accuracy
I0428 19:48:40.554074 26540 net.cpp:408] accuracy <- ip2_relu2_0_split_0
I0428 19:48:40.554080 26540 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:48:40.554088 26540 net.cpp:382] accuracy -> accuracy
I0428 19:48:40.554100 26540 net.cpp:124] Setting up accuracy
I0428 19:48:40.554110 26540 net.cpp:131] Top shape: (1)
I0428 19:48:40.554118 26540 net.cpp:139] Memory required for data: 1938804
I0428 19:48:40.554124 26540 layer_factory.hpp:77] Creating layer loss
I0428 19:48:40.554134 26540 net.cpp:86] Creating Layer loss
I0428 19:48:40.554141 26540 net.cpp:408] loss <- ip2_relu2_0_split_1
I0428 19:48:40.554147 26540 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:48:40.554155 26540 net.cpp:382] loss -> loss
I0428 19:48:40.554165 26540 layer_factory.hpp:77] Creating layer loss
I0428 19:48:40.554452 26540 net.cpp:124] Setting up loss
I0428 19:48:40.554462 26540 net.cpp:131] Top shape: (1)
I0428 19:48:40.554467 26540 net.cpp:134]     with loss weight 1
I0428 19:48:40.554476 26540 net.cpp:139] Memory required for data: 1938808
I0428 19:48:40.554481 26540 net.cpp:200] loss needs backward computation.
I0428 19:48:40.554487 26540 net.cpp:202] accuracy does not need backward computation.
I0428 19:48:40.554494 26540 net.cpp:200] ip2_relu2_0_split needs backward computation.
I0428 19:48:40.554498 26540 net.cpp:200] relu2 needs backward computation.
I0428 19:48:40.554503 26540 net.cpp:200] ip2 needs backward computation.
I0428 19:48:40.554508 26540 net.cpp:200] relu1 needs backward computation.
I0428 19:48:40.554513 26540 net.cpp:200] ip1 needs backward computation.
I0428 19:48:40.554518 26540 net.cpp:200] pool1 needs backward computation.
I0428 19:48:40.554523 26540 net.cpp:200] conv1 needs backward computation.
I0428 19:48:40.554530 26540 net.cpp:200] pool0 needs backward computation.
I0428 19:48:40.554535 26540 net.cpp:200] conv0 needs backward computation.
I0428 19:48:40.554541 26540 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:48:40.554546 26540 net.cpp:202] mnist does not need backward computation.
I0428 19:48:40.554551 26540 net.cpp:244] This network produces output accuracy
I0428 19:48:40.554556 26540 net.cpp:244] This network produces output loss
I0428 19:48:40.554571 26540 net.cpp:257] Network initialization done.
I0428 19:48:40.554615 26540 solver.cpp:56] Solver scaffolding done.
I0428 19:48:40.554939 26540 caffe.cpp:248] Starting Optimization
I0428 19:48:40.554946 26540 solver.cpp:273] Solving LeNet
I0428 19:48:40.554950 26540 solver.cpp:274] Learning Rate Policy: inv
I0428 19:48:40.555840 26540 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:48:40.559355 26540 blocking_queue.cpp:49] Waiting for data
I0428 19:48:40.629925 26547 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:48:40.630385 26540 solver.cpp:398]     Test net output #0: accuracy = 0.0956
I0428 19:48:40.630409 26540 solver.cpp:398]     Test net output #1: loss = 2.33159 (* 1 = 2.33159 loss)
I0428 19:48:40.632185 26540 solver.cpp:219] Iteration 0 (2.94884 iter/s, 0.0771994s/100 iters), loss = 2.3338
I0428 19:48:40.632222 26540 solver.cpp:238]     Train net output #0: loss = 2.3338 (* 1 = 2.3338 loss)
I0428 19:48:40.632241 26540 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:48:40.695621 26540 solver.cpp:219] Iteration 100 (1577.48 iter/s, 0.0633921s/100 iters), loss = 0.652049
I0428 19:48:40.695648 26540 solver.cpp:238]     Train net output #0: loss = 0.652049 (* 1 = 0.652049 loss)
I0428 19:48:40.695673 26540 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:48:40.758859 26540 solver.cpp:219] Iteration 200 (1582.22 iter/s, 0.0632023s/100 iters), loss = 0.419856
I0428 19:48:40.758908 26540 solver.cpp:238]     Train net output #0: loss = 0.419856 (* 1 = 0.419856 loss)
I0428 19:48:40.758920 26540 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:48:40.822720 26540 solver.cpp:219] Iteration 300 (1567.21 iter/s, 0.0638075s/100 iters), loss = 0.358414
I0428 19:48:40.822746 26540 solver.cpp:238]     Train net output #0: loss = 0.358414 (* 1 = 0.358414 loss)
I0428 19:48:40.822754 26540 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:48:40.887063 26540 solver.cpp:219] Iteration 400 (1554.94 iter/s, 0.0643113s/100 iters), loss = 0.232499
I0428 19:48:40.887104 26540 solver.cpp:238]     Train net output #0: loss = 0.232499 (* 1 = 0.232499 loss)
I0428 19:48:40.887112 26540 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:48:40.951032 26540 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:48:41.004262 26547 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:48:41.004746 26540 solver.cpp:398]     Test net output #0: accuracy = 0.9295
I0428 19:48:41.004781 26540 solver.cpp:398]     Test net output #1: loss = 0.221423 (* 1 = 0.221423 loss)
I0428 19:48:41.005494 26540 solver.cpp:219] Iteration 500 (844.726 iter/s, 0.118382s/100 iters), loss = 0.243826
I0428 19:48:41.005532 26540 solver.cpp:238]     Train net output #0: loss = 0.243826 (* 1 = 0.243826 loss)
I0428 19:48:41.005539 26540 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:48:41.069540 26540 solver.cpp:219] Iteration 600 (1562.13 iter/s, 0.0640153s/100 iters), loss = 0.192542
I0428 19:48:41.069581 26540 solver.cpp:238]     Train net output #0: loss = 0.192542 (* 1 = 0.192542 loss)
I0428 19:48:41.069586 26540 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:48:41.136103 26540 solver.cpp:219] Iteration 700 (1503.24 iter/s, 0.0665232s/100 iters), loss = 0.299577
I0428 19:48:41.136158 26540 solver.cpp:238]     Train net output #0: loss = 0.299577 (* 1 = 0.299577 loss)
I0428 19:48:41.136176 26540 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:48:41.219604 26540 solver.cpp:219] Iteration 800 (1198.46 iter/s, 0.0834406s/100 iters), loss = 0.40657
I0428 19:48:41.219640 26540 solver.cpp:238]     Train net output #0: loss = 0.40657 (* 1 = 0.40657 loss)
I0428 19:48:41.219648 26540 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:48:41.290156 26540 solver.cpp:219] Iteration 900 (1418.27 iter/s, 0.0705085s/100 iters), loss = 0.246502
I0428 19:48:41.290186 26540 solver.cpp:238]     Train net output #0: loss = 0.246502 (* 1 = 0.246502 loss)
I0428 19:48:41.290194 26540 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:48:41.313316 26546 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:48:41.358295 26540 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:48:41.358932 26540 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:48:41.359336 26540 solver.cpp:311] Iteration 1000, loss = 0.169886
I0428 19:48:41.359356 26540 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:48:41.435083 26547 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:48:41.435717 26540 solver.cpp:398]     Test net output #0: accuracy = 0.9563
I0428 19:48:41.435751 26540 solver.cpp:398]     Test net output #1: loss = 0.142162 (* 1 = 0.142162 loss)
I0428 19:48:41.435761 26540 solver.cpp:316] Optimization Done.
I0428 19:48:41.435767 26540 caffe.cpp:259] Optimization Done.
