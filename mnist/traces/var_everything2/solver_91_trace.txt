I0428 19:30:27.062716 22292 caffe.cpp:218] Using GPUs 0
I0428 19:30:27.103152 22292 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:30:27.623447 22292 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test91.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:30:27.623586 22292 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test91.prototxt
I0428 19:30:27.623961 22292 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:30:27.623980 22292 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:30:27.624065 22292 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool0"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:30:27.624142 22292 layer_factory.hpp:77] Creating layer mnist
I0428 19:30:27.624241 22292 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:30:27.624267 22292 net.cpp:86] Creating Layer mnist
I0428 19:30:27.624276 22292 net.cpp:382] mnist -> data
I0428 19:30:27.624301 22292 net.cpp:382] mnist -> label
I0428 19:30:27.625412 22292 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:30:27.627874 22292 net.cpp:124] Setting up mnist
I0428 19:30:27.627892 22292 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:30:27.627897 22292 net.cpp:131] Top shape: 64 (64)
I0428 19:30:27.627900 22292 net.cpp:139] Memory required for data: 200960
I0428 19:30:27.627907 22292 layer_factory.hpp:77] Creating layer conv0
I0428 19:30:27.627954 22292 net.cpp:86] Creating Layer conv0
I0428 19:30:27.627964 22292 net.cpp:408] conv0 <- data
I0428 19:30:27.627975 22292 net.cpp:382] conv0 -> conv0
I0428 19:30:27.918542 22292 net.cpp:124] Setting up conv0
I0428 19:30:27.918571 22292 net.cpp:131] Top shape: 64 5 24 24 (184320)
I0428 19:30:27.918576 22292 net.cpp:139] Memory required for data: 938240
I0428 19:30:27.918593 22292 layer_factory.hpp:77] Creating layer pool0
I0428 19:30:27.918609 22292 net.cpp:86] Creating Layer pool0
I0428 19:30:27.918633 22292 net.cpp:408] pool0 <- conv0
I0428 19:30:27.918642 22292 net.cpp:382] pool0 -> pool0
I0428 19:30:27.918699 22292 net.cpp:124] Setting up pool0
I0428 19:30:27.918707 22292 net.cpp:131] Top shape: 64 5 12 12 (46080)
I0428 19:30:27.918710 22292 net.cpp:139] Memory required for data: 1122560
I0428 19:30:27.918715 22292 layer_factory.hpp:77] Creating layer ip1
I0428 19:30:27.918723 22292 net.cpp:86] Creating Layer ip1
I0428 19:30:27.918727 22292 net.cpp:408] ip1 <- pool0
I0428 19:30:27.918735 22292 net.cpp:382] ip1 -> ip1
I0428 19:30:27.919852 22292 net.cpp:124] Setting up ip1
I0428 19:30:27.919867 22292 net.cpp:131] Top shape: 64 10 (640)
I0428 19:30:27.919872 22292 net.cpp:139] Memory required for data: 1125120
I0428 19:30:27.919880 22292 layer_factory.hpp:77] Creating layer relu1
I0428 19:30:27.919889 22292 net.cpp:86] Creating Layer relu1
I0428 19:30:27.919893 22292 net.cpp:408] relu1 <- ip1
I0428 19:30:27.919899 22292 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:30:27.920125 22292 net.cpp:124] Setting up relu1
I0428 19:30:27.920135 22292 net.cpp:131] Top shape: 64 10 (640)
I0428 19:30:27.920138 22292 net.cpp:139] Memory required for data: 1127680
I0428 19:30:27.920141 22292 layer_factory.hpp:77] Creating layer ip2
I0428 19:30:27.920151 22292 net.cpp:86] Creating Layer ip2
I0428 19:30:27.920155 22292 net.cpp:408] ip2 <- ip1
I0428 19:30:27.920162 22292 net.cpp:382] ip2 -> ip2
I0428 19:30:27.920274 22292 net.cpp:124] Setting up ip2
I0428 19:30:27.920282 22292 net.cpp:131] Top shape: 64 10 (640)
I0428 19:30:27.920285 22292 net.cpp:139] Memory required for data: 1130240
I0428 19:30:27.920295 22292 layer_factory.hpp:77] Creating layer relu2
I0428 19:30:27.920301 22292 net.cpp:86] Creating Layer relu2
I0428 19:30:27.920305 22292 net.cpp:408] relu2 <- ip2
I0428 19:30:27.920310 22292 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:30:27.921151 22292 net.cpp:124] Setting up relu2
I0428 19:30:27.921165 22292 net.cpp:131] Top shape: 64 10 (640)
I0428 19:30:27.921169 22292 net.cpp:139] Memory required for data: 1132800
I0428 19:30:27.921174 22292 layer_factory.hpp:77] Creating layer ip3
I0428 19:30:27.921182 22292 net.cpp:86] Creating Layer ip3
I0428 19:30:27.921185 22292 net.cpp:408] ip3 <- ip2
I0428 19:30:27.921192 22292 net.cpp:382] ip3 -> ip3
I0428 19:30:27.921314 22292 net.cpp:124] Setting up ip3
I0428 19:30:27.921322 22292 net.cpp:131] Top shape: 64 10 (640)
I0428 19:30:27.921325 22292 net.cpp:139] Memory required for data: 1135360
I0428 19:30:27.921332 22292 layer_factory.hpp:77] Creating layer relu3
I0428 19:30:27.921339 22292 net.cpp:86] Creating Layer relu3
I0428 19:30:27.921342 22292 net.cpp:408] relu3 <- ip3
I0428 19:30:27.921347 22292 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:30:27.921527 22292 net.cpp:124] Setting up relu3
I0428 19:30:27.921538 22292 net.cpp:131] Top shape: 64 10 (640)
I0428 19:30:27.921541 22292 net.cpp:139] Memory required for data: 1137920
I0428 19:30:27.921545 22292 layer_factory.hpp:77] Creating layer loss
I0428 19:30:27.921551 22292 net.cpp:86] Creating Layer loss
I0428 19:30:27.921555 22292 net.cpp:408] loss <- ip3
I0428 19:30:27.921560 22292 net.cpp:408] loss <- label
I0428 19:30:27.921564 22292 net.cpp:382] loss -> loss
I0428 19:30:27.921584 22292 layer_factory.hpp:77] Creating layer loss
I0428 19:30:27.921867 22292 net.cpp:124] Setting up loss
I0428 19:30:27.921877 22292 net.cpp:131] Top shape: (1)
I0428 19:30:27.921881 22292 net.cpp:134]     with loss weight 1
I0428 19:30:27.921896 22292 net.cpp:139] Memory required for data: 1137924
I0428 19:30:27.921900 22292 net.cpp:200] loss needs backward computation.
I0428 19:30:27.921905 22292 net.cpp:200] relu3 needs backward computation.
I0428 19:30:27.921908 22292 net.cpp:200] ip3 needs backward computation.
I0428 19:30:27.921911 22292 net.cpp:200] relu2 needs backward computation.
I0428 19:30:27.921914 22292 net.cpp:200] ip2 needs backward computation.
I0428 19:30:27.921917 22292 net.cpp:200] relu1 needs backward computation.
I0428 19:30:27.921921 22292 net.cpp:200] ip1 needs backward computation.
I0428 19:30:27.921936 22292 net.cpp:200] pool0 needs backward computation.
I0428 19:30:27.921939 22292 net.cpp:200] conv0 needs backward computation.
I0428 19:30:27.921943 22292 net.cpp:202] mnist does not need backward computation.
I0428 19:30:27.921947 22292 net.cpp:244] This network produces output loss
I0428 19:30:27.921955 22292 net.cpp:257] Network initialization done.
I0428 19:30:27.922286 22292 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test91.prototxt
I0428 19:30:27.922314 22292 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:30:27.922406 22292 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool0"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:30:27.922484 22292 layer_factory.hpp:77] Creating layer mnist
I0428 19:30:27.922535 22292 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:30:27.922550 22292 net.cpp:86] Creating Layer mnist
I0428 19:30:27.922555 22292 net.cpp:382] mnist -> data
I0428 19:30:27.922564 22292 net.cpp:382] mnist -> label
I0428 19:30:27.922663 22292 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:30:27.924767 22292 net.cpp:124] Setting up mnist
I0428 19:30:27.924782 22292 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:30:27.924789 22292 net.cpp:131] Top shape: 100 (100)
I0428 19:30:27.924793 22292 net.cpp:139] Memory required for data: 314000
I0428 19:30:27.924798 22292 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:30:27.924804 22292 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:30:27.924808 22292 net.cpp:408] label_mnist_1_split <- label
I0428 19:30:27.924821 22292 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:30:27.924829 22292 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:30:27.924895 22292 net.cpp:124] Setting up label_mnist_1_split
I0428 19:30:27.924901 22292 net.cpp:131] Top shape: 100 (100)
I0428 19:30:27.924904 22292 net.cpp:131] Top shape: 100 (100)
I0428 19:30:27.924907 22292 net.cpp:139] Memory required for data: 314800
I0428 19:30:27.924922 22292 layer_factory.hpp:77] Creating layer conv0
I0428 19:30:27.924934 22292 net.cpp:86] Creating Layer conv0
I0428 19:30:27.924938 22292 net.cpp:408] conv0 <- data
I0428 19:30:27.924945 22292 net.cpp:382] conv0 -> conv0
I0428 19:30:27.926658 22292 net.cpp:124] Setting up conv0
I0428 19:30:27.926674 22292 net.cpp:131] Top shape: 100 5 24 24 (288000)
I0428 19:30:27.926678 22292 net.cpp:139] Memory required for data: 1466800
I0428 19:30:27.926689 22292 layer_factory.hpp:77] Creating layer pool0
I0428 19:30:27.926697 22292 net.cpp:86] Creating Layer pool0
I0428 19:30:27.926702 22292 net.cpp:408] pool0 <- conv0
I0428 19:30:27.926707 22292 net.cpp:382] pool0 -> pool0
I0428 19:30:27.926751 22292 net.cpp:124] Setting up pool0
I0428 19:30:27.926758 22292 net.cpp:131] Top shape: 100 5 12 12 (72000)
I0428 19:30:27.926760 22292 net.cpp:139] Memory required for data: 1754800
I0428 19:30:27.926764 22292 layer_factory.hpp:77] Creating layer ip1
I0428 19:30:27.926772 22292 net.cpp:86] Creating Layer ip1
I0428 19:30:27.926776 22292 net.cpp:408] ip1 <- pool0
I0428 19:30:27.926781 22292 net.cpp:382] ip1 -> ip1
I0428 19:30:27.926944 22292 net.cpp:124] Setting up ip1
I0428 19:30:27.926952 22292 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:30:27.926956 22292 net.cpp:139] Memory required for data: 1758800
I0428 19:30:27.926964 22292 layer_factory.hpp:77] Creating layer relu1
I0428 19:30:27.926970 22292 net.cpp:86] Creating Layer relu1
I0428 19:30:27.926972 22292 net.cpp:408] relu1 <- ip1
I0428 19:30:27.926978 22292 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:30:27.927168 22292 net.cpp:124] Setting up relu1
I0428 19:30:27.927177 22292 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:30:27.927181 22292 net.cpp:139] Memory required for data: 1762800
I0428 19:30:27.927186 22292 layer_factory.hpp:77] Creating layer ip2
I0428 19:30:27.927192 22292 net.cpp:86] Creating Layer ip2
I0428 19:30:27.927196 22292 net.cpp:408] ip2 <- ip1
I0428 19:30:27.927202 22292 net.cpp:382] ip2 -> ip2
I0428 19:30:27.927314 22292 net.cpp:124] Setting up ip2
I0428 19:30:27.927323 22292 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:30:27.927326 22292 net.cpp:139] Memory required for data: 1766800
I0428 19:30:27.927335 22292 layer_factory.hpp:77] Creating layer relu2
I0428 19:30:27.927340 22292 net.cpp:86] Creating Layer relu2
I0428 19:30:27.927345 22292 net.cpp:408] relu2 <- ip2
I0428 19:30:27.927350 22292 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:30:27.928264 22292 net.cpp:124] Setting up relu2
I0428 19:30:27.928277 22292 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:30:27.928282 22292 net.cpp:139] Memory required for data: 1770800
I0428 19:30:27.928285 22292 layer_factory.hpp:77] Creating layer ip3
I0428 19:30:27.928295 22292 net.cpp:86] Creating Layer ip3
I0428 19:30:27.928299 22292 net.cpp:408] ip3 <- ip2
I0428 19:30:27.928306 22292 net.cpp:382] ip3 -> ip3
I0428 19:30:27.928463 22292 net.cpp:124] Setting up ip3
I0428 19:30:27.928472 22292 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:30:27.928477 22292 net.cpp:139] Memory required for data: 1774800
I0428 19:30:27.928483 22292 layer_factory.hpp:77] Creating layer relu3
I0428 19:30:27.928488 22292 net.cpp:86] Creating Layer relu3
I0428 19:30:27.928491 22292 net.cpp:408] relu3 <- ip3
I0428 19:30:27.928499 22292 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:30:27.928684 22292 net.cpp:124] Setting up relu3
I0428 19:30:27.928694 22292 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:30:27.928699 22292 net.cpp:139] Memory required for data: 1778800
I0428 19:30:27.928702 22292 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:30:27.928709 22292 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:30:27.928712 22292 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:30:27.928719 22292 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:30:27.928725 22292 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:30:27.928766 22292 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:30:27.928774 22292 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:30:27.928778 22292 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:30:27.928792 22292 net.cpp:139] Memory required for data: 1786800
I0428 19:30:27.928797 22292 layer_factory.hpp:77] Creating layer accuracy
I0428 19:30:27.928807 22292 net.cpp:86] Creating Layer accuracy
I0428 19:30:27.928820 22292 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:30:27.928825 22292 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:30:27.928833 22292 net.cpp:382] accuracy -> accuracy
I0428 19:30:27.928841 22292 net.cpp:124] Setting up accuracy
I0428 19:30:27.928846 22292 net.cpp:131] Top shape: (1)
I0428 19:30:27.928850 22292 net.cpp:139] Memory required for data: 1786804
I0428 19:30:27.928853 22292 layer_factory.hpp:77] Creating layer loss
I0428 19:30:27.928858 22292 net.cpp:86] Creating Layer loss
I0428 19:30:27.928863 22292 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:30:27.928867 22292 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:30:27.928872 22292 net.cpp:382] loss -> loss
I0428 19:30:27.928879 22292 layer_factory.hpp:77] Creating layer loss
I0428 19:30:27.929157 22292 net.cpp:124] Setting up loss
I0428 19:30:27.929168 22292 net.cpp:131] Top shape: (1)
I0428 19:30:27.929172 22292 net.cpp:134]     with loss weight 1
I0428 19:30:27.929179 22292 net.cpp:139] Memory required for data: 1786808
I0428 19:30:27.929183 22292 net.cpp:200] loss needs backward computation.
I0428 19:30:27.929188 22292 net.cpp:202] accuracy does not need backward computation.
I0428 19:30:27.929191 22292 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:30:27.929195 22292 net.cpp:200] relu3 needs backward computation.
I0428 19:30:27.929198 22292 net.cpp:200] ip3 needs backward computation.
I0428 19:30:27.929201 22292 net.cpp:200] relu2 needs backward computation.
I0428 19:30:27.929204 22292 net.cpp:200] ip2 needs backward computation.
I0428 19:30:27.929208 22292 net.cpp:200] relu1 needs backward computation.
I0428 19:30:27.929211 22292 net.cpp:200] ip1 needs backward computation.
I0428 19:30:27.929214 22292 net.cpp:200] pool0 needs backward computation.
I0428 19:30:27.929219 22292 net.cpp:200] conv0 needs backward computation.
I0428 19:30:27.929222 22292 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:30:27.929226 22292 net.cpp:202] mnist does not need backward computation.
I0428 19:30:27.929229 22292 net.cpp:244] This network produces output accuracy
I0428 19:30:27.929234 22292 net.cpp:244] This network produces output loss
I0428 19:30:27.929244 22292 net.cpp:257] Network initialization done.
I0428 19:30:27.929299 22292 solver.cpp:56] Solver scaffolding done.
I0428 19:30:27.929639 22292 caffe.cpp:248] Starting Optimization
I0428 19:30:27.929646 22292 solver.cpp:273] Solving LeNet
I0428 19:30:27.929649 22292 solver.cpp:274] Learning Rate Policy: inv
I0428 19:30:27.929790 22292 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:30:27.933358 22292 blocking_queue.cpp:49] Waiting for data
I0428 19:30:28.004954 22299 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:30:28.005527 22292 solver.cpp:398]     Test net output #0: accuracy = 0.0577
I0428 19:30:28.005565 22292 solver.cpp:398]     Test net output #1: loss = 2.32459 (* 1 = 2.32459 loss)
I0428 19:30:28.008008 22292 solver.cpp:219] Iteration 0 (-1.82465e-31 iter/s, 0.0783281s/100 iters), loss = 2.31706
I0428 19:30:28.008050 22292 solver.cpp:238]     Train net output #0: loss = 2.31706 (* 1 = 2.31706 loss)
I0428 19:30:28.008067 22292 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:30:28.078781 22292 solver.cpp:219] Iteration 100 (1414.02 iter/s, 0.0707205s/100 iters), loss = 1.20267
I0428 19:30:28.078814 22292 solver.cpp:238]     Train net output #0: loss = 1.20267 (* 1 = 1.20267 loss)
I0428 19:30:28.078822 22292 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:30:28.141176 22292 solver.cpp:219] Iteration 200 (1603.76 iter/s, 0.0623534s/100 iters), loss = 0.874309
I0428 19:30:28.141206 22292 solver.cpp:238]     Train net output #0: loss = 0.874309 (* 1 = 0.874309 loss)
I0428 19:30:28.141213 22292 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:30:28.202905 22292 solver.cpp:219] Iteration 300 (1620.96 iter/s, 0.0616918s/100 iters), loss = 0.650464
I0428 19:30:28.202935 22292 solver.cpp:238]     Train net output #0: loss = 0.650464 (* 1 = 0.650464 loss)
I0428 19:30:28.202944 22292 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:30:28.264710 22292 solver.cpp:219] Iteration 400 (1618.99 iter/s, 0.0617668s/100 iters), loss = 0.734436
I0428 19:30:28.264740 22292 solver.cpp:238]     Train net output #0: loss = 0.734436 (* 1 = 0.734436 loss)
I0428 19:30:28.264747 22292 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:30:28.325702 22292 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:30:28.378211 22299 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:30:28.378654 22292 solver.cpp:398]     Test net output #0: accuracy = 0.8613
I0428 19:30:28.378676 22292 solver.cpp:398]     Test net output #1: loss = 0.580896 (* 1 = 0.580896 loss)
I0428 19:30:28.379374 22292 solver.cpp:219] Iteration 500 (872.429 iter/s, 0.114623s/100 iters), loss = 0.648414
I0428 19:30:28.379402 22292 solver.cpp:238]     Train net output #0: loss = 0.648414 (* 1 = 0.648414 loss)
I0428 19:30:28.379410 22292 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:30:28.453299 22292 solver.cpp:219] Iteration 600 (1353.52 iter/s, 0.0738814s/100 iters), loss = 0.406156
I0428 19:30:28.453348 22292 solver.cpp:238]     Train net output #0: loss = 0.406156 (* 1 = 0.406156 loss)
I0428 19:30:28.453361 22292 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:30:28.525733 22292 solver.cpp:219] Iteration 700 (1381.63 iter/s, 0.0723784s/100 iters), loss = 0.667118
I0428 19:30:28.525768 22292 solver.cpp:238]     Train net output #0: loss = 0.667119 (* 1 = 0.667119 loss)
I0428 19:30:28.525775 22292 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:30:28.587702 22292 solver.cpp:219] Iteration 800 (1614.82 iter/s, 0.0619264s/100 iters), loss = 0.457956
I0428 19:30:28.587731 22292 solver.cpp:238]     Train net output #0: loss = 0.457956 (* 1 = 0.457956 loss)
I0428 19:30:28.587739 22292 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:30:28.648820 22292 solver.cpp:219] Iteration 900 (1637.24 iter/s, 0.0610785s/100 iters), loss = 0.56136
I0428 19:30:28.648849 22292 solver.cpp:238]     Train net output #0: loss = 0.56136 (* 1 = 0.56136 loss)
I0428 19:30:28.648856 22292 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:30:28.668995 22298 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:30:28.708032 22292 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:30:28.708747 22292 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:30:28.709239 22292 solver.cpp:311] Iteration 1000, loss = 0.498282
I0428 19:30:28.709256 22292 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:30:28.754997 22299 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:30:28.755453 22292 solver.cpp:398]     Test net output #0: accuracy = 0.9215
I0428 19:30:28.755475 22292 solver.cpp:398]     Test net output #1: loss = 0.419063 (* 1 = 0.419063 loss)
I0428 19:30:28.755492 22292 solver.cpp:316] Optimization Done.
I0428 19:30:28.755496 22292 caffe.cpp:259] Optimization Done.
