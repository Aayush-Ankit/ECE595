I0428 19:39:57.379812 24431 caffe.cpp:218] Using GPUs 0
I0428 19:39:57.413797 24431 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:39:57.928772 24431 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test320.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:39:57.928947 24431 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test320.prototxt
I0428 19:39:57.929340 24431 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:39:57.929364 24431 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:39:57.929468 24431 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0428 19:39:57.929579 24431 layer_factory.hpp:77] Creating layer mnist
I0428 19:39:57.929713 24431 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:39:57.929745 24431 net.cpp:86] Creating Layer mnist
I0428 19:39:57.929759 24431 net.cpp:382] mnist -> data
I0428 19:39:57.929790 24431 net.cpp:382] mnist -> label
I0428 19:39:57.931023 24431 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:39:57.933504 24431 net.cpp:124] Setting up mnist
I0428 19:39:57.933526 24431 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:39:57.933535 24431 net.cpp:131] Top shape: 64 (64)
I0428 19:39:57.933542 24431 net.cpp:139] Memory required for data: 200960
I0428 19:39:57.933552 24431 layer_factory.hpp:77] Creating layer conv0
I0428 19:39:57.933578 24431 net.cpp:86] Creating Layer conv0
I0428 19:39:57.933588 24431 net.cpp:408] conv0 <- data
I0428 19:39:57.933609 24431 net.cpp:382] conv0 -> conv0
I0428 19:39:58.223744 24431 net.cpp:124] Setting up conv0
I0428 19:39:58.223779 24431 net.cpp:131] Top shape: 64 2 24 24 (73728)
I0428 19:39:58.223788 24431 net.cpp:139] Memory required for data: 495872
I0428 19:39:58.223850 24431 layer_factory.hpp:77] Creating layer pool0
I0428 19:39:58.223870 24431 net.cpp:86] Creating Layer pool0
I0428 19:39:58.223881 24431 net.cpp:408] pool0 <- conv0
I0428 19:39:58.223892 24431 net.cpp:382] pool0 -> pool0
I0428 19:39:58.223960 24431 net.cpp:124] Setting up pool0
I0428 19:39:58.223973 24431 net.cpp:131] Top shape: 64 2 12 12 (18432)
I0428 19:39:58.223978 24431 net.cpp:139] Memory required for data: 569600
I0428 19:39:58.223984 24431 layer_factory.hpp:77] Creating layer conv1
I0428 19:39:58.224002 24431 net.cpp:86] Creating Layer conv1
I0428 19:39:58.224010 24431 net.cpp:408] conv1 <- pool0
I0428 19:39:58.224021 24431 net.cpp:382] conv1 -> conv1
I0428 19:39:58.226215 24431 net.cpp:124] Setting up conv1
I0428 19:39:58.226236 24431 net.cpp:131] Top shape: 64 5 8 8 (20480)
I0428 19:39:58.226243 24431 net.cpp:139] Memory required for data: 651520
I0428 19:39:58.226263 24431 layer_factory.hpp:77] Creating layer pool1
I0428 19:39:58.226277 24431 net.cpp:86] Creating Layer pool1
I0428 19:39:58.226292 24431 net.cpp:408] pool1 <- conv1
I0428 19:39:58.226305 24431 net.cpp:382] pool1 -> pool1
I0428 19:39:58.226366 24431 net.cpp:124] Setting up pool1
I0428 19:39:58.226375 24431 net.cpp:131] Top shape: 64 5 4 4 (5120)
I0428 19:39:58.226382 24431 net.cpp:139] Memory required for data: 672000
I0428 19:39:58.226387 24431 layer_factory.hpp:77] Creating layer ip1
I0428 19:39:58.226402 24431 net.cpp:86] Creating Layer ip1
I0428 19:39:58.226408 24431 net.cpp:408] ip1 <- pool1
I0428 19:39:58.226419 24431 net.cpp:382] ip1 -> ip1
I0428 19:39:58.227541 24431 net.cpp:124] Setting up ip1
I0428 19:39:58.227558 24431 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:39:58.227565 24431 net.cpp:139] Memory required for data: 678400
I0428 19:39:58.227581 24431 layer_factory.hpp:77] Creating layer relu1
I0428 19:39:58.227593 24431 net.cpp:86] Creating Layer relu1
I0428 19:39:58.227601 24431 net.cpp:408] relu1 <- ip1
I0428 19:39:58.227612 24431 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:39:58.227834 24431 net.cpp:124] Setting up relu1
I0428 19:39:58.227847 24431 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:39:58.227854 24431 net.cpp:139] Memory required for data: 684800
I0428 19:39:58.227860 24431 layer_factory.hpp:77] Creating layer ip2
I0428 19:39:58.227874 24431 net.cpp:86] Creating Layer ip2
I0428 19:39:58.227881 24431 net.cpp:408] ip2 <- ip1
I0428 19:39:58.227891 24431 net.cpp:382] ip2 -> ip2
I0428 19:39:58.228022 24431 net.cpp:124] Setting up ip2
I0428 19:39:58.228034 24431 net.cpp:131] Top shape: 64 10 (640)
I0428 19:39:58.228040 24431 net.cpp:139] Memory required for data: 687360
I0428 19:39:58.228051 24431 layer_factory.hpp:77] Creating layer relu2
I0428 19:39:58.228065 24431 net.cpp:86] Creating Layer relu2
I0428 19:39:58.228073 24431 net.cpp:408] relu2 <- ip2
I0428 19:39:58.228082 24431 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:39:58.228966 24431 net.cpp:124] Setting up relu2
I0428 19:39:58.228987 24431 net.cpp:131] Top shape: 64 10 (640)
I0428 19:39:58.228994 24431 net.cpp:139] Memory required for data: 689920
I0428 19:39:58.229001 24431 layer_factory.hpp:77] Creating layer loss
I0428 19:39:58.229018 24431 net.cpp:86] Creating Layer loss
I0428 19:39:58.229024 24431 net.cpp:408] loss <- ip2
I0428 19:39:58.229033 24431 net.cpp:408] loss <- label
I0428 19:39:58.229043 24431 net.cpp:382] loss -> loss
I0428 19:39:58.229068 24431 layer_factory.hpp:77] Creating layer loss
I0428 19:39:58.229383 24431 net.cpp:124] Setting up loss
I0428 19:39:58.229398 24431 net.cpp:131] Top shape: (1)
I0428 19:39:58.229404 24431 net.cpp:134]     with loss weight 1
I0428 19:39:58.229427 24431 net.cpp:139] Memory required for data: 689924
I0428 19:39:58.229435 24431 net.cpp:200] loss needs backward computation.
I0428 19:39:58.229442 24431 net.cpp:200] relu2 needs backward computation.
I0428 19:39:58.229449 24431 net.cpp:200] ip2 needs backward computation.
I0428 19:39:58.229454 24431 net.cpp:200] relu1 needs backward computation.
I0428 19:39:58.229460 24431 net.cpp:200] ip1 needs backward computation.
I0428 19:39:58.229467 24431 net.cpp:200] pool1 needs backward computation.
I0428 19:39:58.229487 24431 net.cpp:200] conv1 needs backward computation.
I0428 19:39:58.229495 24431 net.cpp:200] pool0 needs backward computation.
I0428 19:39:58.229501 24431 net.cpp:200] conv0 needs backward computation.
I0428 19:39:58.229507 24431 net.cpp:202] mnist does not need backward computation.
I0428 19:39:58.229513 24431 net.cpp:244] This network produces output loss
I0428 19:39:58.229529 24431 net.cpp:257] Network initialization done.
I0428 19:39:58.229885 24431 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test320.prototxt
I0428 19:39:58.229924 24431 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:39:58.230037 24431 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0428 19:39:58.230160 24431 layer_factory.hpp:77] Creating layer mnist
I0428 19:39:58.230231 24431 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:39:58.230252 24431 net.cpp:86] Creating Layer mnist
I0428 19:39:58.230263 24431 net.cpp:382] mnist -> data
I0428 19:39:58.230278 24431 net.cpp:382] mnist -> label
I0428 19:39:58.230430 24431 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:39:58.232775 24431 net.cpp:124] Setting up mnist
I0428 19:39:58.232794 24431 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:39:58.232803 24431 net.cpp:131] Top shape: 100 (100)
I0428 19:39:58.232817 24431 net.cpp:139] Memory required for data: 314000
I0428 19:39:58.232875 24431 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:39:58.232897 24431 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:39:58.232906 24431 net.cpp:408] label_mnist_1_split <- label
I0428 19:39:58.232923 24431 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:39:58.232941 24431 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:39:58.233119 24431 net.cpp:124] Setting up label_mnist_1_split
I0428 19:39:58.233161 24431 net.cpp:131] Top shape: 100 (100)
I0428 19:39:58.233172 24431 net.cpp:131] Top shape: 100 (100)
I0428 19:39:58.233180 24431 net.cpp:139] Memory required for data: 314800
I0428 19:39:58.233187 24431 layer_factory.hpp:77] Creating layer conv0
I0428 19:39:58.233209 24431 net.cpp:86] Creating Layer conv0
I0428 19:39:58.233218 24431 net.cpp:408] conv0 <- data
I0428 19:39:58.233235 24431 net.cpp:382] conv0 -> conv0
I0428 19:39:58.236273 24431 net.cpp:124] Setting up conv0
I0428 19:39:58.236301 24431 net.cpp:131] Top shape: 100 2 24 24 (115200)
I0428 19:39:58.236310 24431 net.cpp:139] Memory required for data: 775600
I0428 19:39:58.236330 24431 layer_factory.hpp:77] Creating layer pool0
I0428 19:39:58.236348 24431 net.cpp:86] Creating Layer pool0
I0428 19:39:58.236357 24431 net.cpp:408] pool0 <- conv0
I0428 19:39:58.236367 24431 net.cpp:382] pool0 -> pool0
I0428 19:39:58.236446 24431 net.cpp:124] Setting up pool0
I0428 19:39:58.236465 24431 net.cpp:131] Top shape: 100 2 12 12 (28800)
I0428 19:39:58.236474 24431 net.cpp:139] Memory required for data: 890800
I0428 19:39:58.236481 24431 layer_factory.hpp:77] Creating layer conv1
I0428 19:39:58.236505 24431 net.cpp:86] Creating Layer conv1
I0428 19:39:58.236516 24431 net.cpp:408] conv1 <- pool0
I0428 19:39:58.236531 24431 net.cpp:382] conv1 -> conv1
I0428 19:39:58.239490 24431 net.cpp:124] Setting up conv1
I0428 19:39:58.239521 24431 net.cpp:131] Top shape: 100 5 8 8 (32000)
I0428 19:39:58.239532 24431 net.cpp:139] Memory required for data: 1018800
I0428 19:39:58.239550 24431 layer_factory.hpp:77] Creating layer pool1
I0428 19:39:58.239564 24431 net.cpp:86] Creating Layer pool1
I0428 19:39:58.239573 24431 net.cpp:408] pool1 <- conv1
I0428 19:39:58.239585 24431 net.cpp:382] pool1 -> pool1
I0428 19:39:58.239658 24431 net.cpp:124] Setting up pool1
I0428 19:39:58.239671 24431 net.cpp:131] Top shape: 100 5 4 4 (8000)
I0428 19:39:58.239679 24431 net.cpp:139] Memory required for data: 1050800
I0428 19:39:58.239686 24431 layer_factory.hpp:77] Creating layer ip1
I0428 19:39:58.239699 24431 net.cpp:86] Creating Layer ip1
I0428 19:39:58.239707 24431 net.cpp:408] ip1 <- pool1
I0428 19:39:58.239720 24431 net.cpp:382] ip1 -> ip1
I0428 19:39:58.239948 24431 net.cpp:124] Setting up ip1
I0428 19:39:58.239961 24431 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:39:58.239969 24431 net.cpp:139] Memory required for data: 1060800
I0428 19:39:58.239984 24431 layer_factory.hpp:77] Creating layer relu1
I0428 19:39:58.239997 24431 net.cpp:86] Creating Layer relu1
I0428 19:39:58.240006 24431 net.cpp:408] relu1 <- ip1
I0428 19:39:58.240018 24431 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:39:58.240406 24431 net.cpp:124] Setting up relu1
I0428 19:39:58.240422 24431 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:39:58.240430 24431 net.cpp:139] Memory required for data: 1070800
I0428 19:39:58.240437 24431 layer_factory.hpp:77] Creating layer ip2
I0428 19:39:58.240453 24431 net.cpp:86] Creating Layer ip2
I0428 19:39:58.240463 24431 net.cpp:408] ip2 <- ip1
I0428 19:39:58.240473 24431 net.cpp:382] ip2 -> ip2
I0428 19:39:58.240685 24431 net.cpp:124] Setting up ip2
I0428 19:39:58.240700 24431 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:39:58.240706 24431 net.cpp:139] Memory required for data: 1074800
I0428 19:39:58.240717 24431 layer_factory.hpp:77] Creating layer relu2
I0428 19:39:58.240728 24431 net.cpp:86] Creating Layer relu2
I0428 19:39:58.240738 24431 net.cpp:408] relu2 <- ip2
I0428 19:39:58.240746 24431 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:39:58.241091 24431 net.cpp:124] Setting up relu2
I0428 19:39:58.241112 24431 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:39:58.241123 24431 net.cpp:139] Memory required for data: 1078800
I0428 19:39:58.241129 24431 layer_factory.hpp:77] Creating layer ip2_relu2_0_split
I0428 19:39:58.241139 24431 net.cpp:86] Creating Layer ip2_relu2_0_split
I0428 19:39:58.241147 24431 net.cpp:408] ip2_relu2_0_split <- ip2
I0428 19:39:58.241158 24431 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_0
I0428 19:39:58.241190 24431 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_1
I0428 19:39:58.241264 24431 net.cpp:124] Setting up ip2_relu2_0_split
I0428 19:39:58.241276 24431 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:39:58.241286 24431 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:39:58.241291 24431 net.cpp:139] Memory required for data: 1086800
I0428 19:39:58.241298 24431 layer_factory.hpp:77] Creating layer accuracy
I0428 19:39:58.241310 24431 net.cpp:86] Creating Layer accuracy
I0428 19:39:58.241319 24431 net.cpp:408] accuracy <- ip2_relu2_0_split_0
I0428 19:39:58.241328 24431 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:39:58.241340 24431 net.cpp:382] accuracy -> accuracy
I0428 19:39:58.241354 24431 net.cpp:124] Setting up accuracy
I0428 19:39:58.241364 24431 net.cpp:131] Top shape: (1)
I0428 19:39:58.241370 24431 net.cpp:139] Memory required for data: 1086804
I0428 19:39:58.241377 24431 layer_factory.hpp:77] Creating layer loss
I0428 19:39:58.241386 24431 net.cpp:86] Creating Layer loss
I0428 19:39:58.241394 24431 net.cpp:408] loss <- ip2_relu2_0_split_1
I0428 19:39:58.241401 24431 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:39:58.241411 24431 net.cpp:382] loss -> loss
I0428 19:39:58.241425 24431 layer_factory.hpp:77] Creating layer loss
I0428 19:39:58.241895 24431 net.cpp:124] Setting up loss
I0428 19:39:58.241914 24431 net.cpp:131] Top shape: (1)
I0428 19:39:58.241922 24431 net.cpp:134]     with loss weight 1
I0428 19:39:58.241936 24431 net.cpp:139] Memory required for data: 1086808
I0428 19:39:58.241943 24431 net.cpp:200] loss needs backward computation.
I0428 19:39:58.241951 24431 net.cpp:202] accuracy does not need backward computation.
I0428 19:39:58.241961 24431 net.cpp:200] ip2_relu2_0_split needs backward computation.
I0428 19:39:58.241966 24431 net.cpp:200] relu2 needs backward computation.
I0428 19:39:58.241974 24431 net.cpp:200] ip2 needs backward computation.
I0428 19:39:58.241981 24431 net.cpp:200] relu1 needs backward computation.
I0428 19:39:58.241986 24431 net.cpp:200] ip1 needs backward computation.
I0428 19:39:58.241992 24431 net.cpp:200] pool1 needs backward computation.
I0428 19:39:58.242000 24431 net.cpp:200] conv1 needs backward computation.
I0428 19:39:58.242007 24431 net.cpp:200] pool0 needs backward computation.
I0428 19:39:58.242015 24431 net.cpp:200] conv0 needs backward computation.
I0428 19:39:58.242022 24431 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:39:58.242030 24431 net.cpp:202] mnist does not need backward computation.
I0428 19:39:58.242038 24431 net.cpp:244] This network produces output accuracy
I0428 19:39:58.242044 24431 net.cpp:244] This network produces output loss
I0428 19:39:58.242063 24431 net.cpp:257] Network initialization done.
I0428 19:39:58.242143 24431 solver.cpp:56] Solver scaffolding done.
I0428 19:39:58.242699 24431 caffe.cpp:248] Starting Optimization
I0428 19:39:58.242710 24431 solver.cpp:273] Solving LeNet
I0428 19:39:58.242717 24431 solver.cpp:274] Learning Rate Policy: inv
I0428 19:39:58.242985 24431 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:39:58.248574 24431 blocking_queue.cpp:49] Waiting for data
I0428 19:39:58.318373 24438 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:39:58.318918 24431 solver.cpp:398]     Test net output #0: accuracy = 0.1484
I0428 19:39:58.318955 24431 solver.cpp:398]     Test net output #1: loss = 2.323 (* 1 = 2.323 loss)
I0428 19:39:58.321570 24431 solver.cpp:219] Iteration 0 (0 iter/s, 0.0788132s/100 iters), loss = 2.32645
I0428 19:39:58.321615 24431 solver.cpp:238]     Train net output #0: loss = 2.32645 (* 1 = 2.32645 loss)
I0428 19:39:58.321635 24431 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:39:58.400259 24431 solver.cpp:219] Iteration 100 (1271.7 iter/s, 0.078635s/100 iters), loss = 0.623213
I0428 19:39:58.400295 24431 solver.cpp:238]     Train net output #0: loss = 0.623213 (* 1 = 0.623213 loss)
I0428 19:39:58.400302 24431 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:39:58.469329 24431 solver.cpp:219] Iteration 200 (1448.71 iter/s, 0.0690269s/100 iters), loss = 0.446306
I0428 19:39:58.469377 24431 solver.cpp:238]     Train net output #0: loss = 0.446306 (* 1 = 0.446306 loss)
I0428 19:39:58.469386 24431 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:39:58.538177 24431 solver.cpp:219] Iteration 300 (1453.67 iter/s, 0.0687913s/100 iters), loss = 0.547597
I0428 19:39:58.538208 24431 solver.cpp:238]     Train net output #0: loss = 0.547597 (* 1 = 0.547597 loss)
I0428 19:39:58.538215 24431 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:39:58.606968 24431 solver.cpp:219] Iteration 400 (1454.5 iter/s, 0.0687524s/100 iters), loss = 0.340711
I0428 19:39:58.606999 24431 solver.cpp:238]     Train net output #0: loss = 0.340711 (* 1 = 0.340711 loss)
I0428 19:39:58.607007 24431 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:39:58.675123 24431 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:39:58.727576 24438 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:39:58.728009 24431 solver.cpp:398]     Test net output #0: accuracy = 0.8604
I0428 19:39:58.728034 24431 solver.cpp:398]     Test net output #1: loss = 0.39981 (* 1 = 0.39981 loss)
I0428 19:39:58.728783 24431 solver.cpp:219] Iteration 500 (821.2 iter/s, 0.121773s/100 iters), loss = 0.44268
I0428 19:39:58.728822 24431 solver.cpp:238]     Train net output #0: loss = 0.44268 (* 1 = 0.44268 loss)
I0428 19:39:58.728837 24431 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:39:58.799093 24431 solver.cpp:219] Iteration 600 (1423.25 iter/s, 0.0702618s/100 iters), loss = 0.451523
I0428 19:39:58.799124 24431 solver.cpp:238]     Train net output #0: loss = 0.451523 (* 1 = 0.451523 loss)
I0428 19:39:58.799130 24431 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:39:58.868602 24431 solver.cpp:219] Iteration 700 (1439.49 iter/s, 0.0694692s/100 iters), loss = 0.505231
I0428 19:39:58.868634 24431 solver.cpp:238]     Train net output #0: loss = 0.505231 (* 1 = 0.505231 loss)
I0428 19:39:58.868643 24431 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:39:58.937398 24431 solver.cpp:219] Iteration 800 (1454.43 iter/s, 0.0687555s/100 iters), loss = 0.564079
I0428 19:39:58.937430 24431 solver.cpp:238]     Train net output #0: loss = 0.564079 (* 1 = 0.564079 loss)
I0428 19:39:58.937438 24431 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:39:59.006136 24431 solver.cpp:219] Iteration 900 (1455.67 iter/s, 0.0686971s/100 iters), loss = 0.503402
I0428 19:39:59.006167 24431 solver.cpp:238]     Train net output #0: loss = 0.503402 (* 1 = 0.503402 loss)
I0428 19:39:59.006175 24431 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:39:59.029160 24437 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:39:59.074169 24431 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:39:59.074836 24431 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:39:59.075260 24431 solver.cpp:311] Iteration 1000, loss = 0.450837
I0428 19:39:59.075281 24431 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:39:59.152568 24438 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:39:59.153015 24431 solver.cpp:398]     Test net output #0: accuracy = 0.8685
I0428 19:39:59.153038 24431 solver.cpp:398]     Test net output #1: loss = 0.356825 (* 1 = 0.356825 loss)
I0428 19:39:59.153044 24431 solver.cpp:316] Optimization Done.
I0428 19:39:59.153048 24431 caffe.cpp:259] Optimization Done.
