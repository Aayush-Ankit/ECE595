I0428 20:35:42.552090  4664 caffe.cpp:218] Using GPUs 0
I0428 20:35:42.583492  4664 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:35:43.031038  4664 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1625.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:35:43.031193  4664 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1625.prototxt
I0428 20:35:43.031539  4664 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:35:43.031558  4664 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:35:43.031646  4664 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:35:43.031734  4664 layer_factory.hpp:77] Creating layer mnist
I0428 20:35:43.031838  4664 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:35:43.031864  4664 net.cpp:86] Creating Layer mnist
I0428 20:35:43.031875  4664 net.cpp:382] mnist -> data
I0428 20:35:43.031899  4664 net.cpp:382] mnist -> label
I0428 20:35:43.032941  4664 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:35:43.035148  4664 net.cpp:124] Setting up mnist
I0428 20:35:43.035182  4664 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:35:43.035190  4664 net.cpp:131] Top shape: 64 (64)
I0428 20:35:43.035195  4664 net.cpp:139] Memory required for data: 200960
I0428 20:35:43.035204  4664 layer_factory.hpp:77] Creating layer conv0
I0428 20:35:43.035224  4664 net.cpp:86] Creating Layer conv0
I0428 20:35:43.035243  4664 net.cpp:408] conv0 <- data
I0428 20:35:43.035259  4664 net.cpp:382] conv0 -> conv0
I0428 20:35:43.262013  4664 net.cpp:124] Setting up conv0
I0428 20:35:43.262056  4664 net.cpp:131] Top shape: 64 100 24 24 (3686400)
I0428 20:35:43.262063  4664 net.cpp:139] Memory required for data: 14946560
I0428 20:35:43.262096  4664 layer_factory.hpp:77] Creating layer pool0
I0428 20:35:43.262114  4664 net.cpp:86] Creating Layer pool0
I0428 20:35:43.262127  4664 net.cpp:408] pool0 <- conv0
I0428 20:35:43.262136  4664 net.cpp:382] pool0 -> pool0
I0428 20:35:43.262193  4664 net.cpp:124] Setting up pool0
I0428 20:35:43.262202  4664 net.cpp:131] Top shape: 64 100 12 12 (921600)
I0428 20:35:43.262207  4664 net.cpp:139] Memory required for data: 18632960
I0428 20:35:43.262212  4664 layer_factory.hpp:77] Creating layer conv1
I0428 20:35:43.262228  4664 net.cpp:86] Creating Layer conv1
I0428 20:35:43.262235  4664 net.cpp:408] conv1 <- pool0
I0428 20:35:43.262243  4664 net.cpp:382] conv1 -> conv1
I0428 20:35:43.265763  4664 net.cpp:124] Setting up conv1
I0428 20:35:43.265779  4664 net.cpp:131] Top shape: 64 50 8 8 (204800)
I0428 20:35:43.265784  4664 net.cpp:139] Memory required for data: 19452160
I0428 20:35:43.265795  4664 layer_factory.hpp:77] Creating layer pool1
I0428 20:35:43.265805  4664 net.cpp:86] Creating Layer pool1
I0428 20:35:43.265811  4664 net.cpp:408] pool1 <- conv1
I0428 20:35:43.265820  4664 net.cpp:382] pool1 -> pool1
I0428 20:35:43.265861  4664 net.cpp:124] Setting up pool1
I0428 20:35:43.265869  4664 net.cpp:131] Top shape: 64 50 4 4 (51200)
I0428 20:35:43.265874  4664 net.cpp:139] Memory required for data: 19656960
I0428 20:35:43.265878  4664 layer_factory.hpp:77] Creating layer ip1
I0428 20:35:43.265890  4664 net.cpp:86] Creating Layer ip1
I0428 20:35:43.265899  4664 net.cpp:408] ip1 <- pool1
I0428 20:35:43.265908  4664 net.cpp:382] ip1 -> ip1
I0428 20:35:43.266105  4664 net.cpp:124] Setting up ip1
I0428 20:35:43.266114  4664 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:35:43.266119  4664 net.cpp:139] Memory required for data: 19663360
I0428 20:35:43.266129  4664 layer_factory.hpp:77] Creating layer relu1
I0428 20:35:43.266141  4664 net.cpp:86] Creating Layer relu1
I0428 20:35:43.266149  4664 net.cpp:408] relu1 <- ip1
I0428 20:35:43.266157  4664 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:35:43.266322  4664 net.cpp:124] Setting up relu1
I0428 20:35:43.266331  4664 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:35:43.266336  4664 net.cpp:139] Memory required for data: 19669760
I0428 20:35:43.266341  4664 layer_factory.hpp:77] Creating layer ip2
I0428 20:35:43.266352  4664 net.cpp:86] Creating Layer ip2
I0428 20:35:43.266358  4664 net.cpp:408] ip2 <- ip1
I0428 20:35:43.266366  4664 net.cpp:382] ip2 -> ip2
I0428 20:35:43.266484  4664 net.cpp:124] Setting up ip2
I0428 20:35:43.266491  4664 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:35:43.266495  4664 net.cpp:139] Memory required for data: 19676160
I0428 20:35:43.266505  4664 layer_factory.hpp:77] Creating layer relu2
I0428 20:35:43.266515  4664 net.cpp:86] Creating Layer relu2
I0428 20:35:43.266520  4664 net.cpp:408] relu2 <- ip2
I0428 20:35:43.266527  4664 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:35:43.267323  4664 net.cpp:124] Setting up relu2
I0428 20:35:43.267336  4664 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:35:43.267341  4664 net.cpp:139] Memory required for data: 19682560
I0428 20:35:43.267348  4664 layer_factory.hpp:77] Creating layer ip3
I0428 20:35:43.267357  4664 net.cpp:86] Creating Layer ip3
I0428 20:35:43.267362  4664 net.cpp:408] ip3 <- ip2
I0428 20:35:43.267372  4664 net.cpp:382] ip3 -> ip3
I0428 20:35:43.267475  4664 net.cpp:124] Setting up ip3
I0428 20:35:43.267484  4664 net.cpp:131] Top shape: 64 10 (640)
I0428 20:35:43.267489  4664 net.cpp:139] Memory required for data: 19685120
I0428 20:35:43.267518  4664 layer_factory.hpp:77] Creating layer relu3
I0428 20:35:43.267526  4664 net.cpp:86] Creating Layer relu3
I0428 20:35:43.267531  4664 net.cpp:408] relu3 <- ip3
I0428 20:35:43.267554  4664 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:35:43.267735  4664 net.cpp:124] Setting up relu3
I0428 20:35:43.267745  4664 net.cpp:131] Top shape: 64 10 (640)
I0428 20:35:43.267750  4664 net.cpp:139] Memory required for data: 19687680
I0428 20:35:43.267755  4664 layer_factory.hpp:77] Creating layer loss
I0428 20:35:43.267763  4664 net.cpp:86] Creating Layer loss
I0428 20:35:43.267768  4664 net.cpp:408] loss <- ip3
I0428 20:35:43.267776  4664 net.cpp:408] loss <- label
I0428 20:35:43.267786  4664 net.cpp:382] loss -> loss
I0428 20:35:43.267807  4664 layer_factory.hpp:77] Creating layer loss
I0428 20:35:43.268096  4664 net.cpp:124] Setting up loss
I0428 20:35:43.268121  4664 net.cpp:131] Top shape: (1)
I0428 20:35:43.268126  4664 net.cpp:134]     with loss weight 1
I0428 20:35:43.268146  4664 net.cpp:139] Memory required for data: 19687684
I0428 20:35:43.268151  4664 net.cpp:200] loss needs backward computation.
I0428 20:35:43.268157  4664 net.cpp:200] relu3 needs backward computation.
I0428 20:35:43.268162  4664 net.cpp:200] ip3 needs backward computation.
I0428 20:35:43.268167  4664 net.cpp:200] relu2 needs backward computation.
I0428 20:35:43.268172  4664 net.cpp:200] ip2 needs backward computation.
I0428 20:35:43.268177  4664 net.cpp:200] relu1 needs backward computation.
I0428 20:35:43.268182  4664 net.cpp:200] ip1 needs backward computation.
I0428 20:35:43.268187  4664 net.cpp:200] pool1 needs backward computation.
I0428 20:35:43.268191  4664 net.cpp:200] conv1 needs backward computation.
I0428 20:35:43.268196  4664 net.cpp:200] pool0 needs backward computation.
I0428 20:35:43.268201  4664 net.cpp:200] conv0 needs backward computation.
I0428 20:35:43.268208  4664 net.cpp:202] mnist does not need backward computation.
I0428 20:35:43.268211  4664 net.cpp:244] This network produces output loss
I0428 20:35:43.268229  4664 net.cpp:257] Network initialization done.
I0428 20:35:43.268566  4664 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1625.prototxt
I0428 20:35:43.268613  4664 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:35:43.268710  4664 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:35:43.268898  4664 layer_factory.hpp:77] Creating layer mnist
I0428 20:35:43.268971  4664 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:35:43.268993  4664 net.cpp:86] Creating Layer mnist
I0428 20:35:43.269004  4664 net.cpp:382] mnist -> data
I0428 20:35:43.269016  4664 net.cpp:382] mnist -> label
I0428 20:35:43.269176  4664 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:35:43.270452  4664 net.cpp:124] Setting up mnist
I0428 20:35:43.270494  4664 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:35:43.270501  4664 net.cpp:131] Top shape: 100 (100)
I0428 20:35:43.270504  4664 net.cpp:139] Memory required for data: 314000
I0428 20:35:43.270508  4664 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:35:43.270526  4664 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:35:43.270530  4664 net.cpp:408] label_mnist_1_split <- label
I0428 20:35:43.270535  4664 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:35:43.270541  4664 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:35:43.270632  4664 net.cpp:124] Setting up label_mnist_1_split
I0428 20:35:43.270638  4664 net.cpp:131] Top shape: 100 (100)
I0428 20:35:43.270642  4664 net.cpp:131] Top shape: 100 (100)
I0428 20:35:43.270644  4664 net.cpp:139] Memory required for data: 314800
I0428 20:35:43.270648  4664 layer_factory.hpp:77] Creating layer conv0
I0428 20:35:43.270656  4664 net.cpp:86] Creating Layer conv0
I0428 20:35:43.270661  4664 net.cpp:408] conv0 <- data
I0428 20:35:43.270684  4664 net.cpp:382] conv0 -> conv0
I0428 20:35:43.272452  4664 net.cpp:124] Setting up conv0
I0428 20:35:43.272464  4664 net.cpp:131] Top shape: 100 100 24 24 (5760000)
I0428 20:35:43.272483  4664 net.cpp:139] Memory required for data: 23354800
I0428 20:35:43.272491  4664 layer_factory.hpp:77] Creating layer pool0
I0428 20:35:43.272498  4664 net.cpp:86] Creating Layer pool0
I0428 20:35:43.272501  4664 net.cpp:408] pool0 <- conv0
I0428 20:35:43.272506  4664 net.cpp:382] pool0 -> pool0
I0428 20:35:43.272557  4664 net.cpp:124] Setting up pool0
I0428 20:35:43.272563  4664 net.cpp:131] Top shape: 100 100 12 12 (1440000)
I0428 20:35:43.272567  4664 net.cpp:139] Memory required for data: 29114800
I0428 20:35:43.272569  4664 layer_factory.hpp:77] Creating layer conv1
I0428 20:35:43.272578  4664 net.cpp:86] Creating Layer conv1
I0428 20:35:43.272583  4664 net.cpp:408] conv1 <- pool0
I0428 20:35:43.272604  4664 net.cpp:382] conv1 -> conv1
I0428 20:35:43.275765  4664 net.cpp:124] Setting up conv1
I0428 20:35:43.275780  4664 net.cpp:131] Top shape: 100 50 8 8 (320000)
I0428 20:35:43.275799  4664 net.cpp:139] Memory required for data: 30394800
I0428 20:35:43.275809  4664 layer_factory.hpp:77] Creating layer pool1
I0428 20:35:43.275830  4664 net.cpp:86] Creating Layer pool1
I0428 20:35:43.275835  4664 net.cpp:408] pool1 <- conv1
I0428 20:35:43.275840  4664 net.cpp:382] pool1 -> pool1
I0428 20:35:43.275888  4664 net.cpp:124] Setting up pool1
I0428 20:35:43.275897  4664 net.cpp:131] Top shape: 100 50 4 4 (80000)
I0428 20:35:43.275900  4664 net.cpp:139] Memory required for data: 30714800
I0428 20:35:43.275902  4664 layer_factory.hpp:77] Creating layer ip1
I0428 20:35:43.275908  4664 net.cpp:86] Creating Layer ip1
I0428 20:35:43.275912  4664 net.cpp:408] ip1 <- pool1
I0428 20:35:43.275918  4664 net.cpp:382] ip1 -> ip1
I0428 20:35:43.276125  4664 net.cpp:124] Setting up ip1
I0428 20:35:43.276144  4664 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:35:43.276147  4664 net.cpp:139] Memory required for data: 30724800
I0428 20:35:43.276155  4664 layer_factory.hpp:77] Creating layer relu1
I0428 20:35:43.276160  4664 net.cpp:86] Creating Layer relu1
I0428 20:35:43.276163  4664 net.cpp:408] relu1 <- ip1
I0428 20:35:43.276170  4664 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:35:43.276340  4664 net.cpp:124] Setting up relu1
I0428 20:35:43.276347  4664 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:35:43.276351  4664 net.cpp:139] Memory required for data: 30734800
I0428 20:35:43.276355  4664 layer_factory.hpp:77] Creating layer ip2
I0428 20:35:43.276362  4664 net.cpp:86] Creating Layer ip2
I0428 20:35:43.276368  4664 net.cpp:408] ip2 <- ip1
I0428 20:35:43.276388  4664 net.cpp:382] ip2 -> ip2
I0428 20:35:43.276510  4664 net.cpp:124] Setting up ip2
I0428 20:35:43.276516  4664 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:35:43.276520  4664 net.cpp:139] Memory required for data: 30744800
I0428 20:35:43.276526  4664 layer_factory.hpp:77] Creating layer relu2
I0428 20:35:43.276531  4664 net.cpp:86] Creating Layer relu2
I0428 20:35:43.276535  4664 net.cpp:408] relu2 <- ip2
I0428 20:35:43.276540  4664 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:35:43.276787  4664 net.cpp:124] Setting up relu2
I0428 20:35:43.276794  4664 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:35:43.276798  4664 net.cpp:139] Memory required for data: 30754800
I0428 20:35:43.276800  4664 layer_factory.hpp:77] Creating layer ip3
I0428 20:35:43.276808  4664 net.cpp:86] Creating Layer ip3
I0428 20:35:43.276828  4664 net.cpp:408] ip3 <- ip2
I0428 20:35:43.276832  4664 net.cpp:382] ip3 -> ip3
I0428 20:35:43.277001  4664 net.cpp:124] Setting up ip3
I0428 20:35:43.277009  4664 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:35:43.277014  4664 net.cpp:139] Memory required for data: 30758800
I0428 20:35:43.277022  4664 layer_factory.hpp:77] Creating layer relu3
I0428 20:35:43.277029  4664 net.cpp:86] Creating Layer relu3
I0428 20:35:43.277031  4664 net.cpp:408] relu3 <- ip3
I0428 20:35:43.277035  4664 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:35:43.277906  4664 net.cpp:124] Setting up relu3
I0428 20:35:43.277918  4664 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:35:43.277937  4664 net.cpp:139] Memory required for data: 30762800
I0428 20:35:43.277940  4664 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:35:43.277947  4664 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:35:43.277953  4664 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:35:43.277958  4664 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:35:43.277964  4664 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:35:43.278024  4664 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:35:43.278030  4664 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:35:43.278034  4664 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:35:43.278051  4664 net.cpp:139] Memory required for data: 30770800
I0428 20:35:43.278054  4664 layer_factory.hpp:77] Creating layer accuracy
I0428 20:35:43.278060  4664 net.cpp:86] Creating Layer accuracy
I0428 20:35:43.278064  4664 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:35:43.278067  4664 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:35:43.278071  4664 net.cpp:382] accuracy -> accuracy
I0428 20:35:43.278079  4664 net.cpp:124] Setting up accuracy
I0428 20:35:43.278082  4664 net.cpp:131] Top shape: (1)
I0428 20:35:43.278085  4664 net.cpp:139] Memory required for data: 30770804
I0428 20:35:43.278089  4664 layer_factory.hpp:77] Creating layer loss
I0428 20:35:43.278093  4664 net.cpp:86] Creating Layer loss
I0428 20:35:43.278096  4664 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:35:43.278100  4664 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:35:43.278105  4664 net.cpp:382] loss -> loss
I0428 20:35:43.278111  4664 layer_factory.hpp:77] Creating layer loss
I0428 20:35:43.278364  4664 net.cpp:124] Setting up loss
I0428 20:35:43.278373  4664 net.cpp:131] Top shape: (1)
I0428 20:35:43.278375  4664 net.cpp:134]     with loss weight 1
I0428 20:35:43.278405  4664 net.cpp:139] Memory required for data: 30770808
I0428 20:35:43.278409  4664 net.cpp:200] loss needs backward computation.
I0428 20:35:43.278412  4664 net.cpp:202] accuracy does not need backward computation.
I0428 20:35:43.278415  4664 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:35:43.278419  4664 net.cpp:200] relu3 needs backward computation.
I0428 20:35:43.278421  4664 net.cpp:200] ip3 needs backward computation.
I0428 20:35:43.278424  4664 net.cpp:200] relu2 needs backward computation.
I0428 20:35:43.278426  4664 net.cpp:200] ip2 needs backward computation.
I0428 20:35:43.278429  4664 net.cpp:200] relu1 needs backward computation.
I0428 20:35:43.278431  4664 net.cpp:200] ip1 needs backward computation.
I0428 20:35:43.278434  4664 net.cpp:200] pool1 needs backward computation.
I0428 20:35:43.278437  4664 net.cpp:200] conv1 needs backward computation.
I0428 20:35:43.278440  4664 net.cpp:200] pool0 needs backward computation.
I0428 20:35:43.278443  4664 net.cpp:200] conv0 needs backward computation.
I0428 20:35:43.278447  4664 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:35:43.278450  4664 net.cpp:202] mnist does not need backward computation.
I0428 20:35:43.278453  4664 net.cpp:244] This network produces output accuracy
I0428 20:35:43.278456  4664 net.cpp:244] This network produces output loss
I0428 20:35:43.278467  4664 net.cpp:257] Network initialization done.
I0428 20:35:43.278530  4664 solver.cpp:56] Solver scaffolding done.
I0428 20:35:43.278882  4664 caffe.cpp:248] Starting Optimization
I0428 20:35:43.278887  4664 solver.cpp:273] Solving LeNet
I0428 20:35:43.278889  4664 solver.cpp:274] Learning Rate Policy: inv
I0428 20:35:43.279768  4664 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:35:43.339038  4664 blocking_queue.cpp:49] Waiting for data
I0428 20:35:43.389370  4671 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:35:43.391129  4664 solver.cpp:398]     Test net output #0: accuracy = 0.0758
I0428 20:35:43.391162  4664 solver.cpp:398]     Test net output #1: loss = 2.30682 (* 1 = 2.30682 loss)
I0428 20:35:43.395828  4664 solver.cpp:219] Iteration 0 (-1.02025e-30 iter/s, 0.116896s/100 iters), loss = 2.30709
I0428 20:35:43.395867  4664 solver.cpp:238]     Train net output #0: loss = 2.30709 (* 1 = 2.30709 loss)
I0428 20:35:43.395879  4664 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:35:43.623106  4664 solver.cpp:219] Iteration 100 (440.103 iter/s, 0.227219s/100 iters), loss = 0.694381
I0428 20:35:43.623150  4664 solver.cpp:238]     Train net output #0: loss = 0.694381 (* 1 = 0.694381 loss)
I0428 20:35:43.623157  4664 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:35:43.853798  4664 solver.cpp:219] Iteration 200 (433.561 iter/s, 0.230648s/100 iters), loss = 0.614931
I0428 20:35:43.853824  4664 solver.cpp:238]     Train net output #0: loss = 0.614931 (* 1 = 0.614931 loss)
I0428 20:35:43.853830  4664 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:35:44.086974  4664 solver.cpp:219] Iteration 300 (428.938 iter/s, 0.233134s/100 iters), loss = 0.48069
I0428 20:35:44.086999  4664 solver.cpp:238]     Train net output #0: loss = 0.48069 (* 1 = 0.48069 loss)
I0428 20:35:44.087005  4664 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:35:44.319245  4664 solver.cpp:219] Iteration 400 (430.612 iter/s, 0.232228s/100 iters), loss = 0.419983
I0428 20:35:44.319274  4664 solver.cpp:238]     Train net output #0: loss = 0.419983 (* 1 = 0.419983 loss)
I0428 20:35:44.319281  4664 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:35:44.547745  4664 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:35:44.655824  4671 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:35:44.658936  4664 solver.cpp:398]     Test net output #0: accuracy = 0.792
I0428 20:35:44.658972  4664 solver.cpp:398]     Test net output #1: loss = 0.522251 (* 1 = 0.522251 loss)
I0428 20:35:44.661201  4664 solver.cpp:219] Iteration 500 (292.479 iter/s, 0.341905s/100 iters), loss = 0.600649
I0428 20:35:44.661253  4664 solver.cpp:238]     Train net output #0: loss = 0.600649 (* 1 = 0.600649 loss)
I0428 20:35:44.661262  4664 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:35:44.913739  4664 solver.cpp:219] Iteration 600 (396.102 iter/s, 0.25246s/100 iters), loss = 0.519214
I0428 20:35:44.913794  4664 solver.cpp:238]     Train net output #0: loss = 0.519214 (* 1 = 0.519214 loss)
I0428 20:35:44.913805  4664 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:35:45.165160  4664 solver.cpp:219] Iteration 700 (397.855 iter/s, 0.251348s/100 iters), loss = 0.753749
I0428 20:35:45.165215  4664 solver.cpp:238]     Train net output #0: loss = 0.753749 (* 1 = 0.753749 loss)
I0428 20:35:45.165227  4664 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:35:45.416827  4664 solver.cpp:219] Iteration 800 (397.482 iter/s, 0.251583s/100 iters), loss = 0.543143
I0428 20:35:45.416874  4664 solver.cpp:238]     Train net output #0: loss = 0.543143 (* 1 = 0.543143 loss)
I0428 20:35:45.416887  4664 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:35:45.670397  4664 solver.cpp:219] Iteration 900 (394.475 iter/s, 0.253501s/100 iters), loss = 0.244163
I0428 20:35:45.670454  4664 solver.cpp:238]     Train net output #0: loss = 0.244163 (* 1 = 0.244163 loss)
I0428 20:35:45.670467  4664 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:35:45.756268  4670 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:35:45.928254  4664 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:35:45.935025  4664 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:35:45.938037  4664 solver.cpp:311] Iteration 1000, loss = 0.121642
I0428 20:35:45.938068  4664 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:35:46.056305  4671 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:35:46.058351  4664 solver.cpp:398]     Test net output #0: accuracy = 0.9722
I0428 20:35:46.058378  4664 solver.cpp:398]     Test net output #1: loss = 0.0915454 (* 1 = 0.0915454 loss)
I0428 20:35:46.058384  4664 solver.cpp:316] Optimization Done.
I0428 20:35:46.058388  4664 caffe.cpp:259] Optimization Done.
