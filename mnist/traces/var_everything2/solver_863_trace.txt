I0428 19:59:57.997411 29368 caffe.cpp:218] Using GPUs 0
I0428 19:59:58.033996 29368 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:59:58.545186 29368 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test863.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:59:58.545330 29368 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test863.prototxt
I0428 19:59:58.545701 29368 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:59:58.545716 29368 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:59:58.545802 29368 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0428 19:59:58.545871 29368 layer_factory.hpp:77] Creating layer mnist
I0428 19:59:58.545970 29368 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:59:58.545994 29368 net.cpp:86] Creating Layer mnist
I0428 19:59:58.546000 29368 net.cpp:382] mnist -> data
I0428 19:59:58.546022 29368 net.cpp:382] mnist -> label
I0428 19:59:58.547118 29368 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:59:58.549569 29368 net.cpp:124] Setting up mnist
I0428 19:59:58.549587 29368 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:59:58.549594 29368 net.cpp:131] Top shape: 64 (64)
I0428 19:59:58.549597 29368 net.cpp:139] Memory required for data: 200960
I0428 19:59:58.549605 29368 layer_factory.hpp:77] Creating layer conv0
I0428 19:59:58.549620 29368 net.cpp:86] Creating Layer conv0
I0428 19:59:58.549626 29368 net.cpp:408] conv0 <- data
I0428 19:59:58.549638 29368 net.cpp:382] conv0 -> conv0
I0428 19:59:58.817353 29368 net.cpp:124] Setting up conv0
I0428 19:59:58.817394 29368 net.cpp:131] Top shape: 64 10 24 24 (368640)
I0428 19:59:58.817399 29368 net.cpp:139] Memory required for data: 1675520
I0428 19:59:58.817432 29368 layer_factory.hpp:77] Creating layer pool0
I0428 19:59:58.817445 29368 net.cpp:86] Creating Layer pool0
I0428 19:59:58.817448 29368 net.cpp:408] pool0 <- conv0
I0428 19:59:58.817453 29368 net.cpp:382] pool0 -> pool0
I0428 19:59:58.817498 29368 net.cpp:124] Setting up pool0
I0428 19:59:58.817504 29368 net.cpp:131] Top shape: 64 10 12 12 (92160)
I0428 19:59:58.817507 29368 net.cpp:139] Memory required for data: 2044160
I0428 19:59:58.817510 29368 layer_factory.hpp:77] Creating layer conv1
I0428 19:59:58.817520 29368 net.cpp:86] Creating Layer conv1
I0428 19:59:58.817523 29368 net.cpp:408] conv1 <- pool0
I0428 19:59:58.817528 29368 net.cpp:382] conv1 -> conv1
I0428 19:59:58.820365 29368 net.cpp:124] Setting up conv1
I0428 19:59:58.820395 29368 net.cpp:131] Top shape: 64 25 8 8 (102400)
I0428 19:59:58.820399 29368 net.cpp:139] Memory required for data: 2453760
I0428 19:59:58.820410 29368 layer_factory.hpp:77] Creating layer pool1
I0428 19:59:58.820417 29368 net.cpp:86] Creating Layer pool1
I0428 19:59:58.820420 29368 net.cpp:408] pool1 <- conv1
I0428 19:59:58.820425 29368 net.cpp:382] pool1 -> pool1
I0428 19:59:58.820477 29368 net.cpp:124] Setting up pool1
I0428 19:59:58.820484 29368 net.cpp:131] Top shape: 64 25 4 4 (25600)
I0428 19:59:58.820487 29368 net.cpp:139] Memory required for data: 2556160
I0428 19:59:58.820489 29368 layer_factory.hpp:77] Creating layer ip1
I0428 19:59:58.820497 29368 net.cpp:86] Creating Layer ip1
I0428 19:59:58.820500 29368 net.cpp:408] ip1 <- pool1
I0428 19:59:58.820504 29368 net.cpp:382] ip1 -> ip1
I0428 19:59:58.821491 29368 net.cpp:124] Setting up ip1
I0428 19:59:58.821519 29368 net.cpp:131] Top shape: 64 10 (640)
I0428 19:59:58.821538 29368 net.cpp:139] Memory required for data: 2558720
I0428 19:59:58.821547 29368 layer_factory.hpp:77] Creating layer relu1
I0428 19:59:58.821553 29368 net.cpp:86] Creating Layer relu1
I0428 19:59:58.821557 29368 net.cpp:408] relu1 <- ip1
I0428 19:59:58.821561 29368 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:59:58.821764 29368 net.cpp:124] Setting up relu1
I0428 19:59:58.821774 29368 net.cpp:131] Top shape: 64 10 (640)
I0428 19:59:58.821776 29368 net.cpp:139] Memory required for data: 2561280
I0428 19:59:58.821779 29368 layer_factory.hpp:77] Creating layer ip2
I0428 19:59:58.821786 29368 net.cpp:86] Creating Layer ip2
I0428 19:59:58.821789 29368 net.cpp:408] ip2 <- ip1
I0428 19:59:58.821795 29368 net.cpp:382] ip2 -> ip2
I0428 19:59:58.821919 29368 net.cpp:124] Setting up ip2
I0428 19:59:58.821926 29368 net.cpp:131] Top shape: 64 10 (640)
I0428 19:59:58.821929 29368 net.cpp:139] Memory required for data: 2563840
I0428 19:59:58.821934 29368 layer_factory.hpp:77] Creating layer relu2
I0428 19:59:58.821959 29368 net.cpp:86] Creating Layer relu2
I0428 19:59:58.821961 29368 net.cpp:408] relu2 <- ip2
I0428 19:59:58.821965 29368 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:59:58.822703 29368 net.cpp:124] Setting up relu2
I0428 19:59:58.822731 29368 net.cpp:131] Top shape: 64 10 (640)
I0428 19:59:58.822734 29368 net.cpp:139] Memory required for data: 2566400
I0428 19:59:58.822737 29368 layer_factory.hpp:77] Creating layer loss
I0428 19:59:58.822744 29368 net.cpp:86] Creating Layer loss
I0428 19:59:58.822747 29368 net.cpp:408] loss <- ip2
I0428 19:59:58.822751 29368 net.cpp:408] loss <- label
I0428 19:59:58.822757 29368 net.cpp:382] loss -> loss
I0428 19:59:58.822772 29368 layer_factory.hpp:77] Creating layer loss
I0428 19:59:58.823017 29368 net.cpp:124] Setting up loss
I0428 19:59:58.823041 29368 net.cpp:131] Top shape: (1)
I0428 19:59:58.823045 29368 net.cpp:134]     with loss weight 1
I0428 19:59:58.823060 29368 net.cpp:139] Memory required for data: 2566404
I0428 19:59:58.823063 29368 net.cpp:200] loss needs backward computation.
I0428 19:59:58.823067 29368 net.cpp:200] relu2 needs backward computation.
I0428 19:59:58.823071 29368 net.cpp:200] ip2 needs backward computation.
I0428 19:59:58.823073 29368 net.cpp:200] relu1 needs backward computation.
I0428 19:59:58.823076 29368 net.cpp:200] ip1 needs backward computation.
I0428 19:59:58.823091 29368 net.cpp:200] pool1 needs backward computation.
I0428 19:59:58.823093 29368 net.cpp:200] conv1 needs backward computation.
I0428 19:59:58.823096 29368 net.cpp:200] pool0 needs backward computation.
I0428 19:59:58.823101 29368 net.cpp:200] conv0 needs backward computation.
I0428 19:59:58.823103 29368 net.cpp:202] mnist does not need backward computation.
I0428 19:59:58.823107 29368 net.cpp:244] This network produces output loss
I0428 19:59:58.823115 29368 net.cpp:257] Network initialization done.
I0428 19:59:58.823452 29368 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test863.prototxt
I0428 19:59:58.823493 29368 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:59:58.823571 29368 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0428 19:59:58.823647 29368 layer_factory.hpp:77] Creating layer mnist
I0428 19:59:58.823690 29368 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:59:58.823714 29368 net.cpp:86] Creating Layer mnist
I0428 19:59:58.823719 29368 net.cpp:382] mnist -> data
I0428 19:59:58.823725 29368 net.cpp:382] mnist -> label
I0428 19:59:58.823824 29368 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:59:58.826196 29368 net.cpp:124] Setting up mnist
I0428 19:59:58.826225 29368 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:59:58.826231 29368 net.cpp:131] Top shape: 100 (100)
I0428 19:59:58.826234 29368 net.cpp:139] Memory required for data: 314000
I0428 19:59:58.826238 29368 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:59:58.826246 29368 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:59:58.826248 29368 net.cpp:408] label_mnist_1_split <- label
I0428 19:59:58.826253 29368 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:59:58.826261 29368 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:59:58.826370 29368 net.cpp:124] Setting up label_mnist_1_split
I0428 19:59:58.826387 29368 net.cpp:131] Top shape: 100 (100)
I0428 19:59:58.826391 29368 net.cpp:131] Top shape: 100 (100)
I0428 19:59:58.826395 29368 net.cpp:139] Memory required for data: 314800
I0428 19:59:58.826397 29368 layer_factory.hpp:77] Creating layer conv0
I0428 19:59:58.826406 29368 net.cpp:86] Creating Layer conv0
I0428 19:59:58.826409 29368 net.cpp:408] conv0 <- data
I0428 19:59:58.826414 29368 net.cpp:382] conv0 -> conv0
I0428 19:59:58.828074 29368 net.cpp:124] Setting up conv0
I0428 19:59:58.828086 29368 net.cpp:131] Top shape: 100 10 24 24 (576000)
I0428 19:59:58.828106 29368 net.cpp:139] Memory required for data: 2618800
I0428 19:59:58.828115 29368 layer_factory.hpp:77] Creating layer pool0
I0428 19:59:58.828121 29368 net.cpp:86] Creating Layer pool0
I0428 19:59:58.828125 29368 net.cpp:408] pool0 <- conv0
I0428 19:59:58.828128 29368 net.cpp:382] pool0 -> pool0
I0428 19:59:58.828161 29368 net.cpp:124] Setting up pool0
I0428 19:59:58.828171 29368 net.cpp:131] Top shape: 100 10 12 12 (144000)
I0428 19:59:58.828173 29368 net.cpp:139] Memory required for data: 3194800
I0428 19:59:58.828176 29368 layer_factory.hpp:77] Creating layer conv1
I0428 19:59:58.828184 29368 net.cpp:86] Creating Layer conv1
I0428 19:59:58.828187 29368 net.cpp:408] conv1 <- pool0
I0428 19:59:58.828191 29368 net.cpp:382] conv1 -> conv1
I0428 19:59:58.830344 29368 net.cpp:124] Setting up conv1
I0428 19:59:58.830356 29368 net.cpp:131] Top shape: 100 25 8 8 (160000)
I0428 19:59:58.830376 29368 net.cpp:139] Memory required for data: 3834800
I0428 19:59:58.830384 29368 layer_factory.hpp:77] Creating layer pool1
I0428 19:59:58.830391 29368 net.cpp:86] Creating Layer pool1
I0428 19:59:58.830394 29368 net.cpp:408] pool1 <- conv1
I0428 19:59:58.830399 29368 net.cpp:382] pool1 -> pool1
I0428 19:59:58.830467 29368 net.cpp:124] Setting up pool1
I0428 19:59:58.830476 29368 net.cpp:131] Top shape: 100 25 4 4 (40000)
I0428 19:59:58.830479 29368 net.cpp:139] Memory required for data: 3994800
I0428 19:59:58.830482 29368 layer_factory.hpp:77] Creating layer ip1
I0428 19:59:58.830487 29368 net.cpp:86] Creating Layer ip1
I0428 19:59:58.830490 29368 net.cpp:408] ip1 <- pool1
I0428 19:59:58.830495 29368 net.cpp:382] ip1 -> ip1
I0428 19:59:58.830610 29368 net.cpp:124] Setting up ip1
I0428 19:59:58.830617 29368 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:59:58.830621 29368 net.cpp:139] Memory required for data: 3998800
I0428 19:59:58.830628 29368 layer_factory.hpp:77] Creating layer relu1
I0428 19:59:58.830633 29368 net.cpp:86] Creating Layer relu1
I0428 19:59:58.830636 29368 net.cpp:408] relu1 <- ip1
I0428 19:59:58.830641 29368 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:59:58.830811 29368 net.cpp:124] Setting up relu1
I0428 19:59:58.830834 29368 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:59:58.830837 29368 net.cpp:139] Memory required for data: 4002800
I0428 19:59:58.830840 29368 layer_factory.hpp:77] Creating layer ip2
I0428 19:59:58.830847 29368 net.cpp:86] Creating Layer ip2
I0428 19:59:58.830850 29368 net.cpp:408] ip2 <- ip1
I0428 19:59:58.830855 29368 net.cpp:382] ip2 -> ip2
I0428 19:59:58.830942 29368 net.cpp:124] Setting up ip2
I0428 19:59:58.830955 29368 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:59:58.830958 29368 net.cpp:139] Memory required for data: 4006800
I0428 19:59:58.830965 29368 layer_factory.hpp:77] Creating layer relu2
I0428 19:59:58.830968 29368 net.cpp:86] Creating Layer relu2
I0428 19:59:58.830971 29368 net.cpp:408] relu2 <- ip2
I0428 19:59:58.830976 29368 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:59:58.831115 29368 net.cpp:124] Setting up relu2
I0428 19:59:58.831123 29368 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:59:58.831126 29368 net.cpp:139] Memory required for data: 4010800
I0428 19:59:58.831130 29368 layer_factory.hpp:77] Creating layer ip2_relu2_0_split
I0428 19:59:58.831133 29368 net.cpp:86] Creating Layer ip2_relu2_0_split
I0428 19:59:58.831137 29368 net.cpp:408] ip2_relu2_0_split <- ip2
I0428 19:59:58.831141 29368 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_0
I0428 19:59:58.831157 29368 net.cpp:382] ip2_relu2_0_split -> ip2_relu2_0_split_1
I0428 19:59:58.831189 29368 net.cpp:124] Setting up ip2_relu2_0_split
I0428 19:59:58.831197 29368 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:59:58.831199 29368 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:59:58.831202 29368 net.cpp:139] Memory required for data: 4018800
I0428 19:59:58.831205 29368 layer_factory.hpp:77] Creating layer accuracy
I0428 19:59:58.831210 29368 net.cpp:86] Creating Layer accuracy
I0428 19:59:58.831213 29368 net.cpp:408] accuracy <- ip2_relu2_0_split_0
I0428 19:59:58.831218 29368 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:59:58.831221 29368 net.cpp:382] accuracy -> accuracy
I0428 19:59:58.831228 29368 net.cpp:124] Setting up accuracy
I0428 19:59:58.831231 29368 net.cpp:131] Top shape: (1)
I0428 19:59:58.831234 29368 net.cpp:139] Memory required for data: 4018804
I0428 19:59:58.831236 29368 layer_factory.hpp:77] Creating layer loss
I0428 19:59:58.831240 29368 net.cpp:86] Creating Layer loss
I0428 19:59:58.831243 29368 net.cpp:408] loss <- ip2_relu2_0_split_1
I0428 19:59:58.831246 29368 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:59:58.831250 29368 net.cpp:382] loss -> loss
I0428 19:59:58.831255 29368 layer_factory.hpp:77] Creating layer loss
I0428 19:59:58.831626 29368 net.cpp:124] Setting up loss
I0428 19:59:58.831635 29368 net.cpp:131] Top shape: (1)
I0428 19:59:58.831660 29368 net.cpp:134]     with loss weight 1
I0428 19:59:58.831666 29368 net.cpp:139] Memory required for data: 4018808
I0428 19:59:58.831671 29368 net.cpp:200] loss needs backward computation.
I0428 19:59:58.831676 29368 net.cpp:202] accuracy does not need backward computation.
I0428 19:59:58.831679 29368 net.cpp:200] ip2_relu2_0_split needs backward computation.
I0428 19:59:58.831682 29368 net.cpp:200] relu2 needs backward computation.
I0428 19:59:58.831686 29368 net.cpp:200] ip2 needs backward computation.
I0428 19:59:58.831696 29368 net.cpp:200] relu1 needs backward computation.
I0428 19:59:58.831699 29368 net.cpp:200] ip1 needs backward computation.
I0428 19:59:58.831702 29368 net.cpp:200] pool1 needs backward computation.
I0428 19:59:58.831707 29368 net.cpp:200] conv1 needs backward computation.
I0428 19:59:58.831710 29368 net.cpp:200] pool0 needs backward computation.
I0428 19:59:58.831714 29368 net.cpp:200] conv0 needs backward computation.
I0428 19:59:58.831718 29368 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:59:58.831722 29368 net.cpp:202] mnist does not need backward computation.
I0428 19:59:58.831724 29368 net.cpp:244] This network produces output accuracy
I0428 19:59:58.831728 29368 net.cpp:244] This network produces output loss
I0428 19:59:58.831738 29368 net.cpp:257] Network initialization done.
I0428 19:59:58.831778 29368 solver.cpp:56] Solver scaffolding done.
I0428 19:59:58.832062 29368 caffe.cpp:248] Starting Optimization
I0428 19:59:58.832067 29368 solver.cpp:273] Solving LeNet
I0428 19:59:58.832070 29368 solver.cpp:274] Learning Rate Policy: inv
I0428 19:59:58.832187 29368 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:59:58.836030 29368 blocking_queue.cpp:49] Waiting for data
I0428 19:59:58.910570 29375 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:59:58.911088 29368 solver.cpp:398]     Test net output #0: accuracy = 0.0907
I0428 19:59:58.911123 29368 solver.cpp:398]     Test net output #1: loss = 2.31678 (* 1 = 2.31678 loss)
I0428 19:59:58.913373 29368 solver.cpp:219] Iteration 0 (-1.46904e-31 iter/s, 0.0812801s/100 iters), loss = 2.31674
I0428 19:59:58.913420 29368 solver.cpp:238]     Train net output #0: loss = 2.31674 (* 1 = 2.31674 loss)
I0428 19:59:58.913432 29368 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:59:59.008420 29368 solver.cpp:219] Iteration 100 (1052.73 iter/s, 0.0949915s/100 iters), loss = 1.09113
I0428 19:59:59.008461 29368 solver.cpp:238]     Train net output #0: loss = 1.09113 (* 1 = 1.09113 loss)
I0428 19:59:59.008467 29368 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:59:59.090585 29368 solver.cpp:219] Iteration 200 (1217.81 iter/s, 0.0821146s/100 iters), loss = 0.676763
I0428 19:59:59.090641 29368 solver.cpp:238]     Train net output #0: loss = 0.676763 (* 1 = 0.676763 loss)
I0428 19:59:59.090648 29368 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:59:59.171089 29368 solver.cpp:219] Iteration 300 (1243.14 iter/s, 0.0804413s/100 iters), loss = 0.335572
I0428 19:59:59.171128 29368 solver.cpp:238]     Train net output #0: loss = 0.335572 (* 1 = 0.335572 loss)
I0428 19:59:59.171134 29368 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:59:59.252029 29368 solver.cpp:219] Iteration 400 (1235.98 iter/s, 0.0809073s/100 iters), loss = 0.1807
I0428 19:59:59.252069 29368 solver.cpp:238]     Train net output #0: loss = 0.1807 (* 1 = 0.1807 loss)
I0428 19:59:59.252075 29368 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:59:59.343078 29368 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:59:59.418530 29375 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:59:59.419042 29368 solver.cpp:398]     Test net output #0: accuracy = 0.9378
I0428 19:59:59.419078 29368 solver.cpp:398]     Test net output #1: loss = 0.2068 (* 1 = 0.2068 loss)
I0428 19:59:59.419986 29368 solver.cpp:219] Iteration 500 (595.549 iter/s, 0.167912s/100 iters), loss = 0.245128
I0428 19:59:59.420027 29368 solver.cpp:238]     Train net output #0: loss = 0.245128 (* 1 = 0.245128 loss)
I0428 19:59:59.420033 29368 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:59:59.514859 29368 solver.cpp:219] Iteration 600 (1054.41 iter/s, 0.0948398s/100 iters), loss = 0.136179
I0428 19:59:59.514899 29368 solver.cpp:238]     Train net output #0: loss = 0.136179 (* 1 = 0.136179 loss)
I0428 19:59:59.514905 29368 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:59:59.596983 29368 solver.cpp:219] Iteration 700 (1218.15 iter/s, 0.082092s/100 iters), loss = 0.163645
I0428 19:59:59.597024 29368 solver.cpp:238]     Train net output #0: loss = 0.163645 (* 1 = 0.163645 loss)
I0428 19:59:59.597031 29368 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:59:59.695703 29368 solver.cpp:219] Iteration 800 (1013.48 iter/s, 0.0986696s/100 iters), loss = 0.270769
I0428 19:59:59.695744 29368 solver.cpp:238]     Train net output #0: loss = 0.270769 (* 1 = 0.270769 loss)
I0428 19:59:59.695751 29368 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:59:59.779680 29368 solver.cpp:219] Iteration 900 (1191.26 iter/s, 0.0839444s/100 iters), loss = 0.23527
I0428 19:59:59.779721 29368 solver.cpp:238]     Train net output #0: loss = 0.23527 (* 1 = 0.23527 loss)
I0428 19:59:59.779726 29368 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:59:59.806782 29374 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:59:59.865219 29368 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:59:59.866091 29368 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:59:59.866674 29368 solver.cpp:311] Iteration 1000, loss = 0.171399
I0428 19:59:59.866696 29368 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:59:59.967938 29375 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:59:59.968641 29368 solver.cpp:398]     Test net output #0: accuracy = 0.9697
I0428 19:59:59.968673 29368 solver.cpp:398]     Test net output #1: loss = 0.0996854 (* 1 = 0.0996854 loss)
I0428 19:59:59.968683 29368 solver.cpp:316] Optimization Done.
I0428 19:59:59.968689 29368 caffe.cpp:259] Optimization Done.
