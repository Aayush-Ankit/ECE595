I0428 20:10:51.793498 31958 caffe.cpp:218] Using GPUs 0
I0428 20:10:51.828579 31958 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:10:52.285465 31958 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1145.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:10:52.285599 31958 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1145.prototxt
I0428 20:10:52.285931 31958 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:10:52.285960 31958 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:10:52.286051 31958 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:10:52.286108 31958 layer_factory.hpp:77] Creating layer mnist
I0428 20:10:52.286187 31958 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:10:52.286206 31958 net.cpp:86] Creating Layer mnist
I0428 20:10:52.286212 31958 net.cpp:382] mnist -> data
I0428 20:10:52.286231 31958 net.cpp:382] mnist -> label
I0428 20:10:52.287140 31958 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:10:52.289391 31958 net.cpp:124] Setting up mnist
I0428 20:10:52.289420 31958 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:10:52.289425 31958 net.cpp:131] Top shape: 64 (64)
I0428 20:10:52.289428 31958 net.cpp:139] Memory required for data: 200960
I0428 20:10:52.289433 31958 layer_factory.hpp:77] Creating layer conv0
I0428 20:10:52.289446 31958 net.cpp:86] Creating Layer conv0
I0428 20:10:52.289463 31958 net.cpp:408] conv0 <- data
I0428 20:10:52.289471 31958 net.cpp:382] conv0 -> conv0
I0428 20:10:52.516149 31958 net.cpp:124] Setting up conv0
I0428 20:10:52.516175 31958 net.cpp:131] Top shape: 64 25 24 24 (921600)
I0428 20:10:52.516180 31958 net.cpp:139] Memory required for data: 3887360
I0428 20:10:52.516193 31958 layer_factory.hpp:77] Creating layer pool0
I0428 20:10:52.516206 31958 net.cpp:86] Creating Layer pool0
I0428 20:10:52.516209 31958 net.cpp:408] pool0 <- conv0
I0428 20:10:52.516216 31958 net.cpp:382] pool0 -> pool0
I0428 20:10:52.516273 31958 net.cpp:124] Setting up pool0
I0428 20:10:52.516279 31958 net.cpp:131] Top shape: 64 25 12 12 (230400)
I0428 20:10:52.516281 31958 net.cpp:139] Memory required for data: 4808960
I0428 20:10:52.516284 31958 layer_factory.hpp:77] Creating layer conv1
I0428 20:10:52.516296 31958 net.cpp:86] Creating Layer conv1
I0428 20:10:52.516300 31958 net.cpp:408] conv1 <- pool0
I0428 20:10:52.516304 31958 net.cpp:382] conv1 -> conv1
I0428 20:10:52.518453 31958 net.cpp:124] Setting up conv1
I0428 20:10:52.518483 31958 net.cpp:131] Top shape: 64 50 8 8 (204800)
I0428 20:10:52.518487 31958 net.cpp:139] Memory required for data: 5628160
I0428 20:10:52.518496 31958 layer_factory.hpp:77] Creating layer pool1
I0428 20:10:52.518507 31958 net.cpp:86] Creating Layer pool1
I0428 20:10:52.518510 31958 net.cpp:408] pool1 <- conv1
I0428 20:10:52.518515 31958 net.cpp:382] pool1 -> pool1
I0428 20:10:52.518568 31958 net.cpp:124] Setting up pool1
I0428 20:10:52.518573 31958 net.cpp:131] Top shape: 64 50 4 4 (51200)
I0428 20:10:52.518575 31958 net.cpp:139] Memory required for data: 5832960
I0428 20:10:52.518579 31958 layer_factory.hpp:77] Creating layer ip1
I0428 20:10:52.518600 31958 net.cpp:86] Creating Layer ip1
I0428 20:10:52.518604 31958 net.cpp:408] ip1 <- pool1
I0428 20:10:52.518609 31958 net.cpp:382] ip1 -> ip1
I0428 20:10:52.519647 31958 net.cpp:124] Setting up ip1
I0428 20:10:52.519659 31958 net.cpp:131] Top shape: 64 10 (640)
I0428 20:10:52.519678 31958 net.cpp:139] Memory required for data: 5835520
I0428 20:10:52.519687 31958 layer_factory.hpp:77] Creating layer relu1
I0428 20:10:52.519693 31958 net.cpp:86] Creating Layer relu1
I0428 20:10:52.519698 31958 net.cpp:408] relu1 <- ip1
I0428 20:10:52.519703 31958 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:10:52.519927 31958 net.cpp:124] Setting up relu1
I0428 20:10:52.519937 31958 net.cpp:131] Top shape: 64 10 (640)
I0428 20:10:52.519939 31958 net.cpp:139] Memory required for data: 5838080
I0428 20:10:52.519943 31958 layer_factory.hpp:77] Creating layer ip2
I0428 20:10:52.519950 31958 net.cpp:86] Creating Layer ip2
I0428 20:10:52.519953 31958 net.cpp:408] ip2 <- ip1
I0428 20:10:52.519958 31958 net.cpp:382] ip2 -> ip2
I0428 20:10:52.520056 31958 net.cpp:124] Setting up ip2
I0428 20:10:52.520063 31958 net.cpp:131] Top shape: 64 10 (640)
I0428 20:10:52.520066 31958 net.cpp:139] Memory required for data: 5840640
I0428 20:10:52.520071 31958 layer_factory.hpp:77] Creating layer relu2
I0428 20:10:52.520078 31958 net.cpp:86] Creating Layer relu2
I0428 20:10:52.520081 31958 net.cpp:408] relu2 <- ip2
I0428 20:10:52.520086 31958 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:10:52.520920 31958 net.cpp:124] Setting up relu2
I0428 20:10:52.520949 31958 net.cpp:131] Top shape: 64 10 (640)
I0428 20:10:52.520952 31958 net.cpp:139] Memory required for data: 5843200
I0428 20:10:52.520956 31958 layer_factory.hpp:77] Creating layer ip3
I0428 20:10:52.520979 31958 net.cpp:86] Creating Layer ip3
I0428 20:10:52.520983 31958 net.cpp:408] ip3 <- ip2
I0428 20:10:52.520990 31958 net.cpp:382] ip3 -> ip3
I0428 20:10:52.521116 31958 net.cpp:124] Setting up ip3
I0428 20:10:52.521139 31958 net.cpp:131] Top shape: 64 10 (640)
I0428 20:10:52.521142 31958 net.cpp:139] Memory required for data: 5845760
I0428 20:10:52.521164 31958 layer_factory.hpp:77] Creating layer relu3
I0428 20:10:52.521169 31958 net.cpp:86] Creating Layer relu3
I0428 20:10:52.521173 31958 net.cpp:408] relu3 <- ip3
I0428 20:10:52.521176 31958 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:10:52.521363 31958 net.cpp:124] Setting up relu3
I0428 20:10:52.521370 31958 net.cpp:131] Top shape: 64 10 (640)
I0428 20:10:52.521373 31958 net.cpp:139] Memory required for data: 5848320
I0428 20:10:52.521376 31958 layer_factory.hpp:77] Creating layer loss
I0428 20:10:52.521381 31958 net.cpp:86] Creating Layer loss
I0428 20:10:52.521384 31958 net.cpp:408] loss <- ip3
I0428 20:10:52.521387 31958 net.cpp:408] loss <- label
I0428 20:10:52.521392 31958 net.cpp:382] loss -> loss
I0428 20:10:52.521411 31958 layer_factory.hpp:77] Creating layer loss
I0428 20:10:52.521638 31958 net.cpp:124] Setting up loss
I0428 20:10:52.521647 31958 net.cpp:131] Top shape: (1)
I0428 20:10:52.521651 31958 net.cpp:134]     with loss weight 1
I0428 20:10:52.521677 31958 net.cpp:139] Memory required for data: 5848324
I0428 20:10:52.521679 31958 net.cpp:200] loss needs backward computation.
I0428 20:10:52.521682 31958 net.cpp:200] relu3 needs backward computation.
I0428 20:10:52.521685 31958 net.cpp:200] ip3 needs backward computation.
I0428 20:10:52.521688 31958 net.cpp:200] relu2 needs backward computation.
I0428 20:10:52.521690 31958 net.cpp:200] ip2 needs backward computation.
I0428 20:10:52.521693 31958 net.cpp:200] relu1 needs backward computation.
I0428 20:10:52.521695 31958 net.cpp:200] ip1 needs backward computation.
I0428 20:10:52.521698 31958 net.cpp:200] pool1 needs backward computation.
I0428 20:10:52.521702 31958 net.cpp:200] conv1 needs backward computation.
I0428 20:10:52.521703 31958 net.cpp:200] pool0 needs backward computation.
I0428 20:10:52.521706 31958 net.cpp:200] conv0 needs backward computation.
I0428 20:10:52.521709 31958 net.cpp:202] mnist does not need backward computation.
I0428 20:10:52.521713 31958 net.cpp:244] This network produces output loss
I0428 20:10:52.521721 31958 net.cpp:257] Network initialization done.
I0428 20:10:52.522110 31958 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1145.prototxt
I0428 20:10:52.522155 31958 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:10:52.522279 31958 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:10:52.522358 31958 layer_factory.hpp:77] Creating layer mnist
I0428 20:10:52.522416 31958 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:10:52.522428 31958 net.cpp:86] Creating Layer mnist
I0428 20:10:52.522434 31958 net.cpp:382] mnist -> data
I0428 20:10:52.522441 31958 net.cpp:382] mnist -> label
I0428 20:10:52.522526 31958 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:10:52.524529 31958 net.cpp:124] Setting up mnist
I0428 20:10:52.524572 31958 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:10:52.524579 31958 net.cpp:131] Top shape: 100 (100)
I0428 20:10:52.524581 31958 net.cpp:139] Memory required for data: 314000
I0428 20:10:52.524585 31958 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:10:52.524611 31958 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:10:52.524616 31958 net.cpp:408] label_mnist_1_split <- label
I0428 20:10:52.524621 31958 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:10:52.524626 31958 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:10:52.524682 31958 net.cpp:124] Setting up label_mnist_1_split
I0428 20:10:52.524688 31958 net.cpp:131] Top shape: 100 (100)
I0428 20:10:52.524690 31958 net.cpp:131] Top shape: 100 (100)
I0428 20:10:52.524693 31958 net.cpp:139] Memory required for data: 314800
I0428 20:10:52.524695 31958 layer_factory.hpp:77] Creating layer conv0
I0428 20:10:52.524703 31958 net.cpp:86] Creating Layer conv0
I0428 20:10:52.524708 31958 net.cpp:408] conv0 <- data
I0428 20:10:52.524713 31958 net.cpp:382] conv0 -> conv0
I0428 20:10:52.526649 31958 net.cpp:124] Setting up conv0
I0428 20:10:52.526664 31958 net.cpp:131] Top shape: 100 25 24 24 (1440000)
I0428 20:10:52.526667 31958 net.cpp:139] Memory required for data: 6074800
I0428 20:10:52.526675 31958 layer_factory.hpp:77] Creating layer pool0
I0428 20:10:52.526684 31958 net.cpp:86] Creating Layer pool0
I0428 20:10:52.526686 31958 net.cpp:408] pool0 <- conv0
I0428 20:10:52.526690 31958 net.cpp:382] pool0 -> pool0
I0428 20:10:52.526728 31958 net.cpp:124] Setting up pool0
I0428 20:10:52.526734 31958 net.cpp:131] Top shape: 100 25 12 12 (360000)
I0428 20:10:52.526736 31958 net.cpp:139] Memory required for data: 7514800
I0428 20:10:52.526739 31958 layer_factory.hpp:77] Creating layer conv1
I0428 20:10:52.526748 31958 net.cpp:86] Creating Layer conv1
I0428 20:10:52.526762 31958 net.cpp:408] conv1 <- pool0
I0428 20:10:52.526768 31958 net.cpp:382] conv1 -> conv1
I0428 20:10:52.528666 31958 net.cpp:124] Setting up conv1
I0428 20:10:52.528695 31958 net.cpp:131] Top shape: 100 50 8 8 (320000)
I0428 20:10:52.528699 31958 net.cpp:139] Memory required for data: 8794800
I0428 20:10:52.528707 31958 layer_factory.hpp:77] Creating layer pool1
I0428 20:10:52.528729 31958 net.cpp:86] Creating Layer pool1
I0428 20:10:52.528733 31958 net.cpp:408] pool1 <- conv1
I0428 20:10:52.528738 31958 net.cpp:382] pool1 -> pool1
I0428 20:10:52.528791 31958 net.cpp:124] Setting up pool1
I0428 20:10:52.528797 31958 net.cpp:131] Top shape: 100 50 4 4 (80000)
I0428 20:10:52.528833 31958 net.cpp:139] Memory required for data: 9114800
I0428 20:10:52.528837 31958 layer_factory.hpp:77] Creating layer ip1
I0428 20:10:52.528844 31958 net.cpp:86] Creating Layer ip1
I0428 20:10:52.528848 31958 net.cpp:408] ip1 <- pool1
I0428 20:10:52.528852 31958 net.cpp:382] ip1 -> ip1
I0428 20:10:52.529005 31958 net.cpp:124] Setting up ip1
I0428 20:10:52.529013 31958 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:10:52.529026 31958 net.cpp:139] Memory required for data: 9118800
I0428 20:10:52.529042 31958 layer_factory.hpp:77] Creating layer relu1
I0428 20:10:52.529048 31958 net.cpp:86] Creating Layer relu1
I0428 20:10:52.529052 31958 net.cpp:408] relu1 <- ip1
I0428 20:10:52.529057 31958 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:10:52.529228 31958 net.cpp:124] Setting up relu1
I0428 20:10:52.529237 31958 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:10:52.529240 31958 net.cpp:139] Memory required for data: 9122800
I0428 20:10:52.529243 31958 layer_factory.hpp:77] Creating layer ip2
I0428 20:10:52.529249 31958 net.cpp:86] Creating Layer ip2
I0428 20:10:52.529259 31958 net.cpp:408] ip2 <- ip1
I0428 20:10:52.529279 31958 net.cpp:382] ip2 -> ip2
I0428 20:10:52.529371 31958 net.cpp:124] Setting up ip2
I0428 20:10:52.529376 31958 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:10:52.529379 31958 net.cpp:139] Memory required for data: 9126800
I0428 20:10:52.529384 31958 layer_factory.hpp:77] Creating layer relu2
I0428 20:10:52.529391 31958 net.cpp:86] Creating Layer relu2
I0428 20:10:52.529392 31958 net.cpp:408] relu2 <- ip2
I0428 20:10:52.529397 31958 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:10:52.529631 31958 net.cpp:124] Setting up relu2
I0428 20:10:52.529639 31958 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:10:52.529641 31958 net.cpp:139] Memory required for data: 9130800
I0428 20:10:52.529644 31958 layer_factory.hpp:77] Creating layer ip3
I0428 20:10:52.529650 31958 net.cpp:86] Creating Layer ip3
I0428 20:10:52.529654 31958 net.cpp:408] ip3 <- ip2
I0428 20:10:52.529659 31958 net.cpp:382] ip3 -> ip3
I0428 20:10:52.529798 31958 net.cpp:124] Setting up ip3
I0428 20:10:52.529803 31958 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:10:52.529808 31958 net.cpp:139] Memory required for data: 9134800
I0428 20:10:52.529814 31958 layer_factory.hpp:77] Creating layer relu3
I0428 20:10:52.529819 31958 net.cpp:86] Creating Layer relu3
I0428 20:10:52.529822 31958 net.cpp:408] relu3 <- ip3
I0428 20:10:52.529827 31958 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:10:52.530630 31958 net.cpp:124] Setting up relu3
I0428 20:10:52.530642 31958 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:10:52.530645 31958 net.cpp:139] Memory required for data: 9138800
I0428 20:10:52.530648 31958 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:10:52.530653 31958 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:10:52.530656 31958 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:10:52.530663 31958 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:10:52.530674 31958 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:10:52.530714 31958 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:10:52.530720 31958 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:10:52.530725 31958 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:10:52.530726 31958 net.cpp:139] Memory required for data: 9146800
I0428 20:10:52.530730 31958 layer_factory.hpp:77] Creating layer accuracy
I0428 20:10:52.530733 31958 net.cpp:86] Creating Layer accuracy
I0428 20:10:52.530737 31958 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:10:52.530740 31958 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:10:52.530745 31958 net.cpp:382] accuracy -> accuracy
I0428 20:10:52.530751 31958 net.cpp:124] Setting up accuracy
I0428 20:10:52.530755 31958 net.cpp:131] Top shape: (1)
I0428 20:10:52.530757 31958 net.cpp:139] Memory required for data: 9146804
I0428 20:10:52.530761 31958 layer_factory.hpp:77] Creating layer loss
I0428 20:10:52.530766 31958 net.cpp:86] Creating Layer loss
I0428 20:10:52.530768 31958 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:10:52.530772 31958 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:10:52.530776 31958 net.cpp:382] loss -> loss
I0428 20:10:52.530781 31958 layer_factory.hpp:77] Creating layer loss
I0428 20:10:52.531035 31958 net.cpp:124] Setting up loss
I0428 20:10:52.531044 31958 net.cpp:131] Top shape: (1)
I0428 20:10:52.531047 31958 net.cpp:134]     with loss weight 1
I0428 20:10:52.531070 31958 net.cpp:139] Memory required for data: 9146808
I0428 20:10:52.531080 31958 net.cpp:200] loss needs backward computation.
I0428 20:10:52.531083 31958 net.cpp:202] accuracy does not need backward computation.
I0428 20:10:52.531086 31958 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:10:52.531090 31958 net.cpp:200] relu3 needs backward computation.
I0428 20:10:52.531097 31958 net.cpp:200] ip3 needs backward computation.
I0428 20:10:52.531100 31958 net.cpp:200] relu2 needs backward computation.
I0428 20:10:52.531103 31958 net.cpp:200] ip2 needs backward computation.
I0428 20:10:52.531111 31958 net.cpp:200] relu1 needs backward computation.
I0428 20:10:52.531113 31958 net.cpp:200] ip1 needs backward computation.
I0428 20:10:52.531116 31958 net.cpp:200] pool1 needs backward computation.
I0428 20:10:52.531119 31958 net.cpp:200] conv1 needs backward computation.
I0428 20:10:52.531137 31958 net.cpp:200] pool0 needs backward computation.
I0428 20:10:52.531142 31958 net.cpp:200] conv0 needs backward computation.
I0428 20:10:52.531146 31958 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:10:52.531149 31958 net.cpp:202] mnist does not need backward computation.
I0428 20:10:52.531152 31958 net.cpp:244] This network produces output accuracy
I0428 20:10:52.531155 31958 net.cpp:244] This network produces output loss
I0428 20:10:52.531182 31958 net.cpp:257] Network initialization done.
I0428 20:10:52.531236 31958 solver.cpp:56] Solver scaffolding done.
I0428 20:10:52.531589 31958 caffe.cpp:248] Starting Optimization
I0428 20:10:52.531595 31958 solver.cpp:273] Solving LeNet
I0428 20:10:52.531597 31958 solver.cpp:274] Learning Rate Policy: inv
I0428 20:10:52.532429 31958 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:10:52.538179 31958 blocking_queue.cpp:49] Waiting for data
I0428 20:10:52.608536 31965 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:10:52.609246 31958 solver.cpp:398]     Test net output #0: accuracy = 0.0898
I0428 20:10:52.609268 31958 solver.cpp:398]     Test net output #1: loss = 2.32049 (* 1 = 2.32049 loss)
I0428 20:10:52.613101 31958 solver.cpp:219] Iteration 0 (0 iter/s, 0.0814746s/100 iters), loss = 2.33847
I0428 20:10:52.613140 31958 solver.cpp:238]     Train net output #0: loss = 2.33847 (* 1 = 2.33847 loss)
I0428 20:10:52.613165 31958 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:10:52.734640 31958 solver.cpp:219] Iteration 100 (823.027 iter/s, 0.121503s/100 iters), loss = 1.24696
I0428 20:10:52.734665 31958 solver.cpp:238]     Train net output #0: loss = 1.24696 (* 1 = 1.24696 loss)
I0428 20:10:52.734671 31958 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:10:52.854380 31958 solver.cpp:219] Iteration 200 (835.397 iter/s, 0.119704s/100 iters), loss = 0.881463
I0428 20:10:52.854405 31958 solver.cpp:238]     Train net output #0: loss = 0.881463 (* 1 = 0.881463 loss)
I0428 20:10:52.854411 31958 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:10:52.974263 31958 solver.cpp:219] Iteration 300 (834.404 iter/s, 0.119846s/100 iters), loss = 0.934999
I0428 20:10:52.974292 31958 solver.cpp:238]     Train net output #0: loss = 0.934999 (* 1 = 0.934999 loss)
I0428 20:10:52.974301 31958 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:10:53.093264 31958 solver.cpp:219] Iteration 400 (840.603 iter/s, 0.118962s/100 iters), loss = 0.950283
I0428 20:10:53.093288 31958 solver.cpp:238]     Train net output #0: loss = 0.950283 (* 1 = 0.950283 loss)
I0428 20:10:53.093294 31958 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:10:53.211526 31958 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:10:53.269451 31965 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:10:53.270262 31958 solver.cpp:398]     Test net output #0: accuracy = 0.6848
I0428 20:10:53.270283 31958 solver.cpp:398]     Test net output #1: loss = 0.80281 (* 1 = 0.80281 loss)
I0428 20:10:53.271574 31958 solver.cpp:219] Iteration 500 (560.94 iter/s, 0.178272s/100 iters), loss = 1.00425
I0428 20:10:53.271595 31958 solver.cpp:238]     Train net output #0: loss = 1.00425 (* 1 = 1.00425 loss)
I0428 20:10:53.271631 31958 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:10:53.397891 31958 solver.cpp:219] Iteration 600 (791.867 iter/s, 0.126284s/100 iters), loss = 0.467099
I0428 20:10:53.397917 31958 solver.cpp:238]     Train net output #0: loss = 0.467099 (* 1 = 0.467099 loss)
I0428 20:10:53.397923 31958 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:10:53.517101 31958 solver.cpp:219] Iteration 700 (839.117 iter/s, 0.119173s/100 iters), loss = 0.826852
I0428 20:10:53.517139 31958 solver.cpp:238]     Train net output #0: loss = 0.826852 (* 1 = 0.826852 loss)
I0428 20:10:53.517161 31958 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:10:53.642408 31958 solver.cpp:219] Iteration 800 (798.262 iter/s, 0.125272s/100 iters), loss = 0.874427
I0428 20:10:53.642447 31958 solver.cpp:238]     Train net output #0: loss = 0.874427 (* 1 = 0.874427 loss)
I0428 20:10:53.642453 31958 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:10:53.767609 31958 solver.cpp:219] Iteration 900 (798.942 iter/s, 0.125166s/100 iters), loss = 0.560173
I0428 20:10:53.767632 31958 solver.cpp:238]     Train net output #0: loss = 0.560173 (* 1 = 0.560173 loss)
I0428 20:10:53.767639 31958 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:10:53.807001 31964 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:10:53.884678 31958 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:10:53.886222 31958 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:10:53.887181 31958 solver.cpp:311] Iteration 1000, loss = 0.72782
I0428 20:10:53.887197 31958 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:10:53.962680 31965 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:10:53.963321 31958 solver.cpp:398]     Test net output #0: accuracy = 0.6914
I0428 20:10:53.963342 31958 solver.cpp:398]     Test net output #1: loss = 0.764932 (* 1 = 0.764932 loss)
I0428 20:10:53.963347 31958 solver.cpp:316] Optimization Done.
I0428 20:10:53.963349 31958 caffe.cpp:259] Optimization Done.
