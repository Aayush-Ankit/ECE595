I0428 20:33:00.616709  4229 caffe.cpp:218] Using GPUs 0
I0428 20:33:00.654067  4229 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:33:01.166546  4229 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1581.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:33:01.166687  4229 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1581.prototxt
I0428 20:33:01.167101  4229 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:33:01.167122  4229 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:33:01.167223  4229 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:33:01.167302  4229 layer_factory.hpp:77] Creating layer mnist
I0428 20:33:01.167397  4229 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:33:01.167419  4229 net.cpp:86] Creating Layer mnist
I0428 20:33:01.167428  4229 net.cpp:382] mnist -> data
I0428 20:33:01.167450  4229 net.cpp:382] mnist -> label
I0428 20:33:01.168535  4229 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:33:01.170999  4229 net.cpp:124] Setting up mnist
I0428 20:33:01.171016  4229 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:33:01.171023  4229 net.cpp:131] Top shape: 64 (64)
I0428 20:33:01.171027  4229 net.cpp:139] Memory required for data: 200960
I0428 20:33:01.171035  4229 layer_factory.hpp:77] Creating layer conv0
I0428 20:33:01.171051  4229 net.cpp:86] Creating Layer conv0
I0428 20:33:01.171082  4229 net.cpp:408] conv0 <- data
I0428 20:33:01.171097  4229 net.cpp:382] conv0 -> conv0
I0428 20:33:01.448242  4229 net.cpp:124] Setting up conv0
I0428 20:33:01.448269  4229 net.cpp:131] Top shape: 64 100 24 24 (3686400)
I0428 20:33:01.448273  4229 net.cpp:139] Memory required for data: 14946560
I0428 20:33:01.448288  4229 layer_factory.hpp:77] Creating layer pool0
I0428 20:33:01.448302  4229 net.cpp:86] Creating Layer pool0
I0428 20:33:01.448305  4229 net.cpp:408] pool0 <- conv0
I0428 20:33:01.448310  4229 net.cpp:382] pool0 -> pool0
I0428 20:33:01.448354  4229 net.cpp:124] Setting up pool0
I0428 20:33:01.448360  4229 net.cpp:131] Top shape: 64 100 12 12 (921600)
I0428 20:33:01.448362  4229 net.cpp:139] Memory required for data: 18632960
I0428 20:33:01.448365  4229 layer_factory.hpp:77] Creating layer conv1
I0428 20:33:01.448376  4229 net.cpp:86] Creating Layer conv1
I0428 20:33:01.448379  4229 net.cpp:408] conv1 <- pool0
I0428 20:33:01.448384  4229 net.cpp:382] conv1 -> conv1
I0428 20:33:01.450654  4229 net.cpp:124] Setting up conv1
I0428 20:33:01.450683  4229 net.cpp:131] Top shape: 64 25 8 8 (102400)
I0428 20:33:01.450686  4229 net.cpp:139] Memory required for data: 19042560
I0428 20:33:01.450695  4229 layer_factory.hpp:77] Creating layer pool1
I0428 20:33:01.450701  4229 net.cpp:86] Creating Layer pool1
I0428 20:33:01.450706  4229 net.cpp:408] pool1 <- conv1
I0428 20:33:01.450711  4229 net.cpp:382] pool1 -> pool1
I0428 20:33:01.450762  4229 net.cpp:124] Setting up pool1
I0428 20:33:01.450767  4229 net.cpp:131] Top shape: 64 25 4 4 (25600)
I0428 20:33:01.450770  4229 net.cpp:139] Memory required for data: 19144960
I0428 20:33:01.450773  4229 layer_factory.hpp:77] Creating layer ip1
I0428 20:33:01.450780  4229 net.cpp:86] Creating Layer ip1
I0428 20:33:01.450783  4229 net.cpp:408] ip1 <- pool1
I0428 20:33:01.450788  4229 net.cpp:382] ip1 -> ip1
I0428 20:33:01.451774  4229 net.cpp:124] Setting up ip1
I0428 20:33:01.451786  4229 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:33:01.451805  4229 net.cpp:139] Memory required for data: 19151360
I0428 20:33:01.451813  4229 layer_factory.hpp:77] Creating layer relu1
I0428 20:33:01.451819  4229 net.cpp:86] Creating Layer relu1
I0428 20:33:01.451823  4229 net.cpp:408] relu1 <- ip1
I0428 20:33:01.451828  4229 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:33:01.451990  4229 net.cpp:124] Setting up relu1
I0428 20:33:01.451999  4229 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:33:01.452003  4229 net.cpp:139] Memory required for data: 19157760
I0428 20:33:01.452005  4229 layer_factory.hpp:77] Creating layer ip2
I0428 20:33:01.452013  4229 net.cpp:86] Creating Layer ip2
I0428 20:33:01.452015  4229 net.cpp:408] ip2 <- ip1
I0428 20:33:01.452020  4229 net.cpp:382] ip2 -> ip2
I0428 20:33:01.452126  4229 net.cpp:124] Setting up ip2
I0428 20:33:01.452133  4229 net.cpp:131] Top shape: 64 10 (640)
I0428 20:33:01.452136  4229 net.cpp:139] Memory required for data: 19160320
I0428 20:33:01.452142  4229 layer_factory.hpp:77] Creating layer relu2
I0428 20:33:01.452147  4229 net.cpp:86] Creating Layer relu2
I0428 20:33:01.452150  4229 net.cpp:408] relu2 <- ip2
I0428 20:33:01.452154  4229 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:33:01.452922  4229 net.cpp:124] Setting up relu2
I0428 20:33:01.452935  4229 net.cpp:131] Top shape: 64 10 (640)
I0428 20:33:01.452955  4229 net.cpp:139] Memory required for data: 19162880
I0428 20:33:01.452958  4229 layer_factory.hpp:77] Creating layer ip3
I0428 20:33:01.452965  4229 net.cpp:86] Creating Layer ip3
I0428 20:33:01.452970  4229 net.cpp:408] ip3 <- ip2
I0428 20:33:01.452975  4229 net.cpp:382] ip3 -> ip3
I0428 20:33:01.453086  4229 net.cpp:124] Setting up ip3
I0428 20:33:01.453095  4229 net.cpp:131] Top shape: 64 10 (640)
I0428 20:33:01.453099  4229 net.cpp:139] Memory required for data: 19165440
I0428 20:33:01.453106  4229 layer_factory.hpp:77] Creating layer relu3
I0428 20:33:01.453110  4229 net.cpp:86] Creating Layer relu3
I0428 20:33:01.453114  4229 net.cpp:408] relu3 <- ip3
I0428 20:33:01.453119  4229 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:33:01.453279  4229 net.cpp:124] Setting up relu3
I0428 20:33:01.453289  4229 net.cpp:131] Top shape: 64 10 (640)
I0428 20:33:01.453291  4229 net.cpp:139] Memory required for data: 19168000
I0428 20:33:01.453310  4229 layer_factory.hpp:77] Creating layer loss
I0428 20:33:01.453315  4229 net.cpp:86] Creating Layer loss
I0428 20:33:01.453318  4229 net.cpp:408] loss <- ip3
I0428 20:33:01.453322  4229 net.cpp:408] loss <- label
I0428 20:33:01.453327  4229 net.cpp:382] loss -> loss
I0428 20:33:01.453341  4229 layer_factory.hpp:77] Creating layer loss
I0428 20:33:01.453580  4229 net.cpp:124] Setting up loss
I0428 20:33:01.453590  4229 net.cpp:131] Top shape: (1)
I0428 20:33:01.453593  4229 net.cpp:134]     with loss weight 1
I0428 20:33:01.453608  4229 net.cpp:139] Memory required for data: 19168004
I0428 20:33:01.453610  4229 net.cpp:200] loss needs backward computation.
I0428 20:33:01.453614  4229 net.cpp:200] relu3 needs backward computation.
I0428 20:33:01.453618  4229 net.cpp:200] ip3 needs backward computation.
I0428 20:33:01.453619  4229 net.cpp:200] relu2 needs backward computation.
I0428 20:33:01.453622  4229 net.cpp:200] ip2 needs backward computation.
I0428 20:33:01.453625  4229 net.cpp:200] relu1 needs backward computation.
I0428 20:33:01.453629  4229 net.cpp:200] ip1 needs backward computation.
I0428 20:33:01.453631  4229 net.cpp:200] pool1 needs backward computation.
I0428 20:33:01.453634  4229 net.cpp:200] conv1 needs backward computation.
I0428 20:33:01.453636  4229 net.cpp:200] pool0 needs backward computation.
I0428 20:33:01.453639  4229 net.cpp:200] conv0 needs backward computation.
I0428 20:33:01.453644  4229 net.cpp:202] mnist does not need backward computation.
I0428 20:33:01.453645  4229 net.cpp:244] This network produces output loss
I0428 20:33:01.453655  4229 net.cpp:257] Network initialization done.
I0428 20:33:01.454010  4229 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1581.prototxt
I0428 20:33:01.454051  4229 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:33:01.454144  4229 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:33:01.454231  4229 layer_factory.hpp:77] Creating layer mnist
I0428 20:33:01.454277  4229 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:33:01.454288  4229 net.cpp:86] Creating Layer mnist
I0428 20:33:01.454293  4229 net.cpp:382] mnist -> data
I0428 20:33:01.454300  4229 net.cpp:382] mnist -> label
I0428 20:33:01.454396  4229 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:33:01.456408  4229 net.cpp:124] Setting up mnist
I0428 20:33:01.456436  4229 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:33:01.456441  4229 net.cpp:131] Top shape: 100 (100)
I0428 20:33:01.456444  4229 net.cpp:139] Memory required for data: 314000
I0428 20:33:01.456449  4229 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:33:01.456454  4229 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:33:01.456459  4229 net.cpp:408] label_mnist_1_split <- label
I0428 20:33:01.456462  4229 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:33:01.456468  4229 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:33:01.456503  4229 net.cpp:124] Setting up label_mnist_1_split
I0428 20:33:01.456509  4229 net.cpp:131] Top shape: 100 (100)
I0428 20:33:01.456512  4229 net.cpp:131] Top shape: 100 (100)
I0428 20:33:01.456516  4229 net.cpp:139] Memory required for data: 314800
I0428 20:33:01.456518  4229 layer_factory.hpp:77] Creating layer conv0
I0428 20:33:01.456526  4229 net.cpp:86] Creating Layer conv0
I0428 20:33:01.456529  4229 net.cpp:408] conv0 <- data
I0428 20:33:01.456533  4229 net.cpp:382] conv0 -> conv0
I0428 20:33:01.458317  4229 net.cpp:124] Setting up conv0
I0428 20:33:01.458331  4229 net.cpp:131] Top shape: 100 100 24 24 (5760000)
I0428 20:33:01.458335  4229 net.cpp:139] Memory required for data: 23354800
I0428 20:33:01.458344  4229 layer_factory.hpp:77] Creating layer pool0
I0428 20:33:01.458350  4229 net.cpp:86] Creating Layer pool0
I0428 20:33:01.458353  4229 net.cpp:408] pool0 <- conv0
I0428 20:33:01.458359  4229 net.cpp:382] pool0 -> pool0
I0428 20:33:01.458392  4229 net.cpp:124] Setting up pool0
I0428 20:33:01.458398  4229 net.cpp:131] Top shape: 100 100 12 12 (1440000)
I0428 20:33:01.458401  4229 net.cpp:139] Memory required for data: 29114800
I0428 20:33:01.458403  4229 layer_factory.hpp:77] Creating layer conv1
I0428 20:33:01.458412  4229 net.cpp:86] Creating Layer conv1
I0428 20:33:01.458415  4229 net.cpp:408] conv1 <- pool0
I0428 20:33:01.458420  4229 net.cpp:382] conv1 -> conv1
I0428 20:33:01.460330  4229 net.cpp:124] Setting up conv1
I0428 20:33:01.460343  4229 net.cpp:131] Top shape: 100 25 8 8 (160000)
I0428 20:33:01.460347  4229 net.cpp:139] Memory required for data: 29754800
I0428 20:33:01.460355  4229 layer_factory.hpp:77] Creating layer pool1
I0428 20:33:01.460362  4229 net.cpp:86] Creating Layer pool1
I0428 20:33:01.460366  4229 net.cpp:408] pool1 <- conv1
I0428 20:33:01.460371  4229 net.cpp:382] pool1 -> pool1
I0428 20:33:01.460407  4229 net.cpp:124] Setting up pool1
I0428 20:33:01.460412  4229 net.cpp:131] Top shape: 100 25 4 4 (40000)
I0428 20:33:01.460415  4229 net.cpp:139] Memory required for data: 29914800
I0428 20:33:01.460418  4229 layer_factory.hpp:77] Creating layer ip1
I0428 20:33:01.460424  4229 net.cpp:86] Creating Layer ip1
I0428 20:33:01.460427  4229 net.cpp:408] ip1 <- pool1
I0428 20:33:01.460443  4229 net.cpp:382] ip1 -> ip1
I0428 20:33:01.460602  4229 net.cpp:124] Setting up ip1
I0428 20:33:01.460621  4229 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:33:01.460624  4229 net.cpp:139] Memory required for data: 29924800
I0428 20:33:01.460633  4229 layer_factory.hpp:77] Creating layer relu1
I0428 20:33:01.460636  4229 net.cpp:86] Creating Layer relu1
I0428 20:33:01.460639  4229 net.cpp:408] relu1 <- ip1
I0428 20:33:01.460644  4229 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:33:01.460804  4229 net.cpp:124] Setting up relu1
I0428 20:33:01.460819  4229 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:33:01.460822  4229 net.cpp:139] Memory required for data: 29934800
I0428 20:33:01.460826  4229 layer_factory.hpp:77] Creating layer ip2
I0428 20:33:01.460834  4229 net.cpp:86] Creating Layer ip2
I0428 20:33:01.460836  4229 net.cpp:408] ip2 <- ip1
I0428 20:33:01.460841  4229 net.cpp:382] ip2 -> ip2
I0428 20:33:01.460949  4229 net.cpp:124] Setting up ip2
I0428 20:33:01.460958  4229 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:33:01.460960  4229 net.cpp:139] Memory required for data: 29938800
I0428 20:33:01.460965  4229 layer_factory.hpp:77] Creating layer relu2
I0428 20:33:01.460970  4229 net.cpp:86] Creating Layer relu2
I0428 20:33:01.460973  4229 net.cpp:408] relu2 <- ip2
I0428 20:33:01.460978  4229 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:33:01.461174  4229 net.cpp:124] Setting up relu2
I0428 20:33:01.461182  4229 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:33:01.461185  4229 net.cpp:139] Memory required for data: 29942800
I0428 20:33:01.461189  4229 layer_factory.hpp:77] Creating layer ip3
I0428 20:33:01.461194  4229 net.cpp:86] Creating Layer ip3
I0428 20:33:01.461197  4229 net.cpp:408] ip3 <- ip2
I0428 20:33:01.461202  4229 net.cpp:382] ip3 -> ip3
I0428 20:33:01.461339  4229 net.cpp:124] Setting up ip3
I0428 20:33:01.461347  4229 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:33:01.461350  4229 net.cpp:139] Memory required for data: 29946800
I0428 20:33:01.461359  4229 layer_factory.hpp:77] Creating layer relu3
I0428 20:33:01.461362  4229 net.cpp:86] Creating Layer relu3
I0428 20:33:01.461365  4229 net.cpp:408] relu3 <- ip3
I0428 20:33:01.461371  4229 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:33:01.462180  4229 net.cpp:124] Setting up relu3
I0428 20:33:01.462193  4229 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:33:01.462196  4229 net.cpp:139] Memory required for data: 29950800
I0428 20:33:01.462200  4229 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:33:01.462211  4229 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:33:01.462215  4229 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:33:01.462227  4229 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:33:01.462234  4229 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:33:01.462271  4229 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:33:01.462277  4229 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:33:01.462281  4229 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:33:01.462285  4229 net.cpp:139] Memory required for data: 29958800
I0428 20:33:01.462287  4229 layer_factory.hpp:77] Creating layer accuracy
I0428 20:33:01.462293  4229 net.cpp:86] Creating Layer accuracy
I0428 20:33:01.462296  4229 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:33:01.462301  4229 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:33:01.462306  4229 net.cpp:382] accuracy -> accuracy
I0428 20:33:01.462312  4229 net.cpp:124] Setting up accuracy
I0428 20:33:01.462316  4229 net.cpp:131] Top shape: (1)
I0428 20:33:01.462319  4229 net.cpp:139] Memory required for data: 29958804
I0428 20:33:01.462322  4229 layer_factory.hpp:77] Creating layer loss
I0428 20:33:01.462332  4229 net.cpp:86] Creating Layer loss
I0428 20:33:01.462335  4229 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:33:01.462339  4229 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:33:01.462343  4229 net.cpp:382] loss -> loss
I0428 20:33:01.462348  4229 layer_factory.hpp:77] Creating layer loss
I0428 20:33:01.462600  4229 net.cpp:124] Setting up loss
I0428 20:33:01.462610  4229 net.cpp:131] Top shape: (1)
I0428 20:33:01.462613  4229 net.cpp:134]     with loss weight 1
I0428 20:33:01.462630  4229 net.cpp:139] Memory required for data: 29958808
I0428 20:33:01.462633  4229 net.cpp:200] loss needs backward computation.
I0428 20:33:01.462644  4229 net.cpp:202] accuracy does not need backward computation.
I0428 20:33:01.462647  4229 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:33:01.462651  4229 net.cpp:200] relu3 needs backward computation.
I0428 20:33:01.462653  4229 net.cpp:200] ip3 needs backward computation.
I0428 20:33:01.462656  4229 net.cpp:200] relu2 needs backward computation.
I0428 20:33:01.462664  4229 net.cpp:200] ip2 needs backward computation.
I0428 20:33:01.462667  4229 net.cpp:200] relu1 needs backward computation.
I0428 20:33:01.462669  4229 net.cpp:200] ip1 needs backward computation.
I0428 20:33:01.462693  4229 net.cpp:200] pool1 needs backward computation.
I0428 20:33:01.462697  4229 net.cpp:200] conv1 needs backward computation.
I0428 20:33:01.462699  4229 net.cpp:200] pool0 needs backward computation.
I0428 20:33:01.462719  4229 net.cpp:200] conv0 needs backward computation.
I0428 20:33:01.462723  4229 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:33:01.462733  4229 net.cpp:202] mnist does not need backward computation.
I0428 20:33:01.462734  4229 net.cpp:244] This network produces output accuracy
I0428 20:33:01.462738  4229 net.cpp:244] This network produces output loss
I0428 20:33:01.462749  4229 net.cpp:257] Network initialization done.
I0428 20:33:01.462801  4229 solver.cpp:56] Solver scaffolding done.
I0428 20:33:01.463179  4229 caffe.cpp:248] Starting Optimization
I0428 20:33:01.463186  4229 solver.cpp:273] Solving LeNet
I0428 20:33:01.463189  4229 solver.cpp:274] Learning Rate Policy: inv
I0428 20:33:01.463395  4229 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:33:01.561070  4236 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:33:01.563477  4229 solver.cpp:398]     Test net output #0: accuracy = 0.1282
I0428 20:33:01.563495  4229 solver.cpp:398]     Test net output #1: loss = 2.31648 (* 1 = 2.31648 loss)
I0428 20:33:01.567829  4229 solver.cpp:219] Iteration 0 (-7.76319e-43 iter/s, 0.104613s/100 iters), loss = 2.30976
I0428 20:33:01.567853  4229 solver.cpp:238]     Train net output #0: loss = 2.30976 (* 1 = 2.30976 loss)
I0428 20:33:01.567864  4229 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:33:01.783210  4229 solver.cpp:219] Iteration 100 (464.395 iter/s, 0.215334s/100 iters), loss = 0.360516
I0428 20:33:01.783246  4229 solver.cpp:238]     Train net output #0: loss = 0.360516 (* 1 = 0.360516 loss)
I0428 20:33:01.783254  4229 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:33:02.002394  4229 solver.cpp:219] Iteration 200 (456.357 iter/s, 0.219127s/100 iters), loss = 0.211668
I0428 20:33:02.002437  4229 solver.cpp:238]     Train net output #0: loss = 0.211668 (* 1 = 0.211668 loss)
I0428 20:33:02.002449  4229 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:33:02.221824  4229 solver.cpp:219] Iteration 300 (455.846 iter/s, 0.219373s/100 iters), loss = 0.26495
I0428 20:33:02.221861  4229 solver.cpp:238]     Train net output #0: loss = 0.26495 (* 1 = 0.26495 loss)
I0428 20:33:02.221870  4229 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:33:02.433791  4229 solver.cpp:219] Iteration 400 (471.888 iter/s, 0.211915s/100 iters), loss = 0.139207
I0428 20:33:02.433826  4229 solver.cpp:238]     Train net output #0: loss = 0.139207 (* 1 = 0.139207 loss)
I0428 20:33:02.433835  4229 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:33:02.644410  4229 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:33:02.745916  4236 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:33:02.748754  4229 solver.cpp:398]     Test net output #0: accuracy = 0.9591
I0428 20:33:02.748780  4229 solver.cpp:398]     Test net output #1: loss = 0.125881 (* 1 = 0.125881 loss)
I0428 20:33:02.750839  4229 solver.cpp:219] Iteration 500 (315.465 iter/s, 0.316993s/100 iters), loss = 0.143975
I0428 20:33:02.750867  4229 solver.cpp:238]     Train net output #0: loss = 0.143975 (* 1 = 0.143975 loss)
I0428 20:33:02.750892  4229 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:33:02.965661  4229 solver.cpp:219] Iteration 600 (465.604 iter/s, 0.214775s/100 iters), loss = 0.0980611
I0428 20:33:02.965695  4229 solver.cpp:238]     Train net output #0: loss = 0.098061 (* 1 = 0.098061 loss)
I0428 20:33:02.965704  4229 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:33:03.185866  4229 solver.cpp:219] Iteration 700 (454.226 iter/s, 0.220155s/100 iters), loss = 0.123307
I0428 20:33:03.185905  4229 solver.cpp:238]     Train net output #0: loss = 0.123307 (* 1 = 0.123307 loss)
I0428 20:33:03.185920  4229 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:33:03.398736  4229 solver.cpp:219] Iteration 800 (469.896 iter/s, 0.212813s/100 iters), loss = 0.386996
I0428 20:33:03.398771  4229 solver.cpp:238]     Train net output #0: loss = 0.386996 (* 1 = 0.386996 loss)
I0428 20:33:03.398778  4229 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:33:03.610888  4229 solver.cpp:219] Iteration 900 (471.48 iter/s, 0.212098s/100 iters), loss = 0.14999
I0428 20:33:03.610927  4229 solver.cpp:238]     Train net output #0: loss = 0.149989 (* 1 = 0.149989 loss)
I0428 20:33:03.610935  4229 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:33:03.681310  4235 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:33:03.820075  4229 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:33:03.822764  4229 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:33:03.824458  4229 solver.cpp:311] Iteration 1000, loss = 0.144581
I0428 20:33:03.824494  4229 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:33:03.838691  4229 blocking_queue.cpp:49] Waiting for data
I0428 20:33:03.942468  4236 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:33:03.944350  4229 solver.cpp:398]     Test net output #0: accuracy = 0.9682
I0428 20:33:03.944381  4229 solver.cpp:398]     Test net output #1: loss = 0.0927873 (* 1 = 0.0927873 loss)
I0428 20:33:03.944388  4229 solver.cpp:316] Optimization Done.
I0428 20:33:03.944392  4229 caffe.cpp:259] Optimization Done.
