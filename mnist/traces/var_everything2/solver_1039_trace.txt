I0428 20:06:50.800606 30968 caffe.cpp:218] Using GPUs 0
I0428 20:06:50.836360 30968 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:06:51.288079 30968 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1039.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:06:51.288209 30968 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1039.prototxt
I0428 20:06:51.288583 30968 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:06:51.288612 30968 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:06:51.288686 30968 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:06:51.288743 30968 layer_factory.hpp:77] Creating layer mnist
I0428 20:06:51.288842 30968 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:06:51.288862 30968 net.cpp:86] Creating Layer mnist
I0428 20:06:51.288869 30968 net.cpp:382] mnist -> data
I0428 20:06:51.288888 30968 net.cpp:382] mnist -> label
I0428 20:06:51.289851 30968 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:06:51.292057 30968 net.cpp:124] Setting up mnist
I0428 20:06:51.292104 30968 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:06:51.292109 30968 net.cpp:131] Top shape: 64 (64)
I0428 20:06:51.292111 30968 net.cpp:139] Memory required for data: 200960
I0428 20:06:51.292116 30968 layer_factory.hpp:77] Creating layer conv0
I0428 20:06:51.292131 30968 net.cpp:86] Creating Layer conv0
I0428 20:06:51.292148 30968 net.cpp:408] conv0 <- data
I0428 20:06:51.292158 30968 net.cpp:382] conv0 -> conv0
I0428 20:06:51.519243 30968 net.cpp:124] Setting up conv0
I0428 20:06:51.519269 30968 net.cpp:131] Top shape: 64 25 24 24 (921600)
I0428 20:06:51.519273 30968 net.cpp:139] Memory required for data: 3887360
I0428 20:06:51.519287 30968 layer_factory.hpp:77] Creating layer pool0
I0428 20:06:51.519299 30968 net.cpp:86] Creating Layer pool0
I0428 20:06:51.519304 30968 net.cpp:408] pool0 <- conv0
I0428 20:06:51.519309 30968 net.cpp:382] pool0 -> pool0
I0428 20:06:51.519366 30968 net.cpp:124] Setting up pool0
I0428 20:06:51.519371 30968 net.cpp:131] Top shape: 64 25 12 12 (230400)
I0428 20:06:51.519373 30968 net.cpp:139] Memory required for data: 4808960
I0428 20:06:51.519376 30968 layer_factory.hpp:77] Creating layer conv1
I0428 20:06:51.519387 30968 net.cpp:86] Creating Layer conv1
I0428 20:06:51.519390 30968 net.cpp:408] conv1 <- pool0
I0428 20:06:51.519394 30968 net.cpp:382] conv1 -> conv1
I0428 20:06:51.522176 30968 net.cpp:124] Setting up conv1
I0428 20:06:51.522205 30968 net.cpp:131] Top shape: 64 5 8 8 (20480)
I0428 20:06:51.522208 30968 net.cpp:139] Memory required for data: 4890880
I0428 20:06:51.522217 30968 layer_factory.hpp:77] Creating layer pool1
I0428 20:06:51.522224 30968 net.cpp:86] Creating Layer pool1
I0428 20:06:51.522228 30968 net.cpp:408] pool1 <- conv1
I0428 20:06:51.522233 30968 net.cpp:382] pool1 -> pool1
I0428 20:06:51.522285 30968 net.cpp:124] Setting up pool1
I0428 20:06:51.522290 30968 net.cpp:131] Top shape: 64 5 4 4 (5120)
I0428 20:06:51.522294 30968 net.cpp:139] Memory required for data: 4911360
I0428 20:06:51.522296 30968 layer_factory.hpp:77] Creating layer ip1
I0428 20:06:51.522303 30968 net.cpp:86] Creating Layer ip1
I0428 20:06:51.522306 30968 net.cpp:408] ip1 <- pool1
I0428 20:06:51.522310 30968 net.cpp:382] ip1 -> ip1
I0428 20:06:51.522429 30968 net.cpp:124] Setting up ip1
I0428 20:06:51.522438 30968 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:06:51.522440 30968 net.cpp:139] Memory required for data: 4917760
I0428 20:06:51.522447 30968 layer_factory.hpp:77] Creating layer relu1
I0428 20:06:51.522454 30968 net.cpp:86] Creating Layer relu1
I0428 20:06:51.522456 30968 net.cpp:408] relu1 <- ip1
I0428 20:06:51.522460 30968 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:06:51.522616 30968 net.cpp:124] Setting up relu1
I0428 20:06:51.522624 30968 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:06:51.522627 30968 net.cpp:139] Memory required for data: 4924160
I0428 20:06:51.522630 30968 layer_factory.hpp:77] Creating layer ip2
I0428 20:06:51.522637 30968 net.cpp:86] Creating Layer ip2
I0428 20:06:51.522640 30968 net.cpp:408] ip2 <- ip1
I0428 20:06:51.522644 30968 net.cpp:382] ip2 -> ip2
I0428 20:06:51.522753 30968 net.cpp:124] Setting up ip2
I0428 20:06:51.522759 30968 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:06:51.522763 30968 net.cpp:139] Memory required for data: 4930560
I0428 20:06:51.522768 30968 layer_factory.hpp:77] Creating layer relu2
I0428 20:06:51.522773 30968 net.cpp:86] Creating Layer relu2
I0428 20:06:51.522776 30968 net.cpp:408] relu2 <- ip2
I0428 20:06:51.522780 30968 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:06:51.523588 30968 net.cpp:124] Setting up relu2
I0428 20:06:51.523602 30968 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:06:51.523620 30968 net.cpp:139] Memory required for data: 4936960
I0428 20:06:51.523623 30968 layer_factory.hpp:77] Creating layer ip3
I0428 20:06:51.523630 30968 net.cpp:86] Creating Layer ip3
I0428 20:06:51.523633 30968 net.cpp:408] ip3 <- ip2
I0428 20:06:51.523638 30968 net.cpp:382] ip3 -> ip3
I0428 20:06:51.523731 30968 net.cpp:124] Setting up ip3
I0428 20:06:51.523738 30968 net.cpp:131] Top shape: 64 10 (640)
I0428 20:06:51.523741 30968 net.cpp:139] Memory required for data: 4939520
I0428 20:06:51.523748 30968 layer_factory.hpp:77] Creating layer relu3
I0428 20:06:51.523753 30968 net.cpp:86] Creating Layer relu3
I0428 20:06:51.523756 30968 net.cpp:408] relu3 <- ip3
I0428 20:06:51.523761 30968 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:06:51.523914 30968 net.cpp:124] Setting up relu3
I0428 20:06:51.523922 30968 net.cpp:131] Top shape: 64 10 (640)
I0428 20:06:51.523926 30968 net.cpp:139] Memory required for data: 4942080
I0428 20:06:51.523929 30968 layer_factory.hpp:77] Creating layer loss
I0428 20:06:51.523934 30968 net.cpp:86] Creating Layer loss
I0428 20:06:51.523937 30968 net.cpp:408] loss <- ip3
I0428 20:06:51.523941 30968 net.cpp:408] loss <- label
I0428 20:06:51.523947 30968 net.cpp:382] loss -> loss
I0428 20:06:51.523963 30968 layer_factory.hpp:77] Creating layer loss
I0428 20:06:51.524180 30968 net.cpp:124] Setting up loss
I0428 20:06:51.524189 30968 net.cpp:131] Top shape: (1)
I0428 20:06:51.524194 30968 net.cpp:134]     with loss weight 1
I0428 20:06:51.524207 30968 net.cpp:139] Memory required for data: 4942084
I0428 20:06:51.524210 30968 net.cpp:200] loss needs backward computation.
I0428 20:06:51.524214 30968 net.cpp:200] relu3 needs backward computation.
I0428 20:06:51.524216 30968 net.cpp:200] ip3 needs backward computation.
I0428 20:06:51.524219 30968 net.cpp:200] relu2 needs backward computation.
I0428 20:06:51.524222 30968 net.cpp:200] ip2 needs backward computation.
I0428 20:06:51.524225 30968 net.cpp:200] relu1 needs backward computation.
I0428 20:06:51.524227 30968 net.cpp:200] ip1 needs backward computation.
I0428 20:06:51.524230 30968 net.cpp:200] pool1 needs backward computation.
I0428 20:06:51.524233 30968 net.cpp:200] conv1 needs backward computation.
I0428 20:06:51.524236 30968 net.cpp:200] pool0 needs backward computation.
I0428 20:06:51.524240 30968 net.cpp:200] conv0 needs backward computation.
I0428 20:06:51.524242 30968 net.cpp:202] mnist does not need backward computation.
I0428 20:06:51.524245 30968 net.cpp:244] This network produces output loss
I0428 20:06:51.524255 30968 net.cpp:257] Network initialization done.
I0428 20:06:51.524739 30968 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1039.prototxt
I0428 20:06:51.524796 30968 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:06:51.524912 30968 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:06:51.525007 30968 layer_factory.hpp:77] Creating layer mnist
I0428 20:06:51.525051 30968 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:06:51.525063 30968 net.cpp:86] Creating Layer mnist
I0428 20:06:51.525068 30968 net.cpp:382] mnist -> data
I0428 20:06:51.525076 30968 net.cpp:382] mnist -> label
I0428 20:06:51.525159 30968 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:06:51.527112 30968 net.cpp:124] Setting up mnist
I0428 20:06:51.527140 30968 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:06:51.527160 30968 net.cpp:131] Top shape: 100 (100)
I0428 20:06:51.527163 30968 net.cpp:139] Memory required for data: 314000
I0428 20:06:51.527166 30968 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:06:51.527187 30968 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:06:51.527191 30968 net.cpp:408] label_mnist_1_split <- label
I0428 20:06:51.527195 30968 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:06:51.527202 30968 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:06:51.527247 30968 net.cpp:124] Setting up label_mnist_1_split
I0428 20:06:51.527253 30968 net.cpp:131] Top shape: 100 (100)
I0428 20:06:51.527256 30968 net.cpp:131] Top shape: 100 (100)
I0428 20:06:51.527259 30968 net.cpp:139] Memory required for data: 314800
I0428 20:06:51.527262 30968 layer_factory.hpp:77] Creating layer conv0
I0428 20:06:51.527269 30968 net.cpp:86] Creating Layer conv0
I0428 20:06:51.527273 30968 net.cpp:408] conv0 <- data
I0428 20:06:51.527281 30968 net.cpp:382] conv0 -> conv0
I0428 20:06:51.529031 30968 net.cpp:124] Setting up conv0
I0428 20:06:51.529062 30968 net.cpp:131] Top shape: 100 25 24 24 (1440000)
I0428 20:06:51.529065 30968 net.cpp:139] Memory required for data: 6074800
I0428 20:06:51.529074 30968 layer_factory.hpp:77] Creating layer pool0
I0428 20:06:51.529083 30968 net.cpp:86] Creating Layer pool0
I0428 20:06:51.529085 30968 net.cpp:408] pool0 <- conv0
I0428 20:06:51.529090 30968 net.cpp:382] pool0 -> pool0
I0428 20:06:51.529157 30968 net.cpp:124] Setting up pool0
I0428 20:06:51.529186 30968 net.cpp:131] Top shape: 100 25 12 12 (360000)
I0428 20:06:51.529189 30968 net.cpp:139] Memory required for data: 7514800
I0428 20:06:51.529192 30968 layer_factory.hpp:77] Creating layer conv1
I0428 20:06:51.529201 30968 net.cpp:86] Creating Layer conv1
I0428 20:06:51.529203 30968 net.cpp:408] conv1 <- pool0
I0428 20:06:51.529208 30968 net.cpp:382] conv1 -> conv1
I0428 20:06:51.531267 30968 net.cpp:124] Setting up conv1
I0428 20:06:51.531296 30968 net.cpp:131] Top shape: 100 5 8 8 (32000)
I0428 20:06:51.531299 30968 net.cpp:139] Memory required for data: 7642800
I0428 20:06:51.531308 30968 layer_factory.hpp:77] Creating layer pool1
I0428 20:06:51.531316 30968 net.cpp:86] Creating Layer pool1
I0428 20:06:51.531318 30968 net.cpp:408] pool1 <- conv1
I0428 20:06:51.531323 30968 net.cpp:382] pool1 -> pool1
I0428 20:06:51.531379 30968 net.cpp:124] Setting up pool1
I0428 20:06:51.531385 30968 net.cpp:131] Top shape: 100 5 4 4 (8000)
I0428 20:06:51.531388 30968 net.cpp:139] Memory required for data: 7674800
I0428 20:06:51.531391 30968 layer_factory.hpp:77] Creating layer ip1
I0428 20:06:51.531399 30968 net.cpp:86] Creating Layer ip1
I0428 20:06:51.531402 30968 net.cpp:408] ip1 <- pool1
I0428 20:06:51.531407 30968 net.cpp:382] ip1 -> ip1
I0428 20:06:51.531527 30968 net.cpp:124] Setting up ip1
I0428 20:06:51.531534 30968 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:06:51.531548 30968 net.cpp:139] Memory required for data: 7684800
I0428 20:06:51.531554 30968 layer_factory.hpp:77] Creating layer relu1
I0428 20:06:51.531561 30968 net.cpp:86] Creating Layer relu1
I0428 20:06:51.531564 30968 net.cpp:408] relu1 <- ip1
I0428 20:06:51.531569 30968 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:06:51.531844 30968 net.cpp:124] Setting up relu1
I0428 20:06:51.531857 30968 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:06:51.531865 30968 net.cpp:139] Memory required for data: 7694800
I0428 20:06:51.531868 30968 layer_factory.hpp:77] Creating layer ip2
I0428 20:06:51.531878 30968 net.cpp:86] Creating Layer ip2
I0428 20:06:51.531882 30968 net.cpp:408] ip2 <- ip1
I0428 20:06:51.531888 30968 net.cpp:382] ip2 -> ip2
I0428 20:06:51.531992 30968 net.cpp:124] Setting up ip2
I0428 20:06:51.531999 30968 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:06:51.532003 30968 net.cpp:139] Memory required for data: 7704800
I0428 20:06:51.532008 30968 layer_factory.hpp:77] Creating layer relu2
I0428 20:06:51.532013 30968 net.cpp:86] Creating Layer relu2
I0428 20:06:51.532016 30968 net.cpp:408] relu2 <- ip2
I0428 20:06:51.532021 30968 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:06:51.532171 30968 net.cpp:124] Setting up relu2
I0428 20:06:51.532178 30968 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:06:51.532181 30968 net.cpp:139] Memory required for data: 7714800
I0428 20:06:51.532186 30968 layer_factory.hpp:77] Creating layer ip3
I0428 20:06:51.532191 30968 net.cpp:86] Creating Layer ip3
I0428 20:06:51.532193 30968 net.cpp:408] ip3 <- ip2
I0428 20:06:51.532199 30968 net.cpp:382] ip3 -> ip3
I0428 20:06:51.532307 30968 net.cpp:124] Setting up ip3
I0428 20:06:51.532320 30968 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:06:51.532325 30968 net.cpp:139] Memory required for data: 7718800
I0428 20:06:51.532336 30968 layer_factory.hpp:77] Creating layer relu3
I0428 20:06:51.532343 30968 net.cpp:86] Creating Layer relu3
I0428 20:06:51.532348 30968 net.cpp:408] relu3 <- ip3
I0428 20:06:51.532359 30968 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:06:51.533295 30968 net.cpp:124] Setting up relu3
I0428 20:06:51.533308 30968 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:06:51.533329 30968 net.cpp:139] Memory required for data: 7722800
I0428 20:06:51.533332 30968 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:06:51.533337 30968 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:06:51.533341 30968 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:06:51.533347 30968 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:06:51.533354 30968 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:06:51.533432 30968 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:06:51.533440 30968 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:06:51.533443 30968 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:06:51.533447 30968 net.cpp:139] Memory required for data: 7730800
I0428 20:06:51.533449 30968 layer_factory.hpp:77] Creating layer accuracy
I0428 20:06:51.533457 30968 net.cpp:86] Creating Layer accuracy
I0428 20:06:51.533459 30968 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:06:51.533463 30968 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:06:51.533468 30968 net.cpp:382] accuracy -> accuracy
I0428 20:06:51.533475 30968 net.cpp:124] Setting up accuracy
I0428 20:06:51.533479 30968 net.cpp:131] Top shape: (1)
I0428 20:06:51.533483 30968 net.cpp:139] Memory required for data: 7730804
I0428 20:06:51.533485 30968 layer_factory.hpp:77] Creating layer loss
I0428 20:06:51.533489 30968 net.cpp:86] Creating Layer loss
I0428 20:06:51.533493 30968 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:06:51.533495 30968 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:06:51.533500 30968 net.cpp:382] loss -> loss
I0428 20:06:51.533505 30968 layer_factory.hpp:77] Creating layer loss
I0428 20:06:51.533737 30968 net.cpp:124] Setting up loss
I0428 20:06:51.533747 30968 net.cpp:131] Top shape: (1)
I0428 20:06:51.533751 30968 net.cpp:134]     with loss weight 1
I0428 20:06:51.533757 30968 net.cpp:139] Memory required for data: 7730808
I0428 20:06:51.533769 30968 net.cpp:200] loss needs backward computation.
I0428 20:06:51.533776 30968 net.cpp:202] accuracy does not need backward computation.
I0428 20:06:51.533782 30968 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:06:51.533785 30968 net.cpp:200] relu3 needs backward computation.
I0428 20:06:51.533807 30968 net.cpp:200] ip3 needs backward computation.
I0428 20:06:51.533812 30968 net.cpp:200] relu2 needs backward computation.
I0428 20:06:51.533815 30968 net.cpp:200] ip2 needs backward computation.
I0428 20:06:51.533820 30968 net.cpp:200] relu1 needs backward computation.
I0428 20:06:51.533824 30968 net.cpp:200] ip1 needs backward computation.
I0428 20:06:51.533829 30968 net.cpp:200] pool1 needs backward computation.
I0428 20:06:51.533834 30968 net.cpp:200] conv1 needs backward computation.
I0428 20:06:51.533840 30968 net.cpp:200] pool0 needs backward computation.
I0428 20:06:51.533845 30968 net.cpp:200] conv0 needs backward computation.
I0428 20:06:51.533851 30968 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:06:51.533871 30968 net.cpp:202] mnist does not need backward computation.
I0428 20:06:51.533876 30968 net.cpp:244] This network produces output accuracy
I0428 20:06:51.533881 30968 net.cpp:244] This network produces output loss
I0428 20:06:51.533895 30968 net.cpp:257] Network initialization done.
I0428 20:06:51.533938 30968 solver.cpp:56] Solver scaffolding done.
I0428 20:06:51.534296 30968 caffe.cpp:248] Starting Optimization
I0428 20:06:51.534301 30968 solver.cpp:273] Solving LeNet
I0428 20:06:51.534303 30968 solver.cpp:274] Learning Rate Policy: inv
I0428 20:06:51.535111 30968 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:06:51.540206 30968 blocking_queue.cpp:49] Waiting for data
I0428 20:06:51.611068 30975 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:06:51.611675 30968 solver.cpp:398]     Test net output #0: accuracy = 0.1095
I0428 20:06:51.611708 30968 solver.cpp:398]     Test net output #1: loss = 2.31266 (* 1 = 2.31266 loss)
I0428 20:06:51.615511 30968 solver.cpp:219] Iteration 0 (-1.01874e-42 iter/s, 0.0811643s/100 iters), loss = 2.31392
I0428 20:06:51.615550 30968 solver.cpp:238]     Train net output #0: loss = 2.31392 (* 1 = 2.31392 loss)
I0428 20:06:51.615561 30968 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:06:51.705458 30968 solver.cpp:219] Iteration 100 (1112.21 iter/s, 0.0899113s/100 iters), loss = 0.410304
I0428 20:06:51.705482 30968 solver.cpp:238]     Train net output #0: loss = 0.410304 (* 1 = 0.410304 loss)
I0428 20:06:51.705489 30968 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:06:51.791651 30968 solver.cpp:219] Iteration 200 (1160.68 iter/s, 0.0861565s/100 iters), loss = 0.226185
I0428 20:06:51.791690 30968 solver.cpp:238]     Train net output #0: loss = 0.226185 (* 1 = 0.226185 loss)
I0428 20:06:51.791697 30968 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:06:51.878610 30968 solver.cpp:219] Iteration 300 (1150.41 iter/s, 0.0869253s/100 iters), loss = 0.262099
I0428 20:06:51.878649 30968 solver.cpp:238]     Train net output #0: loss = 0.262099 (* 1 = 0.262099 loss)
I0428 20:06:51.878655 30968 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:06:51.964247 30968 solver.cpp:219] Iteration 400 (1168.18 iter/s, 0.0856033s/100 iters), loss = 0.121708
I0428 20:06:51.964274 30968 solver.cpp:238]     Train net output #0: loss = 0.121708 (* 1 = 0.121708 loss)
I0428 20:06:51.964280 30968 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:06:52.049819 30968 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:06:52.101269 30975 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:06:52.102708 30968 solver.cpp:398]     Test net output #0: accuracy = 0.9504
I0428 20:06:52.102726 30968 solver.cpp:398]     Test net output #1: loss = 0.155063 (* 1 = 0.155063 loss)
I0428 20:06:52.103624 30968 solver.cpp:219] Iteration 500 (717.681 iter/s, 0.139338s/100 iters), loss = 0.168809
I0428 20:06:52.103691 30968 solver.cpp:238]     Train net output #0: loss = 0.168809 (* 1 = 0.168809 loss)
I0428 20:06:52.103700 30968 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:06:52.202611 30968 solver.cpp:219] Iteration 600 (1010.85 iter/s, 0.0989266s/100 iters), loss = 0.159283
I0428 20:06:52.202635 30968 solver.cpp:238]     Train net output #0: loss = 0.159283 (* 1 = 0.159283 loss)
I0428 20:06:52.202657 30968 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:06:52.300446 30968 solver.cpp:219] Iteration 700 (1022.55 iter/s, 0.0977951s/100 iters), loss = 0.209017
I0428 20:06:52.300472 30968 solver.cpp:238]     Train net output #0: loss = 0.209017 (* 1 = 0.209017 loss)
I0428 20:06:52.300498 30968 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:06:52.392711 30968 solver.cpp:219] Iteration 800 (1084.24 iter/s, 0.0922306s/100 iters), loss = 0.228012
I0428 20:06:52.392752 30968 solver.cpp:238]     Train net output #0: loss = 0.228012 (* 1 = 0.228012 loss)
I0428 20:06:52.392774 30968 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:06:52.480418 30968 solver.cpp:219] Iteration 900 (1140.59 iter/s, 0.0876738s/100 iters), loss = 0.195221
I0428 20:06:52.480459 30968 solver.cpp:238]     Train net output #0: loss = 0.195221 (* 1 = 0.195221 loss)
I0428 20:06:52.480465 30968 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:06:52.509861 30974 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:06:52.567091 30968 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:06:52.567910 30968 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:06:52.568521 30968 solver.cpp:311] Iteration 1000, loss = 0.120755
I0428 20:06:52.568537 30968 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:06:52.644251 30975 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:06:52.644888 30968 solver.cpp:398]     Test net output #0: accuracy = 0.9637
I0428 20:06:52.644911 30968 solver.cpp:398]     Test net output #1: loss = 0.11732 (* 1 = 0.11732 loss)
I0428 20:06:52.644915 30968 solver.cpp:316] Optimization Done.
I0428 20:06:52.644919 30968 caffe.cpp:259] Optimization Done.
