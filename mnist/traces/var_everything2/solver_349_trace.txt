I0428 19:40:55.142877 24694 caffe.cpp:218] Using GPUs 0
I0428 19:40:55.183637 24694 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:40:55.697528 24694 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test349.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:40:55.697721 24694 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test349.prototxt
I0428 19:40:55.698496 24694 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:40:55.698516 24694 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:40:55.698621 24694 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:40:55.698699 24694 layer_factory.hpp:77] Creating layer mnist
I0428 19:40:55.698801 24694 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:40:55.698827 24694 net.cpp:86] Creating Layer mnist
I0428 19:40:55.698835 24694 net.cpp:382] mnist -> data
I0428 19:40:55.698860 24694 net.cpp:382] mnist -> label
I0428 19:40:55.699970 24694 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:40:55.702446 24694 net.cpp:124] Setting up mnist
I0428 19:40:55.702463 24694 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:40:55.702471 24694 net.cpp:131] Top shape: 64 (64)
I0428 19:40:55.702474 24694 net.cpp:139] Memory required for data: 200960
I0428 19:40:55.702481 24694 layer_factory.hpp:77] Creating layer conv0
I0428 19:40:55.702497 24694 net.cpp:86] Creating Layer conv0
I0428 19:40:55.702518 24694 net.cpp:408] conv0 <- data
I0428 19:40:55.702533 24694 net.cpp:382] conv0 -> conv0
I0428 19:40:55.983253 24694 net.cpp:124] Setting up conv0
I0428 19:40:55.983278 24694 net.cpp:131] Top shape: 64 2 24 24 (73728)
I0428 19:40:55.983283 24694 net.cpp:139] Memory required for data: 495872
I0428 19:40:55.983297 24694 layer_factory.hpp:77] Creating layer pool0
I0428 19:40:55.983309 24694 net.cpp:86] Creating Layer pool0
I0428 19:40:55.983314 24694 net.cpp:408] pool0 <- conv0
I0428 19:40:55.983319 24694 net.cpp:382] pool0 -> pool0
I0428 19:40:55.983362 24694 net.cpp:124] Setting up pool0
I0428 19:40:55.983371 24694 net.cpp:131] Top shape: 64 2 12 12 (18432)
I0428 19:40:55.983372 24694 net.cpp:139] Memory required for data: 569600
I0428 19:40:55.983391 24694 layer_factory.hpp:77] Creating layer conv1
I0428 19:40:55.983402 24694 net.cpp:86] Creating Layer conv1
I0428 19:40:55.983420 24694 net.cpp:408] conv1 <- pool0
I0428 19:40:55.983424 24694 net.cpp:382] conv1 -> conv1
I0428 19:40:55.985435 24694 net.cpp:124] Setting up conv1
I0428 19:40:55.985466 24694 net.cpp:131] Top shape: 64 5 8 8 (20480)
I0428 19:40:55.985469 24694 net.cpp:139] Memory required for data: 651520
I0428 19:40:55.985478 24694 layer_factory.hpp:77] Creating layer pool1
I0428 19:40:55.985488 24694 net.cpp:86] Creating Layer pool1
I0428 19:40:55.985492 24694 net.cpp:408] pool1 <- conv1
I0428 19:40:55.985497 24694 net.cpp:382] pool1 -> pool1
I0428 19:40:55.985538 24694 net.cpp:124] Setting up pool1
I0428 19:40:55.985544 24694 net.cpp:131] Top shape: 64 5 4 4 (5120)
I0428 19:40:55.985548 24694 net.cpp:139] Memory required for data: 672000
I0428 19:40:55.985550 24694 layer_factory.hpp:77] Creating layer ip1
I0428 19:40:55.985558 24694 net.cpp:86] Creating Layer ip1
I0428 19:40:55.985561 24694 net.cpp:408] ip1 <- pool1
I0428 19:40:55.985566 24694 net.cpp:382] ip1 -> ip1
I0428 19:40:55.986688 24694 net.cpp:124] Setting up ip1
I0428 19:40:55.986717 24694 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:40:55.986721 24694 net.cpp:139] Memory required for data: 684800
I0428 19:40:55.986729 24694 layer_factory.hpp:77] Creating layer relu1
I0428 19:40:55.986737 24694 net.cpp:86] Creating Layer relu1
I0428 19:40:55.986742 24694 net.cpp:408] relu1 <- ip1
I0428 19:40:55.986745 24694 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:40:55.986939 24694 net.cpp:124] Setting up relu1
I0428 19:40:55.986948 24694 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:40:55.986951 24694 net.cpp:139] Memory required for data: 697600
I0428 19:40:55.986955 24694 layer_factory.hpp:77] Creating layer ip2
I0428 19:40:55.986961 24694 net.cpp:86] Creating Layer ip2
I0428 19:40:55.986965 24694 net.cpp:408] ip2 <- ip1
I0428 19:40:55.986985 24694 net.cpp:382] ip2 -> ip2
I0428 19:40:55.987108 24694 net.cpp:124] Setting up ip2
I0428 19:40:55.987115 24694 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:40:55.987118 24694 net.cpp:139] Memory required for data: 710400
I0428 19:40:55.987123 24694 layer_factory.hpp:77] Creating layer relu2
I0428 19:40:55.987129 24694 net.cpp:86] Creating Layer relu2
I0428 19:40:55.987133 24694 net.cpp:408] relu2 <- ip2
I0428 19:40:55.987136 24694 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:40:55.987947 24694 net.cpp:124] Setting up relu2
I0428 19:40:55.987977 24694 net.cpp:131] Top shape: 64 50 (3200)
I0428 19:40:55.987980 24694 net.cpp:139] Memory required for data: 723200
I0428 19:40:55.987983 24694 layer_factory.hpp:77] Creating layer ip3
I0428 19:40:55.987990 24694 net.cpp:86] Creating Layer ip3
I0428 19:40:55.987993 24694 net.cpp:408] ip3 <- ip2
I0428 19:40:55.987999 24694 net.cpp:382] ip3 -> ip3
I0428 19:40:55.988121 24694 net.cpp:124] Setting up ip3
I0428 19:40:55.988128 24694 net.cpp:131] Top shape: 64 10 (640)
I0428 19:40:55.988132 24694 net.cpp:139] Memory required for data: 725760
I0428 19:40:55.988157 24694 layer_factory.hpp:77] Creating layer relu3
I0428 19:40:55.988162 24694 net.cpp:86] Creating Layer relu3
I0428 19:40:55.988165 24694 net.cpp:408] relu3 <- ip3
I0428 19:40:55.988169 24694 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:40:55.988338 24694 net.cpp:124] Setting up relu3
I0428 19:40:55.988346 24694 net.cpp:131] Top shape: 64 10 (640)
I0428 19:40:55.988349 24694 net.cpp:139] Memory required for data: 728320
I0428 19:40:55.988353 24694 layer_factory.hpp:77] Creating layer loss
I0428 19:40:55.988358 24694 net.cpp:86] Creating Layer loss
I0428 19:40:55.988363 24694 net.cpp:408] loss <- ip3
I0428 19:40:55.988366 24694 net.cpp:408] loss <- label
I0428 19:40:55.988371 24694 net.cpp:382] loss -> loss
I0428 19:40:55.988390 24694 layer_factory.hpp:77] Creating layer loss
I0428 19:40:55.988642 24694 net.cpp:124] Setting up loss
I0428 19:40:55.988651 24694 net.cpp:131] Top shape: (1)
I0428 19:40:55.988656 24694 net.cpp:134]     with loss weight 1
I0428 19:40:55.988669 24694 net.cpp:139] Memory required for data: 728324
I0428 19:40:55.988673 24694 net.cpp:200] loss needs backward computation.
I0428 19:40:55.988677 24694 net.cpp:200] relu3 needs backward computation.
I0428 19:40:55.988680 24694 net.cpp:200] ip3 needs backward computation.
I0428 19:40:55.988683 24694 net.cpp:200] relu2 needs backward computation.
I0428 19:40:55.988687 24694 net.cpp:200] ip2 needs backward computation.
I0428 19:40:55.988689 24694 net.cpp:200] relu1 needs backward computation.
I0428 19:40:55.988692 24694 net.cpp:200] ip1 needs backward computation.
I0428 19:40:55.988695 24694 net.cpp:200] pool1 needs backward computation.
I0428 19:40:55.988698 24694 net.cpp:200] conv1 needs backward computation.
I0428 19:40:55.988701 24694 net.cpp:200] pool0 needs backward computation.
I0428 19:40:55.988704 24694 net.cpp:200] conv0 needs backward computation.
I0428 19:40:55.988708 24694 net.cpp:202] mnist does not need backward computation.
I0428 19:40:55.988711 24694 net.cpp:244] This network produces output loss
I0428 19:40:55.988720 24694 net.cpp:257] Network initialization done.
I0428 19:40:55.989140 24694 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test349.prototxt
I0428 19:40:55.989195 24694 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:40:55.989302 24694 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:40:55.989382 24694 layer_factory.hpp:77] Creating layer mnist
I0428 19:40:55.989428 24694 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:40:55.989441 24694 net.cpp:86] Creating Layer mnist
I0428 19:40:55.989446 24694 net.cpp:382] mnist -> data
I0428 19:40:55.989454 24694 net.cpp:382] mnist -> label
I0428 19:40:55.989538 24694 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:40:55.991740 24694 net.cpp:124] Setting up mnist
I0428 19:40:55.991770 24694 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:40:55.991786 24694 net.cpp:131] Top shape: 100 (100)
I0428 19:40:55.991789 24694 net.cpp:139] Memory required for data: 314000
I0428 19:40:55.991793 24694 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:40:55.991824 24694 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:40:55.991829 24694 net.cpp:408] label_mnist_1_split <- label
I0428 19:40:55.991834 24694 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:40:55.991842 24694 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:40:55.991884 24694 net.cpp:124] Setting up label_mnist_1_split
I0428 19:40:55.991891 24694 net.cpp:131] Top shape: 100 (100)
I0428 19:40:55.991895 24694 net.cpp:131] Top shape: 100 (100)
I0428 19:40:55.991899 24694 net.cpp:139] Memory required for data: 314800
I0428 19:40:55.991900 24694 layer_factory.hpp:77] Creating layer conv0
I0428 19:40:55.991916 24694 net.cpp:86] Creating Layer conv0
I0428 19:40:55.991919 24694 net.cpp:408] conv0 <- data
I0428 19:40:55.991925 24694 net.cpp:382] conv0 -> conv0
I0428 19:40:55.993701 24694 net.cpp:124] Setting up conv0
I0428 19:40:55.993727 24694 net.cpp:131] Top shape: 100 2 24 24 (115200)
I0428 19:40:55.993731 24694 net.cpp:139] Memory required for data: 775600
I0428 19:40:55.993741 24694 layer_factory.hpp:77] Creating layer pool0
I0428 19:40:55.993747 24694 net.cpp:86] Creating Layer pool0
I0428 19:40:55.993752 24694 net.cpp:408] pool0 <- conv0
I0428 19:40:55.993772 24694 net.cpp:382] pool0 -> pool0
I0428 19:40:55.993824 24694 net.cpp:124] Setting up pool0
I0428 19:40:55.993830 24694 net.cpp:131] Top shape: 100 2 12 12 (28800)
I0428 19:40:55.993834 24694 net.cpp:139] Memory required for data: 890800
I0428 19:40:55.993836 24694 layer_factory.hpp:77] Creating layer conv1
I0428 19:40:55.993844 24694 net.cpp:86] Creating Layer conv1
I0428 19:40:55.993849 24694 net.cpp:408] conv1 <- pool0
I0428 19:40:55.993854 24694 net.cpp:382] conv1 -> conv1
I0428 19:40:55.995445 24694 net.cpp:124] Setting up conv1
I0428 19:40:55.995457 24694 net.cpp:131] Top shape: 100 5 8 8 (32000)
I0428 19:40:55.995478 24694 net.cpp:139] Memory required for data: 1018800
I0428 19:40:55.995488 24694 layer_factory.hpp:77] Creating layer pool1
I0428 19:40:55.995496 24694 net.cpp:86] Creating Layer pool1
I0428 19:40:55.995499 24694 net.cpp:408] pool1 <- conv1
I0428 19:40:55.995506 24694 net.cpp:382] pool1 -> pool1
I0428 19:40:55.995540 24694 net.cpp:124] Setting up pool1
I0428 19:40:55.995545 24694 net.cpp:131] Top shape: 100 5 4 4 (8000)
I0428 19:40:55.995548 24694 net.cpp:139] Memory required for data: 1050800
I0428 19:40:55.995551 24694 layer_factory.hpp:77] Creating layer ip1
I0428 19:40:55.995558 24694 net.cpp:86] Creating Layer ip1
I0428 19:40:55.995561 24694 net.cpp:408] ip1 <- pool1
I0428 19:40:55.995565 24694 net.cpp:382] ip1 -> ip1
I0428 19:40:55.995705 24694 net.cpp:124] Setting up ip1
I0428 19:40:55.995713 24694 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:40:55.995728 24694 net.cpp:139] Memory required for data: 1070800
I0428 19:40:55.995736 24694 layer_factory.hpp:77] Creating layer relu1
I0428 19:40:55.995743 24694 net.cpp:86] Creating Layer relu1
I0428 19:40:55.995745 24694 net.cpp:408] relu1 <- ip1
I0428 19:40:55.995750 24694 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:40:55.995915 24694 net.cpp:124] Setting up relu1
I0428 19:40:55.995925 24694 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:40:55.995929 24694 net.cpp:139] Memory required for data: 1090800
I0428 19:40:55.995932 24694 layer_factory.hpp:77] Creating layer ip2
I0428 19:40:55.995939 24694 net.cpp:86] Creating Layer ip2
I0428 19:40:55.995942 24694 net.cpp:408] ip2 <- ip1
I0428 19:40:55.995949 24694 net.cpp:382] ip2 -> ip2
I0428 19:40:55.996062 24694 net.cpp:124] Setting up ip2
I0428 19:40:55.996069 24694 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:40:55.996073 24694 net.cpp:139] Memory required for data: 1110800
I0428 19:40:55.996078 24694 layer_factory.hpp:77] Creating layer relu2
I0428 19:40:55.996083 24694 net.cpp:86] Creating Layer relu2
I0428 19:40:55.996086 24694 net.cpp:408] relu2 <- ip2
I0428 19:40:55.996090 24694 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:40:55.996309 24694 net.cpp:124] Setting up relu2
I0428 19:40:55.996316 24694 net.cpp:131] Top shape: 100 50 (5000)
I0428 19:40:55.996320 24694 net.cpp:139] Memory required for data: 1130800
I0428 19:40:55.996322 24694 layer_factory.hpp:77] Creating layer ip3
I0428 19:40:55.996328 24694 net.cpp:86] Creating Layer ip3
I0428 19:40:55.996331 24694 net.cpp:408] ip3 <- ip2
I0428 19:40:55.996336 24694 net.cpp:382] ip3 -> ip3
I0428 19:40:55.996489 24694 net.cpp:124] Setting up ip3
I0428 19:40:55.996496 24694 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:40:55.996500 24694 net.cpp:139] Memory required for data: 1134800
I0428 19:40:55.996507 24694 layer_factory.hpp:77] Creating layer relu3
I0428 19:40:55.996512 24694 net.cpp:86] Creating Layer relu3
I0428 19:40:55.996515 24694 net.cpp:408] relu3 <- ip3
I0428 19:40:55.996520 24694 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:40:55.997402 24694 net.cpp:124] Setting up relu3
I0428 19:40:55.997413 24694 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:40:55.997428 24694 net.cpp:139] Memory required for data: 1138800
I0428 19:40:55.997431 24694 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:40:55.997437 24694 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:40:55.997440 24694 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:40:55.997447 24694 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:40:55.997452 24694 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:40:55.997505 24694 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:40:55.997514 24694 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:40:55.997534 24694 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:40:55.997535 24694 net.cpp:139] Memory required for data: 1146800
I0428 19:40:55.997539 24694 layer_factory.hpp:77] Creating layer accuracy
I0428 19:40:55.997544 24694 net.cpp:86] Creating Layer accuracy
I0428 19:40:55.997547 24694 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:40:55.997551 24694 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:40:55.997556 24694 net.cpp:382] accuracy -> accuracy
I0428 19:40:55.997563 24694 net.cpp:124] Setting up accuracy
I0428 19:40:55.997567 24694 net.cpp:131] Top shape: (1)
I0428 19:40:55.997570 24694 net.cpp:139] Memory required for data: 1146804
I0428 19:40:55.997572 24694 layer_factory.hpp:77] Creating layer loss
I0428 19:40:55.997577 24694 net.cpp:86] Creating Layer loss
I0428 19:40:55.997581 24694 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:40:55.997591 24694 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:40:55.997596 24694 net.cpp:382] loss -> loss
I0428 19:40:55.997602 24694 layer_factory.hpp:77] Creating layer loss
I0428 19:40:55.997845 24694 net.cpp:124] Setting up loss
I0428 19:40:55.997855 24694 net.cpp:131] Top shape: (1)
I0428 19:40:55.997859 24694 net.cpp:134]     with loss weight 1
I0428 19:40:55.997866 24694 net.cpp:139] Memory required for data: 1146808
I0428 19:40:55.997897 24694 net.cpp:200] loss needs backward computation.
I0428 19:40:55.997917 24694 net.cpp:202] accuracy does not need backward computation.
I0428 19:40:55.997921 24694 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:40:55.997941 24694 net.cpp:200] relu3 needs backward computation.
I0428 19:40:55.997943 24694 net.cpp:200] ip3 needs backward computation.
I0428 19:40:55.997946 24694 net.cpp:200] relu2 needs backward computation.
I0428 19:40:55.997951 24694 net.cpp:200] ip2 needs backward computation.
I0428 19:40:55.997953 24694 net.cpp:200] relu1 needs backward computation.
I0428 19:40:55.997956 24694 net.cpp:200] ip1 needs backward computation.
I0428 19:40:55.997959 24694 net.cpp:200] pool1 needs backward computation.
I0428 19:40:55.997973 24694 net.cpp:200] conv1 needs backward computation.
I0428 19:40:55.997977 24694 net.cpp:200] pool0 needs backward computation.
I0428 19:40:55.997987 24694 net.cpp:200] conv0 needs backward computation.
I0428 19:40:55.997992 24694 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:40:55.997995 24694 net.cpp:202] mnist does not need backward computation.
I0428 19:40:55.997998 24694 net.cpp:244] This network produces output accuracy
I0428 19:40:55.998003 24694 net.cpp:244] This network produces output loss
I0428 19:40:55.998013 24694 net.cpp:257] Network initialization done.
I0428 19:40:55.998070 24694 solver.cpp:56] Solver scaffolding done.
I0428 19:40:55.998458 24694 caffe.cpp:248] Starting Optimization
I0428 19:40:55.998464 24694 solver.cpp:273] Solving LeNet
I0428 19:40:55.998467 24694 solver.cpp:274] Learning Rate Policy: inv
I0428 19:40:55.999397 24694 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:40:56.002106 24694 blocking_queue.cpp:49] Waiting for data
I0428 19:40:56.074272 24701 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:40:56.074702 24694 solver.cpp:398]     Test net output #0: accuracy = 0.0851
I0428 19:40:56.074728 24694 solver.cpp:398]     Test net output #1: loss = 2.32559 (* 1 = 2.32559 loss)
I0428 19:40:56.076627 24694 solver.cpp:219] Iteration 0 (-3.02548e-31 iter/s, 0.0781174s/100 iters), loss = 2.31273
I0428 19:40:56.076666 24694 solver.cpp:238]     Train net output #0: loss = 2.31273 (* 1 = 2.31273 loss)
I0428 19:40:56.076679 24694 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:40:56.151531 24694 solver.cpp:219] Iteration 100 (1335.61 iter/s, 0.0748721s/100 iters), loss = 1.11537
I0428 19:40:56.151572 24694 solver.cpp:238]     Train net output #0: loss = 1.11537 (* 1 = 1.11537 loss)
I0428 19:40:56.151578 24694 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:40:56.222952 24694 solver.cpp:219] Iteration 200 (1401.11 iter/s, 0.0713717s/100 iters), loss = 1.25131
I0428 19:40:56.222993 24694 solver.cpp:238]     Train net output #0: loss = 1.25131 (* 1 = 1.25131 loss)
I0428 19:40:56.223016 24694 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:40:56.292203 24694 solver.cpp:219] Iteration 300 (1445.07 iter/s, 0.0692009s/100 iters), loss = 1.09801
I0428 19:40:56.292227 24694 solver.cpp:238]     Train net output #0: loss = 1.09801 (* 1 = 1.09801 loss)
I0428 19:40:56.292232 24694 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:40:56.362330 24694 solver.cpp:219] Iteration 400 (1426.64 iter/s, 0.0700948s/100 iters), loss = 1.31472
I0428 19:40:56.362370 24694 solver.cpp:238]     Train net output #0: loss = 1.31472 (* 1 = 1.31472 loss)
I0428 19:40:56.362376 24694 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:40:56.431460 24694 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:40:56.506386 24701 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:40:56.506927 24694 solver.cpp:398]     Test net output #0: accuracy = 0.7402
I0428 19:40:56.506959 24694 solver.cpp:398]     Test net output #1: loss = 0.898755 (* 1 = 0.898755 loss)
I0428 19:40:56.507917 24694 solver.cpp:219] Iteration 500 (687.057 iter/s, 0.145548s/100 iters), loss = 0.992007
I0428 19:40:56.507951 24694 solver.cpp:238]     Train net output #0: loss = 0.992007 (* 1 = 0.992007 loss)
I0428 19:40:56.507982 24694 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:40:56.588594 24694 solver.cpp:219] Iteration 600 (1240.22 iter/s, 0.0806308s/100 iters), loss = 0.58496
I0428 19:40:56.588621 24694 solver.cpp:238]     Train net output #0: loss = 0.58496 (* 1 = 0.58496 loss)
I0428 19:40:56.588629 24694 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:40:56.659090 24694 solver.cpp:219] Iteration 700 (1419.26 iter/s, 0.0704595s/100 iters), loss = 0.999288
I0428 19:40:56.659129 24694 solver.cpp:238]     Train net output #0: loss = 0.999288 (* 1 = 0.999288 loss)
I0428 19:40:56.659137 24694 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:40:56.727865 24694 solver.cpp:219] Iteration 800 (1454.69 iter/s, 0.0687432s/100 iters), loss = 0.747434
I0428 19:40:56.727888 24694 solver.cpp:238]     Train net output #0: loss = 0.747434 (* 1 = 0.747434 loss)
I0428 19:40:56.727895 24694 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:40:56.796524 24694 solver.cpp:219] Iteration 900 (1457.17 iter/s, 0.0686263s/100 iters), loss = 0.592082
I0428 19:40:56.796563 24694 solver.cpp:238]     Train net output #0: loss = 0.592082 (* 1 = 0.592082 loss)
I0428 19:40:56.796569 24694 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:40:56.821341 24700 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:40:56.869113 24694 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:40:56.869868 24694 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:40:56.870365 24694 solver.cpp:311] Iteration 1000, loss = 0.64235
I0428 19:40:56.870379 24694 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:40:56.945701 24701 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:40:56.946125 24694 solver.cpp:398]     Test net output #0: accuracy = 0.7659
I0428 19:40:56.946146 24694 solver.cpp:398]     Test net output #1: loss = 0.648327 (* 1 = 0.648327 loss)
I0428 19:40:56.946151 24694 solver.cpp:316] Optimization Done.
I0428 19:40:56.946154 24694 caffe.cpp:259] Optimization Done.
