I0428 19:59:21.065479 29218 caffe.cpp:218] Using GPUs 0
I0428 19:59:21.099467 29218 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 19:59:21.556946 29218 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test845.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 19:59:21.557126 29218 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test845.prototxt
I0428 19:59:21.557518 29218 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 19:59:21.557548 29218 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 19:59:21.557626 29218 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:59:21.557688 29218 layer_factory.hpp:77] Creating layer mnist
I0428 19:59:21.557770 29218 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 19:59:21.557788 29218 net.cpp:86] Creating Layer mnist
I0428 19:59:21.557796 29218 net.cpp:382] mnist -> data
I0428 19:59:21.557813 29218 net.cpp:382] mnist -> label
I0428 19:59:21.558809 29218 data_layer.cpp:45] output data size: 64,1,28,28
I0428 19:59:21.561122 29218 net.cpp:124] Setting up mnist
I0428 19:59:21.561147 29218 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 19:59:21.561152 29218 net.cpp:131] Top shape: 64 (64)
I0428 19:59:21.561156 29218 net.cpp:139] Memory required for data: 200960
I0428 19:59:21.561161 29218 layer_factory.hpp:77] Creating layer conv0
I0428 19:59:21.561174 29218 net.cpp:86] Creating Layer conv0
I0428 19:59:21.561190 29218 net.cpp:408] conv0 <- data
I0428 19:59:21.561200 29218 net.cpp:382] conv0 -> conv0
I0428 19:59:21.788611 29218 net.cpp:124] Setting up conv0
I0428 19:59:21.788637 29218 net.cpp:131] Top shape: 64 10 24 24 (368640)
I0428 19:59:21.788640 29218 net.cpp:139] Memory required for data: 1675520
I0428 19:59:21.788655 29218 layer_factory.hpp:77] Creating layer pool0
I0428 19:59:21.788667 29218 net.cpp:86] Creating Layer pool0
I0428 19:59:21.788671 29218 net.cpp:408] pool0 <- conv0
I0428 19:59:21.788676 29218 net.cpp:382] pool0 -> pool0
I0428 19:59:21.788753 29218 net.cpp:124] Setting up pool0
I0428 19:59:21.788759 29218 net.cpp:131] Top shape: 64 10 12 12 (92160)
I0428 19:59:21.788763 29218 net.cpp:139] Memory required for data: 2044160
I0428 19:59:21.788765 29218 layer_factory.hpp:77] Creating layer conv1
I0428 19:59:21.788775 29218 net.cpp:86] Creating Layer conv1
I0428 19:59:21.788780 29218 net.cpp:408] conv1 <- pool0
I0428 19:59:21.788784 29218 net.cpp:382] conv1 -> conv1
I0428 19:59:21.791584 29218 net.cpp:124] Setting up conv1
I0428 19:59:21.791613 29218 net.cpp:131] Top shape: 64 10 8 8 (40960)
I0428 19:59:21.791617 29218 net.cpp:139] Memory required for data: 2208000
I0428 19:59:21.791625 29218 layer_factory.hpp:77] Creating layer pool1
I0428 19:59:21.791633 29218 net.cpp:86] Creating Layer pool1
I0428 19:59:21.791651 29218 net.cpp:408] pool1 <- conv1
I0428 19:59:21.791656 29218 net.cpp:382] pool1 -> pool1
I0428 19:59:21.791693 29218 net.cpp:124] Setting up pool1
I0428 19:59:21.791699 29218 net.cpp:131] Top shape: 64 10 4 4 (10240)
I0428 19:59:21.791702 29218 net.cpp:139] Memory required for data: 2248960
I0428 19:59:21.791705 29218 layer_factory.hpp:77] Creating layer ip1
I0428 19:59:21.791712 29218 net.cpp:86] Creating Layer ip1
I0428 19:59:21.791715 29218 net.cpp:408] ip1 <- pool1
I0428 19:59:21.791720 29218 net.cpp:382] ip1 -> ip1
I0428 19:59:21.791836 29218 net.cpp:124] Setting up ip1
I0428 19:59:21.791843 29218 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:59:21.791846 29218 net.cpp:139] Memory required for data: 2255360
I0428 19:59:21.791853 29218 layer_factory.hpp:77] Creating layer relu1
I0428 19:59:21.791859 29218 net.cpp:86] Creating Layer relu1
I0428 19:59:21.791864 29218 net.cpp:408] relu1 <- ip1
I0428 19:59:21.791868 29218 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:59:21.792042 29218 net.cpp:124] Setting up relu1
I0428 19:59:21.792050 29218 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:59:21.792053 29218 net.cpp:139] Memory required for data: 2261760
I0428 19:59:21.792057 29218 layer_factory.hpp:77] Creating layer ip2
I0428 19:59:21.792062 29218 net.cpp:86] Creating Layer ip2
I0428 19:59:21.792065 29218 net.cpp:408] ip2 <- ip1
I0428 19:59:21.792070 29218 net.cpp:382] ip2 -> ip2
I0428 19:59:21.792163 29218 net.cpp:124] Setting up ip2
I0428 19:59:21.792171 29218 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:59:21.792173 29218 net.cpp:139] Memory required for data: 2268160
I0428 19:59:21.792178 29218 layer_factory.hpp:77] Creating layer relu2
I0428 19:59:21.792184 29218 net.cpp:86] Creating Layer relu2
I0428 19:59:21.792187 29218 net.cpp:408] relu2 <- ip2
I0428 19:59:21.792191 29218 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:59:21.793020 29218 net.cpp:124] Setting up relu2
I0428 19:59:21.793048 29218 net.cpp:131] Top shape: 64 25 (1600)
I0428 19:59:21.793052 29218 net.cpp:139] Memory required for data: 2274560
I0428 19:59:21.793056 29218 layer_factory.hpp:77] Creating layer ip3
I0428 19:59:21.793062 29218 net.cpp:86] Creating Layer ip3
I0428 19:59:21.793066 29218 net.cpp:408] ip3 <- ip2
I0428 19:59:21.793071 29218 net.cpp:382] ip3 -> ip3
I0428 19:59:21.793195 29218 net.cpp:124] Setting up ip3
I0428 19:59:21.793201 29218 net.cpp:131] Top shape: 64 10 (640)
I0428 19:59:21.793205 29218 net.cpp:139] Memory required for data: 2277120
I0428 19:59:21.793213 29218 layer_factory.hpp:77] Creating layer relu3
I0428 19:59:21.793220 29218 net.cpp:86] Creating Layer relu3
I0428 19:59:21.793223 29218 net.cpp:408] relu3 <- ip3
I0428 19:59:21.793227 29218 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:59:21.793402 29218 net.cpp:124] Setting up relu3
I0428 19:59:21.793411 29218 net.cpp:131] Top shape: 64 10 (640)
I0428 19:59:21.793429 29218 net.cpp:139] Memory required for data: 2279680
I0428 19:59:21.793432 29218 layer_factory.hpp:77] Creating layer loss
I0428 19:59:21.793438 29218 net.cpp:86] Creating Layer loss
I0428 19:59:21.793442 29218 net.cpp:408] loss <- ip3
I0428 19:59:21.793444 29218 net.cpp:408] loss <- label
I0428 19:59:21.793450 29218 net.cpp:382] loss -> loss
I0428 19:59:21.793468 29218 layer_factory.hpp:77] Creating layer loss
I0428 19:59:21.793682 29218 net.cpp:124] Setting up loss
I0428 19:59:21.793691 29218 net.cpp:131] Top shape: (1)
I0428 19:59:21.793694 29218 net.cpp:134]     with loss weight 1
I0428 19:59:21.793725 29218 net.cpp:139] Memory required for data: 2279684
I0428 19:59:21.793730 29218 net.cpp:200] loss needs backward computation.
I0428 19:59:21.793732 29218 net.cpp:200] relu3 needs backward computation.
I0428 19:59:21.793735 29218 net.cpp:200] ip3 needs backward computation.
I0428 19:59:21.793738 29218 net.cpp:200] relu2 needs backward computation.
I0428 19:59:21.793741 29218 net.cpp:200] ip2 needs backward computation.
I0428 19:59:21.793745 29218 net.cpp:200] relu1 needs backward computation.
I0428 19:59:21.793746 29218 net.cpp:200] ip1 needs backward computation.
I0428 19:59:21.793751 29218 net.cpp:200] pool1 needs backward computation.
I0428 19:59:21.793753 29218 net.cpp:200] conv1 needs backward computation.
I0428 19:59:21.793756 29218 net.cpp:200] pool0 needs backward computation.
I0428 19:59:21.793776 29218 net.cpp:200] conv0 needs backward computation.
I0428 19:59:21.793781 29218 net.cpp:202] mnist does not need backward computation.
I0428 19:59:21.793786 29218 net.cpp:244] This network produces output loss
I0428 19:59:21.793799 29218 net.cpp:257] Network initialization done.
I0428 19:59:21.794174 29218 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test845.prototxt
I0428 19:59:21.794230 29218 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 19:59:21.794333 29218 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 19:59:21.794409 29218 layer_factory.hpp:77] Creating layer mnist
I0428 19:59:21.794450 29218 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 19:59:21.794464 29218 net.cpp:86] Creating Layer mnist
I0428 19:59:21.794468 29218 net.cpp:382] mnist -> data
I0428 19:59:21.794476 29218 net.cpp:382] mnist -> label
I0428 19:59:21.794569 29218 data_layer.cpp:45] output data size: 100,1,28,28
I0428 19:59:21.796571 29218 net.cpp:124] Setting up mnist
I0428 19:59:21.796614 29218 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 19:59:21.796619 29218 net.cpp:131] Top shape: 100 (100)
I0428 19:59:21.796623 29218 net.cpp:139] Memory required for data: 314000
I0428 19:59:21.796627 29218 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 19:59:21.796636 29218 net.cpp:86] Creating Layer label_mnist_1_split
I0428 19:59:21.796640 29218 net.cpp:408] label_mnist_1_split <- label
I0428 19:59:21.796644 29218 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 19:59:21.796651 29218 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 19:59:21.796746 29218 net.cpp:124] Setting up label_mnist_1_split
I0428 19:59:21.796753 29218 net.cpp:131] Top shape: 100 (100)
I0428 19:59:21.796756 29218 net.cpp:131] Top shape: 100 (100)
I0428 19:59:21.796759 29218 net.cpp:139] Memory required for data: 314800
I0428 19:59:21.796762 29218 layer_factory.hpp:77] Creating layer conv0
I0428 19:59:21.796771 29218 net.cpp:86] Creating Layer conv0
I0428 19:59:21.796773 29218 net.cpp:408] conv0 <- data
I0428 19:59:21.796778 29218 net.cpp:382] conv0 -> conv0
I0428 19:59:21.798300 29218 net.cpp:124] Setting up conv0
I0428 19:59:21.798313 29218 net.cpp:131] Top shape: 100 10 24 24 (576000)
I0428 19:59:21.798317 29218 net.cpp:139] Memory required for data: 2618800
I0428 19:59:21.798341 29218 layer_factory.hpp:77] Creating layer pool0
I0428 19:59:21.798346 29218 net.cpp:86] Creating Layer pool0
I0428 19:59:21.798349 29218 net.cpp:408] pool0 <- conv0
I0428 19:59:21.798354 29218 net.cpp:382] pool0 -> pool0
I0428 19:59:21.798399 29218 net.cpp:124] Setting up pool0
I0428 19:59:21.798408 29218 net.cpp:131] Top shape: 100 10 12 12 (144000)
I0428 19:59:21.798410 29218 net.cpp:139] Memory required for data: 3194800
I0428 19:59:21.798413 29218 layer_factory.hpp:77] Creating layer conv1
I0428 19:59:21.798420 29218 net.cpp:86] Creating Layer conv1
I0428 19:59:21.798424 29218 net.cpp:408] conv1 <- pool0
I0428 19:59:21.798429 29218 net.cpp:382] conv1 -> conv1
I0428 19:59:21.800509 29218 net.cpp:124] Setting up conv1
I0428 19:59:21.800535 29218 net.cpp:131] Top shape: 100 10 8 8 (64000)
I0428 19:59:21.800539 29218 net.cpp:139] Memory required for data: 3450800
I0428 19:59:21.800562 29218 layer_factory.hpp:77] Creating layer pool1
I0428 19:59:21.800568 29218 net.cpp:86] Creating Layer pool1
I0428 19:59:21.800571 29218 net.cpp:408] pool1 <- conv1
I0428 19:59:21.800576 29218 net.cpp:382] pool1 -> pool1
I0428 19:59:21.800647 29218 net.cpp:124] Setting up pool1
I0428 19:59:21.800655 29218 net.cpp:131] Top shape: 100 10 4 4 (16000)
I0428 19:59:21.800658 29218 net.cpp:139] Memory required for data: 3514800
I0428 19:59:21.800662 29218 layer_factory.hpp:77] Creating layer ip1
I0428 19:59:21.800668 29218 net.cpp:86] Creating Layer ip1
I0428 19:59:21.800671 29218 net.cpp:408] ip1 <- pool1
I0428 19:59:21.800675 29218 net.cpp:382] ip1 -> ip1
I0428 19:59:21.800856 29218 net.cpp:124] Setting up ip1
I0428 19:59:21.800864 29218 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:59:21.800879 29218 net.cpp:139] Memory required for data: 3524800
I0428 19:59:21.800886 29218 layer_factory.hpp:77] Creating layer relu1
I0428 19:59:21.800891 29218 net.cpp:86] Creating Layer relu1
I0428 19:59:21.800900 29218 net.cpp:408] relu1 <- ip1
I0428 19:59:21.800905 29218 net.cpp:369] relu1 -> ip1 (in-place)
I0428 19:59:21.801081 29218 net.cpp:124] Setting up relu1
I0428 19:59:21.801091 29218 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:59:21.801095 29218 net.cpp:139] Memory required for data: 3534800
I0428 19:59:21.801097 29218 layer_factory.hpp:77] Creating layer ip2
I0428 19:59:21.801105 29218 net.cpp:86] Creating Layer ip2
I0428 19:59:21.801108 29218 net.cpp:408] ip2 <- ip1
I0428 19:59:21.801123 29218 net.cpp:382] ip2 -> ip2
I0428 19:59:21.801256 29218 net.cpp:124] Setting up ip2
I0428 19:59:21.801264 29218 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:59:21.801266 29218 net.cpp:139] Memory required for data: 3544800
I0428 19:59:21.801270 29218 layer_factory.hpp:77] Creating layer relu2
I0428 19:59:21.801275 29218 net.cpp:86] Creating Layer relu2
I0428 19:59:21.801278 29218 net.cpp:408] relu2 <- ip2
I0428 19:59:21.801281 29218 net.cpp:369] relu2 -> ip2 (in-place)
I0428 19:59:21.801481 29218 net.cpp:124] Setting up relu2
I0428 19:59:21.801489 29218 net.cpp:131] Top shape: 100 25 (2500)
I0428 19:59:21.801492 29218 net.cpp:139] Memory required for data: 3554800
I0428 19:59:21.801496 29218 layer_factory.hpp:77] Creating layer ip3
I0428 19:59:21.801512 29218 net.cpp:86] Creating Layer ip3
I0428 19:59:21.801514 29218 net.cpp:408] ip3 <- ip2
I0428 19:59:21.801518 29218 net.cpp:382] ip3 -> ip3
I0428 19:59:21.801632 29218 net.cpp:124] Setting up ip3
I0428 19:59:21.801638 29218 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:59:21.801642 29218 net.cpp:139] Memory required for data: 3558800
I0428 19:59:21.801650 29218 layer_factory.hpp:77] Creating layer relu3
I0428 19:59:21.801653 29218 net.cpp:86] Creating Layer relu3
I0428 19:59:21.801656 29218 net.cpp:408] relu3 <- ip3
I0428 19:59:21.801661 29218 net.cpp:369] relu3 -> ip3 (in-place)
I0428 19:59:21.802467 29218 net.cpp:124] Setting up relu3
I0428 19:59:21.802480 29218 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:59:21.802484 29218 net.cpp:139] Memory required for data: 3562800
I0428 19:59:21.802486 29218 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 19:59:21.802491 29218 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 19:59:21.802495 29218 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 19:59:21.802500 29218 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 19:59:21.802506 29218 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 19:59:21.802543 29218 net.cpp:124] Setting up ip3_relu3_0_split
I0428 19:59:21.802549 29218 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:59:21.802553 29218 net.cpp:131] Top shape: 100 10 (1000)
I0428 19:59:21.802556 29218 net.cpp:139] Memory required for data: 3570800
I0428 19:59:21.802558 29218 layer_factory.hpp:77] Creating layer accuracy
I0428 19:59:21.802564 29218 net.cpp:86] Creating Layer accuracy
I0428 19:59:21.802567 29218 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 19:59:21.802572 29218 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 19:59:21.802575 29218 net.cpp:382] accuracy -> accuracy
I0428 19:59:21.802582 29218 net.cpp:124] Setting up accuracy
I0428 19:59:21.802587 29218 net.cpp:131] Top shape: (1)
I0428 19:59:21.802590 29218 net.cpp:139] Memory required for data: 3570804
I0428 19:59:21.802593 29218 layer_factory.hpp:77] Creating layer loss
I0428 19:59:21.802597 29218 net.cpp:86] Creating Layer loss
I0428 19:59:21.802600 29218 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 19:59:21.802603 29218 net.cpp:408] loss <- label_mnist_1_split_1
I0428 19:59:21.802608 29218 net.cpp:382] loss -> loss
I0428 19:59:21.802613 29218 layer_factory.hpp:77] Creating layer loss
I0428 19:59:21.802834 29218 net.cpp:124] Setting up loss
I0428 19:59:21.802844 29218 net.cpp:131] Top shape: (1)
I0428 19:59:21.802846 29218 net.cpp:134]     with loss weight 1
I0428 19:59:21.802862 29218 net.cpp:139] Memory required for data: 3570808
I0428 19:59:21.802865 29218 net.cpp:200] loss needs backward computation.
I0428 19:59:21.802870 29218 net.cpp:202] accuracy does not need backward computation.
I0428 19:59:21.802888 29218 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 19:59:21.802891 29218 net.cpp:200] relu3 needs backward computation.
I0428 19:59:21.802894 29218 net.cpp:200] ip3 needs backward computation.
I0428 19:59:21.802896 29218 net.cpp:200] relu2 needs backward computation.
I0428 19:59:21.802899 29218 net.cpp:200] ip2 needs backward computation.
I0428 19:59:21.802901 29218 net.cpp:200] relu1 needs backward computation.
I0428 19:59:21.802904 29218 net.cpp:200] ip1 needs backward computation.
I0428 19:59:21.802907 29218 net.cpp:200] pool1 needs backward computation.
I0428 19:59:21.802911 29218 net.cpp:200] conv1 needs backward computation.
I0428 19:59:21.802913 29218 net.cpp:200] pool0 needs backward computation.
I0428 19:59:21.802916 29218 net.cpp:200] conv0 needs backward computation.
I0428 19:59:21.802920 29218 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 19:59:21.802925 29218 net.cpp:202] mnist does not need backward computation.
I0428 19:59:21.802927 29218 net.cpp:244] This network produces output accuracy
I0428 19:59:21.802932 29218 net.cpp:244] This network produces output loss
I0428 19:59:21.802942 29218 net.cpp:257] Network initialization done.
I0428 19:59:21.802984 29218 solver.cpp:56] Solver scaffolding done.
I0428 19:59:21.803365 29218 caffe.cpp:248] Starting Optimization
I0428 19:59:21.803371 29218 solver.cpp:273] Solving LeNet
I0428 19:59:21.803375 29218 solver.cpp:274] Learning Rate Policy: inv
I0428 19:59:21.804090 29218 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 19:59:21.816781 29218 blocking_queue.cpp:49] Waiting for data
I0428 19:59:21.851505 29226 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:59:21.852023 29218 solver.cpp:398]     Test net output #0: accuracy = 0.0948
I0428 19:59:21.852041 29218 solver.cpp:398]     Test net output #1: loss = 2.30529 (* 1 = 2.30529 loss)
I0428 19:59:21.854568 29218 solver.cpp:219] Iteration 0 (0 iter/s, 0.0511682s/100 iters), loss = 2.31626
I0428 19:59:21.854593 29218 solver.cpp:238]     Train net output #0: loss = 2.31626 (* 1 = 2.31626 loss)
I0428 19:59:21.854626 29218 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 19:59:21.932827 29218 solver.cpp:219] Iteration 100 (1278.38 iter/s, 0.0782243s/100 iters), loss = 0.791439
I0428 19:59:21.932852 29218 solver.cpp:238]     Train net output #0: loss = 0.791439 (* 1 = 0.791439 loss)
I0428 19:59:21.932874 29218 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 19:59:22.013818 29218 solver.cpp:219] Iteration 200 (1235.23 iter/s, 0.0809564s/100 iters), loss = 0.77794
I0428 19:59:22.013842 29218 solver.cpp:238]     Train net output #0: loss = 0.77794 (* 1 = 0.77794 loss)
I0428 19:59:22.013849 29218 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 19:59:22.101390 29218 solver.cpp:219] Iteration 300 (1142.35 iter/s, 0.0875386s/100 iters), loss = 0.719445
I0428 19:59:22.101429 29218 solver.cpp:238]     Train net output #0: loss = 0.719445 (* 1 = 0.719445 loss)
I0428 19:59:22.101435 29218 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 19:59:22.183588 29218 solver.cpp:219] Iteration 400 (1217.29 iter/s, 0.0821494s/100 iters), loss = 0.770946
I0428 19:59:22.183611 29218 solver.cpp:238]     Train net output #0: loss = 0.770946 (* 1 = 0.770946 loss)
I0428 19:59:22.183617 29218 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 19:59:22.260622 29218 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 19:59:22.335045 29226 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:59:22.335566 29218 solver.cpp:398]     Test net output #0: accuracy = 0.8165
I0428 19:59:22.335589 29218 solver.cpp:398]     Test net output #1: loss = 0.648402 (* 1 = 0.648402 loss)
I0428 19:59:22.336444 29218 solver.cpp:219] Iteration 500 (654.366 iter/s, 0.15282s/100 iters), loss = 0.543725
I0428 19:59:22.336489 29218 solver.cpp:238]     Train net output #0: loss = 0.543725 (* 1 = 0.543725 loss)
I0428 19:59:22.336496 29218 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 19:59:22.423157 29218 solver.cpp:219] Iteration 600 (1153.94 iter/s, 0.0866599s/100 iters), loss = 0.431817
I0428 19:59:22.423183 29218 solver.cpp:238]     Train net output #0: loss = 0.431816 (* 1 = 0.431816 loss)
I0428 19:59:22.423189 29218 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 19:59:22.503346 29218 solver.cpp:219] Iteration 700 (1247.59 iter/s, 0.0801546s/100 iters), loss = 0.860911
I0428 19:59:22.503386 29218 solver.cpp:238]     Train net output #0: loss = 0.860911 (* 1 = 0.860911 loss)
I0428 19:59:22.503392 29218 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 19:59:22.583834 29218 solver.cpp:219] Iteration 800 (1243.22 iter/s, 0.0804362s/100 iters), loss = 0.698652
I0428 19:59:22.583874 29218 solver.cpp:238]     Train net output #0: loss = 0.698652 (* 1 = 0.698652 loss)
I0428 19:59:22.583884 29218 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 19:59:22.670152 29218 solver.cpp:219] Iteration 900 (1159.13 iter/s, 0.0862718s/100 iters), loss = 0.605947
I0428 19:59:22.670182 29218 solver.cpp:238]     Train net output #0: loss = 0.605947 (* 1 = 0.605947 loss)
I0428 19:59:22.670191 29218 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 19:59:22.697427 29225 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:59:22.751582 29218 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 19:59:22.752337 29218 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 19:59:22.752861 29218 solver.cpp:311] Iteration 1000, loss = 0.646865
I0428 19:59:22.752882 29218 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 19:59:22.827347 29226 data_layer.cpp:73] Restarting data prefetching from start.
I0428 19:59:22.827888 29218 solver.cpp:398]     Test net output #0: accuracy = 0.8643
I0428 19:59:22.827908 29218 solver.cpp:398]     Test net output #1: loss = 0.564017 (* 1 = 0.564017 loss)
I0428 19:59:22.827927 29218 solver.cpp:316] Optimization Done.
I0428 19:59:22.827931 29218 caffe.cpp:259] Optimization Done.
