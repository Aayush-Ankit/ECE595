I0428 20:35:50.181562  4708 caffe.cpp:218] Using GPUs 0
I0428 20:35:50.217020  4708 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:35:50.674307  4708 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1627.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:35:50.674449  4708 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1627.prototxt
I0428 20:35:50.674818  4708 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:35:50.674850  4708 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:35:50.674933  4708 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:35:50.674998  4708 layer_factory.hpp:77] Creating layer mnist
I0428 20:35:50.675081  4708 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:35:50.675101  4708 net.cpp:86] Creating Layer mnist
I0428 20:35:50.675109  4708 net.cpp:382] mnist -> data
I0428 20:35:50.675127  4708 net.cpp:382] mnist -> label
I0428 20:35:50.676107  4708 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:35:50.678397  4708 net.cpp:124] Setting up mnist
I0428 20:35:50.678442  4708 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:35:50.678447  4708 net.cpp:131] Top shape: 64 (64)
I0428 20:35:50.678450  4708 net.cpp:139] Memory required for data: 200960
I0428 20:35:50.678457  4708 layer_factory.hpp:77] Creating layer conv0
I0428 20:35:50.678470  4708 net.cpp:86] Creating Layer conv0
I0428 20:35:50.678488  4708 net.cpp:408] conv0 <- data
I0428 20:35:50.678498  4708 net.cpp:382] conv0 -> conv0
I0428 20:35:50.904038  4708 net.cpp:124] Setting up conv0
I0428 20:35:50.904065  4708 net.cpp:131] Top shape: 64 100 24 24 (3686400)
I0428 20:35:50.904068  4708 net.cpp:139] Memory required for data: 14946560
I0428 20:35:50.904083  4708 layer_factory.hpp:77] Creating layer pool0
I0428 20:35:50.904096  4708 net.cpp:86] Creating Layer pool0
I0428 20:35:50.904100  4708 net.cpp:408] pool0 <- conv0
I0428 20:35:50.904105  4708 net.cpp:382] pool0 -> pool0
I0428 20:35:50.904165  4708 net.cpp:124] Setting up pool0
I0428 20:35:50.904170  4708 net.cpp:131] Top shape: 64 100 12 12 (921600)
I0428 20:35:50.904172  4708 net.cpp:139] Memory required for data: 18632960
I0428 20:35:50.904175  4708 layer_factory.hpp:77] Creating layer conv1
I0428 20:35:50.904186  4708 net.cpp:86] Creating Layer conv1
I0428 20:35:50.904188  4708 net.cpp:408] conv1 <- pool0
I0428 20:35:50.904192  4708 net.cpp:382] conv1 -> conv1
I0428 20:35:50.907558  4708 net.cpp:124] Setting up conv1
I0428 20:35:50.907573  4708 net.cpp:131] Top shape: 64 50 8 8 (204800)
I0428 20:35:50.907577  4708 net.cpp:139] Memory required for data: 19452160
I0428 20:35:50.907600  4708 layer_factory.hpp:77] Creating layer pool1
I0428 20:35:50.907608  4708 net.cpp:86] Creating Layer pool1
I0428 20:35:50.907611  4708 net.cpp:408] pool1 <- conv1
I0428 20:35:50.907618  4708 net.cpp:382] pool1 -> pool1
I0428 20:35:50.907667  4708 net.cpp:124] Setting up pool1
I0428 20:35:50.907675  4708 net.cpp:131] Top shape: 64 50 4 4 (51200)
I0428 20:35:50.907677  4708 net.cpp:139] Memory required for data: 19656960
I0428 20:35:50.907680  4708 layer_factory.hpp:77] Creating layer ip1
I0428 20:35:50.907686  4708 net.cpp:86] Creating Layer ip1
I0428 20:35:50.907690  4708 net.cpp:408] ip1 <- pool1
I0428 20:35:50.907694  4708 net.cpp:382] ip1 -> ip1
I0428 20:35:50.907908  4708 net.cpp:124] Setting up ip1
I0428 20:35:50.907917  4708 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:35:50.907920  4708 net.cpp:139] Memory required for data: 19663360
I0428 20:35:50.907927  4708 layer_factory.hpp:77] Creating layer relu1
I0428 20:35:50.907934  4708 net.cpp:86] Creating Layer relu1
I0428 20:35:50.907938  4708 net.cpp:408] relu1 <- ip1
I0428 20:35:50.907943  4708 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:35:50.908120  4708 net.cpp:124] Setting up relu1
I0428 20:35:50.908130  4708 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:35:50.908134  4708 net.cpp:139] Memory required for data: 19669760
I0428 20:35:50.908138  4708 layer_factory.hpp:77] Creating layer ip2
I0428 20:35:50.908144  4708 net.cpp:86] Creating Layer ip2
I0428 20:35:50.908148  4708 net.cpp:408] ip2 <- ip1
I0428 20:35:50.908152  4708 net.cpp:382] ip2 -> ip2
I0428 20:35:50.908275  4708 net.cpp:124] Setting up ip2
I0428 20:35:50.908282  4708 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:35:50.908285  4708 net.cpp:139] Memory required for data: 19682560
I0428 20:35:50.908289  4708 layer_factory.hpp:77] Creating layer relu2
I0428 20:35:50.908296  4708 net.cpp:86] Creating Layer relu2
I0428 20:35:50.908315  4708 net.cpp:408] relu2 <- ip2
I0428 20:35:50.908319  4708 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:35:50.909211  4708 net.cpp:124] Setting up relu2
I0428 20:35:50.909252  4708 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:35:50.909255  4708 net.cpp:139] Memory required for data: 19695360
I0428 20:35:50.909258  4708 layer_factory.hpp:77] Creating layer ip3
I0428 20:35:50.909281  4708 net.cpp:86] Creating Layer ip3
I0428 20:35:50.909284  4708 net.cpp:408] ip3 <- ip2
I0428 20:35:50.909291  4708 net.cpp:382] ip3 -> ip3
I0428 20:35:50.909409  4708 net.cpp:124] Setting up ip3
I0428 20:35:50.909416  4708 net.cpp:131] Top shape: 64 10 (640)
I0428 20:35:50.909420  4708 net.cpp:139] Memory required for data: 19697920
I0428 20:35:50.909427  4708 layer_factory.hpp:77] Creating layer relu3
I0428 20:35:50.909432  4708 net.cpp:86] Creating Layer relu3
I0428 20:35:50.909435  4708 net.cpp:408] relu3 <- ip3
I0428 20:35:50.909453  4708 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:35:50.909603  4708 net.cpp:124] Setting up relu3
I0428 20:35:50.909611  4708 net.cpp:131] Top shape: 64 10 (640)
I0428 20:35:50.909615  4708 net.cpp:139] Memory required for data: 19700480
I0428 20:35:50.909617  4708 layer_factory.hpp:77] Creating layer loss
I0428 20:35:50.909624  4708 net.cpp:86] Creating Layer loss
I0428 20:35:50.909627  4708 net.cpp:408] loss <- ip3
I0428 20:35:50.909631  4708 net.cpp:408] loss <- label
I0428 20:35:50.909636  4708 net.cpp:382] loss -> loss
I0428 20:35:50.909653  4708 layer_factory.hpp:77] Creating layer loss
I0428 20:35:50.909878  4708 net.cpp:124] Setting up loss
I0428 20:35:50.909885  4708 net.cpp:131] Top shape: (1)
I0428 20:35:50.909888  4708 net.cpp:134]     with loss weight 1
I0428 20:35:50.909903  4708 net.cpp:139] Memory required for data: 19700484
I0428 20:35:50.909905  4708 net.cpp:200] loss needs backward computation.
I0428 20:35:50.909909  4708 net.cpp:200] relu3 needs backward computation.
I0428 20:35:50.909911  4708 net.cpp:200] ip3 needs backward computation.
I0428 20:35:50.909914  4708 net.cpp:200] relu2 needs backward computation.
I0428 20:35:50.909916  4708 net.cpp:200] ip2 needs backward computation.
I0428 20:35:50.909919  4708 net.cpp:200] relu1 needs backward computation.
I0428 20:35:50.909921  4708 net.cpp:200] ip1 needs backward computation.
I0428 20:35:50.909924  4708 net.cpp:200] pool1 needs backward computation.
I0428 20:35:50.909927  4708 net.cpp:200] conv1 needs backward computation.
I0428 20:35:50.909929  4708 net.cpp:200] pool0 needs backward computation.
I0428 20:35:50.909932  4708 net.cpp:200] conv0 needs backward computation.
I0428 20:35:50.909935  4708 net.cpp:202] mnist does not need backward computation.
I0428 20:35:50.909937  4708 net.cpp:244] This network produces output loss
I0428 20:35:50.909948  4708 net.cpp:257] Network initialization done.
I0428 20:35:50.910280  4708 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1627.prototxt
I0428 20:35:50.910306  4708 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:35:50.910414  4708 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:35:50.910496  4708 layer_factory.hpp:77] Creating layer mnist
I0428 20:35:50.910540  4708 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:35:50.910554  4708 net.cpp:86] Creating Layer mnist
I0428 20:35:50.910559  4708 net.cpp:382] mnist -> data
I0428 20:35:50.910567  4708 net.cpp:382] mnist -> label
I0428 20:35:50.910665  4708 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:35:50.911725  4708 net.cpp:124] Setting up mnist
I0428 20:35:50.911756  4708 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:35:50.911761  4708 net.cpp:131] Top shape: 100 (100)
I0428 20:35:50.911763  4708 net.cpp:139] Memory required for data: 314000
I0428 20:35:50.911767  4708 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:35:50.911793  4708 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:35:50.911798  4708 net.cpp:408] label_mnist_1_split <- label
I0428 20:35:50.911803  4708 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:35:50.911809  4708 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:35:50.911855  4708 net.cpp:124] Setting up label_mnist_1_split
I0428 20:35:50.911862  4708 net.cpp:131] Top shape: 100 (100)
I0428 20:35:50.911866  4708 net.cpp:131] Top shape: 100 (100)
I0428 20:35:50.911869  4708 net.cpp:139] Memory required for data: 314800
I0428 20:35:50.911871  4708 layer_factory.hpp:77] Creating layer conv0
I0428 20:35:50.911881  4708 net.cpp:86] Creating Layer conv0
I0428 20:35:50.911885  4708 net.cpp:408] conv0 <- data
I0428 20:35:50.911890  4708 net.cpp:382] conv0 -> conv0
I0428 20:35:50.913720  4708 net.cpp:124] Setting up conv0
I0428 20:35:50.913750  4708 net.cpp:131] Top shape: 100 100 24 24 (5760000)
I0428 20:35:50.913753  4708 net.cpp:139] Memory required for data: 23354800
I0428 20:35:50.913763  4708 layer_factory.hpp:77] Creating layer pool0
I0428 20:35:50.913770  4708 net.cpp:86] Creating Layer pool0
I0428 20:35:50.913774  4708 net.cpp:408] pool0 <- conv0
I0428 20:35:50.913779  4708 net.cpp:382] pool0 -> pool0
I0428 20:35:50.913832  4708 net.cpp:124] Setting up pool0
I0428 20:35:50.913852  4708 net.cpp:131] Top shape: 100 100 12 12 (1440000)
I0428 20:35:50.913856  4708 net.cpp:139] Memory required for data: 29114800
I0428 20:35:50.913857  4708 layer_factory.hpp:77] Creating layer conv1
I0428 20:35:50.913882  4708 net.cpp:86] Creating Layer conv1
I0428 20:35:50.913885  4708 net.cpp:408] conv1 <- pool0
I0428 20:35:50.913892  4708 net.cpp:382] conv1 -> conv1
I0428 20:35:50.917062  4708 net.cpp:124] Setting up conv1
I0428 20:35:50.917074  4708 net.cpp:131] Top shape: 100 50 8 8 (320000)
I0428 20:35:50.917094  4708 net.cpp:139] Memory required for data: 30394800
I0428 20:35:50.917102  4708 layer_factory.hpp:77] Creating layer pool1
I0428 20:35:50.917111  4708 net.cpp:86] Creating Layer pool1
I0428 20:35:50.917115  4708 net.cpp:408] pool1 <- conv1
I0428 20:35:50.917120  4708 net.cpp:382] pool1 -> pool1
I0428 20:35:50.917171  4708 net.cpp:124] Setting up pool1
I0428 20:35:50.917176  4708 net.cpp:131] Top shape: 100 50 4 4 (80000)
I0428 20:35:50.917179  4708 net.cpp:139] Memory required for data: 30714800
I0428 20:35:50.917182  4708 layer_factory.hpp:77] Creating layer ip1
I0428 20:35:50.917188  4708 net.cpp:86] Creating Layer ip1
I0428 20:35:50.917191  4708 net.cpp:408] ip1 <- pool1
I0428 20:35:50.917196  4708 net.cpp:382] ip1 -> ip1
I0428 20:35:50.917419  4708 net.cpp:124] Setting up ip1
I0428 20:35:50.917438  4708 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:35:50.917441  4708 net.cpp:139] Memory required for data: 30724800
I0428 20:35:50.917448  4708 layer_factory.hpp:77] Creating layer relu1
I0428 20:35:50.917454  4708 net.cpp:86] Creating Layer relu1
I0428 20:35:50.917459  4708 net.cpp:408] relu1 <- ip1
I0428 20:35:50.917464  4708 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:35:50.917626  4708 net.cpp:124] Setting up relu1
I0428 20:35:50.917649  4708 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:35:50.917654  4708 net.cpp:139] Memory required for data: 30734800
I0428 20:35:50.917655  4708 layer_factory.hpp:77] Creating layer ip2
I0428 20:35:50.917664  4708 net.cpp:86] Creating Layer ip2
I0428 20:35:50.917673  4708 net.cpp:408] ip2 <- ip1
I0428 20:35:50.917678  4708 net.cpp:382] ip2 -> ip2
I0428 20:35:50.917804  4708 net.cpp:124] Setting up ip2
I0428 20:35:50.917812  4708 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:35:50.917814  4708 net.cpp:139] Memory required for data: 30754800
I0428 20:35:50.917820  4708 layer_factory.hpp:77] Creating layer relu2
I0428 20:35:50.917829  4708 net.cpp:86] Creating Layer relu2
I0428 20:35:50.917832  4708 net.cpp:408] relu2 <- ip2
I0428 20:35:50.917839  4708 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:35:50.918097  4708 net.cpp:124] Setting up relu2
I0428 20:35:50.918105  4708 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:35:50.918108  4708 net.cpp:139] Memory required for data: 30774800
I0428 20:35:50.918112  4708 layer_factory.hpp:77] Creating layer ip3
I0428 20:35:50.918118  4708 net.cpp:86] Creating Layer ip3
I0428 20:35:50.918126  4708 net.cpp:408] ip3 <- ip2
I0428 20:35:50.918133  4708 net.cpp:382] ip3 -> ip3
I0428 20:35:50.918232  4708 net.cpp:124] Setting up ip3
I0428 20:35:50.918241  4708 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:35:50.918244  4708 net.cpp:139] Memory required for data: 30778800
I0428 20:35:50.918252  4708 layer_factory.hpp:77] Creating layer relu3
I0428 20:35:50.918257  4708 net.cpp:86] Creating Layer relu3
I0428 20:35:50.918261  4708 net.cpp:408] relu3 <- ip3
I0428 20:35:50.918270  4708 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:35:50.919040  4708 net.cpp:124] Setting up relu3
I0428 20:35:50.919051  4708 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:35:50.919070  4708 net.cpp:139] Memory required for data: 30782800
I0428 20:35:50.919073  4708 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:35:50.919080  4708 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:35:50.919083  4708 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:35:50.919090  4708 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:35:50.919096  4708 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:35:50.919178  4708 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:35:50.919185  4708 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:35:50.919188  4708 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:35:50.919191  4708 net.cpp:139] Memory required for data: 30790800
I0428 20:35:50.919194  4708 layer_factory.hpp:77] Creating layer accuracy
I0428 20:35:50.919200  4708 net.cpp:86] Creating Layer accuracy
I0428 20:35:50.919204  4708 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:35:50.919209  4708 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:35:50.919212  4708 net.cpp:382] accuracy -> accuracy
I0428 20:35:50.919219  4708 net.cpp:124] Setting up accuracy
I0428 20:35:50.919224  4708 net.cpp:131] Top shape: (1)
I0428 20:35:50.919226  4708 net.cpp:139] Memory required for data: 30790804
I0428 20:35:50.919229  4708 layer_factory.hpp:77] Creating layer loss
I0428 20:35:50.919234  4708 net.cpp:86] Creating Layer loss
I0428 20:35:50.919239  4708 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:35:50.919242  4708 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:35:50.919245  4708 net.cpp:382] loss -> loss
I0428 20:35:50.919251  4708 layer_factory.hpp:77] Creating layer loss
I0428 20:35:50.919490  4708 net.cpp:124] Setting up loss
I0428 20:35:50.919498  4708 net.cpp:131] Top shape: (1)
I0428 20:35:50.919502  4708 net.cpp:134]     with loss weight 1
I0428 20:35:50.919517  4708 net.cpp:139] Memory required for data: 30790808
I0428 20:35:50.919520  4708 net.cpp:200] loss needs backward computation.
I0428 20:35:50.919524  4708 net.cpp:202] accuracy does not need backward computation.
I0428 20:35:50.919528  4708 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:35:50.919538  4708 net.cpp:200] relu3 needs backward computation.
I0428 20:35:50.919541  4708 net.cpp:200] ip3 needs backward computation.
I0428 20:35:50.919544  4708 net.cpp:200] relu2 needs backward computation.
I0428 20:35:50.919546  4708 net.cpp:200] ip2 needs backward computation.
I0428 20:35:50.919550  4708 net.cpp:200] relu1 needs backward computation.
I0428 20:35:50.919553  4708 net.cpp:200] ip1 needs backward computation.
I0428 20:35:50.919555  4708 net.cpp:200] pool1 needs backward computation.
I0428 20:35:50.919564  4708 net.cpp:200] conv1 needs backward computation.
I0428 20:35:50.919566  4708 net.cpp:200] pool0 needs backward computation.
I0428 20:35:50.919569  4708 net.cpp:200] conv0 needs backward computation.
I0428 20:35:50.919574  4708 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:35:50.919576  4708 net.cpp:202] mnist does not need backward computation.
I0428 20:35:50.919579  4708 net.cpp:244] This network produces output accuracy
I0428 20:35:50.919582  4708 net.cpp:244] This network produces output loss
I0428 20:35:50.919595  4708 net.cpp:257] Network initialization done.
I0428 20:35:50.919636  4708 solver.cpp:56] Solver scaffolding done.
I0428 20:35:50.920011  4708 caffe.cpp:248] Starting Optimization
I0428 20:35:50.920018  4708 solver.cpp:273] Solving LeNet
I0428 20:35:50.920020  4708 solver.cpp:274] Learning Rate Policy: inv
I0428 20:35:50.920846  4708 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:35:50.974103  4708 blocking_queue.cpp:49] Waiting for data
I0428 20:35:51.031116  4715 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:35:51.033018  4708 solver.cpp:398]     Test net output #0: accuracy = 0.0983
I0428 20:35:51.033058  4708 solver.cpp:398]     Test net output #1: loss = 2.34551 (* 1 = 2.34551 loss)
I0428 20:35:51.041939  4708 solver.cpp:219] Iteration 0 (0 iter/s, 0.121871s/100 iters), loss = 2.34991
I0428 20:35:51.041976  4708 solver.cpp:238]     Train net output #0: loss = 2.34991 (* 1 = 2.34991 loss)
I0428 20:35:51.041988  4708 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:35:51.273507  4708 solver.cpp:219] Iteration 100 (431.945 iter/s, 0.231511s/100 iters), loss = 0.168337
I0428 20:35:51.273550  4708 solver.cpp:238]     Train net output #0: loss = 0.168337 (* 1 = 0.168337 loss)
I0428 20:35:51.273557  4708 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:35:51.501652  4708 solver.cpp:219] Iteration 200 (438.434 iter/s, 0.228085s/100 iters), loss = 0.187199
I0428 20:35:51.501691  4708 solver.cpp:238]     Train net output #0: loss = 0.187199 (* 1 = 0.187199 loss)
I0428 20:35:51.501698  4708 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:35:51.730423  4708 solver.cpp:219] Iteration 300 (437.197 iter/s, 0.22873s/100 iters), loss = 0.261533
I0428 20:35:51.730453  4708 solver.cpp:238]     Train net output #0: loss = 0.261533 (* 1 = 0.261533 loss)
I0428 20:35:51.730460  4708 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:35:51.963644  4708 solver.cpp:219] Iteration 400 (428.863 iter/s, 0.233175s/100 iters), loss = 0.118907
I0428 20:35:51.963685  4708 solver.cpp:238]     Train net output #0: loss = 0.118907 (* 1 = 0.118907 loss)
I0428 20:35:51.963707  4708 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:35:52.194070  4708 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:35:52.301991  4715 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:35:52.305081  4708 solver.cpp:398]     Test net output #0: accuracy = 0.9637
I0428 20:35:52.305119  4708 solver.cpp:398]     Test net output #1: loss = 0.10931 (* 1 = 0.10931 loss)
I0428 20:35:52.307376  4708 solver.cpp:219] Iteration 500 (290.963 iter/s, 0.343686s/100 iters), loss = 0.070964
I0428 20:35:52.307430  4708 solver.cpp:238]     Train net output #0: loss = 0.0709641 (* 1 = 0.0709641 loss)
I0428 20:35:52.307437  4708 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:35:52.546399  4708 solver.cpp:219] Iteration 600 (418.463 iter/s, 0.23897s/100 iters), loss = 0.0849884
I0428 20:35:52.546427  4708 solver.cpp:238]     Train net output #0: loss = 0.0849884 (* 1 = 0.0849884 loss)
I0428 20:35:52.546438  4708 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:35:52.783468  4708 solver.cpp:219] Iteration 700 (421.896 iter/s, 0.237025s/100 iters), loss = 0.0594733
I0428 20:35:52.783495  4708 solver.cpp:238]     Train net output #0: loss = 0.0594733 (* 1 = 0.0594733 loss)
I0428 20:35:52.783506  4708 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:35:53.024386  4708 solver.cpp:219] Iteration 800 (415.154 iter/s, 0.240874s/100 iters), loss = 0.302436
I0428 20:35:53.024428  4708 solver.cpp:238]     Train net output #0: loss = 0.302436 (* 1 = 0.302436 loss)
I0428 20:35:53.024435  4708 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:35:53.262820  4708 solver.cpp:219] Iteration 900 (419.478 iter/s, 0.238391s/100 iters), loss = 0.1443
I0428 20:35:53.262862  4708 solver.cpp:238]     Train net output #0: loss = 0.1443 (* 1 = 0.1443 loss)
I0428 20:35:53.262884  4708 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:35:53.343202  4714 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:35:53.499544  4708 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:35:53.504055  4708 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:35:53.506207  4708 solver.cpp:311] Iteration 1000, loss = 0.0748361
I0428 20:35:53.506224  4708 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:35:53.616935  4715 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:35:53.618736  4708 solver.cpp:398]     Test net output #0: accuracy = 0.9789
I0428 20:35:53.618772  4708 solver.cpp:398]     Test net output #1: loss = 0.0669792 (* 1 = 0.0669792 loss)
I0428 20:35:53.618777  4708 solver.cpp:316] Optimization Done.
I0428 20:35:53.618780  4708 caffe.cpp:259] Optimization Done.
