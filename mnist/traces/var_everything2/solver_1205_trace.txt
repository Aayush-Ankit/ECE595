I0428 20:13:44.257145 32511 caffe.cpp:218] Using GPUs 0
I0428 20:13:44.292956 32511 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:13:44.806931 32511 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1205.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:13:44.807078 32511 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1205.prototxt
I0428 20:13:44.807493 32511 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:13:44.807512 32511 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:13:44.807613 32511 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:13:44.807696 32511 layer_factory.hpp:77] Creating layer mnist
I0428 20:13:44.807798 32511 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:13:44.807821 32511 net.cpp:86] Creating Layer mnist
I0428 20:13:44.807831 32511 net.cpp:382] mnist -> data
I0428 20:13:44.807852 32511 net.cpp:382] mnist -> label
I0428 20:13:44.808946 32511 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:13:44.811419 32511 net.cpp:124] Setting up mnist
I0428 20:13:44.811439 32511 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:13:44.811450 32511 net.cpp:131] Top shape: 64 (64)
I0428 20:13:44.811455 32511 net.cpp:139] Memory required for data: 200960
I0428 20:13:44.811463 32511 layer_factory.hpp:77] Creating layer conv0
I0428 20:13:44.811480 32511 net.cpp:86] Creating Layer conv0
I0428 20:13:44.811501 32511 net.cpp:408] conv0 <- data
I0428 20:13:44.811512 32511 net.cpp:382] conv0 -> conv0
I0428 20:13:45.090561 32511 net.cpp:124] Setting up conv0
I0428 20:13:45.090605 32511 net.cpp:131] Top shape: 64 25 24 24 (921600)
I0428 20:13:45.090610 32511 net.cpp:139] Memory required for data: 3887360
I0428 20:13:45.090623 32511 layer_factory.hpp:77] Creating layer pool0
I0428 20:13:45.090636 32511 net.cpp:86] Creating Layer pool0
I0428 20:13:45.090641 32511 net.cpp:408] pool0 <- conv0
I0428 20:13:45.090646 32511 net.cpp:382] pool0 -> pool0
I0428 20:13:45.090704 32511 net.cpp:124] Setting up pool0
I0428 20:13:45.090709 32511 net.cpp:131] Top shape: 64 25 12 12 (230400)
I0428 20:13:45.090713 32511 net.cpp:139] Memory required for data: 4808960
I0428 20:13:45.090715 32511 layer_factory.hpp:77] Creating layer conv1
I0428 20:13:45.090740 32511 net.cpp:86] Creating Layer conv1
I0428 20:13:45.090744 32511 net.cpp:408] conv1 <- pool0
I0428 20:13:45.090747 32511 net.cpp:382] conv1 -> conv1
I0428 20:13:45.093031 32511 net.cpp:124] Setting up conv1
I0428 20:13:45.093060 32511 net.cpp:131] Top shape: 64 100 8 8 (409600)
I0428 20:13:45.093065 32511 net.cpp:139] Memory required for data: 6447360
I0428 20:13:45.093072 32511 layer_factory.hpp:77] Creating layer pool1
I0428 20:13:45.093081 32511 net.cpp:86] Creating Layer pool1
I0428 20:13:45.093086 32511 net.cpp:408] pool1 <- conv1
I0428 20:13:45.093091 32511 net.cpp:382] pool1 -> pool1
I0428 20:13:45.093168 32511 net.cpp:124] Setting up pool1
I0428 20:13:45.093183 32511 net.cpp:131] Top shape: 64 100 4 4 (102400)
I0428 20:13:45.093186 32511 net.cpp:139] Memory required for data: 6856960
I0428 20:13:45.093189 32511 layer_factory.hpp:77] Creating layer ip1
I0428 20:13:45.093196 32511 net.cpp:86] Creating Layer ip1
I0428 20:13:45.093199 32511 net.cpp:408] ip1 <- pool1
I0428 20:13:45.093204 32511 net.cpp:382] ip1 -> ip1
I0428 20:13:45.094565 32511 net.cpp:124] Setting up ip1
I0428 20:13:45.094578 32511 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:13:45.094596 32511 net.cpp:139] Memory required for data: 6869760
I0428 20:13:45.094619 32511 layer_factory.hpp:77] Creating layer relu1
I0428 20:13:45.094625 32511 net.cpp:86] Creating Layer relu1
I0428 20:13:45.094629 32511 net.cpp:408] relu1 <- ip1
I0428 20:13:45.094632 32511 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:13:45.094841 32511 net.cpp:124] Setting up relu1
I0428 20:13:45.094849 32511 net.cpp:131] Top shape: 64 50 (3200)
I0428 20:13:45.094852 32511 net.cpp:139] Memory required for data: 6882560
I0428 20:13:45.094856 32511 layer_factory.hpp:77] Creating layer ip2
I0428 20:13:45.094861 32511 net.cpp:86] Creating Layer ip2
I0428 20:13:45.094866 32511 net.cpp:408] ip2 <- ip1
I0428 20:13:45.094869 32511 net.cpp:382] ip2 -> ip2
I0428 20:13:45.095831 32511 net.cpp:124] Setting up ip2
I0428 20:13:45.095844 32511 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:13:45.095861 32511 net.cpp:139] Memory required for data: 6888960
I0428 20:13:45.095867 32511 layer_factory.hpp:77] Creating layer relu2
I0428 20:13:45.095873 32511 net.cpp:86] Creating Layer relu2
I0428 20:13:45.095877 32511 net.cpp:408] relu2 <- ip2
I0428 20:13:45.095881 32511 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:13:45.096693 32511 net.cpp:124] Setting up relu2
I0428 20:13:45.096704 32511 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:13:45.096722 32511 net.cpp:139] Memory required for data: 6895360
I0428 20:13:45.096725 32511 layer_factory.hpp:77] Creating layer ip3
I0428 20:13:45.096732 32511 net.cpp:86] Creating Layer ip3
I0428 20:13:45.096735 32511 net.cpp:408] ip3 <- ip2
I0428 20:13:45.096741 32511 net.cpp:382] ip3 -> ip3
I0428 20:13:45.096884 32511 net.cpp:124] Setting up ip3
I0428 20:13:45.096894 32511 net.cpp:131] Top shape: 64 10 (640)
I0428 20:13:45.096896 32511 net.cpp:139] Memory required for data: 6897920
I0428 20:13:45.096904 32511 layer_factory.hpp:77] Creating layer relu3
I0428 20:13:45.096909 32511 net.cpp:86] Creating Layer relu3
I0428 20:13:45.096928 32511 net.cpp:408] relu3 <- ip3
I0428 20:13:45.096935 32511 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:13:45.097118 32511 net.cpp:124] Setting up relu3
I0428 20:13:45.097128 32511 net.cpp:131] Top shape: 64 10 (640)
I0428 20:13:45.097131 32511 net.cpp:139] Memory required for data: 6900480
I0428 20:13:45.097136 32511 layer_factory.hpp:77] Creating layer loss
I0428 20:13:45.097141 32511 net.cpp:86] Creating Layer loss
I0428 20:13:45.097144 32511 net.cpp:408] loss <- ip3
I0428 20:13:45.097149 32511 net.cpp:408] loss <- label
I0428 20:13:45.097168 32511 net.cpp:382] loss -> loss
I0428 20:13:45.097185 32511 layer_factory.hpp:77] Creating layer loss
I0428 20:13:45.097429 32511 net.cpp:124] Setting up loss
I0428 20:13:45.097439 32511 net.cpp:131] Top shape: (1)
I0428 20:13:45.097448 32511 net.cpp:134]     with loss weight 1
I0428 20:13:45.097465 32511 net.cpp:139] Memory required for data: 6900484
I0428 20:13:45.097467 32511 net.cpp:200] loss needs backward computation.
I0428 20:13:45.097470 32511 net.cpp:200] relu3 needs backward computation.
I0428 20:13:45.097473 32511 net.cpp:200] ip3 needs backward computation.
I0428 20:13:45.097476 32511 net.cpp:200] relu2 needs backward computation.
I0428 20:13:45.097479 32511 net.cpp:200] ip2 needs backward computation.
I0428 20:13:45.097481 32511 net.cpp:200] relu1 needs backward computation.
I0428 20:13:45.097484 32511 net.cpp:200] ip1 needs backward computation.
I0428 20:13:45.097487 32511 net.cpp:200] pool1 needs backward computation.
I0428 20:13:45.097489 32511 net.cpp:200] conv1 needs backward computation.
I0428 20:13:45.097492 32511 net.cpp:200] pool0 needs backward computation.
I0428 20:13:45.097496 32511 net.cpp:200] conv0 needs backward computation.
I0428 20:13:45.097499 32511 net.cpp:202] mnist does not need backward computation.
I0428 20:13:45.097502 32511 net.cpp:244] This network produces output loss
I0428 20:13:45.097512 32511 net.cpp:257] Network initialization done.
I0428 20:13:45.097828 32511 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1205.prototxt
I0428 20:13:45.097853 32511 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:13:45.097935 32511 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:13:45.098031 32511 layer_factory.hpp:77] Creating layer mnist
I0428 20:13:45.098075 32511 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:13:45.098088 32511 net.cpp:86] Creating Layer mnist
I0428 20:13:45.098091 32511 net.cpp:382] mnist -> data
I0428 20:13:45.098099 32511 net.cpp:382] mnist -> label
I0428 20:13:45.098194 32511 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:13:45.099341 32511 net.cpp:124] Setting up mnist
I0428 20:13:45.099385 32511 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:13:45.099390 32511 net.cpp:131] Top shape: 100 (100)
I0428 20:13:45.099393 32511 net.cpp:139] Memory required for data: 314000
I0428 20:13:45.099397 32511 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:13:45.099405 32511 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:13:45.099408 32511 net.cpp:408] label_mnist_1_split <- label
I0428 20:13:45.099413 32511 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:13:45.099421 32511 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:13:45.099470 32511 net.cpp:124] Setting up label_mnist_1_split
I0428 20:13:45.099475 32511 net.cpp:131] Top shape: 100 (100)
I0428 20:13:45.099479 32511 net.cpp:131] Top shape: 100 (100)
I0428 20:13:45.099498 32511 net.cpp:139] Memory required for data: 314800
I0428 20:13:45.099500 32511 layer_factory.hpp:77] Creating layer conv0
I0428 20:13:45.099519 32511 net.cpp:86] Creating Layer conv0
I0428 20:13:45.099521 32511 net.cpp:408] conv0 <- data
I0428 20:13:45.099526 32511 net.cpp:382] conv0 -> conv0
I0428 20:13:45.101285 32511 net.cpp:124] Setting up conv0
I0428 20:13:45.101315 32511 net.cpp:131] Top shape: 100 25 24 24 (1440000)
I0428 20:13:45.101318 32511 net.cpp:139] Memory required for data: 6074800
I0428 20:13:45.101326 32511 layer_factory.hpp:77] Creating layer pool0
I0428 20:13:45.101332 32511 net.cpp:86] Creating Layer pool0
I0428 20:13:45.101336 32511 net.cpp:408] pool0 <- conv0
I0428 20:13:45.101341 32511 net.cpp:382] pool0 -> pool0
I0428 20:13:45.101390 32511 net.cpp:124] Setting up pool0
I0428 20:13:45.101397 32511 net.cpp:131] Top shape: 100 25 12 12 (360000)
I0428 20:13:45.101398 32511 net.cpp:139] Memory required for data: 7514800
I0428 20:13:45.101402 32511 layer_factory.hpp:77] Creating layer conv1
I0428 20:13:45.101409 32511 net.cpp:86] Creating Layer conv1
I0428 20:13:45.101413 32511 net.cpp:408] conv1 <- pool0
I0428 20:13:45.101418 32511 net.cpp:382] conv1 -> conv1
I0428 20:13:45.103312 32511 net.cpp:124] Setting up conv1
I0428 20:13:45.103338 32511 net.cpp:131] Top shape: 100 100 8 8 (640000)
I0428 20:13:45.103343 32511 net.cpp:139] Memory required for data: 10074800
I0428 20:13:45.103351 32511 layer_factory.hpp:77] Creating layer pool1
I0428 20:13:45.103358 32511 net.cpp:86] Creating Layer pool1
I0428 20:13:45.103360 32511 net.cpp:408] pool1 <- conv1
I0428 20:13:45.103365 32511 net.cpp:382] pool1 -> pool1
I0428 20:13:45.103404 32511 net.cpp:124] Setting up pool1
I0428 20:13:45.103410 32511 net.cpp:131] Top shape: 100 100 4 4 (160000)
I0428 20:13:45.103412 32511 net.cpp:139] Memory required for data: 10714800
I0428 20:13:45.103415 32511 layer_factory.hpp:77] Creating layer ip1
I0428 20:13:45.103421 32511 net.cpp:86] Creating Layer ip1
I0428 20:13:45.103425 32511 net.cpp:408] ip1 <- pool1
I0428 20:13:45.103430 32511 net.cpp:382] ip1 -> ip1
I0428 20:13:45.104054 32511 net.cpp:124] Setting up ip1
I0428 20:13:45.104063 32511 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:13:45.104090 32511 net.cpp:139] Memory required for data: 10734800
I0428 20:13:45.104099 32511 layer_factory.hpp:77] Creating layer relu1
I0428 20:13:45.104104 32511 net.cpp:86] Creating Layer relu1
I0428 20:13:45.104106 32511 net.cpp:408] relu1 <- ip1
I0428 20:13:45.104111 32511 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:13:45.104287 32511 net.cpp:124] Setting up relu1
I0428 20:13:45.104295 32511 net.cpp:131] Top shape: 100 50 (5000)
I0428 20:13:45.104298 32511 net.cpp:139] Memory required for data: 10754800
I0428 20:13:45.104302 32511 layer_factory.hpp:77] Creating layer ip2
I0428 20:13:45.104310 32511 net.cpp:86] Creating Layer ip2
I0428 20:13:45.104315 32511 net.cpp:408] ip2 <- ip1
I0428 20:13:45.104320 32511 net.cpp:382] ip2 -> ip2
I0428 20:13:45.104436 32511 net.cpp:124] Setting up ip2
I0428 20:13:45.104444 32511 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:13:45.104446 32511 net.cpp:139] Memory required for data: 10764800
I0428 20:13:45.104452 32511 layer_factory.hpp:77] Creating layer relu2
I0428 20:13:45.104456 32511 net.cpp:86] Creating Layer relu2
I0428 20:13:45.104460 32511 net.cpp:408] relu2 <- ip2
I0428 20:13:45.104463 32511 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:13:45.104689 32511 net.cpp:124] Setting up relu2
I0428 20:13:45.104698 32511 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:13:45.104701 32511 net.cpp:139] Memory required for data: 10774800
I0428 20:13:45.104704 32511 layer_factory.hpp:77] Creating layer ip3
I0428 20:13:45.104715 32511 net.cpp:86] Creating Layer ip3
I0428 20:13:45.104718 32511 net.cpp:408] ip3 <- ip2
I0428 20:13:45.104723 32511 net.cpp:382] ip3 -> ip3
I0428 20:13:45.104835 32511 net.cpp:124] Setting up ip3
I0428 20:13:45.104842 32511 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:13:45.104873 32511 net.cpp:139] Memory required for data: 10778800
I0428 20:13:45.104882 32511 layer_factory.hpp:77] Creating layer relu3
I0428 20:13:45.104887 32511 net.cpp:86] Creating Layer relu3
I0428 20:13:45.104890 32511 net.cpp:408] relu3 <- ip3
I0428 20:13:45.104902 32511 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:13:45.105767 32511 net.cpp:124] Setting up relu3
I0428 20:13:45.105794 32511 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:13:45.105811 32511 net.cpp:139] Memory required for data: 10782800
I0428 20:13:45.105815 32511 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:13:45.105820 32511 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:13:45.105823 32511 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:13:45.105829 32511 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:13:45.105834 32511 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:13:45.105870 32511 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:13:45.105875 32511 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:13:45.105878 32511 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:13:45.105881 32511 net.cpp:139] Memory required for data: 10790800
I0428 20:13:45.105890 32511 layer_factory.hpp:77] Creating layer accuracy
I0428 20:13:45.105895 32511 net.cpp:86] Creating Layer accuracy
I0428 20:13:45.105897 32511 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:13:45.105901 32511 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:13:45.105906 32511 net.cpp:382] accuracy -> accuracy
I0428 20:13:45.105912 32511 net.cpp:124] Setting up accuracy
I0428 20:13:45.105916 32511 net.cpp:131] Top shape: (1)
I0428 20:13:45.105919 32511 net.cpp:139] Memory required for data: 10790804
I0428 20:13:45.105922 32511 layer_factory.hpp:77] Creating layer loss
I0428 20:13:45.105926 32511 net.cpp:86] Creating Layer loss
I0428 20:13:45.105929 32511 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:13:45.105933 32511 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:13:45.105937 32511 net.cpp:382] loss -> loss
I0428 20:13:45.105943 32511 layer_factory.hpp:77] Creating layer loss
I0428 20:13:45.106168 32511 net.cpp:124] Setting up loss
I0428 20:13:45.106181 32511 net.cpp:131] Top shape: (1)
I0428 20:13:45.106184 32511 net.cpp:134]     with loss weight 1
I0428 20:13:45.106201 32511 net.cpp:139] Memory required for data: 10790808
I0428 20:13:45.106204 32511 net.cpp:200] loss needs backward computation.
I0428 20:13:45.106209 32511 net.cpp:202] accuracy does not need backward computation.
I0428 20:13:45.106212 32511 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:13:45.106216 32511 net.cpp:200] relu3 needs backward computation.
I0428 20:13:45.106235 32511 net.cpp:200] ip3 needs backward computation.
I0428 20:13:45.106238 32511 net.cpp:200] relu2 needs backward computation.
I0428 20:13:45.106242 32511 net.cpp:200] ip2 needs backward computation.
I0428 20:13:45.106245 32511 net.cpp:200] relu1 needs backward computation.
I0428 20:13:45.106247 32511 net.cpp:200] ip1 needs backward computation.
I0428 20:13:45.106256 32511 net.cpp:200] pool1 needs backward computation.
I0428 20:13:45.106259 32511 net.cpp:200] conv1 needs backward computation.
I0428 20:13:45.106262 32511 net.cpp:200] pool0 needs backward computation.
I0428 20:13:45.106266 32511 net.cpp:200] conv0 needs backward computation.
I0428 20:13:45.106268 32511 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:13:45.106272 32511 net.cpp:202] mnist does not need backward computation.
I0428 20:13:45.106276 32511 net.cpp:244] This network produces output accuracy
I0428 20:13:45.106278 32511 net.cpp:244] This network produces output loss
I0428 20:13:45.106289 32511 net.cpp:257] Network initialization done.
I0428 20:13:45.106329 32511 solver.cpp:56] Solver scaffolding done.
I0428 20:13:45.106657 32511 caffe.cpp:248] Starting Optimization
I0428 20:13:45.106662 32511 solver.cpp:273] Solving LeNet
I0428 20:13:45.106665 32511 solver.cpp:274] Learning Rate Policy: inv
I0428 20:13:45.107517 32511 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:13:45.114619 32511 blocking_queue.cpp:49] Waiting for data
I0428 20:13:45.184847 32543 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:13:45.185746 32511 solver.cpp:398]     Test net output #0: accuracy = 0.0935
I0428 20:13:45.185781 32511 solver.cpp:398]     Test net output #1: loss = 2.31069 (* 1 = 2.31069 loss)
I0428 20:13:45.190663 32511 solver.cpp:219] Iteration 0 (-6.84073e-31 iter/s, 0.0839538s/100 iters), loss = 2.29804
I0428 20:13:45.190701 32511 solver.cpp:238]     Train net output #0: loss = 2.29804 (* 1 = 2.29804 loss)
I0428 20:13:45.190712 32511 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:13:45.351352 32511 solver.cpp:219] Iteration 100 (622.462 iter/s, 0.160652s/100 iters), loss = 0.440022
I0428 20:13:45.351392 32511 solver.cpp:238]     Train net output #0: loss = 0.440022 (* 1 = 0.440022 loss)
I0428 20:13:45.351398 32511 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:13:45.510838 32511 solver.cpp:219] Iteration 200 (627.167 iter/s, 0.159447s/100 iters), loss = 0.565536
I0428 20:13:45.510879 32511 solver.cpp:238]     Train net output #0: loss = 0.565536 (* 1 = 0.565536 loss)
I0428 20:13:45.510885 32511 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:13:45.675004 32511 solver.cpp:219] Iteration 300 (609.285 iter/s, 0.164127s/100 iters), loss = 0.469942
I0428 20:13:45.675029 32511 solver.cpp:238]     Train net output #0: loss = 0.469941 (* 1 = 0.469941 loss)
I0428 20:13:45.675035 32511 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:13:45.836663 32511 solver.cpp:219] Iteration 400 (618.735 iter/s, 0.16162s/100 iters), loss = 0.34394
I0428 20:13:45.836702 32511 solver.cpp:238]     Train net output #0: loss = 0.34394 (* 1 = 0.34394 loss)
I0428 20:13:45.836709 32511 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:13:45.998617 32511 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:13:46.066450 32543 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:13:46.068918 32511 solver.cpp:398]     Test net output #0: accuracy = 0.8685
I0428 20:13:46.068970 32511 solver.cpp:398]     Test net output #1: loss = 0.345419 (* 1 = 0.345419 loss)
I0428 20:13:46.070436 32511 solver.cpp:219] Iteration 500 (427.868 iter/s, 0.233717s/100 iters), loss = 0.301768
I0428 20:13:46.070482 32511 solver.cpp:238]     Train net output #0: loss = 0.301768 (* 1 = 0.301768 loss)
I0428 20:13:46.070489 32511 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:13:46.247323 32511 solver.cpp:219] Iteration 600 (565.493 iter/s, 0.176837s/100 iters), loss = 0.452851
I0428 20:13:46.247354 32511 solver.cpp:238]     Train net output #0: loss = 0.452851 (* 1 = 0.452851 loss)
I0428 20:13:46.247362 32511 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:13:46.456617 32511 solver.cpp:219] Iteration 700 (477.908 iter/s, 0.209245s/100 iters), loss = 0.253045
I0428 20:13:46.456670 32511 solver.cpp:238]     Train net output #0: loss = 0.253045 (* 1 = 0.253045 loss)
I0428 20:13:46.456679 32511 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:13:46.675456 32511 solver.cpp:219] Iteration 800 (457.081 iter/s, 0.21878s/100 iters), loss = 0.472941
I0428 20:13:46.675500 32511 solver.cpp:238]     Train net output #0: loss = 0.472941 (* 1 = 0.472941 loss)
I0428 20:13:46.675511 32511 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:13:46.893084 32511 solver.cpp:219] Iteration 900 (459.632 iter/s, 0.217565s/100 iters), loss = 0.61038
I0428 20:13:46.893136 32511 solver.cpp:238]     Train net output #0: loss = 0.61038 (* 1 = 0.61038 loss)
I0428 20:13:46.893149 32511 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:13:46.964370 32542 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:13:47.091418 32511 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:13:47.095070 32511 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:13:47.096868 32511 solver.cpp:311] Iteration 1000, loss = 0.259958
I0428 20:13:47.096887 32511 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:13:47.166793 32543 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:13:47.169338 32511 solver.cpp:398]     Test net output #0: accuracy = 0.8747
I0428 20:13:47.169374 32511 solver.cpp:398]     Test net output #1: loss = 0.313253 (* 1 = 0.313253 loss)
I0428 20:13:47.169380 32511 solver.cpp:316] Optimization Done.
I0428 20:13:47.169384 32511 caffe.cpp:259] Optimization Done.
