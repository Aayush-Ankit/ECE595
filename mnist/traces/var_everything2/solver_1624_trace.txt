I0428 20:35:38.720366  4656 caffe.cpp:218] Using GPUs 0
I0428 20:35:38.757488  4656 caffe.cpp:223] GPU 0: TITAN X (Pascal)
I0428 20:35:39.273641  4656 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "ECE595/mnist/train_test/var_everything2/lenet_train_test1624.prototxt"
train_state {
  level: 0
  stage: ""
}
I0428 20:35:39.273783  4656 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1624.prototxt
I0428 20:35:39.274201  4656 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0428 20:35:39.274220  4656 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0428 20:35:39.274322  4656 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:35:39.274405  4656 layer_factory.hpp:77] Creating layer mnist
I0428 20:35:39.274504  4656 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0428 20:35:39.274528  4656 net.cpp:86] Creating Layer mnist
I0428 20:35:39.274535  4656 net.cpp:382] mnist -> data
I0428 20:35:39.274559  4656 net.cpp:382] mnist -> label
I0428 20:35:39.275642  4656 data_layer.cpp:45] output data size: 64,1,28,28
I0428 20:35:39.278112  4656 net.cpp:124] Setting up mnist
I0428 20:35:39.278131  4656 net.cpp:131] Top shape: 64 1 28 28 (50176)
I0428 20:35:39.278147  4656 net.cpp:131] Top shape: 64 (64)
I0428 20:35:39.278151  4656 net.cpp:139] Memory required for data: 200960
I0428 20:35:39.278158  4656 layer_factory.hpp:77] Creating layer conv0
I0428 20:35:39.278175  4656 net.cpp:86] Creating Layer conv0
I0428 20:35:39.278200  4656 net.cpp:408] conv0 <- data
I0428 20:35:39.278213  4656 net.cpp:382] conv0 -> conv0
I0428 20:35:39.551563  4656 net.cpp:124] Setting up conv0
I0428 20:35:39.551604  4656 net.cpp:131] Top shape: 64 100 24 24 (3686400)
I0428 20:35:39.551606  4656 net.cpp:139] Memory required for data: 14946560
I0428 20:35:39.551621  4656 layer_factory.hpp:77] Creating layer pool0
I0428 20:35:39.551633  4656 net.cpp:86] Creating Layer pool0
I0428 20:35:39.551637  4656 net.cpp:408] pool0 <- conv0
I0428 20:35:39.551642  4656 net.cpp:382] pool0 -> pool0
I0428 20:35:39.551707  4656 net.cpp:124] Setting up pool0
I0428 20:35:39.551712  4656 net.cpp:131] Top shape: 64 100 12 12 (921600)
I0428 20:35:39.551715  4656 net.cpp:139] Memory required for data: 18632960
I0428 20:35:39.551718  4656 layer_factory.hpp:77] Creating layer conv1
I0428 20:35:39.551729  4656 net.cpp:86] Creating Layer conv1
I0428 20:35:39.551733  4656 net.cpp:408] conv1 <- pool0
I0428 20:35:39.551738  4656 net.cpp:382] conv1 -> conv1
I0428 20:35:39.555367  4656 net.cpp:124] Setting up conv1
I0428 20:35:39.555397  4656 net.cpp:131] Top shape: 64 50 8 8 (204800)
I0428 20:35:39.555400  4656 net.cpp:139] Memory required for data: 19452160
I0428 20:35:39.555409  4656 layer_factory.hpp:77] Creating layer pool1
I0428 20:35:39.555416  4656 net.cpp:86] Creating Layer pool1
I0428 20:35:39.555420  4656 net.cpp:408] pool1 <- conv1
I0428 20:35:39.555425  4656 net.cpp:382] pool1 -> pool1
I0428 20:35:39.555479  4656 net.cpp:124] Setting up pool1
I0428 20:35:39.555483  4656 net.cpp:131] Top shape: 64 50 4 4 (51200)
I0428 20:35:39.555486  4656 net.cpp:139] Memory required for data: 19656960
I0428 20:35:39.555490  4656 layer_factory.hpp:77] Creating layer ip1
I0428 20:35:39.555496  4656 net.cpp:86] Creating Layer ip1
I0428 20:35:39.555500  4656 net.cpp:408] ip1 <- pool1
I0428 20:35:39.555505  4656 net.cpp:382] ip1 -> ip1
I0428 20:35:39.555748  4656 net.cpp:124] Setting up ip1
I0428 20:35:39.555757  4656 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:35:39.555759  4656 net.cpp:139] Memory required for data: 19663360
I0428 20:35:39.555766  4656 layer_factory.hpp:77] Creating layer relu1
I0428 20:35:39.555773  4656 net.cpp:86] Creating Layer relu1
I0428 20:35:39.555783  4656 net.cpp:408] relu1 <- ip1
I0428 20:35:39.555788  4656 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:35:39.555975  4656 net.cpp:124] Setting up relu1
I0428 20:35:39.555984  4656 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:35:39.555987  4656 net.cpp:139] Memory required for data: 19669760
I0428 20:35:39.555990  4656 layer_factory.hpp:77] Creating layer ip2
I0428 20:35:39.555996  4656 net.cpp:86] Creating Layer ip2
I0428 20:35:39.555999  4656 net.cpp:408] ip2 <- ip1
I0428 20:35:39.556005  4656 net.cpp:382] ip2 -> ip2
I0428 20:35:39.556095  4656 net.cpp:124] Setting up ip2
I0428 20:35:39.556102  4656 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:35:39.556104  4656 net.cpp:139] Memory required for data: 19676160
I0428 20:35:39.556110  4656 layer_factory.hpp:77] Creating layer relu2
I0428 20:35:39.556115  4656 net.cpp:86] Creating Layer relu2
I0428 20:35:39.556118  4656 net.cpp:408] relu2 <- ip2
I0428 20:35:39.556123  4656 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:35:39.557008  4656 net.cpp:124] Setting up relu2
I0428 20:35:39.557021  4656 net.cpp:131] Top shape: 64 25 (1600)
I0428 20:35:39.557040  4656 net.cpp:139] Memory required for data: 19682560
I0428 20:35:39.557044  4656 layer_factory.hpp:77] Creating layer ip3
I0428 20:35:39.557051  4656 net.cpp:86] Creating Layer ip3
I0428 20:35:39.557054  4656 net.cpp:408] ip3 <- ip2
I0428 20:35:39.557060  4656 net.cpp:382] ip3 -> ip3
I0428 20:35:39.557162  4656 net.cpp:124] Setting up ip3
I0428 20:35:39.557169  4656 net.cpp:131] Top shape: 64 10 (640)
I0428 20:35:39.557173  4656 net.cpp:139] Memory required for data: 19685120
I0428 20:35:39.557191  4656 layer_factory.hpp:77] Creating layer relu3
I0428 20:35:39.557212  4656 net.cpp:86] Creating Layer relu3
I0428 20:35:39.557216  4656 net.cpp:408] relu3 <- ip3
I0428 20:35:39.557236  4656 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:35:39.557411  4656 net.cpp:124] Setting up relu3
I0428 20:35:39.557420  4656 net.cpp:131] Top shape: 64 10 (640)
I0428 20:35:39.557423  4656 net.cpp:139] Memory required for data: 19687680
I0428 20:35:39.557426  4656 layer_factory.hpp:77] Creating layer loss
I0428 20:35:39.557431  4656 net.cpp:86] Creating Layer loss
I0428 20:35:39.557435  4656 net.cpp:408] loss <- ip3
I0428 20:35:39.557440  4656 net.cpp:408] loss <- label
I0428 20:35:39.557446  4656 net.cpp:382] loss -> loss
I0428 20:35:39.557463  4656 layer_factory.hpp:77] Creating layer loss
I0428 20:35:39.557723  4656 net.cpp:124] Setting up loss
I0428 20:35:39.557731  4656 net.cpp:131] Top shape: (1)
I0428 20:35:39.557734  4656 net.cpp:134]     with loss weight 1
I0428 20:35:39.557749  4656 net.cpp:139] Memory required for data: 19687684
I0428 20:35:39.557754  4656 net.cpp:200] loss needs backward computation.
I0428 20:35:39.557756  4656 net.cpp:200] relu3 needs backward computation.
I0428 20:35:39.557760  4656 net.cpp:200] ip3 needs backward computation.
I0428 20:35:39.557762  4656 net.cpp:200] relu2 needs backward computation.
I0428 20:35:39.557765  4656 net.cpp:200] ip2 needs backward computation.
I0428 20:35:39.557768  4656 net.cpp:200] relu1 needs backward computation.
I0428 20:35:39.557771  4656 net.cpp:200] ip1 needs backward computation.
I0428 20:35:39.557775  4656 net.cpp:200] pool1 needs backward computation.
I0428 20:35:39.557777  4656 net.cpp:200] conv1 needs backward computation.
I0428 20:35:39.557780  4656 net.cpp:200] pool0 needs backward computation.
I0428 20:35:39.557783  4656 net.cpp:200] conv0 needs backward computation.
I0428 20:35:39.557787  4656 net.cpp:202] mnist does not need backward computation.
I0428 20:35:39.557790  4656 net.cpp:244] This network produces output loss
I0428 20:35:39.557799  4656 net.cpp:257] Network initialization done.
I0428 20:35:39.558200  4656 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/mnist/train_test/var_everything2/lenet_train_test1624.prototxt
I0428 20:35:39.558243  4656 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0428 20:35:39.558359  4656 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 25
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "ip3"
  top: "ip3"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0428 20:35:39.558434  4656 layer_factory.hpp:77] Creating layer mnist
I0428 20:35:39.558475  4656 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0428 20:35:39.558487  4656 net.cpp:86] Creating Layer mnist
I0428 20:35:39.558491  4656 net.cpp:382] mnist -> data
I0428 20:35:39.558498  4656 net.cpp:382] mnist -> label
I0428 20:35:39.558578  4656 data_layer.cpp:45] output data size: 100,1,28,28
I0428 20:35:39.559942  4656 net.cpp:124] Setting up mnist
I0428 20:35:39.559985  4656 net.cpp:131] Top shape: 100 1 28 28 (78400)
I0428 20:35:39.559990  4656 net.cpp:131] Top shape: 100 (100)
I0428 20:35:39.560010  4656 net.cpp:139] Memory required for data: 314000
I0428 20:35:39.560014  4656 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0428 20:35:39.560025  4656 net.cpp:86] Creating Layer label_mnist_1_split
I0428 20:35:39.560029  4656 net.cpp:408] label_mnist_1_split <- label
I0428 20:35:39.560034  4656 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I0428 20:35:39.560040  4656 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I0428 20:35:39.560112  4656 net.cpp:124] Setting up label_mnist_1_split
I0428 20:35:39.560120  4656 net.cpp:131] Top shape: 100 (100)
I0428 20:35:39.560124  4656 net.cpp:131] Top shape: 100 (100)
I0428 20:35:39.560132  4656 net.cpp:139] Memory required for data: 314800
I0428 20:35:39.560134  4656 layer_factory.hpp:77] Creating layer conv0
I0428 20:35:39.560143  4656 net.cpp:86] Creating Layer conv0
I0428 20:35:39.560147  4656 net.cpp:408] conv0 <- data
I0428 20:35:39.560153  4656 net.cpp:382] conv0 -> conv0
I0428 20:35:39.561764  4656 net.cpp:124] Setting up conv0
I0428 20:35:39.561794  4656 net.cpp:131] Top shape: 100 100 24 24 (5760000)
I0428 20:35:39.561797  4656 net.cpp:139] Memory required for data: 23354800
I0428 20:35:39.561806  4656 layer_factory.hpp:77] Creating layer pool0
I0428 20:35:39.561825  4656 net.cpp:86] Creating Layer pool0
I0428 20:35:39.561828  4656 net.cpp:408] pool0 <- conv0
I0428 20:35:39.561833  4656 net.cpp:382] pool0 -> pool0
I0428 20:35:39.561879  4656 net.cpp:124] Setting up pool0
I0428 20:35:39.561887  4656 net.cpp:131] Top shape: 100 100 12 12 (1440000)
I0428 20:35:39.561889  4656 net.cpp:139] Memory required for data: 29114800
I0428 20:35:39.561892  4656 layer_factory.hpp:77] Creating layer conv1
I0428 20:35:39.561902  4656 net.cpp:86] Creating Layer conv1
I0428 20:35:39.561905  4656 net.cpp:408] conv1 <- pool0
I0428 20:35:39.561910  4656 net.cpp:382] conv1 -> conv1
I0428 20:35:39.565074  4656 net.cpp:124] Setting up conv1
I0428 20:35:39.565104  4656 net.cpp:131] Top shape: 100 50 8 8 (320000)
I0428 20:35:39.565109  4656 net.cpp:139] Memory required for data: 30394800
I0428 20:35:39.565122  4656 layer_factory.hpp:77] Creating layer pool1
I0428 20:35:39.565150  4656 net.cpp:86] Creating Layer pool1
I0428 20:35:39.565155  4656 net.cpp:408] pool1 <- conv1
I0428 20:35:39.565176  4656 net.cpp:382] pool1 -> pool1
I0428 20:35:39.565213  4656 net.cpp:124] Setting up pool1
I0428 20:35:39.565220  4656 net.cpp:131] Top shape: 100 50 4 4 (80000)
I0428 20:35:39.565223  4656 net.cpp:139] Memory required for data: 30714800
I0428 20:35:39.565227  4656 layer_factory.hpp:77] Creating layer ip1
I0428 20:35:39.565234  4656 net.cpp:86] Creating Layer ip1
I0428 20:35:39.565237  4656 net.cpp:408] ip1 <- pool1
I0428 20:35:39.565243  4656 net.cpp:382] ip1 -> ip1
I0428 20:35:39.565506  4656 net.cpp:124] Setting up ip1
I0428 20:35:39.565541  4656 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:35:39.565544  4656 net.cpp:139] Memory required for data: 30724800
I0428 20:35:39.565568  4656 layer_factory.hpp:77] Creating layer relu1
I0428 20:35:39.565590  4656 net.cpp:86] Creating Layer relu1
I0428 20:35:39.565594  4656 net.cpp:408] relu1 <- ip1
I0428 20:35:39.565598  4656 net.cpp:369] relu1 -> ip1 (in-place)
I0428 20:35:39.565788  4656 net.cpp:124] Setting up relu1
I0428 20:35:39.565798  4656 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:35:39.565800  4656 net.cpp:139] Memory required for data: 30734800
I0428 20:35:39.565804  4656 layer_factory.hpp:77] Creating layer ip2
I0428 20:35:39.565811  4656 net.cpp:86] Creating Layer ip2
I0428 20:35:39.565814  4656 net.cpp:408] ip2 <- ip1
I0428 20:35:39.565820  4656 net.cpp:382] ip2 -> ip2
I0428 20:35:39.565917  4656 net.cpp:124] Setting up ip2
I0428 20:35:39.565924  4656 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:35:39.565927  4656 net.cpp:139] Memory required for data: 30744800
I0428 20:35:39.565933  4656 layer_factory.hpp:77] Creating layer relu2
I0428 20:35:39.565946  4656 net.cpp:86] Creating Layer relu2
I0428 20:35:39.565949  4656 net.cpp:408] relu2 <- ip2
I0428 20:35:39.565954  4656 net.cpp:369] relu2 -> ip2 (in-place)
I0428 20:35:39.566148  4656 net.cpp:124] Setting up relu2
I0428 20:35:39.566156  4656 net.cpp:131] Top shape: 100 25 (2500)
I0428 20:35:39.566159  4656 net.cpp:139] Memory required for data: 30754800
I0428 20:35:39.566162  4656 layer_factory.hpp:77] Creating layer ip3
I0428 20:35:39.566167  4656 net.cpp:86] Creating Layer ip3
I0428 20:35:39.566171  4656 net.cpp:408] ip3 <- ip2
I0428 20:35:39.566175  4656 net.cpp:382] ip3 -> ip3
I0428 20:35:39.566305  4656 net.cpp:124] Setting up ip3
I0428 20:35:39.566313  4656 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:35:39.566315  4656 net.cpp:139] Memory required for data: 30758800
I0428 20:35:39.566324  4656 layer_factory.hpp:77] Creating layer relu3
I0428 20:35:39.566329  4656 net.cpp:86] Creating Layer relu3
I0428 20:35:39.566332  4656 net.cpp:408] relu3 <- ip3
I0428 20:35:39.566344  4656 net.cpp:369] relu3 -> ip3 (in-place)
I0428 20:35:39.567137  4656 net.cpp:124] Setting up relu3
I0428 20:35:39.567147  4656 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:35:39.567167  4656 net.cpp:139] Memory required for data: 30762800
I0428 20:35:39.567169  4656 layer_factory.hpp:77] Creating layer ip3_relu3_0_split
I0428 20:35:39.567176  4656 net.cpp:86] Creating Layer ip3_relu3_0_split
I0428 20:35:39.567179  4656 net.cpp:408] ip3_relu3_0_split <- ip3
I0428 20:35:39.567184  4656 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_0
I0428 20:35:39.567190  4656 net.cpp:382] ip3_relu3_0_split -> ip3_relu3_0_split_1
I0428 20:35:39.567227  4656 net.cpp:124] Setting up ip3_relu3_0_split
I0428 20:35:39.567234  4656 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:35:39.567237  4656 net.cpp:131] Top shape: 100 10 (1000)
I0428 20:35:39.567239  4656 net.cpp:139] Memory required for data: 30770800
I0428 20:35:39.567242  4656 layer_factory.hpp:77] Creating layer accuracy
I0428 20:35:39.567247  4656 net.cpp:86] Creating Layer accuracy
I0428 20:35:39.567251  4656 net.cpp:408] accuracy <- ip3_relu3_0_split_0
I0428 20:35:39.567255  4656 net.cpp:408] accuracy <- label_mnist_1_split_0
I0428 20:35:39.567260  4656 net.cpp:382] accuracy -> accuracy
I0428 20:35:39.567266  4656 net.cpp:124] Setting up accuracy
I0428 20:35:39.567270  4656 net.cpp:131] Top shape: (1)
I0428 20:35:39.567273  4656 net.cpp:139] Memory required for data: 30770804
I0428 20:35:39.567276  4656 layer_factory.hpp:77] Creating layer loss
I0428 20:35:39.567292  4656 net.cpp:86] Creating Layer loss
I0428 20:35:39.567296  4656 net.cpp:408] loss <- ip3_relu3_0_split_1
I0428 20:35:39.567299  4656 net.cpp:408] loss <- label_mnist_1_split_1
I0428 20:35:39.567303  4656 net.cpp:382] loss -> loss
I0428 20:35:39.567309  4656 layer_factory.hpp:77] Creating layer loss
I0428 20:35:39.567538  4656 net.cpp:124] Setting up loss
I0428 20:35:39.567548  4656 net.cpp:131] Top shape: (1)
I0428 20:35:39.567550  4656 net.cpp:134]     with loss weight 1
I0428 20:35:39.567567  4656 net.cpp:139] Memory required for data: 30770808
I0428 20:35:39.567571  4656 net.cpp:200] loss needs backward computation.
I0428 20:35:39.567591  4656 net.cpp:202] accuracy does not need backward computation.
I0428 20:35:39.567595  4656 net.cpp:200] ip3_relu3_0_split needs backward computation.
I0428 20:35:39.567598  4656 net.cpp:200] relu3 needs backward computation.
I0428 20:35:39.567601  4656 net.cpp:200] ip3 needs backward computation.
I0428 20:35:39.567605  4656 net.cpp:200] relu2 needs backward computation.
I0428 20:35:39.567607  4656 net.cpp:200] ip2 needs backward computation.
I0428 20:35:39.567610  4656 net.cpp:200] relu1 needs backward computation.
I0428 20:35:39.567613  4656 net.cpp:200] ip1 needs backward computation.
I0428 20:35:39.567616  4656 net.cpp:200] pool1 needs backward computation.
I0428 20:35:39.567620  4656 net.cpp:200] conv1 needs backward computation.
I0428 20:35:39.567623  4656 net.cpp:200] pool0 needs backward computation.
I0428 20:35:39.567626  4656 net.cpp:200] conv0 needs backward computation.
I0428 20:35:39.567631  4656 net.cpp:202] label_mnist_1_split does not need backward computation.
I0428 20:35:39.567634  4656 net.cpp:202] mnist does not need backward computation.
I0428 20:35:39.567637  4656 net.cpp:244] This network produces output accuracy
I0428 20:35:39.567641  4656 net.cpp:244] This network produces output loss
I0428 20:35:39.567651  4656 net.cpp:257] Network initialization done.
I0428 20:35:39.567695  4656 solver.cpp:56] Solver scaffolding done.
I0428 20:35:39.568073  4656 caffe.cpp:248] Starting Optimization
I0428 20:35:39.568078  4656 solver.cpp:273] Solving LeNet
I0428 20:35:39.568080  4656 solver.cpp:274] Learning Rate Policy: inv
I0428 20:35:39.568907  4656 solver.cpp:331] Iteration 0, Testing net (#0)
I0428 20:35:39.611765  4656 blocking_queue.cpp:49] Waiting for data
I0428 20:35:39.679102  4663 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:35:39.680806  4656 solver.cpp:398]     Test net output #0: accuracy = 0.1223
I0428 20:35:39.680860  4656 solver.cpp:398]     Test net output #1: loss = 2.27213 (* 1 = 2.27213 loss)
I0428 20:35:39.685523  4656 solver.cpp:219] Iteration 0 (-1.683e-31 iter/s, 0.1174s/100 iters), loss = 2.24316
I0428 20:35:39.685547  4656 solver.cpp:238]     Train net output #0: loss = 2.24316 (* 1 = 2.24316 loss)
I0428 20:35:39.685559  4656 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0428 20:35:39.916681  4656 solver.cpp:219] Iteration 100 (432.687 iter/s, 0.231114s/100 iters), loss = 0.528868
I0428 20:35:39.916720  4656 solver.cpp:238]     Train net output #0: loss = 0.528868 (* 1 = 0.528868 loss)
I0428 20:35:39.916728  4656 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0428 20:35:40.151360  4656 solver.cpp:219] Iteration 200 (426.214 iter/s, 0.234624s/100 iters), loss = 0.558412
I0428 20:35:40.151401  4656 solver.cpp:238]     Train net output #0: loss = 0.558412 (* 1 = 0.558412 loss)
I0428 20:35:40.151407  4656 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0428 20:35:40.390967  4656 solver.cpp:219] Iteration 300 (417.449 iter/s, 0.23955s/100 iters), loss = 0.340583
I0428 20:35:40.390992  4656 solver.cpp:238]     Train net output #0: loss = 0.340583 (* 1 = 0.340583 loss)
I0428 20:35:40.390998  4656 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0428 20:35:40.627869  4656 solver.cpp:219] Iteration 400 (422.19 iter/s, 0.23686s/100 iters), loss = 0.195552
I0428 20:35:40.627894  4656 solver.cpp:238]     Train net output #0: loss = 0.195552 (* 1 = 0.195552 loss)
I0428 20:35:40.627900  4656 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0428 20:35:40.861171  4656 solver.cpp:331] Iteration 500, Testing net (#0)
I0428 20:35:40.971215  4663 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:35:40.973001  4656 solver.cpp:398]     Test net output #0: accuracy = 0.8579
I0428 20:35:40.973037  4656 solver.cpp:398]     Test net output #1: loss = 0.361701 (* 1 = 0.361701 loss)
I0428 20:35:40.975416  4656 solver.cpp:219] Iteration 500 (287.769 iter/s, 0.347501s/100 iters), loss = 0.383664
I0428 20:35:40.975469  4656 solver.cpp:238]     Train net output #0: loss = 0.383664 (* 1 = 0.383664 loss)
I0428 20:35:40.975492  4656 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0428 20:35:41.214799  4656 solver.cpp:219] Iteration 600 (417.833 iter/s, 0.23933s/100 iters), loss = 0.528742
I0428 20:35:41.214825  4656 solver.cpp:238]     Train net output #0: loss = 0.528742 (* 1 = 0.528742 loss)
I0428 20:35:41.214846  4656 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0428 20:35:41.453745  4656 solver.cpp:219] Iteration 700 (418.578 iter/s, 0.238904s/100 iters), loss = 0.291955
I0428 20:35:41.453770  4656 solver.cpp:238]     Train net output #0: loss = 0.291955 (* 1 = 0.291955 loss)
I0428 20:35:41.453794  4656 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0428 20:35:41.691112  4656 solver.cpp:219] Iteration 800 (421.364 iter/s, 0.237324s/100 iters), loss = 0.453734
I0428 20:35:41.691136  4656 solver.cpp:238]     Train net output #0: loss = 0.453734 (* 1 = 0.453734 loss)
I0428 20:35:41.691161  4656 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0428 20:35:41.929421  4656 solver.cpp:219] Iteration 900 (419.696 iter/s, 0.238267s/100 iters), loss = 0.417956
I0428 20:35:41.929446  4656 solver.cpp:238]     Train net output #0: loss = 0.417956 (* 1 = 0.417956 loss)
I0428 20:35:41.929469  4656 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0428 20:35:42.008399  4662 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:35:42.166000  4656 solver.cpp:448] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I0428 20:35:42.170538  4656 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I0428 20:35:42.172509  4656 solver.cpp:311] Iteration 1000, loss = 0.305498
I0428 20:35:42.172525  4656 solver.cpp:331] Iteration 1000, Testing net (#0)
I0428 20:35:42.280465  4663 data_layer.cpp:73] Restarting data prefetching from start.
I0428 20:35:42.283468  4656 solver.cpp:398]     Test net output #0: accuracy = 0.8681
I0428 20:35:42.283486  4656 solver.cpp:398]     Test net output #1: loss = 0.322686 (* 1 = 0.322686 loss)
I0428 20:35:42.283491  4656 solver.cpp:316] Optimization Done.
I0428 20:35:42.283495  4656 caffe.cpp:259] Optimization Done.
