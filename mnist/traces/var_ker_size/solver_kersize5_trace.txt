I0404 16:21:02.538594 14262 caffe.cpp:211] Use CPU.
I0404 16:21:02.750792 14262 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 10000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: CPU
net: "ECE595/mnist/train_test/var_ker_size/lenet_train_test_kersize5.prototxt"
train_state {
  level: 0
  stage: ""
}
I0404 16:21:02.751003 14262 solver.cpp:87] Creating training net from net file: ECE595/mnist/train_test/var_ker_size/lenet_train_test_kersize5.prototxt
I0404 16:21:02.751247 14262 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0404 16:21:02.751256 14262 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0404 16:21:02.751390 14262 net.cpp:51] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0404 16:21:02.751471 14262 layer_factory.hpp:77] Creating layer mnist
I0404 16:21:02.751579 14262 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0404 16:21:02.751598 14262 net.cpp:84] Creating Layer mnist
I0404 16:21:02.751605 14262 net.cpp:380] mnist -> data
I0404 16:21:02.751621 14262 net.cpp:380] mnist -> label
I0404 16:21:02.751698 14262 data_layer.cpp:45] output data size: 64,1,28,28
I0404 16:21:02.752815 14262 net.cpp:122] Setting up mnist
I0404 16:21:02.752842 14262 net.cpp:129] Top shape: 64 1 28 28 (50176)
I0404 16:21:02.752846 14262 net.cpp:129] Top shape: 64 (64)
I0404 16:21:02.752847 14262 net.cpp:137] Memory required for data: 200960
I0404 16:21:02.752852 14262 layer_factory.hpp:77] Creating layer conv0
I0404 16:21:02.752882 14262 net.cpp:84] Creating Layer conv0
I0404 16:21:02.752915 14262 net.cpp:406] conv0 <- data
I0404 16:21:02.752926 14262 net.cpp:380] conv0 -> conv0
I0404 16:21:02.753056 14262 net.cpp:122] Setting up conv0
I0404 16:21:02.753074 14262 net.cpp:129] Top shape: 64 50 24 24 (1843200)
I0404 16:21:02.753077 14262 net.cpp:137] Memory required for data: 7573760
I0404 16:21:02.753088 14262 layer_factory.hpp:77] Creating layer pool0
I0404 16:21:02.753094 14262 net.cpp:84] Creating Layer pool0
I0404 16:21:02.753096 14262 net.cpp:406] pool0 <- conv0
I0404 16:21:02.753100 14262 net.cpp:380] pool0 -> pool0
I0404 16:21:02.753111 14262 net.cpp:122] Setting up pool0
I0404 16:21:02.753130 14262 net.cpp:129] Top shape: 64 50 12 12 (460800)
I0404 16:21:02.753132 14262 net.cpp:137] Memory required for data: 9416960
I0404 16:21:02.753134 14262 layer_factory.hpp:77] Creating layer conv1
I0404 16:21:02.753154 14262 net.cpp:84] Creating Layer conv1
I0404 16:21:02.753159 14262 net.cpp:406] conv1 <- pool0
I0404 16:21:02.753162 14262 net.cpp:380] conv1 -> conv1
I0404 16:21:02.753530 14262 net.cpp:122] Setting up conv1
I0404 16:21:02.753535 14262 net.cpp:129] Top shape: 64 50 8 8 (204800)
I0404 16:21:02.753556 14262 net.cpp:137] Memory required for data: 10236160
I0404 16:21:02.753561 14262 layer_factory.hpp:77] Creating layer pool1
I0404 16:21:02.753566 14262 net.cpp:84] Creating Layer pool1
I0404 16:21:02.753567 14262 net.cpp:406] pool1 <- conv1
I0404 16:21:02.753571 14262 net.cpp:380] pool1 -> pool1
I0404 16:21:02.753576 14262 net.cpp:122] Setting up pool1
I0404 16:21:02.753579 14262 net.cpp:129] Top shape: 64 50 4 4 (51200)
I0404 16:21:02.753582 14262 net.cpp:137] Memory required for data: 10440960
I0404 16:21:02.753584 14262 layer_factory.hpp:77] Creating layer conv2
I0404 16:21:02.753598 14262 net.cpp:84] Creating Layer conv2
I0404 16:21:02.753619 14262 net.cpp:406] conv2 <- pool1
I0404 16:21:02.753623 14262 net.cpp:380] conv2 -> conv2
F0404 16:21:02.754106 14262 blob.cpp:133] Check failed: data_ 
*** Check failure stack trace: ***
    @     0x7f44c30855cd  google::LogMessage::Fail()
    @     0x7f44c3087433  google::LogMessage::SendToLog()
    @     0x7f44c308515b  google::LogMessage::Flush()
    @     0x7f44c3087e1e  google::LogMessageFatal::~LogMessageFatal()
    @     0x7f44c381636b  caffe::Blob<>::mutable_cpu_data()
    @     0x7f44c374d534  caffe::BaseConvolutionLayer<>::Reshape()
    @     0x7f44c380cd37  caffe::Net<>::Init()
    @     0x7f44c380f46e  caffe::Net<>::Net()
    @     0x7f44c37d5275  caffe::Solver<>::InitTrainNet()
    @     0x7f44c37d66b5  caffe::Solver<>::Init()
    @     0x7f44c37d69df  caffe::Solver<>::Solver()
    @     0x7f44c37e6cf1  caffe::Creator_SGDSolver<>()
    @           0x40a9f8  train()
    @           0x4072f0  main
    @     0x7f44c1ff6830  __libc_start_main
    @           0x407b19  _start
    @              (nil)  (unknown)
