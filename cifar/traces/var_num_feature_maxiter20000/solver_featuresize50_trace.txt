I0426 11:08:54.094107 12550 caffe.cpp:218] Using GPUs 0
I0426 11:08:54.109155 12550 caffe.cpp:223] GPU 0: GeForce GTX 950M
I0426 11:08:54.296330 12550 solver.cpp:44] Initializing solver from parameters: 
test_iter: 10
test_interval: 1000
base_lr: 0.001
display: 100
max_iter: 20000
lr_policy: "step"
gamma: 1
momentum: 0.9
stepsize: 5000
snapshot: 10000
snapshot_prefix: "examples/cifar10_full_sigmoid"
solver_mode: GPU
device_id: 0
net: "ECE595/cifar/train_test/var_num_feature_maxiter20000/cifar_train_test_featuresize50.prototxt"
train_state {
  level: 0
  stage: ""
}
I0426 11:08:54.296497 12550 solver.cpp:87] Creating training net from net file: ECE595/cifar/train_test/var_num_feature_maxiter20000/cifar_train_test_featuresize50.prototxt
I0426 11:08:54.296680 12550 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0426 11:08:54.296691 12550 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0426 11:08:54.296766 12550 net.cpp:51] Initializing net from parameters: 
name: "CIFAR"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb"
    batch_size: 111
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Sigmoid0"
  type: "Sigmoid"
  bottom: "conv0"
  top: "Sigmoid0"
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "Sigmoid0"
  top: "pool0"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Sigmoid1"
  type: "Sigmoid"
  bottom: "conv1"
  top: "Sigmoid1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "Sigmoid1"
  top: "pool1"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Sigmoid2"
  type: "Sigmoid"
  bottom: "conv2"
  top: "Sigmoid2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "Sigmoid2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0426 11:08:54.296814 12550 layer_factory.hpp:77] Creating layer cifar
I0426 11:08:54.296891 12550 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_train_lmdb
I0426 11:08:54.296913 12550 net.cpp:84] Creating Layer cifar
I0426 11:08:54.296919 12550 net.cpp:380] cifar -> data
I0426 11:08:54.296936 12550 net.cpp:380] cifar -> label
I0426 11:08:54.296946 12550 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0426 11:08:54.297732 12550 data_layer.cpp:45] output data size: 111,3,32,32
I0426 11:08:54.301390 12550 net.cpp:122] Setting up cifar
I0426 11:08:54.301406 12550 net.cpp:129] Top shape: 111 3 32 32 (340992)
I0426 11:08:54.301424 12550 net.cpp:129] Top shape: 111 (111)
I0426 11:08:54.301427 12550 net.cpp:137] Memory required for data: 1364412
I0426 11:08:54.301434 12550 layer_factory.hpp:77] Creating layer conv0
I0426 11:08:54.301452 12550 net.cpp:84] Creating Layer conv0
I0426 11:08:54.301457 12550 net.cpp:406] conv0 <- data
I0426 11:08:54.301468 12550 net.cpp:380] conv0 -> conv0
I0426 11:08:54.302038 12550 net.cpp:122] Setting up conv0
I0426 11:08:54.302047 12550 net.cpp:129] Top shape: 111 50 32 32 (5683200)
I0426 11:08:54.302049 12550 net.cpp:137] Memory required for data: 24097212
I0426 11:08:54.302064 12550 layer_factory.hpp:77] Creating layer Sigmoid0
I0426 11:08:54.302070 12550 net.cpp:84] Creating Layer Sigmoid0
I0426 11:08:54.302074 12550 net.cpp:406] Sigmoid0 <- conv0
I0426 11:08:54.302078 12550 net.cpp:380] Sigmoid0 -> Sigmoid0
I0426 11:08:54.302098 12550 net.cpp:122] Setting up Sigmoid0
I0426 11:08:54.302103 12550 net.cpp:129] Top shape: 111 50 32 32 (5683200)
I0426 11:08:54.302104 12550 net.cpp:137] Memory required for data: 46830012
I0426 11:08:54.302106 12550 layer_factory.hpp:77] Creating layer pool0
I0426 11:08:54.302111 12550 net.cpp:84] Creating Layer pool0
I0426 11:08:54.302134 12550 net.cpp:406] pool0 <- Sigmoid0
I0426 11:08:54.302136 12550 net.cpp:380] pool0 -> pool0
I0426 11:08:54.302175 12550 net.cpp:122] Setting up pool0
I0426 11:08:54.302181 12550 net.cpp:129] Top shape: 111 50 16 16 (1420800)
I0426 11:08:54.302182 12550 net.cpp:137] Memory required for data: 52513212
I0426 11:08:54.302183 12550 layer_factory.hpp:77] Creating layer conv1
I0426 11:08:54.302191 12550 net.cpp:84] Creating Layer conv1
I0426 11:08:54.302194 12550 net.cpp:406] conv1 <- pool0
I0426 11:08:54.302199 12550 net.cpp:380] conv1 -> conv1
I0426 11:08:54.303186 12550 net.cpp:122] Setting up conv1
I0426 11:08:54.303194 12550 net.cpp:129] Top shape: 111 50 16 16 (1420800)
I0426 11:08:54.303196 12550 net.cpp:137] Memory required for data: 58196412
I0426 11:08:54.303202 12550 layer_factory.hpp:77] Creating layer Sigmoid1
I0426 11:08:54.303207 12550 net.cpp:84] Creating Layer Sigmoid1
I0426 11:08:54.303210 12550 net.cpp:406] Sigmoid1 <- conv1
I0426 11:08:54.303215 12550 net.cpp:380] Sigmoid1 -> Sigmoid1
I0426 11:08:54.303228 12550 net.cpp:122] Setting up Sigmoid1
I0426 11:08:54.303232 12550 net.cpp:129] Top shape: 111 50 16 16 (1420800)
I0426 11:08:54.303234 12550 net.cpp:137] Memory required for data: 63879612
I0426 11:08:54.303236 12550 layer_factory.hpp:77] Creating layer pool1
I0426 11:08:54.303241 12550 net.cpp:84] Creating Layer pool1
I0426 11:08:54.303242 12550 net.cpp:406] pool1 <- Sigmoid1
I0426 11:08:54.303246 12550 net.cpp:380] pool1 -> pool1
I0426 11:08:54.303280 12550 net.cpp:122] Setting up pool1
I0426 11:08:54.303284 12550 net.cpp:129] Top shape: 111 50 8 8 (355200)
I0426 11:08:54.303287 12550 net.cpp:137] Memory required for data: 65300412
I0426 11:08:54.303289 12550 layer_factory.hpp:77] Creating layer conv2
I0426 11:08:54.303297 12550 net.cpp:84] Creating Layer conv2
I0426 11:08:54.303299 12550 net.cpp:406] conv2 <- pool1
I0426 11:08:54.303303 12550 net.cpp:380] conv2 -> conv2
I0426 11:08:54.304021 12550 net.cpp:122] Setting up conv2
I0426 11:08:54.304028 12550 net.cpp:129] Top shape: 111 50 8 8 (355200)
I0426 11:08:54.304030 12550 net.cpp:137] Memory required for data: 66721212
I0426 11:08:54.304036 12550 layer_factory.hpp:77] Creating layer Sigmoid2
I0426 11:08:54.304041 12550 net.cpp:84] Creating Layer Sigmoid2
I0426 11:08:54.304045 12550 net.cpp:406] Sigmoid2 <- conv2
I0426 11:08:54.304049 12550 net.cpp:380] Sigmoid2 -> Sigmoid2
I0426 11:08:54.304297 12550 net.cpp:122] Setting up Sigmoid2
I0426 11:08:54.304302 12550 net.cpp:129] Top shape: 111 50 8 8 (355200)
I0426 11:08:54.304318 12550 net.cpp:137] Memory required for data: 68142012
I0426 11:08:54.304321 12550 layer_factory.hpp:77] Creating layer pool2
I0426 11:08:54.304324 12550 net.cpp:84] Creating Layer pool2
I0426 11:08:54.304327 12550 net.cpp:406] pool2 <- Sigmoid2
I0426 11:08:54.304330 12550 net.cpp:380] pool2 -> pool2
I0426 11:08:54.304345 12550 net.cpp:122] Setting up pool2
I0426 11:08:54.304373 12550 net.cpp:129] Top shape: 111 50 4 4 (88800)
I0426 11:08:54.304374 12550 net.cpp:137] Memory required for data: 68497212
I0426 11:08:54.304378 12550 layer_factory.hpp:77] Creating layer ip1
I0426 11:08:54.304383 12550 net.cpp:84] Creating Layer ip1
I0426 11:08:54.304388 12550 net.cpp:406] ip1 <- pool2
I0426 11:08:54.304394 12550 net.cpp:380] ip1 -> ip1
I0426 11:08:54.304893 12550 net.cpp:122] Setting up ip1
I0426 11:08:54.304901 12550 net.cpp:129] Top shape: 111 10 (1110)
I0426 11:08:54.304903 12550 net.cpp:137] Memory required for data: 68501652
I0426 11:08:54.304908 12550 layer_factory.hpp:77] Creating layer loss
I0426 11:08:54.304929 12550 net.cpp:84] Creating Layer loss
I0426 11:08:54.304932 12550 net.cpp:406] loss <- ip1
I0426 11:08:54.304935 12550 net.cpp:406] loss <- label
I0426 11:08:54.304939 12550 net.cpp:380] loss -> loss
I0426 11:08:54.304949 12550 layer_factory.hpp:77] Creating layer loss
I0426 11:08:54.305033 12550 net.cpp:122] Setting up loss
I0426 11:08:54.305037 12550 net.cpp:129] Top shape: (1)
I0426 11:08:54.305042 12550 net.cpp:132]     with loss weight 1
I0426 11:08:54.305078 12550 net.cpp:137] Memory required for data: 68501656
I0426 11:08:54.305080 12550 net.cpp:198] loss needs backward computation.
I0426 11:08:54.305085 12550 net.cpp:198] ip1 needs backward computation.
I0426 11:08:54.305088 12550 net.cpp:198] pool2 needs backward computation.
I0426 11:08:54.305090 12550 net.cpp:198] Sigmoid2 needs backward computation.
I0426 11:08:54.305093 12550 net.cpp:198] conv2 needs backward computation.
I0426 11:08:54.305109 12550 net.cpp:198] pool1 needs backward computation.
I0426 11:08:54.305110 12550 net.cpp:198] Sigmoid1 needs backward computation.
I0426 11:08:54.305114 12550 net.cpp:198] conv1 needs backward computation.
I0426 11:08:54.305115 12550 net.cpp:198] pool0 needs backward computation.
I0426 11:08:54.305119 12550 net.cpp:198] Sigmoid0 needs backward computation.
I0426 11:08:54.305120 12550 net.cpp:198] conv0 needs backward computation.
I0426 11:08:54.305124 12550 net.cpp:200] cifar does not need backward computation.
I0426 11:08:54.305125 12550 net.cpp:242] This network produces output loss
I0426 11:08:54.305135 12550 net.cpp:255] Network initialization done.
I0426 11:08:54.305294 12550 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/cifar/train_test/var_num_feature_maxiter20000/cifar_train_test_featuresize50.prototxt
I0426 11:08:54.305326 12550 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0426 11:08:54.305423 12550 net.cpp:51] Initializing net from parameters: 
name: "CIFAR"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb"
    batch_size: 1000
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Sigmoid0"
  type: "Sigmoid"
  bottom: "conv0"
  top: "Sigmoid0"
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "Sigmoid0"
  top: "pool0"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Sigmoid1"
  type: "Sigmoid"
  bottom: "conv1"
  top: "Sigmoid1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "Sigmoid1"
  top: "pool1"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Sigmoid2"
  type: "Sigmoid"
  bottom: "conv2"
  top: "Sigmoid2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "Sigmoid2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0426 11:08:54.305515 12550 layer_factory.hpp:77] Creating layer cifar
I0426 11:08:54.305579 12550 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0426 11:08:54.305598 12550 net.cpp:84] Creating Layer cifar
I0426 11:08:54.305608 12550 net.cpp:380] cifar -> data
I0426 11:08:54.305615 12550 net.cpp:380] cifar -> label
I0426 11:08:54.305621 12550 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0426 11:08:54.306000 12550 data_layer.cpp:45] output data size: 1000,3,32,32
I0426 11:08:54.332948 12550 net.cpp:122] Setting up cifar
I0426 11:08:54.332967 12550 net.cpp:129] Top shape: 1000 3 32 32 (3072000)
I0426 11:08:54.332970 12550 net.cpp:129] Top shape: 1000 (1000)
I0426 11:08:54.332973 12550 net.cpp:137] Memory required for data: 12292000
I0426 11:08:54.332978 12550 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0426 11:08:54.333017 12550 net.cpp:84] Creating Layer label_cifar_1_split
I0426 11:08:54.333020 12550 net.cpp:406] label_cifar_1_split <- label
I0426 11:08:54.333026 12550 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I0426 11:08:54.333034 12550 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I0426 11:08:54.333104 12550 net.cpp:122] Setting up label_cifar_1_split
I0426 11:08:54.333108 12550 net.cpp:129] Top shape: 1000 (1000)
I0426 11:08:54.333112 12550 net.cpp:129] Top shape: 1000 (1000)
I0426 11:08:54.333113 12550 net.cpp:137] Memory required for data: 12300000
I0426 11:08:54.333114 12550 layer_factory.hpp:77] Creating layer conv0
I0426 11:08:54.333125 12550 net.cpp:84] Creating Layer conv0
I0426 11:08:54.333128 12550 net.cpp:406] conv0 <- data
I0426 11:08:54.333151 12550 net.cpp:380] conv0 -> conv0
I0426 11:08:54.333463 12550 net.cpp:122] Setting up conv0
I0426 11:08:54.333469 12550 net.cpp:129] Top shape: 1000 50 32 32 (51200000)
I0426 11:08:54.333472 12550 net.cpp:137] Memory required for data: 217100000
I0426 11:08:54.333498 12550 layer_factory.hpp:77] Creating layer Sigmoid0
I0426 11:08:54.333501 12550 net.cpp:84] Creating Layer Sigmoid0
I0426 11:08:54.333503 12550 net.cpp:406] Sigmoid0 <- conv0
I0426 11:08:54.333508 12550 net.cpp:380] Sigmoid0 -> Sigmoid0
I0426 11:08:54.333562 12550 net.cpp:122] Setting up Sigmoid0
I0426 11:08:54.333586 12550 net.cpp:129] Top shape: 1000 50 32 32 (51200000)
I0426 11:08:54.333588 12550 net.cpp:137] Memory required for data: 421900000
I0426 11:08:54.333591 12550 layer_factory.hpp:77] Creating layer pool0
I0426 11:08:54.333612 12550 net.cpp:84] Creating Layer pool0
I0426 11:08:54.333616 12550 net.cpp:406] pool0 <- Sigmoid0
I0426 11:08:54.333618 12550 net.cpp:380] pool0 -> pool0
I0426 11:08:54.333668 12550 net.cpp:122] Setting up pool0
I0426 11:08:54.333673 12550 net.cpp:129] Top shape: 1000 50 16 16 (12800000)
I0426 11:08:54.333674 12550 net.cpp:137] Memory required for data: 473100000
I0426 11:08:54.333676 12550 layer_factory.hpp:77] Creating layer conv1
I0426 11:08:54.333683 12550 net.cpp:84] Creating Layer conv1
I0426 11:08:54.333696 12550 net.cpp:406] conv1 <- pool0
I0426 11:08:54.333701 12550 net.cpp:380] conv1 -> conv1
I0426 11:08:54.336321 12550 net.cpp:122] Setting up conv1
I0426 11:08:54.336333 12550 net.cpp:129] Top shape: 1000 50 16 16 (12800000)
I0426 11:08:54.336334 12550 net.cpp:137] Memory required for data: 524300000
I0426 11:08:54.336341 12550 layer_factory.hpp:77] Creating layer Sigmoid1
I0426 11:08:54.336349 12550 net.cpp:84] Creating Layer Sigmoid1
I0426 11:08:54.336352 12550 net.cpp:406] Sigmoid1 <- conv1
I0426 11:08:54.336375 12550 net.cpp:380] Sigmoid1 -> Sigmoid1
I0426 11:08:54.336390 12550 net.cpp:122] Setting up Sigmoid1
I0426 11:08:54.336395 12550 net.cpp:129] Top shape: 1000 50 16 16 (12800000)
I0426 11:08:54.336397 12550 net.cpp:137] Memory required for data: 575500000
I0426 11:08:54.336400 12550 layer_factory.hpp:77] Creating layer pool1
I0426 11:08:54.336405 12550 net.cpp:84] Creating Layer pool1
I0426 11:08:54.336408 12550 net.cpp:406] pool1 <- Sigmoid1
I0426 11:08:54.336412 12550 net.cpp:380] pool1 -> pool1
I0426 11:08:54.336428 12550 net.cpp:122] Setting up pool1
I0426 11:08:54.336433 12550 net.cpp:129] Top shape: 1000 50 8 8 (3200000)
I0426 11:08:54.336436 12550 net.cpp:137] Memory required for data: 588300000
I0426 11:08:54.336437 12550 layer_factory.hpp:77] Creating layer conv2
I0426 11:08:54.336463 12550 net.cpp:84] Creating Layer conv2
I0426 11:08:54.336484 12550 net.cpp:406] conv2 <- pool1
I0426 11:08:54.336488 12550 net.cpp:380] conv2 -> conv2
I0426 11:08:54.337200 12550 net.cpp:122] Setting up conv2
I0426 11:08:54.337205 12550 net.cpp:129] Top shape: 1000 50 8 8 (3200000)
I0426 11:08:54.337208 12550 net.cpp:137] Memory required for data: 601100000
I0426 11:08:54.337214 12550 layer_factory.hpp:77] Creating layer Sigmoid2
I0426 11:08:54.337219 12550 net.cpp:84] Creating Layer Sigmoid2
I0426 11:08:54.337221 12550 net.cpp:406] Sigmoid2 <- conv2
I0426 11:08:54.337242 12550 net.cpp:380] Sigmoid2 -> Sigmoid2
I0426 11:08:54.337272 12550 net.cpp:122] Setting up Sigmoid2
I0426 11:08:54.337277 12550 net.cpp:129] Top shape: 1000 50 8 8 (3200000)
I0426 11:08:54.337278 12550 net.cpp:137] Memory required for data: 613900000
I0426 11:08:54.337280 12550 layer_factory.hpp:77] Creating layer pool2
I0426 11:08:54.337285 12550 net.cpp:84] Creating Layer pool2
I0426 11:08:54.337286 12550 net.cpp:406] pool2 <- Sigmoid2
I0426 11:08:54.337291 12550 net.cpp:380] pool2 -> pool2
I0426 11:08:54.337304 12550 net.cpp:122] Setting up pool2
I0426 11:08:54.337308 12550 net.cpp:129] Top shape: 1000 50 4 4 (800000)
I0426 11:08:54.337311 12550 net.cpp:137] Memory required for data: 617100000
I0426 11:08:54.337313 12550 layer_factory.hpp:77] Creating layer ip1
I0426 11:08:54.337319 12550 net.cpp:84] Creating Layer ip1
I0426 11:08:54.337322 12550 net.cpp:406] ip1 <- pool2
I0426 11:08:54.337327 12550 net.cpp:380] ip1 -> ip1
I0426 11:08:54.337486 12550 net.cpp:122] Setting up ip1
I0426 11:08:54.337491 12550 net.cpp:129] Top shape: 1000 10 (10000)
I0426 11:08:54.337492 12550 net.cpp:137] Memory required for data: 617140000
I0426 11:08:54.337497 12550 layer_factory.hpp:77] Creating layer ip1_ip1_0_split
I0426 11:08:54.337501 12550 net.cpp:84] Creating Layer ip1_ip1_0_split
I0426 11:08:54.337502 12550 net.cpp:406] ip1_ip1_0_split <- ip1
I0426 11:08:54.337507 12550 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I0426 11:08:54.337512 12550 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I0426 11:08:54.337564 12550 net.cpp:122] Setting up ip1_ip1_0_split
I0426 11:08:54.337568 12550 net.cpp:129] Top shape: 1000 10 (10000)
I0426 11:08:54.337570 12550 net.cpp:129] Top shape: 1000 10 (10000)
I0426 11:08:54.337574 12550 net.cpp:137] Memory required for data: 617220000
I0426 11:08:54.337575 12550 layer_factory.hpp:77] Creating layer accuracy
I0426 11:08:54.337579 12550 net.cpp:84] Creating Layer accuracy
I0426 11:08:54.337581 12550 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I0426 11:08:54.337585 12550 net.cpp:406] accuracy <- label_cifar_1_split_0
I0426 11:08:54.337589 12550 net.cpp:380] accuracy -> accuracy
I0426 11:08:54.337621 12550 net.cpp:122] Setting up accuracy
I0426 11:08:54.337640 12550 net.cpp:129] Top shape: (1)
I0426 11:08:54.337642 12550 net.cpp:137] Memory required for data: 617220004
I0426 11:08:54.337644 12550 layer_factory.hpp:77] Creating layer loss
I0426 11:08:54.337647 12550 net.cpp:84] Creating Layer loss
I0426 11:08:54.337649 12550 net.cpp:406] loss <- ip1_ip1_0_split_1
I0426 11:08:54.337653 12550 net.cpp:406] loss <- label_cifar_1_split_1
I0426 11:08:54.337656 12550 net.cpp:380] loss -> loss
I0426 11:08:54.337662 12550 layer_factory.hpp:77] Creating layer loss
I0426 11:08:54.337738 12550 net.cpp:122] Setting up loss
I0426 11:08:54.337743 12550 net.cpp:129] Top shape: (1)
I0426 11:08:54.337744 12550 net.cpp:132]     with loss weight 1
I0426 11:08:54.337767 12550 net.cpp:137] Memory required for data: 617220008
I0426 11:08:54.337769 12550 net.cpp:198] loss needs backward computation.
I0426 11:08:54.337787 12550 net.cpp:200] accuracy does not need backward computation.
I0426 11:08:54.337790 12550 net.cpp:198] ip1_ip1_0_split needs backward computation.
I0426 11:08:54.337792 12550 net.cpp:198] ip1 needs backward computation.
I0426 11:08:54.337795 12550 net.cpp:198] pool2 needs backward computation.
I0426 11:08:54.337797 12550 net.cpp:198] Sigmoid2 needs backward computation.
I0426 11:08:54.337800 12550 net.cpp:198] conv2 needs backward computation.
I0426 11:08:54.337801 12550 net.cpp:198] pool1 needs backward computation.
I0426 11:08:54.337805 12550 net.cpp:198] Sigmoid1 needs backward computation.
I0426 11:08:54.337807 12550 net.cpp:198] conv1 needs backward computation.
I0426 11:08:54.337810 12550 net.cpp:198] pool0 needs backward computation.
I0426 11:08:54.337811 12550 net.cpp:198] Sigmoid0 needs backward computation.
I0426 11:08:54.337813 12550 net.cpp:198] conv0 needs backward computation.
I0426 11:08:54.337816 12550 net.cpp:200] label_cifar_1_split does not need backward computation.
I0426 11:08:54.337821 12550 net.cpp:200] cifar does not need backward computation.
I0426 11:08:54.337822 12550 net.cpp:242] This network produces output accuracy
I0426 11:08:54.337824 12550 net.cpp:242] This network produces output loss
I0426 11:08:54.337836 12550 net.cpp:255] Network initialization done.
I0426 11:08:54.337894 12550 solver.cpp:56] Solver scaffolding done.
I0426 11:08:54.338122 12550 caffe.cpp:248] Starting Optimization
I0426 11:08:54.338126 12550 solver.cpp:273] Solving CIFAR
I0426 11:08:54.338129 12550 solver.cpp:274] Learning Rate Policy: step
I0426 11:08:54.339260 12550 solver.cpp:331] Iteration 0, Testing net (#0)
I0426 11:08:54.339438 12550 blocking_queue.cpp:49] Waiting for data
I0426 11:08:56.868649 12560 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:08:58.528970 12550 solver.cpp:398]     Test net output #0: accuracy = 0.1
I0426 11:08:58.529036 12550 solver.cpp:398]     Test net output #1: loss = 2.30757 (* 1 = 2.30757 loss)
I0426 11:08:58.654994 12550 solver.cpp:219] Iteration 0 (-4.09585e-30 iter/s, 4.31613s/100 iters), loss = 2.30823
I0426 11:08:58.655081 12550 solver.cpp:238]     Train net output #0: loss = 2.30823 (* 1 = 2.30823 loss)
I0426 11:08:58.655128 12550 sgd_solver.cpp:105] Iteration 0, lr = 0.001
I0426 11:09:09.560720 12550 solver.cpp:219] Iteration 100 (9.16938 iter/s, 10.9059s/100 iters), loss = 2.32343
I0426 11:09:09.560750 12550 solver.cpp:238]     Train net output #0: loss = 2.32343 (* 1 = 2.32343 loss)
I0426 11:09:09.560755 12550 sgd_solver.cpp:105] Iteration 100, lr = 0.001
I0426 11:09:20.459259 12550 solver.cpp:219] Iteration 200 (9.17539 iter/s, 10.8987s/100 iters), loss = 2.30596
I0426 11:09:20.459296 12550 solver.cpp:238]     Train net output #0: loss = 2.30596 (* 1 = 2.30596 loss)
I0426 11:09:20.459302 12550 sgd_solver.cpp:105] Iteration 200, lr = 0.001
I0426 11:09:31.352851 12550 solver.cpp:219] Iteration 300 (9.17956 iter/s, 10.8938s/100 iters), loss = 2.30147
I0426 11:09:31.353091 12550 solver.cpp:238]     Train net output #0: loss = 2.30147 (* 1 = 2.30147 loss)
I0426 11:09:31.353116 12550 sgd_solver.cpp:105] Iteration 300, lr = 0.001
I0426 11:09:42.239181 12550 solver.cpp:219] Iteration 400 (9.18585 iter/s, 10.8863s/100 iters), loss = 2.30174
I0426 11:09:42.239217 12550 solver.cpp:238]     Train net output #0: loss = 2.30174 (* 1 = 2.30174 loss)
I0426 11:09:42.239224 12550 sgd_solver.cpp:105] Iteration 400, lr = 0.001
I0426 11:09:47.246423 12557 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:09:53.138561 12550 solver.cpp:219] Iteration 500 (9.17468 iter/s, 10.8996s/100 iters), loss = 2.30877
I0426 11:09:53.138597 12550 solver.cpp:238]     Train net output #0: loss = 2.30877 (* 1 = 2.30877 loss)
I0426 11:09:53.138602 12550 sgd_solver.cpp:105] Iteration 500, lr = 0.001
I0426 11:10:04.038421 12550 solver.cpp:219] Iteration 600 (9.17427 iter/s, 10.9s/100 iters), loss = 2.3026
I0426 11:10:04.038619 12550 solver.cpp:238]     Train net output #0: loss = 2.3026 (* 1 = 2.3026 loss)
I0426 11:10:04.038628 12550 sgd_solver.cpp:105] Iteration 600, lr = 0.001
I0426 11:10:14.928601 12550 solver.cpp:219] Iteration 700 (9.18256 iter/s, 10.8902s/100 iters), loss = 2.29388
I0426 11:10:14.928637 12550 solver.cpp:238]     Train net output #0: loss = 2.29388 (* 1 = 2.29388 loss)
I0426 11:10:14.928644 12550 sgd_solver.cpp:105] Iteration 700, lr = 0.001
I0426 11:10:25.863030 12550 solver.cpp:219] Iteration 800 (9.14527 iter/s, 10.9346s/100 iters), loss = 2.30932
I0426 11:10:25.863067 12550 solver.cpp:238]     Train net output #0: loss = 2.30932 (* 1 = 2.30932 loss)
I0426 11:10:25.863075 12550 sgd_solver.cpp:105] Iteration 800, lr = 0.001
I0426 11:10:36.316332 12557 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:10:36.754709 12550 solver.cpp:219] Iteration 900 (9.18117 iter/s, 10.8919s/100 iters), loss = 2.30205
I0426 11:10:36.754743 12550 solver.cpp:238]     Train net output #0: loss = 2.30205 (* 1 = 2.30205 loss)
I0426 11:10:36.754750 12550 sgd_solver.cpp:105] Iteration 900, lr = 0.001
I0426 11:10:47.445107 12550 solver.cpp:331] Iteration 1000, Testing net (#0)
I0426 11:10:49.988982 12560 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:10:51.628056 12550 solver.cpp:398]     Test net output #0: accuracy = 0.1645
I0426 11:10:51.628085 12550 solver.cpp:398]     Test net output #1: loss = 2.30308 (* 1 = 2.30308 loss)
I0426 11:10:51.736714 12550 solver.cpp:219] Iteration 1000 (6.67455 iter/s, 14.9823s/100 iters), loss = 2.3004
I0426 11:10:51.736743 12550 solver.cpp:238]     Train net output #0: loss = 2.3004 (* 1 = 2.3004 loss)
I0426 11:10:51.736750 12550 sgd_solver.cpp:105] Iteration 1000, lr = 0.001
I0426 11:11:02.589001 12550 solver.cpp:219] Iteration 1100 (9.21448 iter/s, 10.8525s/100 iters), loss = 2.29795
I0426 11:11:02.589064 12550 solver.cpp:238]     Train net output #0: loss = 2.29795 (* 1 = 2.29795 loss)
I0426 11:11:02.589071 12550 sgd_solver.cpp:105] Iteration 1100, lr = 0.001
I0426 11:11:13.438886 12550 solver.cpp:219] Iteration 1200 (9.21655 iter/s, 10.85s/100 iters), loss = 2.31933
I0426 11:11:13.438993 12550 solver.cpp:238]     Train net output #0: loss = 2.31933 (* 1 = 2.31933 loss)
I0426 11:11:13.439000 12550 sgd_solver.cpp:105] Iteration 1200, lr = 0.001
I0426 11:11:24.281690 12550 solver.cpp:219] Iteration 1300 (9.22261 iter/s, 10.8429s/100 iters), loss = 2.29598
I0426 11:11:24.281724 12550 solver.cpp:238]     Train net output #0: loss = 2.29598 (* 1 = 2.29598 loss)
I0426 11:11:24.281730 12550 sgd_solver.cpp:105] Iteration 1300, lr = 0.001
I0426 11:11:29.380340 12557 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:11:35.125270 12550 solver.cpp:219] Iteration 1400 (9.22189 iter/s, 10.8438s/100 iters), loss = 2.27378
I0426 11:11:35.125319 12550 solver.cpp:238]     Train net output #0: loss = 2.27378 (* 1 = 2.27378 loss)
I0426 11:11:35.125325 12550 sgd_solver.cpp:105] Iteration 1400, lr = 0.001
I0426 11:11:45.977053 12550 solver.cpp:219] Iteration 1500 (9.21493 iter/s, 10.852s/100 iters), loss = 2.26376
I0426 11:11:45.977216 12550 solver.cpp:238]     Train net output #0: loss = 2.26376 (* 1 = 2.26376 loss)
I0426 11:11:45.977226 12550 sgd_solver.cpp:105] Iteration 1500, lr = 0.001
I0426 11:11:56.839071 12550 solver.cpp:219] Iteration 1600 (9.20634 iter/s, 10.8621s/100 iters), loss = 2.2066
I0426 11:11:56.839107 12550 solver.cpp:238]     Train net output #0: loss = 2.2066 (* 1 = 2.2066 loss)
I0426 11:11:56.839113 12550 sgd_solver.cpp:105] Iteration 1600, lr = 0.001
I0426 11:12:07.697252 12550 solver.cpp:219] Iteration 1700 (9.20949 iter/s, 10.8584s/100 iters), loss = 2.0905
I0426 11:12:07.697288 12550 solver.cpp:238]     Train net output #0: loss = 2.0905 (* 1 = 2.0905 loss)
I0426 11:12:07.697294 12550 sgd_solver.cpp:105] Iteration 1700, lr = 0.001
I0426 11:12:18.218926 12557 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:12:18.543243 12550 solver.cpp:219] Iteration 1800 (9.21985 iter/s, 10.8462s/100 iters), loss = 2.02761
I0426 11:12:18.543277 12550 solver.cpp:238]     Train net output #0: loss = 2.02761 (* 1 = 2.02761 loss)
I0426 11:12:18.543283 12550 sgd_solver.cpp:105] Iteration 1800, lr = 0.001
I0426 11:12:29.401743 12550 solver.cpp:219] Iteration 1900 (9.20922 iter/s, 10.8587s/100 iters), loss = 1.93475
I0426 11:12:29.401778 12550 solver.cpp:238]     Train net output #0: loss = 1.93475 (* 1 = 1.93475 loss)
I0426 11:12:29.401784 12550 sgd_solver.cpp:105] Iteration 1900, lr = 0.001
I0426 11:12:40.210964 12550 solver.cpp:331] Iteration 2000, Testing net (#0)
I0426 11:12:42.760090 12560 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:12:44.417680 12550 solver.cpp:398]     Test net output #0: accuracy = 0.2312
I0426 11:12:44.417713 12550 solver.cpp:398]     Test net output #1: loss = 2.04195 (* 1 = 2.04195 loss)
I0426 11:12:44.525671 12550 solver.cpp:219] Iteration 2000 (6.61192 iter/s, 15.1242s/100 iters), loss = 2.03483
I0426 11:12:44.525699 12550 solver.cpp:238]     Train net output #0: loss = 2.03483 (* 1 = 2.03483 loss)
I0426 11:12:44.525707 12550 sgd_solver.cpp:105] Iteration 2000, lr = 0.001
I0426 11:12:55.386829 12550 solver.cpp:219] Iteration 2100 (9.20695 iter/s, 10.8614s/100 iters), loss = 1.9412
I0426 11:12:55.386900 12550 solver.cpp:238]     Train net output #0: loss = 1.9412 (* 1 = 1.9412 loss)
I0426 11:12:55.386907 12550 sgd_solver.cpp:105] Iteration 2100, lr = 0.001
I0426 11:13:06.284369 12550 solver.cpp:219] Iteration 2200 (9.17626 iter/s, 10.8977s/100 iters), loss = 2.03381
I0426 11:13:06.284409 12550 solver.cpp:238]     Train net output #0: loss = 2.03381 (* 1 = 2.03381 loss)
I0426 11:13:06.284417 12550 sgd_solver.cpp:105] Iteration 2200, lr = 0.001
I0426 11:13:11.518606 12557 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:13:17.185626 12550 solver.cpp:219] Iteration 2300 (9.1731 iter/s, 10.9014s/100 iters), loss = 1.88439
I0426 11:13:17.185660 12550 solver.cpp:238]     Train net output #0: loss = 1.88439 (* 1 = 1.88439 loss)
I0426 11:13:17.185667 12550 sgd_solver.cpp:105] Iteration 2300, lr = 0.001
I0426 11:13:28.049218 12550 solver.cpp:219] Iteration 2400 (9.2049 iter/s, 10.8638s/100 iters), loss = 2.02207
I0426 11:13:28.049361 12550 solver.cpp:238]     Train net output #0: loss = 2.02207 (* 1 = 2.02207 loss)
I0426 11:13:28.049370 12550 sgd_solver.cpp:105] Iteration 2400, lr = 0.001
I0426 11:13:38.944433 12550 solver.cpp:219] Iteration 2500 (9.17827 iter/s, 10.8953s/100 iters), loss = 1.95555
I0426 11:13:38.944469 12550 solver.cpp:238]     Train net output #0: loss = 1.95555 (* 1 = 1.95555 loss)
I0426 11:13:38.944475 12550 sgd_solver.cpp:105] Iteration 2500, lr = 0.001
I0426 11:13:49.832248 12550 solver.cpp:219] Iteration 2600 (9.18442 iter/s, 10.888s/100 iters), loss = 2.02853
I0426 11:13:49.832285 12550 solver.cpp:238]     Train net output #0: loss = 2.02853 (* 1 = 2.02853 loss)
I0426 11:13:49.832293 12550 sgd_solver.cpp:105] Iteration 2600, lr = 0.001
I0426 11:14:00.504518 12557 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:14:00.730166 12550 solver.cpp:219] Iteration 2700 (9.17591 iter/s, 10.8981s/100 iters), loss = 1.97998
I0426 11:14:00.730204 12550 solver.cpp:238]     Train net output #0: loss = 1.97998 (* 1 = 1.97998 loss)
I0426 11:14:00.730212 12550 sgd_solver.cpp:105] Iteration 2700, lr = 0.001
I0426 11:14:11.614326 12550 solver.cpp:219] Iteration 2800 (9.18751 iter/s, 10.8843s/100 iters), loss = 1.97989
I0426 11:14:11.614364 12550 solver.cpp:238]     Train net output #0: loss = 1.97989 (* 1 = 1.97989 loss)
I0426 11:14:11.614370 12550 sgd_solver.cpp:105] Iteration 2800, lr = 0.001
I0426 11:14:22.505121 12550 solver.cpp:219] Iteration 2900 (9.18191 iter/s, 10.891s/100 iters), loss = 1.99173
I0426 11:14:22.505162 12550 solver.cpp:238]     Train net output #0: loss = 1.99173 (* 1 = 1.99173 loss)
I0426 11:14:22.505169 12550 sgd_solver.cpp:105] Iteration 2900, lr = 0.001
I0426 11:14:33.229877 12550 solver.cpp:331] Iteration 3000, Testing net (#0)
I0426 11:14:35.789734 12560 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:14:37.447546 12550 solver.cpp:398]     Test net output #0: accuracy = 0.2615
I0426 11:14:37.447574 12550 solver.cpp:398]     Test net output #1: loss = 1.97006 (* 1 = 1.97006 loss)
I0426 11:14:37.565213 12550 solver.cpp:219] Iteration 3000 (6.63995 iter/s, 15.0604s/100 iters), loss = 2.00869
I0426 11:14:37.565250 12550 solver.cpp:238]     Train net output #0: loss = 2.00869 (* 1 = 2.00869 loss)
I0426 11:14:37.565258 12550 sgd_solver.cpp:105] Iteration 3000, lr = 0.001
I0426 11:14:48.453464 12550 solver.cpp:219] Iteration 3100 (9.18406 iter/s, 10.8884s/100 iters), loss = 2.01611
I0426 11:14:48.453500 12550 solver.cpp:238]     Train net output #0: loss = 2.01611 (* 1 = 2.01611 loss)
I0426 11:14:48.453505 12550 sgd_solver.cpp:105] Iteration 3100, lr = 0.001
I0426 11:14:53.792811 12557 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:14:59.349560 12550 solver.cpp:219] Iteration 3200 (9.17745 iter/s, 10.8963s/100 iters), loss = 2.00936
I0426 11:14:59.349617 12550 solver.cpp:238]     Train net output #0: loss = 2.00936 (* 1 = 2.00936 loss)
I0426 11:14:59.349627 12550 sgd_solver.cpp:105] Iteration 3200, lr = 0.001
I0426 11:15:10.239122 12550 solver.cpp:219] Iteration 3300 (9.18297 iter/s, 10.8897s/100 iters), loss = 2.09375
I0426 11:15:10.239258 12550 solver.cpp:238]     Train net output #0: loss = 2.09375 (* 1 = 2.09375 loss)
I0426 11:15:10.239265 12550 sgd_solver.cpp:105] Iteration 3300, lr = 0.001
I0426 11:15:21.131842 12550 solver.cpp:219] Iteration 3400 (9.18037 iter/s, 10.8928s/100 iters), loss = 2.03064
I0426 11:15:21.131913 12550 solver.cpp:238]     Train net output #0: loss = 2.03064 (* 1 = 2.03064 loss)
I0426 11:15:21.131920 12550 sgd_solver.cpp:105] Iteration 3400, lr = 0.001
I0426 11:15:32.013041 12550 solver.cpp:219] Iteration 3500 (9.19004 iter/s, 10.8813s/100 iters), loss = 1.83118
I0426 11:15:32.013078 12550 solver.cpp:238]     Train net output #0: loss = 1.83118 (* 1 = 1.83118 loss)
I0426 11:15:32.013084 12550 sgd_solver.cpp:105] Iteration 3500, lr = 0.001
I0426 11:15:42.795555 12557 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:15:42.913060 12550 solver.cpp:219] Iteration 3600 (9.17414 iter/s, 10.9002s/100 iters), loss = 1.97572
I0426 11:15:42.913099 12550 solver.cpp:238]     Train net output #0: loss = 1.97572 (* 1 = 1.97572 loss)
I0426 11:15:42.913105 12550 sgd_solver.cpp:105] Iteration 3600, lr = 0.001
I0426 11:15:53.808307 12550 solver.cpp:219] Iteration 3700 (9.17816 iter/s, 10.8954s/100 iters), loss = 1.87801
I0426 11:15:53.808344 12550 solver.cpp:238]     Train net output #0: loss = 1.87801 (* 1 = 1.87801 loss)
I0426 11:15:53.808351 12550 sgd_solver.cpp:105] Iteration 3700, lr = 0.001
I0426 11:16:04.704159 12550 solver.cpp:219] Iteration 3800 (9.17765 iter/s, 10.896s/100 iters), loss = 1.97274
I0426 11:16:04.704196 12550 solver.cpp:238]     Train net output #0: loss = 1.97274 (* 1 = 1.97274 loss)
I0426 11:16:04.704203 12550 sgd_solver.cpp:105] Iteration 3800, lr = 0.001
I0426 11:16:15.599277 12550 solver.cpp:219] Iteration 3900 (9.17827 iter/s, 10.8953s/100 iters), loss = 1.93324
I0426 11:16:15.599412 12550 solver.cpp:238]     Train net output #0: loss = 1.93324 (* 1 = 1.93324 loss)
I0426 11:16:15.599421 12550 sgd_solver.cpp:105] Iteration 3900, lr = 0.001
I0426 11:16:26.333936 12550 solver.cpp:331] Iteration 4000, Testing net (#0)
I0426 11:16:28.894726 12560 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:16:30.553094 12550 solver.cpp:398]     Test net output #0: accuracy = 0.2972
I0426 11:16:30.553122 12550 solver.cpp:398]     Test net output #1: loss = 1.91511 (* 1 = 1.91511 loss)
I0426 11:16:30.669960 12550 solver.cpp:219] Iteration 4000 (6.63532 iter/s, 15.0709s/100 iters), loss = 1.90391
I0426 11:16:30.670003 12550 solver.cpp:238]     Train net output #0: loss = 1.90391 (* 1 = 1.90391 loss)
I0426 11:16:30.670011 12550 sgd_solver.cpp:105] Iteration 4000, lr = 0.001
I0426 11:16:36.117378 12557 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:16:41.558121 12550 solver.cpp:219] Iteration 4100 (9.18413 iter/s, 10.8883s/100 iters), loss = 2.10795
I0426 11:16:41.558157 12550 solver.cpp:238]     Train net output #0: loss = 2.10795 (* 1 = 2.10795 loss)
I0426 11:16:41.558164 12550 sgd_solver.cpp:105] Iteration 4100, lr = 0.001
I0426 11:16:52.447540 12550 solver.cpp:219] Iteration 4200 (9.18307 iter/s, 10.8896s/100 iters), loss = 1.92921
I0426 11:16:52.447698 12550 solver.cpp:238]     Train net output #0: loss = 1.92921 (* 1 = 1.92921 loss)
I0426 11:16:52.447706 12550 sgd_solver.cpp:105] Iteration 4200, lr = 0.001
I0426 11:17:03.340968 12550 solver.cpp:219] Iteration 4300 (9.17979 iter/s, 10.8935s/100 iters), loss = 1.85191
I0426 11:17:03.341006 12550 solver.cpp:238]     Train net output #0: loss = 1.85191 (* 1 = 1.85191 loss)
I0426 11:17:03.341012 12550 sgd_solver.cpp:105] Iteration 4300, lr = 0.001
I0426 11:17:14.243142 12550 solver.cpp:219] Iteration 4400 (9.17233 iter/s, 10.9024s/100 iters), loss = 1.8648
I0426 11:17:14.243180 12550 solver.cpp:238]     Train net output #0: loss = 1.8648 (* 1 = 1.8648 loss)
I0426 11:17:14.243187 12550 sgd_solver.cpp:105] Iteration 4400, lr = 0.001
I0426 11:17:25.124652 12550 solver.cpp:219] Iteration 4500 (9.18975 iter/s, 10.8817s/100 iters), loss = 1.93537
I0426 11:17:25.124805 12550 solver.cpp:238]     Train net output #0: loss = 1.93537 (* 1 = 1.93537 loss)
I0426 11:17:25.124814 12550 sgd_solver.cpp:105] Iteration 4500, lr = 0.001
I0426 11:17:25.125838 12557 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:17:36.015872 12550 solver.cpp:219] Iteration 4600 (9.18165 iter/s, 10.8913s/100 iters), loss = 1.87563
I0426 11:17:36.015909 12550 solver.cpp:238]     Train net output #0: loss = 1.87563 (* 1 = 1.87563 loss)
I0426 11:17:36.015915 12550 sgd_solver.cpp:105] Iteration 4600, lr = 0.001
I0426 11:17:46.912760 12550 solver.cpp:219] Iteration 4700 (9.17677 iter/s, 10.8971s/100 iters), loss = 1.7879
I0426 11:17:46.912796 12550 solver.cpp:238]     Train net output #0: loss = 1.7879 (* 1 = 1.7879 loss)
I0426 11:17:46.912803 12550 sgd_solver.cpp:105] Iteration 4700, lr = 0.001
I0426 11:17:57.815517 12550 solver.cpp:219] Iteration 4800 (9.17183 iter/s, 10.9029s/100 iters), loss = 2.07012
I0426 11:17:57.815578 12550 solver.cpp:238]     Train net output #0: loss = 2.07012 (* 1 = 2.07012 loss)
I0426 11:17:57.815585 12550 sgd_solver.cpp:105] Iteration 4800, lr = 0.001
I0426 11:18:08.714970 12550 solver.cpp:219] Iteration 4900 (9.17464 iter/s, 10.8996s/100 iters), loss = 1.81436
I0426 11:18:08.715009 12550 solver.cpp:238]     Train net output #0: loss = 1.81436 (* 1 = 1.81436 loss)
I0426 11:18:08.715016 12550 sgd_solver.cpp:105] Iteration 4900, lr = 0.001
I0426 11:18:14.174963 12557 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:18:19.467591 12550 solver.cpp:331] Iteration 5000, Testing net (#0)
I0426 11:18:22.042662 12560 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:18:23.707777 12550 solver.cpp:398]     Test net output #0: accuracy = 0.2922
I0426 11:18:23.707806 12550 solver.cpp:398]     Test net output #1: loss = 1.88836 (* 1 = 1.88836 loss)
I0426 11:18:23.823865 12550 solver.cpp:219] Iteration 5000 (6.6185 iter/s, 15.1092s/100 iters), loss = 1.89496
I0426 11:18:23.823902 12550 solver.cpp:238]     Train net output #0: loss = 1.89496 (* 1 = 1.89496 loss)
I0426 11:18:23.823910 12550 sgd_solver.cpp:105] Iteration 5000, lr = 0.001
I0426 11:18:34.709969 12550 solver.cpp:219] Iteration 5100 (9.18586 iter/s, 10.8863s/100 iters), loss = 1.84265
I0426 11:18:34.710142 12550 solver.cpp:238]     Train net output #0: loss = 1.84265 (* 1 = 1.84265 loss)
I0426 11:18:34.710150 12550 sgd_solver.cpp:105] Iteration 5100, lr = 0.001
I0426 11:18:45.598168 12550 solver.cpp:219] Iteration 5200 (9.18421 iter/s, 10.8883s/100 iters), loss = 1.98731
I0426 11:18:45.598206 12550 solver.cpp:238]     Train net output #0: loss = 1.98731 (* 1 = 1.98731 loss)
I0426 11:18:45.598213 12550 sgd_solver.cpp:105] Iteration 5200, lr = 0.001
I0426 11:18:56.491224 12550 solver.cpp:219] Iteration 5300 (9.18001 iter/s, 10.8932s/100 iters), loss = 1.83594
I0426 11:18:56.491266 12550 solver.cpp:238]     Train net output #0: loss = 1.83594 (* 1 = 1.83594 loss)
I0426 11:18:56.491273 12550 sgd_solver.cpp:105] Iteration 5300, lr = 0.001
I0426 11:19:07.385824 12550 solver.cpp:219] Iteration 5400 (9.17871 iter/s, 10.8948s/100 iters), loss = 1.91478
I0426 11:19:07.385884 12550 solver.cpp:238]     Train net output #0: loss = 1.91478 (* 1 = 1.91478 loss)
I0426 11:19:07.385890 12550 sgd_solver.cpp:105] Iteration 5400, lr = 0.001
I0426 11:19:07.503793 12557 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:19:18.279161 12550 solver.cpp:219] Iteration 5500 (9.17978 iter/s, 10.8935s/100 iters), loss = 1.94211
I0426 11:19:18.279198 12550 solver.cpp:238]     Train net output #0: loss = 1.94211 (* 1 = 1.94211 loss)
I0426 11:19:18.279204 12550 sgd_solver.cpp:105] Iteration 5500, lr = 0.001
I0426 11:19:29.165421 12550 solver.cpp:219] Iteration 5600 (9.18573 iter/s, 10.8864s/100 iters), loss = 1.7077
I0426 11:19:29.165459 12550 solver.cpp:238]     Train net output #0: loss = 1.7077 (* 1 = 1.7077 loss)
I0426 11:19:29.165467 12550 sgd_solver.cpp:105] Iteration 5600, lr = 0.001
I0426 11:19:40.055560 12550 solver.cpp:219] Iteration 5700 (9.18246 iter/s, 10.8903s/100 iters), loss = 1.82842
I0426 11:19:40.055697 12550 solver.cpp:238]     Train net output #0: loss = 1.82842 (* 1 = 1.82842 loss)
I0426 11:19:40.055706 12550 sgd_solver.cpp:105] Iteration 5700, lr = 0.001
I0426 11:19:58.361228 12550 solver.cpp:219] Iteration 5800 (5.46271 iter/s, 18.3059s/100 iters), loss = 1.94393
I0426 11:19:58.361263 12550 solver.cpp:238]     Train net output #0: loss = 1.94393 (* 1 = 1.94393 loss)
I0426 11:19:58.361270 12550 sgd_solver.cpp:105] Iteration 5800, lr = 0.001
I0426 11:20:17.373631 12557 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:20:35.624085 12550 solver.cpp:219] Iteration 5900 (2.68358 iter/s, 37.2636s/100 iters), loss = 1.74243
I0426 11:20:35.624124 12550 solver.cpp:238]     Train net output #0: loss = 1.74243 (* 1 = 1.74243 loss)
I0426 11:20:35.624131 12550 sgd_solver.cpp:105] Iteration 5900, lr = 0.001
I0426 11:21:12.272243 12550 solver.cpp:331] Iteration 6000, Testing net (#0)
I0426 11:21:21.470778 12560 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:21:27.457808 12550 solver.cpp:398]     Test net output #0: accuracy = 0.3391
I0426 11:21:27.457849 12550 solver.cpp:398]     Test net output #1: loss = 1.83103 (* 1 = 1.83103 loss)
I0426 11:21:27.843050 12550 solver.cpp:219] Iteration 6000 (1.91498 iter/s, 52.22s/100 iters), loss = 1.74951
I0426 11:21:27.843080 12550 solver.cpp:238]     Train net output #0: loss = 1.74951 (* 1 = 1.74951 loss)
I0426 11:21:27.843103 12550 sgd_solver.cpp:105] Iteration 6000, lr = 0.001
I0426 11:22:05.091378 12550 solver.cpp:219] Iteration 6100 (2.68464 iter/s, 37.249s/100 iters), loss = 1.77781
I0426 11:22:05.091439 12550 solver.cpp:238]     Train net output #0: loss = 1.77781 (* 1 = 1.77781 loss)
I0426 11:22:05.091444 12550 sgd_solver.cpp:105] Iteration 6100, lr = 0.001
I0426 11:22:42.321251 12550 solver.cpp:219] Iteration 6200 (2.68597 iter/s, 37.2305s/100 iters), loss = 1.93421
I0426 11:22:42.321440 12550 solver.cpp:238]     Train net output #0: loss = 1.93421 (* 1 = 1.93421 loss)
I0426 11:22:42.321461 12550 sgd_solver.cpp:105] Iteration 6200, lr = 0.001
I0426 11:23:19.522727 12550 solver.cpp:219] Iteration 6300 (2.68803 iter/s, 37.202s/100 iters), loss = 1.8246
I0426 11:23:19.522845 12550 solver.cpp:238]     Train net output #0: loss = 1.8246 (* 1 = 1.8246 loss)
I0426 11:23:19.522852 12550 sgd_solver.cpp:105] Iteration 6300, lr = 0.001
I0426 11:23:20.286038 12557 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:23:56.776708 12550 solver.cpp:219] Iteration 6400 (2.68423 iter/s, 37.2546s/100 iters), loss = 1.90916
I0426 11:23:56.776846 12550 solver.cpp:238]     Train net output #0: loss = 1.90916 (* 1 = 1.90916 loss)
I0426 11:23:56.776855 12550 sgd_solver.cpp:105] Iteration 6400, lr = 0.001
I0426 11:24:33.967021 12550 solver.cpp:219] Iteration 6500 (2.68883 iter/s, 37.1909s/100 iters), loss = 1.67553
I0426 11:24:33.967198 12550 solver.cpp:238]     Train net output #0: loss = 1.67553 (* 1 = 1.67553 loss)
I0426 11:24:33.967205 12550 sgd_solver.cpp:105] Iteration 6500, lr = 0.001
I0426 11:25:11.149792 12550 solver.cpp:219] Iteration 6600 (2.68938 iter/s, 37.1833s/100 iters), loss = 1.88655
I0426 11:25:11.149922 12550 solver.cpp:238]     Train net output #0: loss = 1.88655 (* 1 = 1.88655 loss)
I0426 11:25:11.149930 12550 sgd_solver.cpp:105] Iteration 6600, lr = 0.001
I0426 11:25:48.356148 12550 solver.cpp:219] Iteration 6700 (2.68767 iter/s, 37.2069s/100 iters), loss = 1.84742
I0426 11:25:48.356230 12550 solver.cpp:238]     Train net output #0: loss = 1.84742 (* 1 = 1.84742 loss)
I0426 11:25:48.356236 12550 sgd_solver.cpp:105] Iteration 6700, lr = 0.001
I0426 11:26:07.753424 12557 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:26:25.634204 12550 solver.cpp:219] Iteration 6800 (2.6825 iter/s, 37.2787s/100 iters), loss = 1.8607
I0426 11:26:25.634376 12550 solver.cpp:238]     Train net output #0: loss = 1.8607 (* 1 = 1.8607 loss)
I0426 11:26:25.634382 12550 sgd_solver.cpp:105] Iteration 6800, lr = 0.001
I0426 11:27:02.849752 12550 solver.cpp:219] Iteration 6900 (2.68701 iter/s, 37.2161s/100 iters), loss = 1.90395
I0426 11:27:02.849922 12550 solver.cpp:238]     Train net output #0: loss = 1.90395 (* 1 = 1.90395 loss)
I0426 11:27:02.849928 12550 sgd_solver.cpp:105] Iteration 6900, lr = 0.001
I0426 11:27:39.506609 12550 solver.cpp:331] Iteration 7000, Testing net (#0)
I0426 11:27:48.729287 12560 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:27:54.803820 12550 solver.cpp:398]     Test net output #0: accuracy = 0.346
I0426 11:27:54.803845 12550 solver.cpp:398]     Test net output #1: loss = 1.80091 (* 1 = 1.80091 loss)
I0426 11:27:55.176766 12550 solver.cpp:219] Iteration 7000 (1.91103 iter/s, 52.3278s/100 iters), loss = 1.92205
I0426 11:27:55.176789 12550 solver.cpp:238]     Train net output #0: loss = 1.92205 (* 1 = 1.92205 loss)
I0426 11:27:55.176812 12550 sgd_solver.cpp:105] Iteration 7000, lr = 0.001
I0426 11:28:32.420418 12550 solver.cpp:219] Iteration 7100 (2.68497 iter/s, 37.2443s/100 iters), loss = 1.85516
I0426 11:28:32.420544 12550 solver.cpp:238]     Train net output #0: loss = 1.85516 (* 1 = 1.85516 loss)
I0426 11:28:32.420552 12550 sgd_solver.cpp:105] Iteration 7100, lr = 0.001
I0426 11:29:09.568070 12550 solver.cpp:219] Iteration 7200 (2.69192 iter/s, 37.1482s/100 iters), loss = 1.75806
I0426 11:29:09.568244 12550 solver.cpp:238]     Train net output #0: loss = 1.75806 (* 1 = 1.75806 loss)
I0426 11:29:09.568251 12550 sgd_solver.cpp:105] Iteration 7200, lr = 0.001
I0426 11:29:10.686044 12557 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:29:46.728497 12550 solver.cpp:219] Iteration 7300 (2.691 iter/s, 37.1609s/100 iters), loss = 1.82914
I0426 11:29:46.728668 12550 solver.cpp:238]     Train net output #0: loss = 1.82914 (* 1 = 1.82914 loss)
I0426 11:29:46.728675 12550 sgd_solver.cpp:105] Iteration 7300, lr = 0.001
I0426 11:30:23.907142 12550 solver.cpp:219] Iteration 7400 (2.68968 iter/s, 37.1792s/100 iters), loss = 1.81675
I0426 11:30:23.907270 12550 solver.cpp:238]     Train net output #0: loss = 1.81675 (* 1 = 1.81675 loss)
I0426 11:30:23.907279 12550 sgd_solver.cpp:105] Iteration 7400, lr = 0.001
I0426 11:31:01.067487 12550 solver.cpp:219] Iteration 7500 (2.691 iter/s, 37.1609s/100 iters), loss = 1.8052
I0426 11:31:01.067683 12550 solver.cpp:238]     Train net output #0: loss = 1.8052 (* 1 = 1.8052 loss)
I0426 11:31:01.067690 12550 sgd_solver.cpp:105] Iteration 7500, lr = 0.001
I0426 11:31:38.221335 12550 solver.cpp:219] Iteration 7600 (2.69148 iter/s, 37.1543s/100 iters), loss = 1.73753
I0426 11:31:38.221429 12550 solver.cpp:238]     Train net output #0: loss = 1.73753 (* 1 = 1.73753 loss)
I0426 11:31:38.221436 12550 sgd_solver.cpp:105] Iteration 7600, lr = 0.001
I0426 11:31:57.908463 12557 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:32:15.372711 12550 solver.cpp:219] Iteration 7700 (2.69165 iter/s, 37.152s/100 iters), loss = 1.69014
I0426 11:32:15.372790 12550 solver.cpp:238]     Train net output #0: loss = 1.69014 (* 1 = 1.69014 loss)
I0426 11:32:15.372797 12550 sgd_solver.cpp:105] Iteration 7700, lr = 0.001
I0426 11:32:52.512132 12550 solver.cpp:219] Iteration 7800 (2.69251 iter/s, 37.14s/100 iters), loss = 1.88298
I0426 11:32:52.512320 12550 solver.cpp:238]     Train net output #0: loss = 1.88298 (* 1 = 1.88298 loss)
I0426 11:32:52.512331 12550 sgd_solver.cpp:105] Iteration 7800, lr = 0.001
I0426 11:33:29.652878 12550 solver.cpp:219] Iteration 7900 (2.69242 iter/s, 37.1413s/100 iters), loss = 1.87291
I0426 11:33:29.653048 12550 solver.cpp:238]     Train net output #0: loss = 1.87291 (* 1 = 1.87291 loss)
I0426 11:33:29.653053 12550 sgd_solver.cpp:105] Iteration 7900, lr = 0.001
I0426 11:34:06.250385 12550 solver.cpp:331] Iteration 8000, Testing net (#0)
I0426 11:34:15.421448 12560 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:34:21.355821 12550 solver.cpp:398]     Test net output #0: accuracy = 0.3702
I0426 11:34:21.355898 12550 solver.cpp:398]     Test net output #1: loss = 1.75609 (* 1 = 1.75609 loss)
I0426 11:34:21.728830 12550 solver.cpp:219] Iteration 8000 (1.92024 iter/s, 52.0767s/100 iters), loss = 1.79895
I0426 11:34:21.728910 12550 solver.cpp:238]     Train net output #0: loss = 1.79895 (* 1 = 1.79895 loss)
I0426 11:34:21.728930 12550 sgd_solver.cpp:105] Iteration 8000, lr = 0.001
I0426 11:34:58.901731 12550 solver.cpp:219] Iteration 8100 (2.69009 iter/s, 37.1735s/100 iters), loss = 1.65723
I0426 11:34:58.901794 12550 solver.cpp:238]     Train net output #0: loss = 1.65723 (* 1 = 1.65723 loss)
I0426 11:34:58.901799 12550 sgd_solver.cpp:105] Iteration 8100, lr = 0.001
I0426 11:35:00.392773 12557 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:35:36.036679 12550 solver.cpp:219] Iteration 8200 (2.69284 iter/s, 37.1356s/100 iters), loss = 1.87847
I0426 11:35:36.036948 12550 solver.cpp:238]     Train net output #0: loss = 1.87847 (* 1 = 1.87847 loss)
I0426 11:35:36.036973 12550 sgd_solver.cpp:105] Iteration 8200, lr = 0.001
I0426 11:36:13.196251 12550 solver.cpp:219] Iteration 8300 (2.69106 iter/s, 37.16s/100 iters), loss = 1.82865
I0426 11:36:13.196298 12550 solver.cpp:238]     Train net output #0: loss = 1.82865 (* 1 = 1.82865 loss)
I0426 11:36:13.196303 12550 sgd_solver.cpp:105] Iteration 8300, lr = 0.001
I0426 11:36:50.342133 12550 solver.cpp:219] Iteration 8400 (2.69204 iter/s, 37.1465s/100 iters), loss = 1.65165
I0426 11:36:50.342396 12550 solver.cpp:238]     Train net output #0: loss = 1.65165 (* 1 = 1.65165 loss)
I0426 11:36:50.342423 12550 sgd_solver.cpp:105] Iteration 8400, lr = 0.001
I0426 11:37:27.498059 12550 solver.cpp:219] Iteration 8500 (2.69133 iter/s, 37.1564s/100 iters), loss = 1.77605
I0426 11:37:27.498174 12550 solver.cpp:238]     Train net output #0: loss = 1.77605 (* 1 = 1.77605 loss)
I0426 11:37:27.498179 12550 sgd_solver.cpp:105] Iteration 8500, lr = 0.001
I0426 11:37:47.556076 12557 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:38:04.642035 12550 solver.cpp:219] Iteration 8600 (2.69218 iter/s, 37.1446s/100 iters), loss = 1.74752
I0426 11:38:04.642138 12550 solver.cpp:238]     Train net output #0: loss = 1.74752 (* 1 = 1.74752 loss)
I0426 11:38:04.642144 12550 sgd_solver.cpp:105] Iteration 8600, lr = 0.001
I0426 11:38:41.789659 12550 solver.cpp:219] Iteration 8700 (2.69192 iter/s, 37.1482s/100 iters), loss = 1.76213
I0426 11:38:41.789839 12550 solver.cpp:238]     Train net output #0: loss = 1.76213 (* 1 = 1.76213 loss)
I0426 11:38:41.789846 12550 sgd_solver.cpp:105] Iteration 8700, lr = 0.001
I0426 11:39:18.904362 12550 solver.cpp:219] Iteration 8800 (2.69431 iter/s, 37.1152s/100 iters), loss = 1.77601
I0426 11:39:18.904412 12550 solver.cpp:238]     Train net output #0: loss = 1.77601 (* 1 = 1.77601 loss)
I0426 11:39:18.904417 12550 sgd_solver.cpp:105] Iteration 8800, lr = 0.001
I0426 11:39:56.020196 12550 solver.cpp:219] Iteration 8900 (2.69422 iter/s, 37.1165s/100 iters), loss = 1.7636
I0426 11:39:56.020287 12550 solver.cpp:238]     Train net output #0: loss = 1.7636 (* 1 = 1.7636 loss)
I0426 11:39:56.020293 12550 sgd_solver.cpp:105] Iteration 8900, lr = 0.001
I0426 11:40:32.618777 12550 solver.cpp:331] Iteration 9000, Testing net (#0)
I0426 11:40:41.703670 12560 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:40:47.695868 12550 solver.cpp:398]     Test net output #0: accuracy = 0.3817
I0426 11:40:47.695894 12550 solver.cpp:398]     Test net output #1: loss = 1.72927 (* 1 = 1.72927 loss)
I0426 11:40:48.065106 12550 solver.cpp:219] Iteration 9000 (1.92138 iter/s, 52.0458s/100 iters), loss = 1.82446
I0426 11:40:48.065135 12550 solver.cpp:238]     Train net output #0: loss = 1.82446 (* 1 = 1.82446 loss)
I0426 11:40:48.065141 12550 sgd_solver.cpp:105] Iteration 9000, lr = 0.001
I0426 11:40:49.921478 12557 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:41:25.179102 12550 solver.cpp:219] Iteration 9100 (2.69435 iter/s, 37.1147s/100 iters), loss = 1.85531
I0426 11:41:25.179299 12550 solver.cpp:238]     Train net output #0: loss = 1.85531 (* 1 = 1.85531 loss)
I0426 11:41:25.179306 12550 sgd_solver.cpp:105] Iteration 9100, lr = 0.001
I0426 11:42:02.301596 12550 solver.cpp:219] Iteration 9200 (2.69375 iter/s, 37.123s/100 iters), loss = 1.59844
I0426 11:42:02.301702 12550 solver.cpp:238]     Train net output #0: loss = 1.59844 (* 1 = 1.59844 loss)
I0426 11:42:02.301707 12550 sgd_solver.cpp:105] Iteration 9200, lr = 0.001
I0426 11:42:39.429061 12550 solver.cpp:219] Iteration 9300 (2.69338 iter/s, 37.128s/100 iters), loss = 1.70805
I0426 11:42:39.429113 12550 solver.cpp:238]     Train net output #0: loss = 1.70805 (* 1 = 1.70805 loss)
I0426 11:42:39.429118 12550 sgd_solver.cpp:105] Iteration 9300, lr = 0.001
I0426 11:43:16.527762 12550 solver.cpp:219] Iteration 9400 (2.69547 iter/s, 37.0993s/100 iters), loss = 1.82763
I0426 11:43:16.527892 12550 solver.cpp:238]     Train net output #0: loss = 1.82763 (* 1 = 1.82763 loss)
I0426 11:43:16.527901 12550 sgd_solver.cpp:105] Iteration 9400, lr = 0.001
I0426 11:43:36.922549 12557 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:43:53.623401 12550 solver.cpp:219] Iteration 9500 (2.69569 iter/s, 37.0962s/100 iters), loss = 1.68486
I0426 11:43:53.623581 12550 solver.cpp:238]     Train net output #0: loss = 1.68486 (* 1 = 1.68486 loss)
I0426 11:43:53.623589 12550 sgd_solver.cpp:105] Iteration 9500, lr = 0.001
I0426 11:44:30.757426 12550 solver.cpp:219] Iteration 9600 (2.69291 iter/s, 37.1345s/100 iters), loss = 1.74517
I0426 11:44:30.757470 12550 solver.cpp:238]     Train net output #0: loss = 1.74517 (* 1 = 1.74517 loss)
I0426 11:44:30.757477 12550 sgd_solver.cpp:105] Iteration 9600, lr = 0.001
I0426 11:45:07.890262 12550 solver.cpp:219] Iteration 9700 (2.69299 iter/s, 37.1335s/100 iters), loss = 1.7464
I0426 11:45:07.890440 12550 solver.cpp:238]     Train net output #0: loss = 1.7464 (* 1 = 1.7464 loss)
I0426 11:45:07.890449 12550 sgd_solver.cpp:105] Iteration 9700, lr = 0.001
I0426 11:45:45.028251 12550 solver.cpp:219] Iteration 9800 (2.69262 iter/s, 37.1385s/100 iters), loss = 1.69773
I0426 11:45:45.028374 12550 solver.cpp:238]     Train net output #0: loss = 1.69773 (* 1 = 1.69773 loss)
I0426 11:45:45.028381 12550 sgd_solver.cpp:105] Iteration 9800, lr = 0.001
I0426 11:46:22.165982 12550 solver.cpp:219] Iteration 9900 (2.69264 iter/s, 37.1383s/100 iters), loss = 1.70102
I0426 11:46:22.166054 12550 solver.cpp:238]     Train net output #0: loss = 1.70102 (* 1 = 1.70102 loss)
I0426 11:46:22.166060 12550 sgd_solver.cpp:105] Iteration 9900, lr = 0.001
I0426 11:46:24.028795 12557 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:46:58.750133 12550 solver.cpp:448] Snapshotting to binary proto file examples/cifar10_full_sigmoid_iter_10000.caffemodel
I0426 11:46:58.925189 12550 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10_full_sigmoid_iter_10000.solverstate
I0426 11:46:58.926017 12550 solver.cpp:331] Iteration 10000, Testing net (#0)
I0426 11:47:07.899807 12560 data_layer.cpp:73] Restarting data prefetching from start.
I0426 11:47:13.829944 12550 solver.cpp:398]     Test net output #0: accuracy = 0.3873
I0426 11:47:13.829967 12550 solver.cpp:398]     Test net output #1: loss = 1.70122 (* 1 = 1.70122 loss)
I0426 11:47:14.199175 12550 solver.cpp:219] Iteration 10000 (1.92182 iter/s, 52.0341s/100 iters), loss = 1.61781
I0426 11:47:14.199200 12550 solver.cpp:238]     Train net output #0: loss = 1.61781 (* 1 = 1.61781 loss)
I0426 11:47:14.199206 12550 sgd_solver.cpp:105] Iteration 10000, lr = 0.001
I0426 11:47:51.301895 12550 solver.cpp:219] Iteration 10100 (2.69517 iter/s, 37.1034s/100 iters), loss = 1.65844
I0426 11:47:51.302007 12550 solver.cpp:238]     Train net output #0: loss = 1.65844 (* 1 = 1.65844 loss)
I0426 11:47:51.302014 12550 sgd_solver.cpp:105] Iteration 10100, lr = 0.001
I0426 11:48:28.451406 12550 solver.cpp:219] Iteration 10200 (2.69178 iter/s, 37.1501s/100 iters), loss = 1.79603
I0426 11:48:28.451581 12550 solver.cpp:238]     Train net output #0: loss = 1.79603 (* 1 = 1.79603 loss)
I0426 11:48:28.451589 12550 sgd_solver.cpp:105] Iteration 10200, lr = 0.001
I0426 11:49:05.649408 12550 solver.cpp:219] Iteration 10300 (2.68828 iter/s, 37.1985s/100 iters), loss = 1.64532
I0426 11:49:05.649551 12550 solver.cpp:238]     Train net output #0: loss = 1.64532 (* 1 = 1.64532 loss)
I0426 11:49:05.649560 12550 sgd_solver.cpp:105] Iteration 10300, lr = 0.001
I0426 11:49:26.428659 12557 data_layer.cpp:73] Restarting data prefetching from start.
