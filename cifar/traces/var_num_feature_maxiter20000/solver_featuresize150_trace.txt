I0426 17:22:49.495946 16582 caffe.cpp:218] Using GPUs 0
I0426 17:22:49.510102 16582 caffe.cpp:223] GPU 0: GeForce GTX 950M
I0426 17:22:49.699806 16582 solver.cpp:44] Initializing solver from parameters: 
test_iter: 10
test_interval: 1000
base_lr: 0.001
display: 100
max_iter: 20000
lr_policy: "step"
gamma: 1
momentum: 0.9
stepsize: 5000
snapshot: 10000
snapshot_prefix: "examples/cifar10_full_sigmoid"
solver_mode: GPU
device_id: 0
net: "ECE595/cifar/train_test/var_num_feature_maxiter20000/cifar_train_test_featuresize150.prototxt"
train_state {
  level: 0
  stage: ""
}
I0426 17:22:49.699920 16582 solver.cpp:87] Creating training net from net file: ECE595/cifar/train_test/var_num_feature_maxiter20000/cifar_train_test_featuresize150.prototxt
I0426 17:22:49.700098 16582 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0426 17:22:49.700110 16582 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0426 17:22:49.700182 16582 net.cpp:51] Initializing net from parameters: 
name: "CIFAR"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb"
    batch_size: 111
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 150
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Sigmoid0"
  type: "Sigmoid"
  bottom: "conv0"
  top: "Sigmoid0"
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "Sigmoid0"
  top: "pool0"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 150
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Sigmoid1"
  type: "Sigmoid"
  bottom: "conv1"
  top: "Sigmoid1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "Sigmoid1"
  top: "pool1"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 150
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Sigmoid2"
  type: "Sigmoid"
  bottom: "conv2"
  top: "Sigmoid2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "Sigmoid2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0426 17:22:49.700232 16582 layer_factory.hpp:77] Creating layer cifar
I0426 17:22:49.700310 16582 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_train_lmdb
I0426 17:22:49.700332 16582 net.cpp:84] Creating Layer cifar
I0426 17:22:49.700338 16582 net.cpp:380] cifar -> data
I0426 17:22:49.700353 16582 net.cpp:380] cifar -> label
I0426 17:22:49.700363 16582 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0426 17:22:49.701105 16582 data_layer.cpp:45] output data size: 111,3,32,32
I0426 17:22:49.704753 16582 net.cpp:122] Setting up cifar
I0426 17:22:49.704769 16582 net.cpp:129] Top shape: 111 3 32 32 (340992)
I0426 17:22:49.704787 16582 net.cpp:129] Top shape: 111 (111)
I0426 17:22:49.704789 16582 net.cpp:137] Memory required for data: 1364412
I0426 17:22:49.704797 16582 layer_factory.hpp:77] Creating layer conv0
I0426 17:22:49.704814 16582 net.cpp:84] Creating Layer conv0
I0426 17:22:49.704818 16582 net.cpp:406] conv0 <- data
I0426 17:22:49.704829 16582 net.cpp:380] conv0 -> conv0
I0426 17:22:49.706127 16582 net.cpp:122] Setting up conv0
I0426 17:22:49.706140 16582 net.cpp:129] Top shape: 111 150 32 32 (17049600)
I0426 17:22:49.706142 16582 net.cpp:137] Memory required for data: 69562812
I0426 17:22:49.706157 16582 layer_factory.hpp:77] Creating layer Sigmoid0
I0426 17:22:49.706163 16582 net.cpp:84] Creating Layer Sigmoid0
I0426 17:22:49.706166 16582 net.cpp:406] Sigmoid0 <- conv0
I0426 17:22:49.706171 16582 net.cpp:380] Sigmoid0 -> Sigmoid0
I0426 17:22:49.706190 16582 net.cpp:122] Setting up Sigmoid0
I0426 17:22:49.706195 16582 net.cpp:129] Top shape: 111 150 32 32 (17049600)
I0426 17:22:49.706197 16582 net.cpp:137] Memory required for data: 137761212
I0426 17:22:49.706199 16582 layer_factory.hpp:77] Creating layer pool0
I0426 17:22:49.706204 16582 net.cpp:84] Creating Layer pool0
I0426 17:22:49.706207 16582 net.cpp:406] pool0 <- Sigmoid0
I0426 17:22:49.706210 16582 net.cpp:380] pool0 -> pool0
I0426 17:22:49.706233 16582 net.cpp:122] Setting up pool0
I0426 17:22:49.706236 16582 net.cpp:129] Top shape: 111 150 16 16 (4262400)
I0426 17:22:49.706239 16582 net.cpp:137] Memory required for data: 154810812
I0426 17:22:49.706241 16582 layer_factory.hpp:77] Creating layer conv1
I0426 17:22:49.706248 16582 net.cpp:84] Creating Layer conv1
I0426 17:22:49.706251 16582 net.cpp:406] conv1 <- pool0
I0426 17:22:49.706257 16582 net.cpp:380] conv1 -> conv1
I0426 17:22:49.711501 16582 net.cpp:122] Setting up conv1
I0426 17:22:49.711514 16582 net.cpp:129] Top shape: 111 150 16 16 (4262400)
I0426 17:22:49.711516 16582 net.cpp:137] Memory required for data: 171860412
I0426 17:22:49.711524 16582 layer_factory.hpp:77] Creating layer Sigmoid1
I0426 17:22:49.711531 16582 net.cpp:84] Creating Layer Sigmoid1
I0426 17:22:49.711534 16582 net.cpp:406] Sigmoid1 <- conv1
I0426 17:22:49.711539 16582 net.cpp:380] Sigmoid1 -> Sigmoid1
I0426 17:22:49.711555 16582 net.cpp:122] Setting up Sigmoid1
I0426 17:22:49.711558 16582 net.cpp:129] Top shape: 111 150 16 16 (4262400)
I0426 17:22:49.711560 16582 net.cpp:137] Memory required for data: 188910012
I0426 17:22:49.711561 16582 layer_factory.hpp:77] Creating layer pool1
I0426 17:22:49.711566 16582 net.cpp:84] Creating Layer pool1
I0426 17:22:49.711567 16582 net.cpp:406] pool1 <- Sigmoid1
I0426 17:22:49.711571 16582 net.cpp:380] pool1 -> pool1
I0426 17:22:49.711588 16582 net.cpp:122] Setting up pool1
I0426 17:22:49.711591 16582 net.cpp:129] Top shape: 111 150 8 8 (1065600)
I0426 17:22:49.711593 16582 net.cpp:137] Memory required for data: 193172412
I0426 17:22:49.711596 16582 layer_factory.hpp:77] Creating layer conv2
I0426 17:22:49.711602 16582 net.cpp:84] Creating Layer conv2
I0426 17:22:49.711606 16582 net.cpp:406] conv2 <- pool1
I0426 17:22:49.711609 16582 net.cpp:380] conv2 -> conv2
I0426 17:22:49.717022 16582 net.cpp:122] Setting up conv2
I0426 17:22:49.717036 16582 net.cpp:129] Top shape: 111 150 8 8 (1065600)
I0426 17:22:49.717038 16582 net.cpp:137] Memory required for data: 197434812
I0426 17:22:49.717046 16582 layer_factory.hpp:77] Creating layer Sigmoid2
I0426 17:22:49.717053 16582 net.cpp:84] Creating Layer Sigmoid2
I0426 17:22:49.717056 16582 net.cpp:406] Sigmoid2 <- conv2
I0426 17:22:49.717061 16582 net.cpp:380] Sigmoid2 -> Sigmoid2
I0426 17:22:49.717077 16582 net.cpp:122] Setting up Sigmoid2
I0426 17:22:49.717080 16582 net.cpp:129] Top shape: 111 150 8 8 (1065600)
I0426 17:22:49.717084 16582 net.cpp:137] Memory required for data: 201697212
I0426 17:22:49.717087 16582 layer_factory.hpp:77] Creating layer pool2
I0426 17:22:49.717092 16582 net.cpp:84] Creating Layer pool2
I0426 17:22:49.717093 16582 net.cpp:406] pool2 <- Sigmoid2
I0426 17:22:49.717097 16582 net.cpp:380] pool2 -> pool2
I0426 17:22:49.717125 16582 net.cpp:122] Setting up pool2
I0426 17:22:49.717130 16582 net.cpp:129] Top shape: 111 150 4 4 (266400)
I0426 17:22:49.717133 16582 net.cpp:137] Memory required for data: 202762812
I0426 17:22:49.717135 16582 layer_factory.hpp:77] Creating layer ip1
I0426 17:22:49.717140 16582 net.cpp:84] Creating Layer ip1
I0426 17:22:49.717144 16582 net.cpp:406] ip1 <- pool2
I0426 17:22:49.717147 16582 net.cpp:380] ip1 -> ip1
I0426 17:22:49.717767 16582 net.cpp:122] Setting up ip1
I0426 17:22:49.717774 16582 net.cpp:129] Top shape: 111 10 (1110)
I0426 17:22:49.717777 16582 net.cpp:137] Memory required for data: 202767252
I0426 17:22:49.717782 16582 layer_factory.hpp:77] Creating layer loss
I0426 17:22:49.717785 16582 net.cpp:84] Creating Layer loss
I0426 17:22:49.717789 16582 net.cpp:406] loss <- ip1
I0426 17:22:49.717792 16582 net.cpp:406] loss <- label
I0426 17:22:49.717797 16582 net.cpp:380] loss -> loss
I0426 17:22:49.717808 16582 layer_factory.hpp:77] Creating layer loss
I0426 17:22:49.717871 16582 net.cpp:122] Setting up loss
I0426 17:22:49.717875 16582 net.cpp:129] Top shape: (1)
I0426 17:22:49.717877 16582 net.cpp:132]     with loss weight 1
I0426 17:22:49.717895 16582 net.cpp:137] Memory required for data: 202767256
I0426 17:22:49.717898 16582 net.cpp:198] loss needs backward computation.
I0426 17:22:49.717905 16582 net.cpp:198] ip1 needs backward computation.
I0426 17:22:49.717906 16582 net.cpp:198] pool2 needs backward computation.
I0426 17:22:49.717910 16582 net.cpp:198] Sigmoid2 needs backward computation.
I0426 17:22:49.717912 16582 net.cpp:198] conv2 needs backward computation.
I0426 17:22:49.717914 16582 net.cpp:198] pool1 needs backward computation.
I0426 17:22:49.717916 16582 net.cpp:198] Sigmoid1 needs backward computation.
I0426 17:22:49.717918 16582 net.cpp:198] conv1 needs backward computation.
I0426 17:22:49.717921 16582 net.cpp:198] pool0 needs backward computation.
I0426 17:22:49.717923 16582 net.cpp:198] Sigmoid0 needs backward computation.
I0426 17:22:49.717926 16582 net.cpp:198] conv0 needs backward computation.
I0426 17:22:49.717927 16582 net.cpp:200] cifar does not need backward computation.
I0426 17:22:49.717929 16582 net.cpp:242] This network produces output loss
I0426 17:22:49.717939 16582 net.cpp:255] Network initialization done.
I0426 17:22:49.718087 16582 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/cifar/train_test/var_num_feature_maxiter20000/cifar_train_test_featuresize150.prototxt
I0426 17:22:49.718106 16582 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0426 17:22:49.718183 16582 net.cpp:51] Initializing net from parameters: 
name: "CIFAR"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb"
    batch_size: 1000
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 150
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Sigmoid0"
  type: "Sigmoid"
  bottom: "conv0"
  top: "Sigmoid0"
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "Sigmoid0"
  top: "pool0"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 150
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Sigmoid1"
  type: "Sigmoid"
  bottom: "conv1"
  top: "Sigmoid1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "Sigmoid1"
  top: "pool1"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 150
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Sigmoid2"
  type: "Sigmoid"
  bottom: "conv2"
  top: "Sigmoid2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "Sigmoid2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0426 17:22:49.718250 16582 layer_factory.hpp:77] Creating layer cifar
I0426 17:22:49.718294 16582 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0426 17:22:49.718305 16582 net.cpp:84] Creating Layer cifar
I0426 17:22:49.718312 16582 net.cpp:380] cifar -> data
I0426 17:22:49.718318 16582 net.cpp:380] cifar -> label
I0426 17:22:49.718324 16582 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0426 17:22:49.718443 16582 data_layer.cpp:45] output data size: 1000,3,32,32
I0426 17:22:49.740434 16582 net.cpp:122] Setting up cifar
I0426 17:22:49.740453 16582 net.cpp:129] Top shape: 1000 3 32 32 (3072000)
I0426 17:22:49.740458 16582 net.cpp:129] Top shape: 1000 (1000)
I0426 17:22:49.740459 16582 net.cpp:137] Memory required for data: 12292000
I0426 17:22:49.740464 16582 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0426 17:22:49.740474 16582 net.cpp:84] Creating Layer label_cifar_1_split
I0426 17:22:49.740478 16582 net.cpp:406] label_cifar_1_split <- label
I0426 17:22:49.740483 16582 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I0426 17:22:49.740490 16582 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I0426 17:22:49.740542 16582 net.cpp:122] Setting up label_cifar_1_split
I0426 17:22:49.740547 16582 net.cpp:129] Top shape: 1000 (1000)
I0426 17:22:49.740550 16582 net.cpp:129] Top shape: 1000 (1000)
I0426 17:22:49.740552 16582 net.cpp:137] Memory required for data: 12300000
I0426 17:22:49.740555 16582 layer_factory.hpp:77] Creating layer conv0
I0426 17:22:49.740566 16582 net.cpp:84] Creating Layer conv0
I0426 17:22:49.740568 16582 net.cpp:406] conv0 <- data
I0426 17:22:49.740573 16582 net.cpp:380] conv0 -> conv0
I0426 17:22:49.741004 16582 net.cpp:122] Setting up conv0
I0426 17:22:49.741016 16582 net.cpp:129] Top shape: 1000 150 32 32 (153600000)
I0426 17:22:49.741019 16582 net.cpp:137] Memory required for data: 626700000
I0426 17:22:49.741027 16582 layer_factory.hpp:77] Creating layer Sigmoid0
I0426 17:22:49.741031 16582 net.cpp:84] Creating Layer Sigmoid0
I0426 17:22:49.741034 16582 net.cpp:406] Sigmoid0 <- conv0
I0426 17:22:49.741039 16582 net.cpp:380] Sigmoid0 -> Sigmoid0
I0426 17:22:49.741056 16582 net.cpp:122] Setting up Sigmoid0
I0426 17:22:49.741060 16582 net.cpp:129] Top shape: 1000 150 32 32 (153600000)
I0426 17:22:49.741062 16582 net.cpp:137] Memory required for data: 1241100000
I0426 17:22:49.741065 16582 layer_factory.hpp:77] Creating layer pool0
I0426 17:22:49.741071 16582 net.cpp:84] Creating Layer pool0
I0426 17:22:49.741073 16582 net.cpp:406] pool0 <- Sigmoid0
I0426 17:22:49.741077 16582 net.cpp:380] pool0 -> pool0
I0426 17:22:49.741093 16582 net.cpp:122] Setting up pool0
I0426 17:22:49.741097 16582 net.cpp:129] Top shape: 1000 150 16 16 (38400000)
I0426 17:22:49.741099 16582 net.cpp:137] Memory required for data: 1394700000
I0426 17:22:49.741101 16582 layer_factory.hpp:77] Creating layer conv1
I0426 17:22:49.741122 16582 net.cpp:84] Creating Layer conv1
I0426 17:22:49.741127 16582 net.cpp:406] conv1 <- pool0
I0426 17:22:49.741130 16582 net.cpp:380] conv1 -> conv1
I0426 17:22:49.746408 16582 net.cpp:122] Setting up conv1
I0426 17:22:49.746421 16582 net.cpp:129] Top shape: 1000 150 16 16 (38400000)
I0426 17:22:49.746425 16582 net.cpp:137] Memory required for data: 1548300000
I0426 17:22:49.746433 16582 layer_factory.hpp:77] Creating layer Sigmoid1
I0426 17:22:49.746438 16582 net.cpp:84] Creating Layer Sigmoid1
I0426 17:22:49.746440 16582 net.cpp:406] Sigmoid1 <- conv1
I0426 17:22:49.746445 16582 net.cpp:380] Sigmoid1 -> Sigmoid1
I0426 17:22:49.746462 16582 net.cpp:122] Setting up Sigmoid1
I0426 17:22:49.746466 16582 net.cpp:129] Top shape: 1000 150 16 16 (38400000)
I0426 17:22:49.746470 16582 net.cpp:137] Memory required for data: 1701900000
I0426 17:22:49.746472 16582 layer_factory.hpp:77] Creating layer pool1
I0426 17:22:49.746477 16582 net.cpp:84] Creating Layer pool1
I0426 17:22:49.746479 16582 net.cpp:406] pool1 <- Sigmoid1
I0426 17:22:49.746482 16582 net.cpp:380] pool1 -> pool1
I0426 17:22:49.746500 16582 net.cpp:122] Setting up pool1
I0426 17:22:49.746505 16582 net.cpp:129] Top shape: 1000 150 8 8 (9600000)
I0426 17:22:49.746506 16582 net.cpp:137] Memory required for data: 1740300000
I0426 17:22:49.746508 16582 layer_factory.hpp:77] Creating layer conv2
I0426 17:22:49.746517 16582 net.cpp:84] Creating Layer conv2
I0426 17:22:49.746520 16582 net.cpp:406] conv2 <- pool1
I0426 17:22:49.746522 16582 net.cpp:380] conv2 -> conv2
I0426 17:22:49.751817 16582 net.cpp:122] Setting up conv2
I0426 17:22:49.751832 16582 net.cpp:129] Top shape: 1000 150 8 8 (9600000)
I0426 17:22:49.751833 16582 net.cpp:137] Memory required for data: 1778700000
I0426 17:22:49.751840 16582 layer_factory.hpp:77] Creating layer Sigmoid2
I0426 17:22:49.751847 16582 net.cpp:84] Creating Layer Sigmoid2
I0426 17:22:49.751849 16582 net.cpp:406] Sigmoid2 <- conv2
I0426 17:22:49.751853 16582 net.cpp:380] Sigmoid2 -> Sigmoid2
I0426 17:22:49.751871 16582 net.cpp:122] Setting up Sigmoid2
I0426 17:22:49.751874 16582 net.cpp:129] Top shape: 1000 150 8 8 (9600000)
I0426 17:22:49.751878 16582 net.cpp:137] Memory required for data: 1817100000
I0426 17:22:49.751880 16582 layer_factory.hpp:77] Creating layer pool2
I0426 17:22:49.751884 16582 net.cpp:84] Creating Layer pool2
I0426 17:22:49.751886 16582 net.cpp:406] pool2 <- Sigmoid2
I0426 17:22:49.751893 16582 net.cpp:380] pool2 -> pool2
I0426 17:22:49.751912 16582 net.cpp:122] Setting up pool2
I0426 17:22:49.751916 16582 net.cpp:129] Top shape: 1000 150 4 4 (2400000)
I0426 17:22:49.751919 16582 net.cpp:137] Memory required for data: 1826700000
I0426 17:22:49.751921 16582 layer_factory.hpp:77] Creating layer ip1
I0426 17:22:49.751926 16582 net.cpp:84] Creating Layer ip1
I0426 17:22:49.751929 16582 net.cpp:406] ip1 <- pool2
I0426 17:22:49.751934 16582 net.cpp:380] ip1 -> ip1
I0426 17:22:49.752219 16582 net.cpp:122] Setting up ip1
I0426 17:22:49.752224 16582 net.cpp:129] Top shape: 1000 10 (10000)
I0426 17:22:49.752226 16582 net.cpp:137] Memory required for data: 1826740000
I0426 17:22:49.752230 16582 layer_factory.hpp:77] Creating layer ip1_ip1_0_split
I0426 17:22:49.752234 16582 net.cpp:84] Creating Layer ip1_ip1_0_split
I0426 17:22:49.752236 16582 net.cpp:406] ip1_ip1_0_split <- ip1
I0426 17:22:49.752239 16582 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I0426 17:22:49.752243 16582 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I0426 17:22:49.752271 16582 net.cpp:122] Setting up ip1_ip1_0_split
I0426 17:22:49.752275 16582 net.cpp:129] Top shape: 1000 10 (10000)
I0426 17:22:49.752277 16582 net.cpp:129] Top shape: 1000 10 (10000)
I0426 17:22:49.752279 16582 net.cpp:137] Memory required for data: 1826820000
I0426 17:22:49.752281 16582 layer_factory.hpp:77] Creating layer accuracy
I0426 17:22:49.752286 16582 net.cpp:84] Creating Layer accuracy
I0426 17:22:49.752288 16582 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I0426 17:22:49.752291 16582 net.cpp:406] accuracy <- label_cifar_1_split_0
I0426 17:22:49.752306 16582 net.cpp:380] accuracy -> accuracy
I0426 17:22:49.752313 16582 net.cpp:122] Setting up accuracy
I0426 17:22:49.752316 16582 net.cpp:129] Top shape: (1)
I0426 17:22:49.752318 16582 net.cpp:137] Memory required for data: 1826820004
I0426 17:22:49.752321 16582 layer_factory.hpp:77] Creating layer loss
I0426 17:22:49.752323 16582 net.cpp:84] Creating Layer loss
I0426 17:22:49.752326 16582 net.cpp:406] loss <- ip1_ip1_0_split_1
I0426 17:22:49.752328 16582 net.cpp:406] loss <- label_cifar_1_split_1
I0426 17:22:49.752332 16582 net.cpp:380] loss -> loss
I0426 17:22:49.752337 16582 layer_factory.hpp:77] Creating layer loss
I0426 17:22:49.752413 16582 net.cpp:122] Setting up loss
I0426 17:22:49.752416 16582 net.cpp:129] Top shape: (1)
I0426 17:22:49.752418 16582 net.cpp:132]     with loss weight 1
I0426 17:22:49.752431 16582 net.cpp:137] Memory required for data: 1826820008
I0426 17:22:49.752434 16582 net.cpp:198] loss needs backward computation.
I0426 17:22:49.752436 16582 net.cpp:200] accuracy does not need backward computation.
I0426 17:22:49.752440 16582 net.cpp:198] ip1_ip1_0_split needs backward computation.
I0426 17:22:49.752442 16582 net.cpp:198] ip1 needs backward computation.
I0426 17:22:49.752444 16582 net.cpp:198] pool2 needs backward computation.
I0426 17:22:49.752446 16582 net.cpp:198] Sigmoid2 needs backward computation.
I0426 17:22:49.752450 16582 net.cpp:198] conv2 needs backward computation.
I0426 17:22:49.752452 16582 net.cpp:198] pool1 needs backward computation.
I0426 17:22:49.752454 16582 net.cpp:198] Sigmoid1 needs backward computation.
I0426 17:22:49.752456 16582 net.cpp:198] conv1 needs backward computation.
I0426 17:22:49.752460 16582 net.cpp:198] pool0 needs backward computation.
I0426 17:22:49.752461 16582 net.cpp:198] Sigmoid0 needs backward computation.
I0426 17:22:49.752465 16582 net.cpp:198] conv0 needs backward computation.
I0426 17:22:49.752467 16582 net.cpp:200] label_cifar_1_split does not need backward computation.
I0426 17:22:49.752471 16582 net.cpp:200] cifar does not need backward computation.
I0426 17:22:49.752473 16582 net.cpp:242] This network produces output accuracy
I0426 17:22:49.752475 16582 net.cpp:242] This network produces output loss
I0426 17:22:49.752486 16582 net.cpp:255] Network initialization done.
I0426 17:22:49.752528 16582 solver.cpp:56] Solver scaffolding done.
I0426 17:22:49.752748 16582 caffe.cpp:248] Starting Optimization
I0426 17:22:49.752751 16582 solver.cpp:273] Solving CIFAR
I0426 17:22:49.752753 16582 solver.cpp:274] Learning Rate Policy: step
I0426 17:22:49.753764 16582 solver.cpp:331] Iteration 0, Testing net (#0)
I0426 17:23:14.268399 16591 data_layer.cpp:73] Restarting data prefetching from start.
I0426 17:23:30.536820 16582 solver.cpp:398]     Test net output #0: accuracy = 0.1
I0426 17:23:30.536870 16582 solver.cpp:398]     Test net output #1: loss = 2.32787 (* 1 = 2.32787 loss)
I0426 17:23:31.705246 16582 solver.cpp:219] Iteration 0 (0 iter/s, 41.9527s/100 iters), loss = 2.30929
I0426 17:23:31.705276 16582 solver.cpp:238]     Train net output #0: loss = 2.30929 (* 1 = 2.30929 loss)
I0426 17:23:31.705291 16582 sgd_solver.cpp:105] Iteration 0, lr = 0.001
I0426 17:25:26.816123 16582 solver.cpp:219] Iteration 100 (0.868712 iter/s, 115.113s/100 iters), loss = 2.34044
I0426 17:25:26.816306 16582 solver.cpp:238]     Train net output #0: loss = 2.34044 (* 1 = 2.34044 loss)
I0426 17:25:26.816313 16582 sgd_solver.cpp:105] Iteration 100, lr = 0.001
I0426 17:27:21.918633 16582 solver.cpp:219] Iteration 200 (0.868776 iter/s, 115.104s/100 iters), loss = 2.34256
I0426 17:27:21.918732 16582 solver.cpp:238]     Train net output #0: loss = 2.34256 (* 1 = 2.34256 loss)
I0426 17:27:21.918738 16582 sgd_solver.cpp:105] Iteration 200, lr = 0.001
I0426 17:29:17.020678 16582 solver.cpp:219] Iteration 300 (0.868779 iter/s, 115.104s/100 iters), loss = 2.29009
I0426 17:29:17.020840 16582 solver.cpp:238]     Train net output #0: loss = 2.29009 (* 1 = 2.29009 loss)
I0426 17:29:17.020848 16582 sgd_solver.cpp:105] Iteration 300, lr = 0.001
I0426 17:31:17.661079 16582 solver.cpp:219] Iteration 400 (0.828896 iter/s, 120.642s/100 iters), loss = 2.28474
I0426 17:31:17.661228 16582 solver.cpp:238]     Train net output #0: loss = 2.28474 (* 1 = 2.28474 loss)
I0426 17:31:17.661236 16582 sgd_solver.cpp:105] Iteration 400, lr = 0.001
I0426 17:32:10.638103 16589 data_layer.cpp:73] Restarting data prefetching from start.
I0426 17:33:12.791465 16582 solver.cpp:219] Iteration 500 (0.868566 iter/s, 115.132s/100 iters), loss = 2.23317
I0426 17:33:12.791640 16582 solver.cpp:238]     Train net output #0: loss = 2.23317 (* 1 = 2.23317 loss)
I0426 17:33:12.791648 16582 sgd_solver.cpp:105] Iteration 500, lr = 0.001
I0426 17:35:07.872966 16582 solver.cpp:219] Iteration 600 (0.868935 iter/s, 115.083s/100 iters), loss = 2.14061
I0426 17:35:07.873014 16582 solver.cpp:238]     Train net output #0: loss = 2.14061 (* 1 = 2.14061 loss)
I0426 17:35:07.873020 16582 sgd_solver.cpp:105] Iteration 600, lr = 0.001
I0426 17:37:02.967933 16582 solver.cpp:219] Iteration 700 (0.868832 iter/s, 115.097s/100 iters), loss = 2.06613
I0426 17:37:02.968111 16582 solver.cpp:238]     Train net output #0: loss = 2.06613 (* 1 = 2.06613 loss)
I0426 17:37:02.968117 16582 sgd_solver.cpp:105] Iteration 700, lr = 0.001
I0426 17:38:58.058378 16582 solver.cpp:219] Iteration 800 (0.868867 iter/s, 115.092s/100 iters), loss = 1.99123
I0426 17:38:58.058552 16582 solver.cpp:238]     Train net output #0: loss = 1.99123 (* 1 = 1.99123 loss)
I0426 17:38:58.058559 16582 sgd_solver.cpp:105] Iteration 800, lr = 0.001
I0426 17:40:48.549518 16589 data_layer.cpp:73] Restarting data prefetching from start.
I0426 17:40:53.147583 16582 solver.cpp:219] Iteration 900 (0.868877 iter/s, 115.091s/100 iters), loss = 2.03334
I0426 17:40:53.147613 16582 solver.cpp:238]     Train net output #0: loss = 2.03334 (* 1 = 2.03334 loss)
I0426 17:40:53.147617 16582 sgd_solver.cpp:105] Iteration 900, lr = 0.001
I0426 17:42:46.500201 16582 solver.cpp:331] Iteration 1000, Testing net (#0)
I0426 17:43:11.526729 16591 data_layer.cpp:73] Restarting data prefetching from start.
I0426 17:43:27.793781 16582 solver.cpp:398]     Test net output #0: accuracy = 0.2515
I0426 17:43:27.793942 16582 solver.cpp:398]     Test net output #1: loss = 2.01395 (* 1 = 2.01395 loss)
I0426 17:43:28.942925 16582 solver.cpp:219] Iteration 1000 (0.641856 iter/s, 155.798s/100 iters), loss = 1.98371
I0426 17:43:28.942950 16582 solver.cpp:238]     Train net output #0: loss = 1.98371 (* 1 = 1.98371 loss)
I0426 17:43:28.942955 16582 sgd_solver.cpp:105] Iteration 1000, lr = 0.001
I0426 17:45:24.020068 16582 solver.cpp:219] Iteration 1100 (0.868967 iter/s, 115.079s/100 iters), loss = 1.99203
I0426 17:45:24.020114 16582 solver.cpp:238]     Train net output #0: loss = 1.99203 (* 1 = 1.99203 loss)
I0426 17:45:24.020119 16582 sgd_solver.cpp:105] Iteration 1100, lr = 0.001
I0426 17:47:19.093019 16582 solver.cpp:219] Iteration 1200 (0.868999 iter/s, 115.075s/100 iters), loss = 2.0633
I0426 17:47:19.093065 16582 solver.cpp:238]     Train net output #0: loss = 2.0633 (* 1 = 2.0633 loss)
I0426 17:47:19.093070 16582 sgd_solver.cpp:105] Iteration 1200, lr = 0.001
I0426 17:49:14.194931 16582 solver.cpp:219] Iteration 1300 (0.86878 iter/s, 115.104s/100 iters), loss = 2.03202
I0426 17:49:14.195101 16582 solver.cpp:238]     Train net output #0: loss = 2.03202 (* 1 = 2.03202 loss)
I0426 17:49:14.195108 16582 sgd_solver.cpp:105] Iteration 1300, lr = 0.001
I0426 17:50:08.295538 16589 data_layer.cpp:73] Restarting data prefetching from start.
I0426 17:51:09.289590 16582 solver.cpp:219] Iteration 1400 (0.868835 iter/s, 115.097s/100 iters), loss = 1.9799
I0426 17:51:09.289638 16582 solver.cpp:238]     Train net output #0: loss = 1.9799 (* 1 = 1.9799 loss)
I0426 17:51:09.289644 16582 sgd_solver.cpp:105] Iteration 1400, lr = 0.001
I0426 17:53:04.394315 16582 solver.cpp:219] Iteration 1500 (0.868759 iter/s, 115.107s/100 iters), loss = 2.02436
I0426 17:53:04.394383 16582 solver.cpp:238]     Train net output #0: loss = 2.02436 (* 1 = 2.02436 loss)
I0426 17:53:04.394389 16582 sgd_solver.cpp:105] Iteration 1500, lr = 0.001
I0426 17:54:59.493734 16582 solver.cpp:219] Iteration 1600 (0.868799 iter/s, 115.101s/100 iters), loss = 2.07731
I0426 17:54:59.493912 16582 solver.cpp:238]     Train net output #0: loss = 2.07731 (* 1 = 2.07731 loss)
I0426 17:54:59.493919 16582 sgd_solver.cpp:105] Iteration 1600, lr = 0.001
I0426 17:56:54.583770 16582 solver.cpp:219] Iteration 1700 (0.86887 iter/s, 115.092s/100 iters), loss = 1.89662
I0426 17:56:54.583951 16582 solver.cpp:238]     Train net output #0: loss = 1.89662 (* 1 = 1.89662 loss)
I0426 17:56:54.583959 16582 sgd_solver.cpp:105] Iteration 1700, lr = 0.001
I0426 17:58:46.225914 16589 data_layer.cpp:73] Restarting data prefetching from start.
I0426 17:58:49.674185 16582 solver.cpp:219] Iteration 1800 (0.868868 iter/s, 115.092s/100 iters), loss = 1.82803
I0426 17:58:49.674212 16582 solver.cpp:238]     Train net output #0: loss = 1.82803 (* 1 = 1.82803 loss)
I0426 17:58:49.674217 16582 sgd_solver.cpp:105] Iteration 1800, lr = 0.001
I0426 18:00:44.756419 16582 solver.cpp:219] Iteration 1900 (0.868928 iter/s, 115.084s/100 iters), loss = 1.80201
I0426 18:00:44.756593 16582 solver.cpp:238]     Train net output #0: loss = 1.80201 (* 1 = 1.80201 loss)
I0426 18:00:44.756600 16582 sgd_solver.cpp:105] Iteration 1900, lr = 0.001
I0426 18:02:38.139078 16582 solver.cpp:331] Iteration 2000, Testing net (#0)
I0426 18:03:03.157317 16591 data_layer.cpp:73] Restarting data prefetching from start.
I0426 18:03:19.429013 16582 solver.cpp:398]     Test net output #0: accuracy = 0.3131
I0426 18:03:19.429177 16582 solver.cpp:398]     Test net output #1: loss = 1.9085 (* 1 = 1.9085 loss)
I0426 18:03:20.578341 16582 solver.cpp:219] Iteration 2000 (0.641747 iter/s, 155.825s/100 iters), loss = 1.8847
I0426 18:03:20.578364 16582 solver.cpp:238]     Train net output #0: loss = 1.8847 (* 1 = 1.8847 loss)
I0426 18:03:20.578371 16582 sgd_solver.cpp:105] Iteration 2000, lr = 0.001
I0426 18:05:15.676982 16582 solver.cpp:219] Iteration 2100 (0.868804 iter/s, 115.101s/100 iters), loss = 1.82273
I0426 18:05:15.677161 16582 solver.cpp:238]     Train net output #0: loss = 1.82273 (* 1 = 1.82273 loss)
I0426 18:05:15.677167 16582 sgd_solver.cpp:105] Iteration 2100, lr = 0.001
I0426 18:07:10.809586 16582 solver.cpp:219] Iteration 2200 (0.868549 iter/s, 115.135s/100 iters), loss = 1.87604
I0426 18:07:10.809767 16582 solver.cpp:238]     Train net output #0: loss = 1.87604 (* 1 = 1.87604 loss)
I0426 18:07:10.809774 16582 sgd_solver.cpp:105] Iteration 2200, lr = 0.001
I0426 18:08:06.073299 16589 data_layer.cpp:73] Restarting data prefetching from start.
I0426 18:09:05.915819 16582 solver.cpp:219] Iteration 2300 (0.868748 iter/s, 115.108s/100 iters), loss = 1.74621
I0426 18:09:05.915946 16582 solver.cpp:238]     Train net output #0: loss = 1.74621 (* 1 = 1.74621 loss)
I0426 18:09:05.915953 16582 sgd_solver.cpp:105] Iteration 2300, lr = 0.001
I0426 18:11:01.003799 16582 solver.cpp:219] Iteration 2400 (0.868886 iter/s, 115.09s/100 iters), loss = 1.85082
I0426 18:11:01.003978 16582 solver.cpp:238]     Train net output #0: loss = 1.85082 (* 1 = 1.85082 loss)
I0426 18:11:01.003985 16582 sgd_solver.cpp:105] Iteration 2400, lr = 0.001
I0426 18:12:56.064330 16582 solver.cpp:219] Iteration 2500 (0.869093 iter/s, 115.062s/100 iters), loss = 1.8515
I0426 18:12:56.064378 16582 solver.cpp:238]     Train net output #0: loss = 1.8515 (* 1 = 1.8515 loss)
I0426 18:12:56.064383 16582 sgd_solver.cpp:105] Iteration 2500, lr = 0.001
I0426 18:14:51.131633 16582 solver.cpp:219] Iteration 2600 (0.869041 iter/s, 115.069s/100 iters), loss = 1.92748
I0426 18:14:51.131681 16582 solver.cpp:238]     Train net output #0: loss = 1.92748 (* 1 = 1.92748 loss)
I0426 18:14:51.131686 16582 sgd_solver.cpp:105] Iteration 2600, lr = 0.001
I0426 18:16:43.949676 16589 data_layer.cpp:73] Restarting data prefetching from start.
I0426 18:16:46.248313 16582 solver.cpp:219] Iteration 2700 (0.868668 iter/s, 115.119s/100 iters), loss = 1.79824
I0426 18:16:46.248340 16582 solver.cpp:238]     Train net output #0: loss = 1.79824 (* 1 = 1.79824 loss)
I0426 18:16:46.248345 16582 sgd_solver.cpp:105] Iteration 2700, lr = 0.001
I0426 18:18:41.344810 16582 solver.cpp:219] Iteration 2800 (0.86882 iter/s, 115.099s/100 iters), loss = 1.84946
I0426 18:18:41.344960 16582 solver.cpp:238]     Train net output #0: loss = 1.84946 (* 1 = 1.84946 loss)
I0426 18:18:41.344967 16582 sgd_solver.cpp:105] Iteration 2800, lr = 0.001
I0426 18:20:36.462317 16582 solver.cpp:219] Iteration 2900 (0.868663 iter/s, 115.119s/100 iters), loss = 1.83219
I0426 18:20:36.462369 16582 solver.cpp:238]     Train net output #0: loss = 1.83219 (* 1 = 1.83219 loss)
I0426 18:20:36.462375 16582 sgd_solver.cpp:105] Iteration 2900, lr = 0.001
I0426 18:22:29.854485 16582 solver.cpp:331] Iteration 3000, Testing net (#0)
I0426 18:22:54.863802 16591 data_layer.cpp:73] Restarting data prefetching from start.
I0426 18:23:11.142004 16582 solver.cpp:398]     Test net output #0: accuracy = 0.3408
I0426 18:23:11.142045 16582 solver.cpp:398]     Test net output #1: loss = 1.8301 (* 1 = 1.8301 loss)
I0426 18:23:12.292237 16582 solver.cpp:219] Iteration 3000 (0.641714 iter/s, 155.833s/100 iters), loss = 1.93393
I0426 18:23:12.292260 16582 solver.cpp:238]     Train net output #0: loss = 1.93393 (* 1 = 1.93393 loss)
I0426 18:23:12.292266 16582 sgd_solver.cpp:105] Iteration 3000, lr = 0.001
I0426 18:25:07.385411 16582 solver.cpp:219] Iteration 3100 (0.868846 iter/s, 115.095s/100 iters), loss = 1.89265
I0426 18:25:07.385459 16582 solver.cpp:238]     Train net output #0: loss = 1.89265 (* 1 = 1.89265 loss)
I0426 18:25:07.385464 16582 sgd_solver.cpp:105] Iteration 3100, lr = 0.001
I0426 18:26:03.800570 16589 data_layer.cpp:73] Restarting data prefetching from start.
I0426 18:27:02.492472 16582 solver.cpp:219] Iteration 3200 (0.868741 iter/s, 115.109s/100 iters), loss = 1.92595
I0426 18:27:02.492519 16582 solver.cpp:238]     Train net output #0: loss = 1.92595 (* 1 = 1.92595 loss)
I0426 18:27:02.492524 16582 sgd_solver.cpp:105] Iteration 3200, lr = 0.001
I0426 18:28:57.572943 16582 solver.cpp:219] Iteration 3300 (0.868942 iter/s, 115.083s/100 iters), loss = 1.92266
I0426 18:28:57.573117 16582 solver.cpp:238]     Train net output #0: loss = 1.92266 (* 1 = 1.92266 loss)
I0426 18:28:57.573124 16582 sgd_solver.cpp:105] Iteration 3300, lr = 0.001
I0426 18:30:52.664504 16582 solver.cpp:219] Iteration 3400 (0.868859 iter/s, 115.093s/100 iters), loss = 1.92118
I0426 18:30:52.664646 16582 solver.cpp:238]     Train net output #0: loss = 1.92118 (* 1 = 1.92118 loss)
I0426 18:30:52.664654 16582 sgd_solver.cpp:105] Iteration 3400, lr = 0.001
I0426 18:32:47.765398 16582 solver.cpp:219] Iteration 3500 (0.868788 iter/s, 115.103s/100 iters), loss = 1.6831
I0426 18:32:47.765566 16582 solver.cpp:238]     Train net output #0: loss = 1.6831 (* 1 = 1.6831 loss)
I0426 18:32:47.765573 16582 sgd_solver.cpp:105] Iteration 3500, lr = 0.001
I0426 18:34:41.714455 16589 data_layer.cpp:73] Restarting data prefetching from start.
I0426 18:34:42.861569 16582 solver.cpp:219] Iteration 3600 (0.868824 iter/s, 115.098s/100 iters), loss = 1.8624
I0426 18:34:42.861598 16582 solver.cpp:238]     Train net output #0: loss = 1.8624 (* 1 = 1.8624 loss)
I0426 18:34:42.861603 16582 sgd_solver.cpp:105] Iteration 3600, lr = 0.001
I0426 18:36:37.949054 16582 solver.cpp:219] Iteration 3700 (0.868889 iter/s, 115.09s/100 iters), loss = 1.74053
I0426 18:36:37.949200 16582 solver.cpp:238]     Train net output #0: loss = 1.74053 (* 1 = 1.74053 loss)
I0426 18:36:37.949208 16582 sgd_solver.cpp:105] Iteration 3700, lr = 0.001
I0426 18:38:33.035630 16582 solver.cpp:219] Iteration 3800 (0.868896 iter/s, 115.089s/100 iters), loss = 1.77453
I0426 18:38:33.035679 16582 solver.cpp:238]     Train net output #0: loss = 1.77453 (* 1 = 1.77453 loss)
I0426 18:38:33.035686 16582 sgd_solver.cpp:105] Iteration 3800, lr = 0.001
I0426 18:40:28.133721 16582 solver.cpp:219] Iteration 3900 (0.868809 iter/s, 115.1s/100 iters), loss = 1.81422
I0426 18:40:28.133893 16582 solver.cpp:238]     Train net output #0: loss = 1.81422 (* 1 = 1.81422 loss)
I0426 18:40:28.133900 16582 sgd_solver.cpp:105] Iteration 3900, lr = 0.001
I0426 18:42:21.518666 16582 solver.cpp:331] Iteration 4000, Testing net (#0)
I0426 18:42:46.531795 16591 data_layer.cpp:73] Restarting data prefetching from start.
I0426 18:43:02.811321 16582 solver.cpp:398]     Test net output #0: accuracy = 0.3713
I0426 18:43:02.811363 16582 solver.cpp:398]     Test net output #1: loss = 1.75401 (* 1 = 1.75401 loss)
I0426 18:43:03.960568 16582 solver.cpp:219] Iteration 4000 (0.641727 iter/s, 155.829s/100 iters), loss = 1.79456
I0426 18:43:03.960597 16582 solver.cpp:238]     Train net output #0: loss = 1.79456 (* 1 = 1.79456 loss)
I0426 18:43:03.960602 16582 sgd_solver.cpp:105] Iteration 4000, lr = 0.001
I0426 18:44:01.517398 16589 data_layer.cpp:73] Restarting data prefetching from start.
I0426 18:44:59.036641 16582 solver.cpp:219] Iteration 4100 (0.868975 iter/s, 115.078s/100 iters), loss = 1.99399
I0426 18:44:59.036689 16582 solver.cpp:238]     Train net output #0: loss = 1.99399 (* 1 = 1.99399 loss)
I0426 18:44:59.036694 16582 sgd_solver.cpp:105] Iteration 4100, lr = 0.001
I0426 18:46:54.154377 16582 solver.cpp:219] Iteration 4200 (0.868661 iter/s, 115.12s/100 iters), loss = 1.75224
I0426 18:46:54.154551 16582 solver.cpp:238]     Train net output #0: loss = 1.75224 (* 1 = 1.75224 loss)
I0426 18:46:54.154557 16582 sgd_solver.cpp:105] Iteration 4200, lr = 0.001
I0426 18:48:49.267793 16582 solver.cpp:219] Iteration 4300 (0.868694 iter/s, 115.115s/100 iters), loss = 1.65639
I0426 18:48:49.267964 16582 solver.cpp:238]     Train net output #0: loss = 1.65639 (* 1 = 1.65639 loss)
I0426 18:48:49.267971 16582 sgd_solver.cpp:105] Iteration 4300, lr = 0.001
I0426 18:50:44.369173 16582 solver.cpp:219] Iteration 4400 (0.868785 iter/s, 115.103s/100 iters), loss = 1.72851
I0426 18:50:44.369349 16582 solver.cpp:238]     Train net output #0: loss = 1.72851 (* 1 = 1.72851 loss)
I0426 18:50:44.369356 16582 sgd_solver.cpp:105] Iteration 4400, lr = 0.001
I0426 18:52:39.474272 16582 solver.cpp:219] Iteration 4500 (0.868757 iter/s, 115.107s/100 iters), loss = 1.73037
I0426 18:52:39.474444 16582 solver.cpp:238]     Train net output #0: loss = 1.73037 (* 1 = 1.73037 loss)
I0426 18:52:39.474452 16582 sgd_solver.cpp:105] Iteration 4500, lr = 0.001
I0426 18:52:39.477788 16589 data_layer.cpp:73] Restarting data prefetching from start.
I0426 18:54:34.559298 16582 solver.cpp:219] Iteration 4600 (0.868908 iter/s, 115.087s/100 iters), loss = 1.73664
I0426 18:54:34.559468 16582 solver.cpp:238]     Train net output #0: loss = 1.73664 (* 1 = 1.73664 loss)
I0426 18:54:34.559476 16582 sgd_solver.cpp:105] Iteration 4600, lr = 0.001
I0426 18:56:29.659447 16582 solver.cpp:219] Iteration 4700 (0.868794 iter/s, 115.102s/100 iters), loss = 1.69753
I0426 18:56:29.659590 16582 solver.cpp:238]     Train net output #0: loss = 1.69753 (* 1 = 1.69753 loss)
I0426 18:56:29.659597 16582 sgd_solver.cpp:105] Iteration 4700, lr = 0.001
I0426 18:58:25.333459 16582 solver.cpp:219] Iteration 4800 (0.864484 iter/s, 115.676s/100 iters), loss = 1.93105
I0426 18:58:25.333643 16582 solver.cpp:238]     Train net output #0: loss = 1.93105 (* 1 = 1.93105 loss)
I0426 18:58:25.333649 16582 sgd_solver.cpp:105] Iteration 4800, lr = 0.001
I0426 19:00:24.476675 16582 solver.cpp:219] Iteration 4900 (0.839312 iter/s, 119.145s/100 iters), loss = 1.65354
I0426 19:00:24.476723 16582 solver.cpp:238]     Train net output #0: loss = 1.65354 (* 1 = 1.65354 loss)
I0426 19:00:24.476729 16582 sgd_solver.cpp:105] Iteration 4900, lr = 0.001
I0426 19:01:22.025966 16589 data_layer.cpp:73] Restarting data prefetching from start.
I0426 19:02:17.839474 16582 solver.cpp:331] Iteration 5000, Testing net (#0)
I0426 19:02:42.863811 16591 data_layer.cpp:73] Restarting data prefetching from start.
I0426 19:02:59.136584 16582 solver.cpp:398]     Test net output #0: accuracy = 0.3774
I0426 19:02:59.136744 16582 solver.cpp:398]     Test net output #1: loss = 1.73317 (* 1 = 1.73317 loss)
I0426 19:03:00.305196 16582 solver.cpp:219] Iteration 5000 (0.64172 iter/s, 155.831s/100 iters), loss = 1.78211
I0426 19:03:00.305233 16582 solver.cpp:238]     Train net output #0: loss = 1.78211 (* 1 = 1.78211 loss)
I0426 19:03:00.305238 16582 sgd_solver.cpp:105] Iteration 5000, lr = 0.001
I0426 19:04:55.391317 16582 solver.cpp:219] Iteration 5100 (0.868899 iter/s, 115.088s/100 iters), loss = 1.69413
I0426 19:04:55.391520 16582 solver.cpp:238]     Train net output #0: loss = 1.69413 (* 1 = 1.69413 loss)
I0426 19:04:55.391528 16582 sgd_solver.cpp:105] Iteration 5100, lr = 0.001
I0426 19:06:50.512895 16582 solver.cpp:219] Iteration 5200 (0.868633 iter/s, 115.123s/100 iters), loss = 1.84098
I0426 19:06:50.513068 16582 solver.cpp:238]     Train net output #0: loss = 1.84098 (* 1 = 1.84098 loss)
I0426 19:06:50.513075 16582 sgd_solver.cpp:105] Iteration 5200, lr = 0.001
I0426 19:08:45.588754 16582 solver.cpp:219] Iteration 5300 (0.868978 iter/s, 115.078s/100 iters), loss = 1.6379
I0426 19:08:45.588928 16582 solver.cpp:238]     Train net output #0: loss = 1.6379 (* 1 = 1.6379 loss)
I0426 19:08:45.588937 16582 sgd_solver.cpp:105] Iteration 5300, lr = 0.001
I0426 19:10:40.671710 16582 solver.cpp:219] Iteration 5400 (0.868924 iter/s, 115.085s/100 iters), loss = 1.7086
I0426 19:10:40.671814 16582 solver.cpp:238]     Train net output #0: loss = 1.7086 (* 1 = 1.7086 loss)
I0426 19:10:40.671819 16582 sgd_solver.cpp:105] Iteration 5400, lr = 0.001
I0426 19:10:41.833889 16589 data_layer.cpp:73] Restarting data prefetching from start.
I0426 19:12:35.768684 16582 solver.cpp:219] Iteration 5500 (0.868818 iter/s, 115.099s/100 iters), loss = 1.81432
I0426 19:12:35.768774 16582 solver.cpp:238]     Train net output #0: loss = 1.81432 (* 1 = 1.81432 loss)
I0426 19:12:35.768780 16582 sgd_solver.cpp:105] Iteration 5500, lr = 0.001
I0426 19:14:30.871327 16582 solver.cpp:219] Iteration 5600 (0.868775 iter/s, 115.105s/100 iters), loss = 1.54456
I0426 19:14:30.871392 16582 solver.cpp:238]     Train net output #0: loss = 1.54456 (* 1 = 1.54456 loss)
I0426 19:14:30.871412 16582 sgd_solver.cpp:105] Iteration 5600, lr = 0.001
I0426 19:16:25.990087 16582 solver.cpp:219] Iteration 5700 (0.868653 iter/s, 115.121s/100 iters), loss = 1.72763
I0426 19:16:25.990176 16582 solver.cpp:238]     Train net output #0: loss = 1.72763 (* 1 = 1.72763 loss)
I0426 19:16:25.990182 16582 sgd_solver.cpp:105] Iteration 5700, lr = 0.001
I0426 19:18:21.058492 16582 solver.cpp:219] Iteration 5800 (0.869033 iter/s, 115.07s/100 iters), loss = 1.75961
I0426 19:18:21.058665 16582 solver.cpp:238]     Train net output #0: loss = 1.75961 (* 1 = 1.75961 loss)
I0426 19:18:21.058672 16582 sgd_solver.cpp:105] Iteration 5800, lr = 0.001
F0426 19:20:25.444567 16582 math_functions.cu:26] Check failed: status == CUBLAS_STATUS_SUCCESS (14 vs. 0)  CUBLAS_STATUS_INTERNAL_ERROR
*** Check failure stack trace: ***
    @     0x7f50630d75cd  google::LogMessage::Fail()
    @     0x7f50630d9433  google::LogMessage::SendToLog()
    @     0x7f50630d715b  google::LogMessage::Flush()
    @     0x7f50630d9e1e  google::LogMessageFatal::~LogMessageFatal()
    @     0x7f50638b3742  caffe::caffe_gpu_gemm<>()
    @     0x7f506379d501  caffe::BaseConvolutionLayer<>::forward_gpu_gemm()
    @     0x7f50638750b6  caffe::ConvolutionLayer<>::Forward_gpu()
    @     0x7f5063850b91  caffe::Net<>::ForwardFromTo()
    @     0x7f5063850c97  caffe::Net<>::Forward()
    @     0x7f506382c8e8  caffe::Solver<>::Step()
    @     0x7f506382d48a  caffe::Solver<>::Solve()
    @           0x40ab04  train()
    @           0x4072f0  main
    @     0x7f5062048830  __libc_start_main
    @           0x407b19  _start
    @              (nil)  (unknown)
