I0430 16:06:02.199897 11918 caffe.cpp:218] Using GPUs 0
I0430 16:06:02.226683 11918 caffe.cpp:223] GPU 0: GeForce GTX 950M
I0430 16:06:02.478121 11918 solver.cpp:44] Initializing solver from parameters: 
test_iter: 10
test_interval: 1000
base_lr: 0.001
display: 100
max_iter: 60000
lr_policy: "step"
gamma: 1
momentum: 0.9
stepsize: 5000
snapshot: 10000
snapshot_prefix: "examples/cifar10_full_sigmoid"
solver_mode: GPU
device_id: 0
net: "ECE595/cifar/train_test/var_everything/cifar_train_test_0.prototxt"
train_state {
  level: 0
  stage: ""
}
I0430 16:06:02.478261 11918 solver.cpp:87] Creating training net from net file: ECE595/cifar/train_test/var_everything/cifar_train_test_0.prototxt
I0430 16:06:02.478479 11918 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0430 16:06:02.478492 11918 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0430 16:06:02.478577 11918 net.cpp:51] Initializing net from parameters: 
name: "CIFAR"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb"
    batch_size: 111
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0430 16:06:02.478649 11918 layer_factory.hpp:77] Creating layer cifar
I0430 16:06:02.478773 11918 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_train_lmdb
I0430 16:06:02.478807 11918 net.cpp:84] Creating Layer cifar
I0430 16:06:02.478818 11918 net.cpp:380] cifar -> data
I0430 16:06:02.478847 11918 net.cpp:380] cifar -> label
I0430 16:06:02.478863 11918 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0430 16:06:02.480092 11918 data_layer.cpp:45] output data size: 111,3,32,32
I0430 16:06:02.484696 11918 net.cpp:122] Setting up cifar
I0430 16:06:02.484717 11918 net.cpp:129] Top shape: 111 3 32 32 (340992)
I0430 16:06:02.484721 11918 net.cpp:129] Top shape: 111 (111)
I0430 16:06:02.484724 11918 net.cpp:137] Memory required for data: 1364412
I0430 16:06:02.484733 11918 layer_factory.hpp:77] Creating layer ip1
I0430 16:06:02.484746 11918 net.cpp:84] Creating Layer ip1
I0430 16:06:02.484751 11918 net.cpp:406] ip1 <- data
I0430 16:06:02.484763 11918 net.cpp:380] ip1 -> ip1
I0430 16:06:02.486064 11918 net.cpp:122] Setting up ip1
I0430 16:06:02.486079 11918 net.cpp:129] Top shape: 111 10 (1110)
I0430 16:06:02.486084 11918 net.cpp:137] Memory required for data: 1368852
I0430 16:06:02.486109 11918 layer_factory.hpp:77] Creating layer relu1
I0430 16:06:02.486124 11918 net.cpp:84] Creating Layer relu1
I0430 16:06:02.486129 11918 net.cpp:406] relu1 <- ip1
I0430 16:06:02.486137 11918 net.cpp:367] relu1 -> ip1 (in-place)
I0430 16:06:02.486167 11918 net.cpp:122] Setting up relu1
I0430 16:06:02.486187 11918 net.cpp:129] Top shape: 111 10 (1110)
I0430 16:06:02.486227 11918 net.cpp:137] Memory required for data: 1373292
I0430 16:06:02.486244 11918 layer_factory.hpp:77] Creating layer ip2
I0430 16:06:02.486276 11918 net.cpp:84] Creating Layer ip2
I0430 16:06:02.486291 11918 net.cpp:406] ip2 <- ip1
I0430 16:06:02.486311 11918 net.cpp:380] ip2 -> ip2
I0430 16:06:02.486501 11918 net.cpp:122] Setting up ip2
I0430 16:06:02.486512 11918 net.cpp:129] Top shape: 111 10 (1110)
I0430 16:06:02.486532 11918 net.cpp:137] Memory required for data: 1377732
I0430 16:06:02.486544 11918 layer_factory.hpp:77] Creating layer loss
I0430 16:06:02.486557 11918 net.cpp:84] Creating Layer loss
I0430 16:06:02.486563 11918 net.cpp:406] loss <- ip2
I0430 16:06:02.486568 11918 net.cpp:406] loss <- label
I0430 16:06:02.486577 11918 net.cpp:380] loss -> loss
I0430 16:06:02.486595 11918 layer_factory.hpp:77] Creating layer loss
I0430 16:06:02.486708 11918 net.cpp:122] Setting up loss
I0430 16:06:02.486716 11918 net.cpp:129] Top shape: (1)
I0430 16:06:02.486721 11918 net.cpp:132]     with loss weight 1
I0430 16:06:02.486744 11918 net.cpp:137] Memory required for data: 1377736
I0430 16:06:02.486749 11918 net.cpp:198] loss needs backward computation.
I0430 16:06:02.486759 11918 net.cpp:198] ip2 needs backward computation.
I0430 16:06:02.486764 11918 net.cpp:198] relu1 needs backward computation.
I0430 16:06:02.486779 11918 net.cpp:198] ip1 needs backward computation.
I0430 16:06:02.486788 11918 net.cpp:200] cifar does not need backward computation.
I0430 16:06:02.486791 11918 net.cpp:242] This network produces output loss
I0430 16:06:02.486801 11918 net.cpp:255] Network initialization done.
I0430 16:06:02.486937 11918 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/cifar/train_test/var_everything/cifar_train_test_0.prototxt
I0430 16:06:02.486960 11918 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0430 16:06:02.487025 11918 net.cpp:51] Initializing net from parameters: 
name: "CIFAR"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb"
    batch_size: 1000
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0430 16:06:02.487085 11918 layer_factory.hpp:77] Creating layer cifar
I0430 16:06:02.487151 11918 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0430 16:06:02.487169 11918 net.cpp:84] Creating Layer cifar
I0430 16:06:02.487177 11918 net.cpp:380] cifar -> data
I0430 16:06:02.487190 11918 net.cpp:380] cifar -> label
I0430 16:06:02.487201 11918 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0430 16:06:02.487349 11918 data_layer.cpp:45] output data size: 1000,3,32,32
I0430 16:06:02.513419 11918 net.cpp:122] Setting up cifar
I0430 16:06:02.513443 11918 net.cpp:129] Top shape: 1000 3 32 32 (3072000)
I0430 16:06:02.513449 11918 net.cpp:129] Top shape: 1000 (1000)
I0430 16:06:02.513453 11918 net.cpp:137] Memory required for data: 12292000
I0430 16:06:02.513460 11918 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0430 16:06:02.513476 11918 net.cpp:84] Creating Layer label_cifar_1_split
I0430 16:06:02.513481 11918 net.cpp:406] label_cifar_1_split <- label
I0430 16:06:02.513490 11918 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I0430 16:06:02.513504 11918 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I0430 16:06:02.513555 11918 net.cpp:122] Setting up label_cifar_1_split
I0430 16:06:02.513561 11918 net.cpp:129] Top shape: 1000 (1000)
I0430 16:06:02.513581 11918 net.cpp:129] Top shape: 1000 (1000)
I0430 16:06:02.513587 11918 net.cpp:137] Memory required for data: 12300000
I0430 16:06:02.513592 11918 layer_factory.hpp:77] Creating layer ip1
I0430 16:06:02.513603 11918 net.cpp:84] Creating Layer ip1
I0430 16:06:02.513608 11918 net.cpp:406] ip1 <- data
I0430 16:06:02.513628 11918 net.cpp:380] ip1 -> ip1
I0430 16:06:02.514039 11918 net.cpp:122] Setting up ip1
I0430 16:06:02.514045 11918 net.cpp:129] Top shape: 1000 10 (10000)
I0430 16:06:02.514050 11918 net.cpp:137] Memory required for data: 12340000
I0430 16:06:02.514065 11918 layer_factory.hpp:77] Creating layer relu1
I0430 16:06:02.514072 11918 net.cpp:84] Creating Layer relu1
I0430 16:06:02.514077 11918 net.cpp:406] relu1 <- ip1
I0430 16:06:02.514086 11918 net.cpp:367] relu1 -> ip1 (in-place)
I0430 16:06:02.514093 11918 net.cpp:122] Setting up relu1
I0430 16:06:02.514099 11918 net.cpp:129] Top shape: 1000 10 (10000)
I0430 16:06:02.514103 11918 net.cpp:137] Memory required for data: 12380000
I0430 16:06:02.514108 11918 layer_factory.hpp:77] Creating layer ip2
I0430 16:06:02.514117 11918 net.cpp:84] Creating Layer ip2
I0430 16:06:02.514122 11918 net.cpp:406] ip2 <- ip1
I0430 16:06:02.514128 11918 net.cpp:380] ip2 -> ip2
I0430 16:06:02.514237 11918 net.cpp:122] Setting up ip2
I0430 16:06:02.514245 11918 net.cpp:129] Top shape: 1000 10 (10000)
I0430 16:06:02.514250 11918 net.cpp:137] Memory required for data: 12420000
I0430 16:06:02.514258 11918 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0430 16:06:02.514268 11918 net.cpp:84] Creating Layer ip2_ip2_0_split
I0430 16:06:02.514274 11918 net.cpp:406] ip2_ip2_0_split <- ip2
I0430 16:06:02.514281 11918 net.cpp:380] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0430 16:06:02.514291 11918 net.cpp:380] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0430 16:06:02.514335 11918 net.cpp:122] Setting up ip2_ip2_0_split
I0430 16:06:02.514343 11918 net.cpp:129] Top shape: 1000 10 (10000)
I0430 16:06:02.514348 11918 net.cpp:129] Top shape: 1000 10 (10000)
I0430 16:06:02.514350 11918 net.cpp:137] Memory required for data: 12500000
I0430 16:06:02.514354 11918 layer_factory.hpp:77] Creating layer accuracy
I0430 16:06:02.514363 11918 net.cpp:84] Creating Layer accuracy
I0430 16:06:02.514367 11918 net.cpp:406] accuracy <- ip2_ip2_0_split_0
I0430 16:06:02.514372 11918 net.cpp:406] accuracy <- label_cifar_1_split_0
I0430 16:06:02.514381 11918 net.cpp:380] accuracy -> accuracy
I0430 16:06:02.514395 11918 net.cpp:122] Setting up accuracy
I0430 16:06:02.514401 11918 net.cpp:129] Top shape: (1)
I0430 16:06:02.514405 11918 net.cpp:137] Memory required for data: 12500004
I0430 16:06:02.514408 11918 layer_factory.hpp:77] Creating layer loss
I0430 16:06:02.514418 11918 net.cpp:84] Creating Layer loss
I0430 16:06:02.514422 11918 net.cpp:406] loss <- ip2_ip2_0_split_1
I0430 16:06:02.514427 11918 net.cpp:406] loss <- label_cifar_1_split_1
I0430 16:06:02.514433 11918 net.cpp:380] loss -> loss
I0430 16:06:02.514444 11918 layer_factory.hpp:77] Creating layer loss
I0430 16:06:02.518187 11918 net.cpp:122] Setting up loss
I0430 16:06:02.518227 11918 net.cpp:129] Top shape: (1)
I0430 16:06:02.518232 11918 net.cpp:132]     with loss weight 1
I0430 16:06:02.518246 11918 net.cpp:137] Memory required for data: 12500008
I0430 16:06:02.518251 11918 net.cpp:198] loss needs backward computation.
I0430 16:06:02.518257 11918 net.cpp:200] accuracy does not need backward computation.
I0430 16:06:02.518265 11918 net.cpp:198] ip2_ip2_0_split needs backward computation.
I0430 16:06:02.518270 11918 net.cpp:198] ip2 needs backward computation.
I0430 16:06:02.518275 11918 net.cpp:198] relu1 needs backward computation.
I0430 16:06:02.518278 11918 net.cpp:198] ip1 needs backward computation.
I0430 16:06:02.518282 11918 net.cpp:200] label_cifar_1_split does not need backward computation.
I0430 16:06:02.518290 11918 net.cpp:200] cifar does not need backward computation.
I0430 16:06:02.518292 11918 net.cpp:242] This network produces output accuracy
I0430 16:06:02.518297 11918 net.cpp:242] This network produces output loss
I0430 16:06:02.518324 11918 net.cpp:255] Network initialization done.
I0430 16:06:02.518360 11918 solver.cpp:56] Solver scaffolding done.
I0430 16:06:02.518494 11918 caffe.cpp:248] Starting Optimization
I0430 16:06:02.518499 11918 solver.cpp:273] Solving CIFAR
I0430 16:06:02.518503 11918 solver.cpp:274] Learning Rate Policy: step
I0430 16:06:02.518836 11918 solver.cpp:331] Iteration 0, Testing net (#0)
I0430 16:06:02.518918 11918 blocking_queue.cpp:49] Waiting for data
I0430 16:06:02.647652 11926 data_layer.cpp:73] Restarting data prefetching from start.
I0430 16:06:02.652983 11918 solver.cpp:398]     Test net output #0: accuracy = 0.1206
I0430 16:06:02.653017 11918 solver.cpp:398]     Test net output #1: loss = 2.51992 (* 1 = 2.51992 loss)
I0430 16:06:02.653990 11918 solver.cpp:219] Iteration 0 (1.6499e-31 iter/s, 0.135456s/100 iters), loss = 2.45573
I0430 16:06:02.654013 11918 solver.cpp:238]     Train net output #0: loss = 2.45573 (* 1 = 2.45573 loss)
I0430 16:06:02.654034 11918 sgd_solver.cpp:105] Iteration 0, lr = 0.001
I0430 16:06:02.815462 11918 solver.cpp:219] Iteration 100 (619.43 iter/s, 0.161439s/100 iters), loss = 2.33317
I0430 16:06:02.815507 11918 solver.cpp:238]     Train net output #0: loss = 2.33317 (* 1 = 2.33317 loss)
I0430 16:06:02.815513 11918 sgd_solver.cpp:105] Iteration 100, lr = 0.001
I0430 16:06:02.969203 11918 solver.cpp:219] Iteration 200 (650.942 iter/s, 0.153623s/100 iters), loss = 2.34093
I0430 16:06:02.969321 11918 solver.cpp:238]     Train net output #0: loss = 2.34093 (* 1 = 2.34093 loss)
I0430 16:06:02.969332 11918 sgd_solver.cpp:105] Iteration 200, lr = 0.001
I0430 16:06:03.244766 11918 solver.cpp:219] Iteration 300 (363.025 iter/s, 0.275463s/100 iters), loss = 2.22101
I0430 16:06:03.244814 11918 solver.cpp:238]     Train net output #0: loss = 2.22101 (* 1 = 2.22101 loss)
I0430 16:06:03.244825 11918 sgd_solver.cpp:105] Iteration 300, lr = 0.001
I0430 16:06:03.821245 11918 solver.cpp:219] Iteration 400 (173.483 iter/s, 0.576424s/100 iters), loss = 2.66539
I0430 16:06:03.821293 11918 solver.cpp:238]     Train net output #0: loss = 2.66539 (* 1 = 2.66539 loss)
I0430 16:06:03.821305 11918 sgd_solver.cpp:105] Iteration 400, lr = 0.001
I0430 16:06:04.113323 11925 data_layer.cpp:73] Restarting data prefetching from start.
I0430 16:06:04.206496 11918 solver.cpp:219] Iteration 500 (259.611 iter/s, 0.385191s/100 iters), loss = 2.06554
I0430 16:06:04.206545 11918 solver.cpp:238]     Train net output #0: loss = 2.06554 (* 1 = 2.06554 loss)
I0430 16:06:04.206553 11918 sgd_solver.cpp:105] Iteration 500, lr = 0.001
I0430 16:06:04.359467 11918 solver.cpp:219] Iteration 600 (653.963 iter/s, 0.152914s/100 iters), loss = 2.06545
I0430 16:06:04.359505 11918 solver.cpp:238]     Train net output #0: loss = 2.06545 (* 1 = 2.06545 loss)
I0430 16:06:04.359513 11918 sgd_solver.cpp:105] Iteration 600, lr = 0.001
I0430 16:06:04.511874 11918 solver.cpp:219] Iteration 700 (656.34 iter/s, 0.15236s/100 iters), loss = 2.09371
I0430 16:06:04.511911 11918 solver.cpp:238]     Train net output #0: loss = 2.09371 (* 1 = 2.09371 loss)
I0430 16:06:04.511916 11918 sgd_solver.cpp:105] Iteration 700, lr = 0.001
I0430 16:06:04.662355 11918 solver.cpp:219] Iteration 800 (664.741 iter/s, 0.150435s/100 iters), loss = 1.92876
I0430 16:06:04.662395 11918 solver.cpp:238]     Train net output #0: loss = 1.92876 (* 1 = 1.92876 loss)
I0430 16:06:04.662400 11918 sgd_solver.cpp:105] Iteration 800, lr = 0.001
I0430 16:06:04.815155 11925 data_layer.cpp:73] Restarting data prefetching from start.
I0430 16:06:04.816234 11918 solver.cpp:219] Iteration 900 (650.058 iter/s, 0.153832s/100 iters), loss = 1.89986
I0430 16:06:04.816279 11918 solver.cpp:238]     Train net output #0: loss = 1.89986 (* 1 = 1.89986 loss)
I0430 16:06:04.816287 11918 sgd_solver.cpp:105] Iteration 900, lr = 0.001
I0430 16:06:04.965785 11918 solver.cpp:331] Iteration 1000, Testing net (#0)
I0430 16:06:05.064797 11926 data_layer.cpp:73] Restarting data prefetching from start.
I0430 16:06:05.070137 11918 solver.cpp:398]     Test net output #0: accuracy = 0.2598
I0430 16:06:05.070188 11918 solver.cpp:398]     Test net output #1: loss = 2.11626 (* 1 = 2.11626 loss)
I0430 16:06:05.071030 11918 solver.cpp:219] Iteration 1000 (392.54 iter/s, 0.254751s/100 iters), loss = 2.11384
I0430 16:06:05.071058 11918 solver.cpp:238]     Train net output #0: loss = 2.11384 (* 1 = 2.11384 loss)
I0430 16:06:05.071066 11918 sgd_solver.cpp:105] Iteration 1000, lr = 0.001
I0430 16:06:05.085371 11918 blocking_queue.cpp:49] Waiting for data
I0430 16:06:05.231941 11918 solver.cpp:219] Iteration 1100 (621.595 iter/s, 0.160876s/100 iters), loss = 2.21062
I0430 16:06:05.231978 11918 solver.cpp:238]     Train net output #0: loss = 2.21062 (* 1 = 2.21062 loss)
I0430 16:06:05.231983 11918 sgd_solver.cpp:105] Iteration 1100, lr = 0.001
I0430 16:06:05.386760 11918 solver.cpp:219] Iteration 1200 (646.11 iter/s, 0.154772s/100 iters), loss = 1.95257
I0430 16:06:05.386801 11918 solver.cpp:238]     Train net output #0: loss = 1.95257 (* 1 = 1.95257 loss)
I0430 16:06:05.386806 11918 sgd_solver.cpp:105] Iteration 1200, lr = 0.001
I0430 16:06:05.541015 11918 solver.cpp:219] Iteration 1300 (648.49 iter/s, 0.154204s/100 iters), loss = 2.36375
I0430 16:06:05.541054 11918 solver.cpp:238]     Train net output #0: loss = 2.36375 (* 1 = 2.36375 loss)
I0430 16:06:05.541059 11918 sgd_solver.cpp:105] Iteration 1300, lr = 0.001
I0430 16:06:05.617934 11925 data_layer.cpp:73] Restarting data prefetching from start.
