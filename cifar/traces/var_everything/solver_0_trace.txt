I0430 16:18:04.633651 12268 caffe.cpp:218] Using GPUs 0
I0430 16:18:04.660624 12268 caffe.cpp:223] GPU 0: GeForce GTX 950M
I0430 16:18:04.925380 12268 solver.cpp:44] Initializing solver from parameters: 
test_iter: 10
test_interval: 1000
base_lr: 0.001
display: 100
max_iter: 60000
lr_policy: "step"
gamma: 1
momentum: 0.9
stepsize: 5000
snapshot: 10000
snapshot_prefix: "examples/cifar10_full_sigmoid"
solver_mode: GPU
device_id: 0
net: "ECE595/cifar/train_test/var_everything/cifar_train_test_0.prototxt"
train_state {
  level: 0
  stage: ""
}
I0430 16:18:04.925525 12268 solver.cpp:87] Creating training net from net file: ECE595/cifar/train_test/var_everything/cifar_train_test_0.prototxt
I0430 16:18:04.925751 12268 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0430 16:18:04.925766 12268 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0430 16:18:04.925851 12268 net.cpp:51] Initializing net from parameters: 
name: "CIFAR"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb"
    batch_size: 111
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0430 16:18:04.925909 12268 layer_factory.hpp:77] Creating layer cifar
I0430 16:18:04.926036 12268 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_train_lmdb
I0430 16:18:04.926071 12268 net.cpp:84] Creating Layer cifar
I0430 16:18:04.926087 12268 net.cpp:380] cifar -> data
I0430 16:18:04.926115 12268 net.cpp:380] cifar -> label
I0430 16:18:04.926132 12268 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0430 16:18:04.927515 12268 data_layer.cpp:45] output data size: 111,3,32,32
I0430 16:18:04.931862 12268 net.cpp:122] Setting up cifar
I0430 16:18:04.931886 12268 net.cpp:129] Top shape: 111 3 32 32 (340992)
I0430 16:18:04.931891 12268 net.cpp:129] Top shape: 111 (111)
I0430 16:18:04.931893 12268 net.cpp:137] Memory required for data: 1364412
I0430 16:18:04.931902 12268 layer_factory.hpp:77] Creating layer ip1
I0430 16:18:04.931915 12268 net.cpp:84] Creating Layer ip1
I0430 16:18:04.931921 12268 net.cpp:406] ip1 <- data
I0430 16:18:04.931934 12268 net.cpp:380] ip1 -> ip1
I0430 16:18:04.933188 12268 net.cpp:122] Setting up ip1
I0430 16:18:04.933214 12268 net.cpp:129] Top shape: 111 10 (1110)
I0430 16:18:04.933218 12268 net.cpp:137] Memory required for data: 1368852
I0430 16:18:04.933235 12268 layer_factory.hpp:77] Creating layer relu1
I0430 16:18:04.933243 12268 net.cpp:84] Creating Layer relu1
I0430 16:18:04.933246 12268 net.cpp:406] relu1 <- ip1
I0430 16:18:04.933253 12268 net.cpp:367] relu1 -> ip1 (in-place)
I0430 16:18:04.933260 12268 net.cpp:122] Setting up relu1
I0430 16:18:04.933269 12268 net.cpp:129] Top shape: 111 10 (1110)
I0430 16:18:04.933272 12268 net.cpp:137] Memory required for data: 1373292
I0430 16:18:04.933275 12268 layer_factory.hpp:77] Creating layer ip2
I0430 16:18:04.933284 12268 net.cpp:84] Creating Layer ip2
I0430 16:18:04.933290 12268 net.cpp:406] ip2 <- ip1
I0430 16:18:04.933300 12268 net.cpp:380] ip2 -> ip2
I0430 16:18:04.933708 12268 net.cpp:122] Setting up ip2
I0430 16:18:04.933722 12268 net.cpp:129] Top shape: 111 10 (1110)
I0430 16:18:04.933743 12268 net.cpp:137] Memory required for data: 1377732
I0430 16:18:04.933756 12268 layer_factory.hpp:77] Creating layer loss
I0430 16:18:04.933766 12268 net.cpp:84] Creating Layer loss
I0430 16:18:04.933773 12268 net.cpp:406] loss <- ip2
I0430 16:18:04.933779 12268 net.cpp:406] loss <- label
I0430 16:18:04.933789 12268 net.cpp:380] loss -> loss
I0430 16:18:04.933804 12268 layer_factory.hpp:77] Creating layer loss
I0430 16:18:04.933923 12268 net.cpp:122] Setting up loss
I0430 16:18:04.933934 12268 net.cpp:129] Top shape: (1)
I0430 16:18:04.933940 12268 net.cpp:132]     with loss weight 1
I0430 16:18:04.933965 12268 net.cpp:137] Memory required for data: 1377736
I0430 16:18:04.933970 12268 net.cpp:198] loss needs backward computation.
I0430 16:18:04.933979 12268 net.cpp:198] ip2 needs backward computation.
I0430 16:18:04.933985 12268 net.cpp:198] relu1 needs backward computation.
I0430 16:18:04.933990 12268 net.cpp:198] ip1 needs backward computation.
I0430 16:18:04.933993 12268 net.cpp:200] cifar does not need backward computation.
I0430 16:18:04.933997 12268 net.cpp:242] This network produces output loss
I0430 16:18:04.934008 12268 net.cpp:255] Network initialization done.
I0430 16:18:04.934222 12268 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/cifar/train_test/var_everything/cifar_train_test_0.prototxt
I0430 16:18:04.934250 12268 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0430 16:18:04.934343 12268 net.cpp:51] Initializing net from parameters: 
name: "CIFAR"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb"
    batch_size: 1000
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0430 16:18:04.934422 12268 layer_factory.hpp:77] Creating layer cifar
I0430 16:18:04.934495 12268 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0430 16:18:04.934514 12268 net.cpp:84] Creating Layer cifar
I0430 16:18:04.934522 12268 net.cpp:380] cifar -> data
I0430 16:18:04.934535 12268 net.cpp:380] cifar -> label
I0430 16:18:04.934548 12268 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0430 16:18:04.934749 12268 data_layer.cpp:45] output data size: 1000,3,32,32
I0430 16:18:04.962131 12268 net.cpp:122] Setting up cifar
I0430 16:18:04.962152 12268 net.cpp:129] Top shape: 1000 3 32 32 (3072000)
I0430 16:18:04.962157 12268 net.cpp:129] Top shape: 1000 (1000)
I0430 16:18:04.962159 12268 net.cpp:137] Memory required for data: 12292000
I0430 16:18:04.962164 12268 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0430 16:18:04.962174 12268 net.cpp:84] Creating Layer label_cifar_1_split
I0430 16:18:04.962177 12268 net.cpp:406] label_cifar_1_split <- label
I0430 16:18:04.962185 12268 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I0430 16:18:04.962214 12268 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I0430 16:18:04.962316 12268 net.cpp:122] Setting up label_cifar_1_split
I0430 16:18:04.962327 12268 net.cpp:129] Top shape: 1000 (1000)
I0430 16:18:04.962353 12268 net.cpp:129] Top shape: 1000 (1000)
I0430 16:18:04.962359 12268 net.cpp:137] Memory required for data: 12300000
I0430 16:18:04.962364 12268 layer_factory.hpp:77] Creating layer ip1
I0430 16:18:04.962376 12268 net.cpp:84] Creating Layer ip1
I0430 16:18:04.962381 12268 net.cpp:406] ip1 <- data
I0430 16:18:04.962389 12268 net.cpp:380] ip1 -> ip1
I0430 16:18:04.962921 12268 net.cpp:122] Setting up ip1
I0430 16:18:04.962929 12268 net.cpp:129] Top shape: 1000 10 (10000)
I0430 16:18:04.962932 12268 net.cpp:137] Memory required for data: 12340000
I0430 16:18:04.962941 12268 layer_factory.hpp:77] Creating layer relu1
I0430 16:18:04.962947 12268 net.cpp:84] Creating Layer relu1
I0430 16:18:04.962950 12268 net.cpp:406] relu1 <- ip1
I0430 16:18:04.962955 12268 net.cpp:367] relu1 -> ip1 (in-place)
I0430 16:18:04.962961 12268 net.cpp:122] Setting up relu1
I0430 16:18:04.962967 12268 net.cpp:129] Top shape: 1000 10 (10000)
I0430 16:18:04.962970 12268 net.cpp:137] Memory required for data: 12380000
I0430 16:18:04.962973 12268 layer_factory.hpp:77] Creating layer ip2
I0430 16:18:04.962981 12268 net.cpp:84] Creating Layer ip2
I0430 16:18:04.962987 12268 net.cpp:406] ip2 <- ip1
I0430 16:18:04.962996 12268 net.cpp:380] ip2 -> ip2
I0430 16:18:04.963186 12268 net.cpp:122] Setting up ip2
I0430 16:18:04.963193 12268 net.cpp:129] Top shape: 1000 10 (10000)
I0430 16:18:04.963201 12268 net.cpp:137] Memory required for data: 12420000
I0430 16:18:04.963209 12268 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0430 16:18:04.963217 12268 net.cpp:84] Creating Layer ip2_ip2_0_split
I0430 16:18:04.963222 12268 net.cpp:406] ip2_ip2_0_split <- ip2
I0430 16:18:04.963232 12268 net.cpp:380] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0430 16:18:04.963241 12268 net.cpp:380] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0430 16:18:04.965154 12268 net.cpp:122] Setting up ip2_ip2_0_split
I0430 16:18:04.965162 12268 net.cpp:129] Top shape: 1000 10 (10000)
I0430 16:18:04.965167 12268 net.cpp:129] Top shape: 1000 10 (10000)
I0430 16:18:04.965168 12268 net.cpp:137] Memory required for data: 12500000
I0430 16:18:04.965170 12268 layer_factory.hpp:77] Creating layer accuracy
I0430 16:18:04.965176 12268 net.cpp:84] Creating Layer accuracy
I0430 16:18:04.965180 12268 net.cpp:406] accuracy <- ip2_ip2_0_split_0
I0430 16:18:04.965184 12268 net.cpp:406] accuracy <- label_cifar_1_split_0
I0430 16:18:04.965189 12268 net.cpp:380] accuracy -> accuracy
I0430 16:18:04.965200 12268 net.cpp:122] Setting up accuracy
I0430 16:18:04.965206 12268 net.cpp:129] Top shape: (1)
I0430 16:18:04.965210 12268 net.cpp:137] Memory required for data: 12500004
I0430 16:18:04.965214 12268 layer_factory.hpp:77] Creating layer loss
I0430 16:18:04.965220 12268 net.cpp:84] Creating Layer loss
I0430 16:18:04.965226 12268 net.cpp:406] loss <- ip2_ip2_0_split_1
I0430 16:18:04.965232 12268 net.cpp:406] loss <- label_cifar_1_split_1
I0430 16:18:04.965241 12268 net.cpp:380] loss -> loss
I0430 16:18:04.965251 12268 layer_factory.hpp:77] Creating layer loss
I0430 16:18:04.965853 12268 net.cpp:122] Setting up loss
I0430 16:18:04.965863 12268 net.cpp:129] Top shape: (1)
I0430 16:18:04.965867 12268 net.cpp:132]     with loss weight 1
I0430 16:18:04.965875 12268 net.cpp:137] Memory required for data: 12500008
I0430 16:18:04.965880 12268 net.cpp:198] loss needs backward computation.
I0430 16:18:04.965884 12268 net.cpp:200] accuracy does not need backward computation.
I0430 16:18:04.965888 12268 net.cpp:198] ip2_ip2_0_split needs backward computation.
I0430 16:18:04.965889 12268 net.cpp:198] ip2 needs backward computation.
I0430 16:18:04.965893 12268 net.cpp:198] relu1 needs backward computation.
I0430 16:18:04.965895 12268 net.cpp:198] ip1 needs backward computation.
I0430 16:18:04.965898 12268 net.cpp:200] label_cifar_1_split does not need backward computation.
I0430 16:18:04.965903 12268 net.cpp:200] cifar does not need backward computation.
I0430 16:18:04.965906 12268 net.cpp:242] This network produces output accuracy
I0430 16:18:04.965911 12268 net.cpp:242] This network produces output loss
I0430 16:18:04.965939 12268 net.cpp:255] Network initialization done.
I0430 16:18:04.965977 12268 solver.cpp:56] Solver scaffolding done.
I0430 16:18:04.966161 12268 caffe.cpp:248] Starting Optimization
I0430 16:18:04.966166 12268 solver.cpp:273] Solving CIFAR
I0430 16:18:04.966171 12268 solver.cpp:274] Learning Rate Policy: step
I0430 16:18:04.966629 12268 solver.cpp:331] Iteration 0, Testing net (#0)
I0430 16:18:04.966711 12268 blocking_queue.cpp:49] Waiting for data
I0430 16:18:05.093926 12277 data_layer.cpp:73] Restarting data prefetching from start.
I0430 16:18:05.099297 12268 solver.cpp:398]     Test net output #0: accuracy = 0.0817
I0430 16:18:05.099329 12268 solver.cpp:398]     Test net output #1: loss = 2.56508 (* 1 = 2.56508 loss)
I0430 16:18:05.100327 12268 solver.cpp:219] Iteration 0 (-1.20189e-07 iter/s, 0.134128s/100 iters), loss = 2.45341
I0430 16:18:05.100348 12268 solver.cpp:238]     Train net output #0: loss = 2.45341 (* 1 = 2.45341 loss)
I0430 16:18:05.100361 12268 sgd_solver.cpp:105] Iteration 0, lr = 0.001
I0430 16:18:05.263664 12268 solver.cpp:219] Iteration 100 (612.363 iter/s, 0.163302s/100 iters), loss = 2.14645
I0430 16:18:05.263700 12268 solver.cpp:238]     Train net output #0: loss = 2.14645 (* 1 = 2.14645 loss)
I0430 16:18:05.263706 12268 sgd_solver.cpp:105] Iteration 100, lr = 0.001
I0430 16:18:05.416173 12268 solver.cpp:219] Iteration 200 (655.909 iter/s, 0.15246s/100 iters), loss = 2.34804
I0430 16:18:05.416216 12268 solver.cpp:238]     Train net output #0: loss = 2.34804 (* 1 = 2.34804 loss)
I0430 16:18:05.416224 12268 sgd_solver.cpp:105] Iteration 200, lr = 0.001
I0430 16:18:05.569880 12268 solver.cpp:219] Iteration 300 (650.815 iter/s, 0.153654s/100 iters), loss = 2.09689
I0430 16:18:05.569917 12268 solver.cpp:238]     Train net output #0: loss = 2.09689 (* 1 = 2.09689 loss)
I0430 16:18:05.569926 12268 sgd_solver.cpp:105] Iteration 300, lr = 0.001
I0430 16:18:05.723665 12268 solver.cpp:219] Iteration 400 (650.46 iter/s, 0.153737s/100 iters), loss = 2.16865
I0430 16:18:05.723701 12268 solver.cpp:238]     Train net output #0: loss = 2.16865 (* 1 = 2.16865 loss)
I0430 16:18:05.723706 12268 sgd_solver.cpp:105] Iteration 400, lr = 0.001
I0430 16:18:05.802266 12276 data_layer.cpp:73] Restarting data prefetching from start.
I0430 16:18:05.879808 12268 solver.cpp:219] Iteration 500 (640.633 iter/s, 0.156096s/100 iters), loss = 2.19354
I0430 16:18:05.879844 12268 solver.cpp:238]     Train net output #0: loss = 2.19354 (* 1 = 2.19354 loss)
I0430 16:18:05.879849 12268 sgd_solver.cpp:105] Iteration 500, lr = 0.001
I0430 16:18:06.030402 12268 solver.cpp:219] Iteration 600 (664.247 iter/s, 0.150546s/100 iters), loss = 2.28085
I0430 16:18:06.030439 12268 solver.cpp:238]     Train net output #0: loss = 2.28085 (* 1 = 2.28085 loss)
I0430 16:18:06.030444 12268 sgd_solver.cpp:105] Iteration 600, lr = 0.001
I0430 16:18:06.184401 12268 solver.cpp:219] Iteration 700 (649.564 iter/s, 0.15395s/100 iters), loss = 2.17123
I0430 16:18:06.184438 12268 solver.cpp:238]     Train net output #0: loss = 2.17123 (* 1 = 2.17123 loss)
I0430 16:18:06.184449 12268 sgd_solver.cpp:105] Iteration 700, lr = 0.001
I0430 16:18:06.334048 12268 solver.cpp:219] Iteration 800 (668.499 iter/s, 0.149589s/100 iters), loss = 2.04998
I0430 16:18:06.334106 12268 solver.cpp:238]     Train net output #0: loss = 2.04998 (* 1 = 2.04998 loss)
I0430 16:18:06.334117 12268 sgd_solver.cpp:105] Iteration 800, lr = 0.001
I0430 16:18:06.483880 12276 data_layer.cpp:73] Restarting data prefetching from start.
I0430 16:18:06.484948 12268 solver.cpp:219] Iteration 900 (662.965 iter/s, 0.150837s/100 iters), loss = 1.99493
I0430 16:18:06.484977 12268 solver.cpp:238]     Train net output #0: loss = 1.99493 (* 1 = 1.99493 loss)
I0430 16:18:06.484984 12268 sgd_solver.cpp:105] Iteration 900, lr = 0.001
I0430 16:18:06.636093 12268 solver.cpp:331] Iteration 1000, Testing net (#0)
