I0425 16:09:34.328395  7883 caffe.cpp:218] Using GPUs 0
I0425 16:09:34.345693  7883 caffe.cpp:223] GPU 0: GeForce GTX 950M
I0425 16:09:34.586048  7883 solver.cpp:44] Initializing solver from parameters: 
test_iter: 10
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 6000
lr_policy: "step"
gamma: 1
momentum: 0.9
stepsize: 5000
snapshot: 10000
snapshot_prefix: "examples/cifar10_full_sigmoid"
solver_mode: GPU
device_id: 0
net: "ECE595/cifar/train_test/var_num_feature_maxiter6000/cifarlenet_train_test_featuresize150.prototxt"
train_state {
  level: 0
  stage: ""
}
I0425 16:09:34.586189  7883 solver.cpp:87] Creating training net from net file: ECE595/cifar/train_test/var_num_feature_maxiter6000/cifarlenet_train_test_featuresize150.prototxt
I0425 16:09:34.586385  7883 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0425 16:09:34.586400  7883 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0425 16:09:34.586483  7883 net.cpp:51] Initializing net from parameters: 
name: "CIFARLeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 150
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 150
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0425 16:09:34.586556  7883 layer_factory.hpp:77] Creating layer cifar
I0425 16:09:34.586680  7883 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_train_lmdb
I0425 16:09:34.586707  7883 net.cpp:84] Creating Layer cifar
I0425 16:09:34.586716  7883 net.cpp:380] cifar -> data
I0425 16:09:34.586736  7883 net.cpp:380] cifar -> label
I0425 16:09:34.586750  7883 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0425 16:09:34.587697  7883 data_layer.cpp:45] output data size: 64,3,32,32
I0425 16:09:34.591331  7883 net.cpp:122] Setting up cifar
I0425 16:09:34.591349  7883 net.cpp:129] Top shape: 64 3 32 32 (196608)
I0425 16:09:34.591353  7883 net.cpp:129] Top shape: 64 (64)
I0425 16:09:34.591357  7883 net.cpp:137] Memory required for data: 786688
I0425 16:09:34.591367  7883 layer_factory.hpp:77] Creating layer conv0
I0425 16:09:34.591387  7883 net.cpp:84] Creating Layer conv0
I0425 16:09:34.591393  7883 net.cpp:406] conv0 <- data
I0425 16:09:34.591406  7883 net.cpp:380] conv0 -> conv0
I0425 16:09:34.592481  7883 net.cpp:122] Setting up conv0
I0425 16:09:34.592494  7883 net.cpp:129] Top shape: 64 150 28 28 (7526400)
I0425 16:09:34.592497  7883 net.cpp:137] Memory required for data: 30892288
I0425 16:09:34.592515  7883 layer_factory.hpp:77] Creating layer pool0
I0425 16:09:34.592525  7883 net.cpp:84] Creating Layer pool0
I0425 16:09:34.592527  7883 net.cpp:406] pool0 <- conv0
I0425 16:09:34.592531  7883 net.cpp:380] pool0 -> pool0
I0425 16:09:34.592577  7883 net.cpp:122] Setting up pool0
I0425 16:09:34.592584  7883 net.cpp:129] Top shape: 64 150 14 14 (1881600)
I0425 16:09:34.592586  7883 net.cpp:137] Memory required for data: 38418688
I0425 16:09:34.592589  7883 layer_factory.hpp:77] Creating layer conv1
I0425 16:09:34.592598  7883 net.cpp:84] Creating Layer conv1
I0425 16:09:34.592602  7883 net.cpp:406] conv1 <- pool0
I0425 16:09:34.592607  7883 net.cpp:380] conv1 -> conv1
I0425 16:09:34.596837  7883 net.cpp:122] Setting up conv1
I0425 16:09:34.596851  7883 net.cpp:129] Top shape: 64 150 10 10 (960000)
I0425 16:09:34.596854  7883 net.cpp:137] Memory required for data: 42258688
I0425 16:09:34.596864  7883 layer_factory.hpp:77] Creating layer pool1
I0425 16:09:34.596873  7883 net.cpp:84] Creating Layer pool1
I0425 16:09:34.596877  7883 net.cpp:406] pool1 <- conv1
I0425 16:09:34.596884  7883 net.cpp:380] pool1 -> pool1
I0425 16:09:34.596920  7883 net.cpp:122] Setting up pool1
I0425 16:09:34.596925  7883 net.cpp:129] Top shape: 64 150 5 5 (240000)
I0425 16:09:34.596928  7883 net.cpp:137] Memory required for data: 43218688
I0425 16:09:34.596931  7883 layer_factory.hpp:77] Creating layer ip1
I0425 16:09:34.596938  7883 net.cpp:84] Creating Layer ip1
I0425 16:09:34.596941  7883 net.cpp:406] ip1 <- pool1
I0425 16:09:34.596946  7883 net.cpp:380] ip1 -> ip1
I0425 16:09:34.610453  7883 net.cpp:122] Setting up ip1
I0425 16:09:34.610472  7883 net.cpp:129] Top shape: 64 500 (32000)
I0425 16:09:34.610476  7883 net.cpp:137] Memory required for data: 43346688
I0425 16:09:34.610487  7883 layer_factory.hpp:77] Creating layer relu1
I0425 16:09:34.610496  7883 net.cpp:84] Creating Layer relu1
I0425 16:09:34.610499  7883 net.cpp:406] relu1 <- ip1
I0425 16:09:34.610505  7883 net.cpp:367] relu1 -> ip1 (in-place)
I0425 16:09:34.610515  7883 net.cpp:122] Setting up relu1
I0425 16:09:34.610518  7883 net.cpp:129] Top shape: 64 500 (32000)
I0425 16:09:34.610522  7883 net.cpp:137] Memory required for data: 43474688
I0425 16:09:34.610523  7883 layer_factory.hpp:77] Creating layer ip2
I0425 16:09:34.610530  7883 net.cpp:84] Creating Layer ip2
I0425 16:09:34.610533  7883 net.cpp:406] ip2 <- ip1
I0425 16:09:34.610539  7883 net.cpp:380] ip2 -> ip2
I0425 16:09:34.610671  7883 net.cpp:122] Setting up ip2
I0425 16:09:34.610677  7883 net.cpp:129] Top shape: 64 10 (640)
I0425 16:09:34.610680  7883 net.cpp:137] Memory required for data: 43477248
I0425 16:09:34.610685  7883 layer_factory.hpp:77] Creating layer relu2
I0425 16:09:34.610692  7883 net.cpp:84] Creating Layer relu2
I0425 16:09:34.610697  7883 net.cpp:406] relu2 <- ip2
I0425 16:09:34.610700  7883 net.cpp:367] relu2 -> ip2 (in-place)
I0425 16:09:34.610704  7883 net.cpp:122] Setting up relu2
I0425 16:09:34.610707  7883 net.cpp:129] Top shape: 64 10 (640)
I0425 16:09:34.610710  7883 net.cpp:137] Memory required for data: 43479808
I0425 16:09:34.610713  7883 layer_factory.hpp:77] Creating layer loss
I0425 16:09:34.610718  7883 net.cpp:84] Creating Layer loss
I0425 16:09:34.610719  7883 net.cpp:406] loss <- ip2
I0425 16:09:34.610723  7883 net.cpp:406] loss <- label
I0425 16:09:34.610728  7883 net.cpp:380] loss -> loss
I0425 16:09:34.610743  7883 layer_factory.hpp:77] Creating layer loss
I0425 16:09:34.610826  7883 net.cpp:122] Setting up loss
I0425 16:09:34.610832  7883 net.cpp:129] Top shape: (1)
I0425 16:09:34.610834  7883 net.cpp:132]     with loss weight 1
I0425 16:09:34.610851  7883 net.cpp:137] Memory required for data: 43479812
I0425 16:09:34.610852  7883 net.cpp:198] loss needs backward computation.
I0425 16:09:34.610860  7883 net.cpp:198] relu2 needs backward computation.
I0425 16:09:34.610880  7883 net.cpp:198] ip2 needs backward computation.
I0425 16:09:34.610883  7883 net.cpp:198] relu1 needs backward computation.
I0425 16:09:34.610885  7883 net.cpp:198] ip1 needs backward computation.
I0425 16:09:34.610889  7883 net.cpp:198] pool1 needs backward computation.
I0425 16:09:34.610891  7883 net.cpp:198] conv1 needs backward computation.
I0425 16:09:34.610894  7883 net.cpp:198] pool0 needs backward computation.
I0425 16:09:34.610896  7883 net.cpp:198] conv0 needs backward computation.
I0425 16:09:34.610900  7883 net.cpp:200] cifar does not need backward computation.
I0425 16:09:34.610903  7883 net.cpp:242] This network produces output loss
I0425 16:09:34.610918  7883 net.cpp:255] Network initialization done.
I0425 16:09:34.611080  7883 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/cifar/train_test/var_num_feature_maxiter6000/cifarlenet_train_test_featuresize150.prototxt
I0425 16:09:34.611101  7883 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0425 16:09:34.611186  7883 net.cpp:51] Initializing net from parameters: 
name: "CIFARLeNet"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 150
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 150
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0425 16:09:34.611249  7883 layer_factory.hpp:77] Creating layer cifar
I0425 16:09:34.611312  7883 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0425 16:09:34.611325  7883 net.cpp:84] Creating Layer cifar
I0425 16:09:34.611333  7883 net.cpp:380] cifar -> data
I0425 16:09:34.611340  7883 net.cpp:380] cifar -> label
I0425 16:09:34.611347  7883 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0425 16:09:34.611488  7883 data_layer.cpp:45] output data size: 100,3,32,32
I0425 16:09:34.615339  7883 net.cpp:122] Setting up cifar
I0425 16:09:34.615360  7883 net.cpp:129] Top shape: 100 3 32 32 (307200)
I0425 16:09:34.615365  7883 net.cpp:129] Top shape: 100 (100)
I0425 16:09:34.615367  7883 net.cpp:137] Memory required for data: 1229200
I0425 16:09:34.615388  7883 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0425 16:09:34.615398  7883 net.cpp:84] Creating Layer label_cifar_1_split
I0425 16:09:34.615401  7883 net.cpp:406] label_cifar_1_split <- label
I0425 16:09:34.615409  7883 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I0425 16:09:34.615418  7883 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I0425 16:09:34.615486  7883 net.cpp:122] Setting up label_cifar_1_split
I0425 16:09:34.615494  7883 net.cpp:129] Top shape: 100 (100)
I0425 16:09:34.615496  7883 net.cpp:129] Top shape: 100 (100)
I0425 16:09:34.615499  7883 net.cpp:137] Memory required for data: 1230000
I0425 16:09:34.615501  7883 layer_factory.hpp:77] Creating layer conv0
I0425 16:09:34.615512  7883 net.cpp:84] Creating Layer conv0
I0425 16:09:34.615515  7883 net.cpp:406] conv0 <- data
I0425 16:09:34.615521  7883 net.cpp:380] conv0 -> conv0
I0425 16:09:34.615828  7883 net.cpp:122] Setting up conv0
I0425 16:09:34.615836  7883 net.cpp:129] Top shape: 100 150 28 28 (11760000)
I0425 16:09:34.615839  7883 net.cpp:137] Memory required for data: 48270000
I0425 16:09:34.615849  7883 layer_factory.hpp:77] Creating layer pool0
I0425 16:09:34.615855  7883 net.cpp:84] Creating Layer pool0
I0425 16:09:34.615857  7883 net.cpp:406] pool0 <- conv0
I0425 16:09:34.615861  7883 net.cpp:380] pool0 -> pool0
I0425 16:09:34.615897  7883 net.cpp:122] Setting up pool0
I0425 16:09:34.615902  7883 net.cpp:129] Top shape: 100 150 14 14 (2940000)
I0425 16:09:34.615906  7883 net.cpp:137] Memory required for data: 60030000
I0425 16:09:34.615907  7883 layer_factory.hpp:77] Creating layer conv1
I0425 16:09:34.615916  7883 net.cpp:84] Creating Layer conv1
I0425 16:09:34.615919  7883 net.cpp:406] conv1 <- pool0
I0425 16:09:34.615926  7883 net.cpp:380] conv1 -> conv1
I0425 16:09:34.620538  7883 net.cpp:122] Setting up conv1
I0425 16:09:34.620559  7883 net.cpp:129] Top shape: 100 150 10 10 (1500000)
I0425 16:09:34.620563  7883 net.cpp:137] Memory required for data: 66030000
I0425 16:09:34.620573  7883 layer_factory.hpp:77] Creating layer pool1
I0425 16:09:34.620582  7883 net.cpp:84] Creating Layer pool1
I0425 16:09:34.620586  7883 net.cpp:406] pool1 <- conv1
I0425 16:09:34.620594  7883 net.cpp:380] pool1 -> pool1
I0425 16:09:34.620635  7883 net.cpp:122] Setting up pool1
I0425 16:09:34.620641  7883 net.cpp:129] Top shape: 100 150 5 5 (375000)
I0425 16:09:34.620645  7883 net.cpp:137] Memory required for data: 67530000
I0425 16:09:34.620647  7883 layer_factory.hpp:77] Creating layer ip1
I0425 16:09:34.620656  7883 net.cpp:84] Creating Layer ip1
I0425 16:09:34.620657  7883 net.cpp:406] ip1 <- pool1
I0425 16:09:34.620663  7883 net.cpp:380] ip1 -> ip1
I0425 16:09:34.635067  7883 net.cpp:122] Setting up ip1
I0425 16:09:34.635090  7883 net.cpp:129] Top shape: 100 500 (50000)
I0425 16:09:34.635093  7883 net.cpp:137] Memory required for data: 67730000
I0425 16:09:34.635107  7883 layer_factory.hpp:77] Creating layer relu1
I0425 16:09:34.635114  7883 net.cpp:84] Creating Layer relu1
I0425 16:09:34.635118  7883 net.cpp:406] relu1 <- ip1
I0425 16:09:34.635123  7883 net.cpp:367] relu1 -> ip1 (in-place)
I0425 16:09:34.635131  7883 net.cpp:122] Setting up relu1
I0425 16:09:34.635134  7883 net.cpp:129] Top shape: 100 500 (50000)
I0425 16:09:34.635136  7883 net.cpp:137] Memory required for data: 67930000
I0425 16:09:34.635139  7883 layer_factory.hpp:77] Creating layer ip2
I0425 16:09:34.635149  7883 net.cpp:84] Creating Layer ip2
I0425 16:09:34.635154  7883 net.cpp:406] ip2 <- ip1
I0425 16:09:34.635157  7883 net.cpp:380] ip2 -> ip2
I0425 16:09:34.635298  7883 net.cpp:122] Setting up ip2
I0425 16:09:34.635308  7883 net.cpp:129] Top shape: 100 10 (1000)
I0425 16:09:34.635309  7883 net.cpp:137] Memory required for data: 67934000
I0425 16:09:34.635314  7883 layer_factory.hpp:77] Creating layer relu2
I0425 16:09:34.635318  7883 net.cpp:84] Creating Layer relu2
I0425 16:09:34.635321  7883 net.cpp:406] relu2 <- ip2
I0425 16:09:34.635324  7883 net.cpp:367] relu2 -> ip2 (in-place)
I0425 16:09:34.635329  7883 net.cpp:122] Setting up relu2
I0425 16:09:34.635346  7883 net.cpp:129] Top shape: 100 10 (1000)
I0425 16:09:34.635349  7883 net.cpp:137] Memory required for data: 67938000
I0425 16:09:34.635351  7883 layer_factory.hpp:77] Creating layer ip2_relu2_0_split
I0425 16:09:34.635356  7883 net.cpp:84] Creating Layer ip2_relu2_0_split
I0425 16:09:34.635359  7883 net.cpp:406] ip2_relu2_0_split <- ip2
I0425 16:09:34.635367  7883 net.cpp:380] ip2_relu2_0_split -> ip2_relu2_0_split_0
I0425 16:09:34.635375  7883 net.cpp:380] ip2_relu2_0_split -> ip2_relu2_0_split_1
I0425 16:09:34.635408  7883 net.cpp:122] Setting up ip2_relu2_0_split
I0425 16:09:34.635412  7883 net.cpp:129] Top shape: 100 10 (1000)
I0425 16:09:34.635416  7883 net.cpp:129] Top shape: 100 10 (1000)
I0425 16:09:34.635418  7883 net.cpp:137] Memory required for data: 67946000
I0425 16:09:34.635421  7883 layer_factory.hpp:77] Creating layer accuracy
I0425 16:09:34.635426  7883 net.cpp:84] Creating Layer accuracy
I0425 16:09:34.635429  7883 net.cpp:406] accuracy <- ip2_relu2_0_split_0
I0425 16:09:34.635433  7883 net.cpp:406] accuracy <- label_cifar_1_split_0
I0425 16:09:34.635438  7883 net.cpp:380] accuracy -> accuracy
I0425 16:09:34.635445  7883 net.cpp:122] Setting up accuracy
I0425 16:09:34.635449  7883 net.cpp:129] Top shape: (1)
I0425 16:09:34.635452  7883 net.cpp:137] Memory required for data: 67946004
I0425 16:09:34.635453  7883 layer_factory.hpp:77] Creating layer loss
I0425 16:09:34.635458  7883 net.cpp:84] Creating Layer loss
I0425 16:09:34.635463  7883 net.cpp:406] loss <- ip2_relu2_0_split_1
I0425 16:09:34.635468  7883 net.cpp:406] loss <- label_cifar_1_split_1
I0425 16:09:34.635474  7883 net.cpp:380] loss -> loss
I0425 16:09:34.635481  7883 layer_factory.hpp:77] Creating layer loss
I0425 16:09:34.635565  7883 net.cpp:122] Setting up loss
I0425 16:09:34.635570  7883 net.cpp:129] Top shape: (1)
I0425 16:09:34.635572  7883 net.cpp:132]     with loss weight 1
I0425 16:09:34.635581  7883 net.cpp:137] Memory required for data: 67946008
I0425 16:09:34.635583  7883 net.cpp:198] loss needs backward computation.
I0425 16:09:34.635587  7883 net.cpp:200] accuracy does not need backward computation.
I0425 16:09:34.635591  7883 net.cpp:198] ip2_relu2_0_split needs backward computation.
I0425 16:09:34.635593  7883 net.cpp:198] relu2 needs backward computation.
I0425 16:09:34.635596  7883 net.cpp:198] ip2 needs backward computation.
I0425 16:09:34.635599  7883 net.cpp:198] relu1 needs backward computation.
I0425 16:09:34.635601  7883 net.cpp:198] ip1 needs backward computation.
I0425 16:09:34.635604  7883 net.cpp:198] pool1 needs backward computation.
I0425 16:09:34.635607  7883 net.cpp:198] conv1 needs backward computation.
I0425 16:09:34.635612  7883 net.cpp:198] pool0 needs backward computation.
I0425 16:09:34.635614  7883 net.cpp:198] conv0 needs backward computation.
I0425 16:09:34.635617  7883 net.cpp:200] label_cifar_1_split does not need backward computation.
I0425 16:09:34.635623  7883 net.cpp:200] cifar does not need backward computation.
I0425 16:09:34.635625  7883 net.cpp:242] This network produces output accuracy
I0425 16:09:34.635628  7883 net.cpp:242] This network produces output loss
I0425 16:09:34.635641  7883 net.cpp:255] Network initialization done.
I0425 16:09:34.635718  7883 solver.cpp:56] Solver scaffolding done.
I0425 16:09:34.635989  7883 caffe.cpp:248] Starting Optimization
I0425 16:09:34.635994  7883 solver.cpp:273] Solving CIFARLeNet
I0425 16:09:34.635996  7883 solver.cpp:274] Learning Rate Policy: step
I0425 16:09:34.636869  7883 solver.cpp:331] Iteration 0, Testing net (#0)
I0425 16:09:36.815706  7883 solver.cpp:398]     Test net output #0: accuracy = 0.127
I0425 16:09:36.815735  7883 solver.cpp:398]     Test net output #1: loss = 35.9536 (* 1 = 35.9536 loss)
I0425 16:09:37.155171  7883 solver.cpp:219] Iteration 0 (-9.64059e-37 iter/s, 2.51917s/100 iters), loss = 36.4262
I0425 16:09:37.155212  7883 solver.cpp:238]     Train net output #0: loss = 36.4262 (* 1 = 36.4262 loss)
I0425 16:09:37.155225  7883 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0425 16:10:09.408087  7883 solver.cpp:219] Iteration 100 (3.10046 iter/s, 32.2533s/100 iters), loss = 87.3365
I0425 16:10:09.408339  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:10:09.408362  7883 sgd_solver.cpp:105] Iteration 100, lr = 0.01
I0425 16:10:41.616051  7883 solver.cpp:219] Iteration 200 (3.10481 iter/s, 32.2081s/100 iters), loss = 87.3365
I0425 16:10:41.616245  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:10:41.616252  7883 sgd_solver.cpp:105] Iteration 200, lr = 0.01
I0425 16:11:13.812924  7883 solver.cpp:219] Iteration 300 (3.10587 iter/s, 32.1971s/100 iters), loss = 87.3365
I0425 16:11:13.812974  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:11:13.812980  7883 sgd_solver.cpp:105] Iteration 300, lr = 0.01
I0425 16:11:46.009919  7883 solver.cpp:219] Iteration 400 (3.10584 iter/s, 32.1974s/100 iters), loss = 87.3365
I0425 16:11:46.010064  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:11:46.010071  7883 sgd_solver.cpp:105] Iteration 400, lr = 0.01
I0425 16:12:18.213210  7883 solver.cpp:219] Iteration 500 (3.10525 iter/s, 32.2035s/100 iters), loss = 87.3365
I0425 16:12:18.213280  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:12:18.213289  7883 sgd_solver.cpp:105] Iteration 500, lr = 0.01
I0425 16:12:50.418337  7883 solver.cpp:219] Iteration 600 (3.10506 iter/s, 32.2055s/100 iters), loss = 87.3365
I0425 16:12:50.418390  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:12:50.418397  7883 sgd_solver.cpp:105] Iteration 600, lr = 0.01
I0425 16:13:22.613644  7883 solver.cpp:219] Iteration 700 (3.10601 iter/s, 32.1957s/100 iters), loss = 87.3365
I0425 16:13:22.613829  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:13:22.613837  7883 sgd_solver.cpp:105] Iteration 700, lr = 0.01
I0425 16:13:47.422581  7891 data_layer.cpp:73] Restarting data prefetching from start.
I0425 16:13:54.828636  7883 solver.cpp:219] Iteration 800 (3.10412 iter/s, 32.2152s/100 iters), loss = 87.3365
I0425 16:13:54.828780  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:13:54.828790  7883 sgd_solver.cpp:105] Iteration 800, lr = 0.01
I0425 16:14:27.026760  7883 solver.cpp:219] Iteration 900 (3.10574 iter/s, 32.1984s/100 iters), loss = 87.3365
I0425 16:14:27.026906  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:14:27.026918  7883 sgd_solver.cpp:105] Iteration 900, lr = 0.01
I0425 16:14:58.729951  7883 solver.cpp:331] Iteration 1000, Testing net (#0)
I0425 16:15:01.078773  7883 solver.cpp:398]     Test net output #0: accuracy = 0.094
I0425 16:15:01.078804  7883 solver.cpp:398]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:15:01.408488  7883 solver.cpp:219] Iteration 1000 (2.9085 iter/s, 34.382s/100 iters), loss = 87.3365
I0425 16:15:01.408526  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:15:01.408534  7883 sgd_solver.cpp:105] Iteration 1000, lr = 0.01
I0425 16:15:33.593932  7883 solver.cpp:219] Iteration 1100 (3.10696 iter/s, 32.1858s/100 iters), loss = 87.3365
I0425 16:15:33.594136  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:15:33.594147  7883 sgd_solver.cpp:105] Iteration 1100, lr = 0.01
I0425 16:16:05.800403  7883 solver.cpp:219] Iteration 1200 (3.10495 iter/s, 32.2067s/100 iters), loss = 87.3365
I0425 16:16:05.800601  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:16:05.800609  7883 sgd_solver.cpp:105] Iteration 1200, lr = 0.01
I0425 16:16:37.982738  7883 solver.cpp:219] Iteration 1300 (3.10727 iter/s, 32.1825s/100 iters), loss = 87.3365
I0425 16:16:37.982905  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:16:37.982915  7883 sgd_solver.cpp:105] Iteration 1300, lr = 0.01
I0425 16:17:10.188405  7883 solver.cpp:219] Iteration 1400 (3.10502 iter/s, 32.2059s/100 iters), loss = 87.3365
I0425 16:17:10.188603  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:17:10.188609  7883 sgd_solver.cpp:105] Iteration 1400, lr = 0.01
I0425 16:17:42.393546  7883 solver.cpp:219] Iteration 1500 (3.10507 iter/s, 32.2054s/100 iters), loss = 87.3365
I0425 16:17:42.393733  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:17:42.393741  7883 sgd_solver.cpp:105] Iteration 1500, lr = 0.01
I0425 16:18:01.076946  7891 data_layer.cpp:73] Restarting data prefetching from start.
I0425 16:18:14.599083  7883 solver.cpp:219] Iteration 1600 (3.10503 iter/s, 32.2058s/100 iters), loss = 87.3365
I0425 16:18:14.599232  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:18:14.599238  7883 sgd_solver.cpp:105] Iteration 1600, lr = 0.01
I0425 16:18:46.789116  7883 solver.cpp:219] Iteration 1700 (3.10652 iter/s, 32.1903s/100 iters), loss = 87.3365
I0425 16:18:46.789189  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:18:46.789197  7883 sgd_solver.cpp:105] Iteration 1700, lr = 0.01
I0425 16:19:18.973433  7883 solver.cpp:219] Iteration 1800 (3.10707 iter/s, 32.1847s/100 iters), loss = 87.3365
I0425 16:19:18.973572  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:19:18.973582  7883 sgd_solver.cpp:105] Iteration 1800, lr = 0.01
I0425 16:19:51.176311  7883 solver.cpp:219] Iteration 1900 (3.10529 iter/s, 32.2032s/100 iters), loss = 87.3365
I0425 16:19:51.176476  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:19:51.176484  7883 sgd_solver.cpp:105] Iteration 1900, lr = 0.01
I0425 16:20:22.874624  7883 solver.cpp:331] Iteration 2000, Testing net (#0)
I0425 16:20:25.216884  7883 solver.cpp:398]     Test net output #0: accuracy = 0.1
I0425 16:20:25.216930  7883 solver.cpp:398]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:20:25.546950  7883 solver.cpp:219] Iteration 2000 (2.90944 iter/s, 34.3709s/100 iters), loss = 87.3365
I0425 16:20:25.546995  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:20:25.547001  7883 sgd_solver.cpp:105] Iteration 2000, lr = 0.01
I0425 16:20:57.752063  7883 solver.cpp:219] Iteration 2100 (3.10506 iter/s, 32.2055s/100 iters), loss = 87.3365
I0425 16:20:57.752233  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:20:57.752239  7883 sgd_solver.cpp:105] Iteration 2100, lr = 0.01
I0425 16:21:29.944003  7883 solver.cpp:219] Iteration 2200 (3.10634 iter/s, 32.1922s/100 iters), loss = 87.3365
I0425 16:21:29.944193  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:21:29.944201  7883 sgd_solver.cpp:105] Iteration 2200, lr = 0.01
I0425 16:22:02.147385  7883 solver.cpp:219] Iteration 2300 (3.10524 iter/s, 32.2036s/100 iters), loss = 87.3365
I0425 16:22:02.147608  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:22:02.147617  7883 sgd_solver.cpp:105] Iteration 2300, lr = 0.01
I0425 16:22:14.715191  7891 data_layer.cpp:73] Restarting data prefetching from start.
I0425 16:22:34.349902  7883 solver.cpp:219] Iteration 2400 (3.10533 iter/s, 32.2027s/100 iters), loss = 87.3365
I0425 16:22:34.350039  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:22:34.350045  7883 sgd_solver.cpp:105] Iteration 2400, lr = 0.01
I0425 16:23:06.558763  7883 solver.cpp:219] Iteration 2500 (3.10471 iter/s, 32.2092s/100 iters), loss = 87.3365
I0425 16:23:06.558908  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:23:06.558919  7883 sgd_solver.cpp:105] Iteration 2500, lr = 0.01
I0425 16:23:39.649236  7883 solver.cpp:219] Iteration 2600 (3.02199 iter/s, 33.0908s/100 iters), loss = 87.3365
I0425 16:23:39.649431  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:23:39.649441  7883 sgd_solver.cpp:105] Iteration 2600, lr = 0.01
I0425 16:24:13.362948  7883 solver.cpp:219] Iteration 2700 (2.96613 iter/s, 33.714s/100 iters), loss = 87.3365
I0425 16:24:13.363121  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:24:13.363131  7883 sgd_solver.cpp:105] Iteration 2700, lr = 0.01
I0425 16:24:45.625840  7883 solver.cpp:219] Iteration 2800 (3.09951 iter/s, 32.2631s/100 iters), loss = 87.3365
I0425 16:24:45.626042  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:24:45.626054  7883 sgd_solver.cpp:105] Iteration 2800, lr = 0.01
I0425 16:25:18.303109  7883 solver.cpp:219] Iteration 2900 (3.09278 iter/s, 32.3333s/100 iters), loss = 87.3365
I0425 16:25:18.303306  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:25:18.303315  7883 sgd_solver.cpp:105] Iteration 2900, lr = 0.01
I0425 16:25:50.100092  7883 solver.cpp:331] Iteration 3000, Testing net (#0)
I0425 16:25:52.444377  7883 solver.cpp:398]     Test net output #0: accuracy = 0.112
I0425 16:25:52.444406  7883 solver.cpp:398]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:25:52.775231  7883 solver.cpp:219] Iteration 3000 (2.90912 iter/s, 34.3747s/100 iters), loss = 87.3365
I0425 16:25:52.775269  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:25:52.775277  7883 sgd_solver.cpp:105] Iteration 3000, lr = 0.01
I0425 16:26:24.984279  7883 solver.cpp:219] Iteration 3100 (3.10646 iter/s, 32.191s/100 iters), loss = 87.3365
I0425 16:26:24.984482  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:26:24.984490  7883 sgd_solver.cpp:105] Iteration 3100, lr = 0.01
I0425 16:26:31.437695  7891 data_layer.cpp:73] Restarting data prefetching from start.
I0425 16:26:57.203387  7883 solver.cpp:219] Iteration 3200 (3.10528 iter/s, 32.2032s/100 iters), loss = 87.3365
I0425 16:26:57.203605  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:26:57.203621  7883 sgd_solver.cpp:105] Iteration 3200, lr = 0.01
I0425 16:27:29.277966  7883 solver.cpp:219] Iteration 3300 (3.10566 iter/s, 32.1993s/100 iters), loss = 87.3365
I0425 16:27:29.278074  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:27:29.278081  7883 sgd_solver.cpp:105] Iteration 3300, lr = 0.01
I0425 16:28:06.083081  7883 solver.cpp:219] Iteration 3400 (2.71529 iter/s, 36.8285s/100 iters), loss = 87.3365
I0425 16:28:06.083230  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:28:06.083248  7883 sgd_solver.cpp:105] Iteration 3400, lr = 0.01
I0425 16:28:38.342684  7883 solver.cpp:219] Iteration 3500 (3.10937 iter/s, 32.1609s/100 iters), loss = 87.3365
I0425 16:28:38.342744  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:28:38.342751  7883 sgd_solver.cpp:105] Iteration 3500, lr = 0.01
I0425 16:29:10.517235  7883 solver.cpp:219] Iteration 3600 (3.10783 iter/s, 32.1768s/100 iters), loss = 87.3365
I0425 16:29:10.517434  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:29:10.517442  7883 sgd_solver.cpp:105] Iteration 3600, lr = 0.01
I0425 16:29:42.757810  7883 solver.cpp:219] Iteration 3700 (3.10259 iter/s, 32.2312s/100 iters), loss = 87.3365
I0425 16:29:42.758000  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:29:42.758008  7883 sgd_solver.cpp:105] Iteration 3700, lr = 0.01
I0425 16:30:14.944623  7883 solver.cpp:219] Iteration 3800 (3.10807 iter/s, 32.1744s/100 iters), loss = 87.3365
I0425 16:30:14.944768  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:30:14.944774  7883 sgd_solver.cpp:105] Iteration 3800, lr = 0.01
I0425 16:30:47.127094  7883 solver.cpp:219] Iteration 3900 (3.10845 iter/s, 32.1704s/100 iters), loss = 87.3365
I0425 16:30:47.127274  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:30:47.127282  7883 sgd_solver.cpp:105] Iteration 3900, lr = 0.01
I0425 16:30:47.783897  7891 data_layer.cpp:73] Restarting data prefetching from start.
I0425 16:31:18.821203  7883 solver.cpp:331] Iteration 4000, Testing net (#0)
I0425 16:31:21.161501  7883 solver.cpp:398]     Test net output #0: accuracy = 0.098
I0425 16:31:21.161528  7883 solver.cpp:398]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:31:21.488399  7883 solver.cpp:219] Iteration 4000 (2.91132 iter/s, 34.3486s/100 iters), loss = 87.3365
I0425 16:31:21.488425  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:31:21.488430  7883 sgd_solver.cpp:105] Iteration 4000, lr = 0.01
I0425 16:31:53.684342  7883 solver.cpp:219] Iteration 4100 (3.10711 iter/s, 32.1843s/100 iters), loss = 87.3365
I0425 16:31:53.684433  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:31:53.684439  7883 sgd_solver.cpp:105] Iteration 4100, lr = 0.01
I0425 16:32:25.795754  7883 solver.cpp:219] Iteration 4200 (3.109 iter/s, 32.1647s/100 iters), loss = 87.3365
I0425 16:32:25.795886  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:32:25.795892  7883 sgd_solver.cpp:105] Iteration 4200, lr = 0.01
I0425 16:32:57.938199  7883 solver.cpp:219] Iteration 4300 (3.10932 iter/s, 32.1613s/100 iters), loss = 87.3365
I0425 16:32:57.938297  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:32:57.938304  7883 sgd_solver.cpp:105] Iteration 4300, lr = 0.01
I0425 16:33:30.104074  7883 solver.cpp:219] Iteration 4400 (3.10829 iter/s, 32.1721s/100 iters), loss = 87.3365
I0425 16:33:30.104248  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:33:30.104255  7883 sgd_solver.cpp:105] Iteration 4400, lr = 0.01
I0425 16:34:02.280570  7883 solver.cpp:219] Iteration 4500 (3.10771 iter/s, 32.178s/100 iters), loss = 87.3365
I0425 16:34:02.280680  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:34:02.280689  7883 sgd_solver.cpp:105] Iteration 4500, lr = 0.01
I0425 16:34:34.471972  7883 solver.cpp:219] Iteration 4600 (3.10642 iter/s, 32.1913s/100 iters), loss = 87.3365
I0425 16:34:34.472159  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:34:34.472167  7883 sgd_solver.cpp:105] Iteration 4600, lr = 0.01
I0425 16:35:01.185341  7891 data_layer.cpp:73] Restarting data prefetching from start.
I0425 16:35:06.651700  7883 solver.cpp:219] Iteration 4700 (3.10762 iter/s, 32.179s/100 iters), loss = 87.3365
I0425 16:35:06.651821  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:35:06.651829  7883 sgd_solver.cpp:105] Iteration 4700, lr = 0.01
I0425 16:35:38.806324  7883 solver.cpp:219] Iteration 4800 (3.11006 iter/s, 32.1537s/100 iters), loss = 87.3365
I0425 16:35:38.806486  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:35:38.806494  7883 sgd_solver.cpp:105] Iteration 4800, lr = 0.01
I0425 16:36:10.976407  7883 solver.cpp:219] Iteration 4900 (3.10908 iter/s, 32.1638s/100 iters), loss = 87.3365
I0425 16:36:10.976558  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:36:10.976567  7883 sgd_solver.cpp:105] Iteration 4900, lr = 0.01
I0425 16:36:42.637882  7883 solver.cpp:331] Iteration 5000, Testing net (#0)
I0425 16:36:44.975272  7883 solver.cpp:398]     Test net output #0: accuracy = 0.092
I0425 16:36:44.975330  7883 solver.cpp:398]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:36:45.301972  7883 solver.cpp:219] Iteration 5000 (2.91353 iter/s, 34.3227s/100 iters), loss = 87.3365
I0425 16:36:45.302000  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:36:45.302006  7883 sgd_solver.cpp:105] Iteration 5000, lr = 0.01
I0425 16:37:17.460675  7883 solver.cpp:219] Iteration 5100 (3.10979 iter/s, 32.1566s/100 iters), loss = 87.3365
I0425 16:37:17.460757  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:37:17.460764  7883 sgd_solver.cpp:105] Iteration 5100, lr = 0.01
I0425 16:37:49.609156  7883 solver.cpp:219] Iteration 5200 (3.11001 iter/s, 32.1543s/100 iters), loss = 87.3365
I0425 16:37:49.609223  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:37:49.609230  7883 sgd_solver.cpp:105] Iteration 5200, lr = 0.01
I0425 16:38:21.782971  7883 solver.cpp:219] Iteration 5300 (3.10807 iter/s, 32.1743s/100 iters), loss = 87.3365
I0425 16:38:21.783057  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:38:21.783078  7883 sgd_solver.cpp:105] Iteration 5300, lr = 0.01
I0425 16:38:53.973633  7883 solver.cpp:219] Iteration 5400 (3.10777 iter/s, 32.1775s/100 iters), loss = 87.3365
I0425 16:38:53.973723  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:38:53.973728  7883 sgd_solver.cpp:105] Iteration 5400, lr = 0.01
I0425 16:39:14.587935  7891 data_layer.cpp:73] Restarting data prefetching from start.
I0425 16:39:26.174342  7883 solver.cpp:219] Iteration 5500 (3.10614 iter/s, 32.1943s/100 iters), loss = 87.3365
I0425 16:39:26.174553  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:39:26.174561  7883 sgd_solver.cpp:105] Iteration 5500, lr = 0.01
I0425 16:39:58.360321  7883 solver.cpp:219] Iteration 5600 (3.10733 iter/s, 32.182s/100 iters), loss = 87.3365
I0425 16:39:58.360520  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:39:58.360527  7883 sgd_solver.cpp:105] Iteration 5600, lr = 0.01
I0425 16:40:30.548101  7883 solver.cpp:219] Iteration 5700 (3.10707 iter/s, 32.1847s/100 iters), loss = 87.3365
I0425 16:40:30.548182  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:40:30.548187  7883 sgd_solver.cpp:105] Iteration 5700, lr = 0.01
I0425 16:41:02.745973  7883 solver.cpp:219] Iteration 5800 (3.10605 iter/s, 32.1952s/100 iters), loss = 87.3365
I0425 16:41:02.746176  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:41:02.746181  7883 sgd_solver.cpp:105] Iteration 5800, lr = 0.01
I0425 16:41:34.917647  7883 solver.cpp:219] Iteration 5900 (3.10858 iter/s, 32.169s/100 iters), loss = 87.3365
I0425 16:41:34.917858  7883 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:41:34.917881  7883 sgd_solver.cpp:105] Iteration 5900, lr = 0.01
I0425 16:42:06.531620  7883 solver.cpp:448] Snapshotting to binary proto file examples/cifar10_full_sigmoid_iter_6000.caffemodel
I0425 16:42:06.751063  7883 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10_full_sigmoid_iter_6000.solverstate
I0425 16:42:06.911434  7883 solver.cpp:311] Iteration 6000, loss = 87.3365
I0425 16:42:06.911455  7883 solver.cpp:331] Iteration 6000, Testing net (#0)
I0425 16:42:09.066094  7883 solver.cpp:398]     Test net output #0: accuracy = 0.088
I0425 16:42:09.066135  7883 solver.cpp:398]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 16:42:09.066139  7883 solver.cpp:316] Optimization Done.
I0425 16:42:09.066141  7883 caffe.cpp:259] Optimization Done.
