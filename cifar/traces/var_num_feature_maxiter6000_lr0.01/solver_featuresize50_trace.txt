I0425 15:52:23.361248  7720 caffe.cpp:218] Using GPUs 0
I0425 15:52:23.375497  7720 caffe.cpp:223] GPU 0: GeForce GTX 950M
I0425 15:52:23.559821  7720 solver.cpp:44] Initializing solver from parameters: 
test_iter: 10
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 6000
lr_policy: "step"
gamma: 1
momentum: 0.9
stepsize: 5000
snapshot: 10000
snapshot_prefix: "examples/cifar10_full_sigmoid"
solver_mode: GPU
device_id: 0
net: "ECE595/cifar/train_test/var_num_feature_maxiter6000/cifarlenet_train_test_featuresize50.prototxt"
train_state {
  level: 0
  stage: ""
}
I0425 15:52:23.560047  7720 solver.cpp:87] Creating training net from net file: ECE595/cifar/train_test/var_num_feature_maxiter6000/cifarlenet_train_test_featuresize50.prototxt
I0425 15:52:23.560272  7720 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0425 15:52:23.560281  7720 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0425 15:52:23.560410  7720 net.cpp:51] Initializing net from parameters: 
name: "CIFARLeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0425 15:52:23.560472  7720 layer_factory.hpp:77] Creating layer cifar
I0425 15:52:23.560600  7720 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_train_lmdb
I0425 15:52:23.560655  7720 net.cpp:84] Creating Layer cifar
I0425 15:52:23.560678  7720 net.cpp:380] cifar -> data
I0425 15:52:23.560726  7720 net.cpp:380] cifar -> label
I0425 15:52:23.560752  7720 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0425 15:52:23.561780  7720 data_layer.cpp:45] output data size: 64,3,32,32
I0425 15:52:23.565280  7720 net.cpp:122] Setting up cifar
I0425 15:52:23.565297  7720 net.cpp:129] Top shape: 64 3 32 32 (196608)
I0425 15:52:23.565301  7720 net.cpp:129] Top shape: 64 (64)
I0425 15:52:23.565302  7720 net.cpp:137] Memory required for data: 786688
I0425 15:52:23.565327  7720 layer_factory.hpp:77] Creating layer conv0
I0425 15:52:23.565358  7720 net.cpp:84] Creating Layer conv0
I0425 15:52:23.565382  7720 net.cpp:406] conv0 <- data
I0425 15:52:23.565393  7720 net.cpp:380] conv0 -> conv0
I0425 15:52:23.566115  7720 net.cpp:122] Setting up conv0
I0425 15:52:23.566138  7720 net.cpp:129] Top shape: 64 50 28 28 (2508800)
I0425 15:52:23.566139  7720 net.cpp:137] Memory required for data: 10821888
I0425 15:52:23.566172  7720 layer_factory.hpp:77] Creating layer pool0
I0425 15:52:23.566179  7720 net.cpp:84] Creating Layer pool0
I0425 15:52:23.566182  7720 net.cpp:406] pool0 <- conv0
I0425 15:52:23.566200  7720 net.cpp:380] pool0 -> pool0
I0425 15:52:23.566236  7720 net.cpp:122] Setting up pool0
I0425 15:52:23.566241  7720 net.cpp:129] Top shape: 64 50 14 14 (627200)
I0425 15:52:23.566243  7720 net.cpp:137] Memory required for data: 13330688
I0425 15:52:23.566246  7720 layer_factory.hpp:77] Creating layer conv1
I0425 15:52:23.566254  7720 net.cpp:84] Creating Layer conv1
I0425 15:52:23.566262  7720 net.cpp:406] conv1 <- pool0
I0425 15:52:23.566267  7720 net.cpp:380] conv1 -> conv1
I0425 15:52:23.567111  7720 net.cpp:122] Setting up conv1
I0425 15:52:23.567117  7720 net.cpp:129] Top shape: 64 50 10 10 (320000)
I0425 15:52:23.567138  7720 net.cpp:137] Memory required for data: 14610688
I0425 15:52:23.567144  7720 layer_factory.hpp:77] Creating layer pool1
I0425 15:52:23.567164  7720 net.cpp:84] Creating Layer pool1
I0425 15:52:23.567167  7720 net.cpp:406] pool1 <- conv1
I0425 15:52:23.567170  7720 net.cpp:380] pool1 -> pool1
I0425 15:52:23.567215  7720 net.cpp:122] Setting up pool1
I0425 15:52:23.567220  7720 net.cpp:129] Top shape: 64 50 5 5 (80000)
I0425 15:52:23.567222  7720 net.cpp:137] Memory required for data: 14930688
I0425 15:52:23.567225  7720 layer_factory.hpp:77] Creating layer ip1
I0425 15:52:23.567230  7720 net.cpp:84] Creating Layer ip1
I0425 15:52:23.567234  7720 net.cpp:406] ip1 <- pool1
I0425 15:52:23.567239  7720 net.cpp:380] ip1 -> ip1
I0425 15:52:23.570968  7720 net.cpp:122] Setting up ip1
I0425 15:52:23.570981  7720 net.cpp:129] Top shape: 64 500 (32000)
I0425 15:52:23.570981  7720 net.cpp:137] Memory required for data: 15058688
I0425 15:52:23.570991  7720 layer_factory.hpp:77] Creating layer relu1
I0425 15:52:23.571013  7720 net.cpp:84] Creating Layer relu1
I0425 15:52:23.571015  7720 net.cpp:406] relu1 <- ip1
I0425 15:52:23.571020  7720 net.cpp:367] relu1 -> ip1 (in-place)
I0425 15:52:23.571040  7720 net.cpp:122] Setting up relu1
I0425 15:52:23.571043  7720 net.cpp:129] Top shape: 64 500 (32000)
I0425 15:52:23.571058  7720 net.cpp:137] Memory required for data: 15186688
I0425 15:52:23.571060  7720 layer_factory.hpp:77] Creating layer ip2
I0425 15:52:23.571064  7720 net.cpp:84] Creating Layer ip2
I0425 15:52:23.571086  7720 net.cpp:406] ip2 <- ip1
I0425 15:52:23.571089  7720 net.cpp:380] ip2 -> ip2
I0425 15:52:23.571610  7720 net.cpp:122] Setting up ip2
I0425 15:52:23.571619  7720 net.cpp:129] Top shape: 64 10 (640)
I0425 15:52:23.571621  7720 net.cpp:137] Memory required for data: 15189248
I0425 15:52:23.571625  7720 layer_factory.hpp:77] Creating layer relu2
I0425 15:52:23.571645  7720 net.cpp:84] Creating Layer relu2
I0425 15:52:23.571646  7720 net.cpp:406] relu2 <- ip2
I0425 15:52:23.571650  7720 net.cpp:367] relu2 -> ip2 (in-place)
I0425 15:52:23.571671  7720 net.cpp:122] Setting up relu2
I0425 15:52:23.571674  7720 net.cpp:129] Top shape: 64 10 (640)
I0425 15:52:23.571676  7720 net.cpp:137] Memory required for data: 15191808
I0425 15:52:23.571678  7720 layer_factory.hpp:77] Creating layer loss
I0425 15:52:23.571681  7720 net.cpp:84] Creating Layer loss
I0425 15:52:23.571683  7720 net.cpp:406] loss <- ip2
I0425 15:52:23.571686  7720 net.cpp:406] loss <- label
I0425 15:52:23.571692  7720 net.cpp:380] loss -> loss
I0425 15:52:23.571702  7720 layer_factory.hpp:77] Creating layer loss
I0425 15:52:23.571799  7720 net.cpp:122] Setting up loss
I0425 15:52:23.571802  7720 net.cpp:129] Top shape: (1)
I0425 15:52:23.571805  7720 net.cpp:132]     with loss weight 1
I0425 15:52:23.571841  7720 net.cpp:137] Memory required for data: 15191812
I0425 15:52:23.571842  7720 net.cpp:198] loss needs backward computation.
I0425 15:52:23.571847  7720 net.cpp:198] relu2 needs backward computation.
I0425 15:52:23.571877  7720 net.cpp:198] ip2 needs backward computation.
I0425 15:52:23.571892  7720 net.cpp:198] relu1 needs backward computation.
I0425 15:52:23.571894  7720 net.cpp:198] ip1 needs backward computation.
I0425 15:52:23.571895  7720 net.cpp:198] pool1 needs backward computation.
I0425 15:52:23.571898  7720 net.cpp:198] conv1 needs backward computation.
I0425 15:52:23.571900  7720 net.cpp:198] pool0 needs backward computation.
I0425 15:52:23.571920  7720 net.cpp:198] conv0 needs backward computation.
I0425 15:52:23.571923  7720 net.cpp:200] cifar does not need backward computation.
I0425 15:52:23.571925  7720 net.cpp:242] This network produces output loss
I0425 15:52:23.571933  7720 net.cpp:255] Network initialization done.
I0425 15:52:23.572157  7720 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/cifar/train_test/var_num_feature_maxiter6000/cifarlenet_train_test_featuresize50.prototxt
I0425 15:52:23.572207  7720 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0425 15:52:23.572309  7720 net.cpp:51] Initializing net from parameters: 
name: "CIFARLeNet"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0425 15:52:23.572393  7720 layer_factory.hpp:77] Creating layer cifar
I0425 15:52:23.572435  7720 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0425 15:52:23.572446  7720 net.cpp:84] Creating Layer cifar
I0425 15:52:23.572468  7720 net.cpp:380] cifar -> data
I0425 15:52:23.572474  7720 net.cpp:380] cifar -> label
I0425 15:52:23.572479  7720 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0425 15:52:23.572676  7720 data_layer.cpp:45] output data size: 100,3,32,32
I0425 15:52:23.576318  7720 net.cpp:122] Setting up cifar
I0425 15:52:23.576335  7720 net.cpp:129] Top shape: 100 3 32 32 (307200)
I0425 15:52:23.576352  7720 net.cpp:129] Top shape: 100 (100)
I0425 15:52:23.576354  7720 net.cpp:137] Memory required for data: 1229200
I0425 15:52:23.576360  7720 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0425 15:52:23.576400  7720 net.cpp:84] Creating Layer label_cifar_1_split
I0425 15:52:23.576403  7720 net.cpp:406] label_cifar_1_split <- label
I0425 15:52:23.576408  7720 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I0425 15:52:23.576417  7720 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I0425 15:52:23.576503  7720 net.cpp:122] Setting up label_cifar_1_split
I0425 15:52:23.576506  7720 net.cpp:129] Top shape: 100 (100)
I0425 15:52:23.576509  7720 net.cpp:129] Top shape: 100 (100)
I0425 15:52:23.576511  7720 net.cpp:137] Memory required for data: 1230000
I0425 15:52:23.576514  7720 layer_factory.hpp:77] Creating layer conv0
I0425 15:52:23.576522  7720 net.cpp:84] Creating Layer conv0
I0425 15:52:23.576525  7720 net.cpp:406] conv0 <- data
I0425 15:52:23.576531  7720 net.cpp:380] conv0 -> conv0
I0425 15:52:23.576764  7720 net.cpp:122] Setting up conv0
I0425 15:52:23.576769  7720 net.cpp:129] Top shape: 100 50 28 28 (3920000)
I0425 15:52:23.576771  7720 net.cpp:137] Memory required for data: 16910000
I0425 15:52:23.576779  7720 layer_factory.hpp:77] Creating layer pool0
I0425 15:52:23.576798  7720 net.cpp:84] Creating Layer pool0
I0425 15:52:23.576802  7720 net.cpp:406] pool0 <- conv0
I0425 15:52:23.576818  7720 net.cpp:380] pool0 -> pool0
I0425 15:52:23.576846  7720 net.cpp:122] Setting up pool0
I0425 15:52:23.576850  7720 net.cpp:129] Top shape: 100 50 14 14 (980000)
I0425 15:52:23.576853  7720 net.cpp:137] Memory required for data: 20830000
I0425 15:52:23.576854  7720 layer_factory.hpp:77] Creating layer conv1
I0425 15:52:23.576874  7720 net.cpp:84] Creating Layer conv1
I0425 15:52:23.576877  7720 net.cpp:406] conv1 <- pool0
I0425 15:52:23.576882  7720 net.cpp:380] conv1 -> conv1
I0425 15:52:23.577469  7720 net.cpp:122] Setting up conv1
I0425 15:52:23.577476  7720 net.cpp:129] Top shape: 100 50 10 10 (500000)
I0425 15:52:23.577478  7720 net.cpp:137] Memory required for data: 22830000
I0425 15:52:23.577504  7720 layer_factory.hpp:77] Creating layer pool1
I0425 15:52:23.577513  7720 net.cpp:84] Creating Layer pool1
I0425 15:52:23.577517  7720 net.cpp:406] pool1 <- conv1
I0425 15:52:23.577523  7720 net.cpp:380] pool1 -> pool1
I0425 15:52:23.577549  7720 net.cpp:122] Setting up pool1
I0425 15:52:23.577553  7720 net.cpp:129] Top shape: 100 50 5 5 (125000)
I0425 15:52:23.577556  7720 net.cpp:137] Memory required for data: 23330000
I0425 15:52:23.577558  7720 layer_factory.hpp:77] Creating layer ip1
I0425 15:52:23.577564  7720 net.cpp:84] Creating Layer ip1
I0425 15:52:23.577567  7720 net.cpp:406] ip1 <- pool1
I0425 15:52:23.577589  7720 net.cpp:380] ip1 -> ip1
I0425 15:52:23.581423  7720 net.cpp:122] Setting up ip1
I0425 15:52:23.581437  7720 net.cpp:129] Top shape: 100 500 (50000)
I0425 15:52:23.581440  7720 net.cpp:137] Memory required for data: 23530000
I0425 15:52:23.581468  7720 layer_factory.hpp:77] Creating layer relu1
I0425 15:52:23.581476  7720 net.cpp:84] Creating Layer relu1
I0425 15:52:23.581480  7720 net.cpp:406] relu1 <- ip1
I0425 15:52:23.581483  7720 net.cpp:367] relu1 -> ip1 (in-place)
I0425 15:52:23.581490  7720 net.cpp:122] Setting up relu1
I0425 15:52:23.581493  7720 net.cpp:129] Top shape: 100 500 (50000)
I0425 15:52:23.581496  7720 net.cpp:137] Memory required for data: 23730000
I0425 15:52:23.581497  7720 layer_factory.hpp:77] Creating layer ip2
I0425 15:52:23.581503  7720 net.cpp:84] Creating Layer ip2
I0425 15:52:23.581507  7720 net.cpp:406] ip2 <- ip1
I0425 15:52:23.581512  7720 net.cpp:380] ip2 -> ip2
I0425 15:52:23.581617  7720 net.cpp:122] Setting up ip2
I0425 15:52:23.581622  7720 net.cpp:129] Top shape: 100 10 (1000)
I0425 15:52:23.581624  7720 net.cpp:137] Memory required for data: 23734000
I0425 15:52:23.581629  7720 layer_factory.hpp:77] Creating layer relu2
I0425 15:52:23.581631  7720 net.cpp:84] Creating Layer relu2
I0425 15:52:23.581634  7720 net.cpp:406] relu2 <- ip2
I0425 15:52:23.581639  7720 net.cpp:367] relu2 -> ip2 (in-place)
I0425 15:52:23.581642  7720 net.cpp:122] Setting up relu2
I0425 15:52:23.581655  7720 net.cpp:129] Top shape: 100 10 (1000)
I0425 15:52:23.581658  7720 net.cpp:137] Memory required for data: 23738000
I0425 15:52:23.581660  7720 layer_factory.hpp:77] Creating layer ip2_relu2_0_split
I0425 15:52:23.581665  7720 net.cpp:84] Creating Layer ip2_relu2_0_split
I0425 15:52:23.581667  7720 net.cpp:406] ip2_relu2_0_split <- ip2
I0425 15:52:23.581671  7720 net.cpp:380] ip2_relu2_0_split -> ip2_relu2_0_split_0
I0425 15:52:23.581676  7720 net.cpp:380] ip2_relu2_0_split -> ip2_relu2_0_split_1
I0425 15:52:23.581701  7720 net.cpp:122] Setting up ip2_relu2_0_split
I0425 15:52:23.581707  7720 net.cpp:129] Top shape: 100 10 (1000)
I0425 15:52:23.581709  7720 net.cpp:129] Top shape: 100 10 (1000)
I0425 15:52:23.581710  7720 net.cpp:137] Memory required for data: 23746000
I0425 15:52:23.581713  7720 layer_factory.hpp:77] Creating layer accuracy
I0425 15:52:23.581718  7720 net.cpp:84] Creating Layer accuracy
I0425 15:52:23.581722  7720 net.cpp:406] accuracy <- ip2_relu2_0_split_0
I0425 15:52:23.581725  7720 net.cpp:406] accuracy <- label_cifar_1_split_0
I0425 15:52:23.581729  7720 net.cpp:380] accuracy -> accuracy
I0425 15:52:23.581734  7720 net.cpp:122] Setting up accuracy
I0425 15:52:23.581738  7720 net.cpp:129] Top shape: (1)
I0425 15:52:23.581740  7720 net.cpp:137] Memory required for data: 23746004
I0425 15:52:23.581743  7720 layer_factory.hpp:77] Creating layer loss
I0425 15:52:23.581745  7720 net.cpp:84] Creating Layer loss
I0425 15:52:23.581749  7720 net.cpp:406] loss <- ip2_relu2_0_split_1
I0425 15:52:23.581753  7720 net.cpp:406] loss <- label_cifar_1_split_1
I0425 15:52:23.581755  7720 net.cpp:380] loss -> loss
I0425 15:52:23.581760  7720 layer_factory.hpp:77] Creating layer loss
I0425 15:52:23.581856  7720 net.cpp:122] Setting up loss
I0425 15:52:23.581859  7720 net.cpp:129] Top shape: (1)
I0425 15:52:23.581861  7720 net.cpp:132]     with loss weight 1
I0425 15:52:23.581869  7720 net.cpp:137] Memory required for data: 23746008
I0425 15:52:23.581872  7720 net.cpp:198] loss needs backward computation.
I0425 15:52:23.581876  7720 net.cpp:200] accuracy does not need backward computation.
I0425 15:52:23.581877  7720 net.cpp:198] ip2_relu2_0_split needs backward computation.
I0425 15:52:23.581879  7720 net.cpp:198] relu2 needs backward computation.
I0425 15:52:23.581882  7720 net.cpp:198] ip2 needs backward computation.
I0425 15:52:23.581884  7720 net.cpp:198] relu1 needs backward computation.
I0425 15:52:23.581887  7720 net.cpp:198] ip1 needs backward computation.
I0425 15:52:23.581888  7720 net.cpp:198] pool1 needs backward computation.
I0425 15:52:23.581892  7720 net.cpp:198] conv1 needs backward computation.
I0425 15:52:23.581893  7720 net.cpp:198] pool0 needs backward computation.
I0425 15:52:23.581895  7720 net.cpp:198] conv0 needs backward computation.
I0425 15:52:23.581897  7720 net.cpp:200] label_cifar_1_split does not need backward computation.
I0425 15:52:23.581902  7720 net.cpp:200] cifar does not need backward computation.
I0425 15:52:23.581902  7720 net.cpp:242] This network produces output accuracy
I0425 15:52:23.581905  7720 net.cpp:242] This network produces output loss
I0425 15:52:23.581914  7720 net.cpp:255] Network initialization done.
I0425 15:52:23.581979  7720 solver.cpp:56] Solver scaffolding done.
I0425 15:52:23.582239  7720 caffe.cpp:248] Starting Optimization
I0425 15:52:23.582243  7720 solver.cpp:273] Solving CIFARLeNet
I0425 15:52:23.582245  7720 solver.cpp:274] Learning Rate Policy: step
I0425 15:52:23.583184  7720 solver.cpp:331] Iteration 0, Testing net (#0)
I0425 15:52:23.802763  7720 solver.cpp:398]     Test net output #0: accuracy = 0.093
I0425 15:52:23.802788  7720 solver.cpp:398]     Test net output #1: loss = 55.2703 (* 1 = 55.2703 loss)
I0425 15:52:23.839897  7720 solver.cpp:219] Iteration 0 (0 iter/s, 0.257415s/100 iters), loss = 62.3526
I0425 15:52:23.839939  7720 solver.cpp:238]     Train net output #0: loss = 62.3526 (* 1 = 62.3526 loss)
I0425 15:52:23.839970  7720 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0425 15:52:27.093822  7720 solver.cpp:219] Iteration 100 (30.7323 iter/s, 3.2539s/100 iters), loss = 87.3365
I0425 15:52:27.093866  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:52:27.093871  7720 sgd_solver.cpp:105] Iteration 100, lr = 0.01
I0425 15:52:30.341850  7720 solver.cpp:219] Iteration 200 (30.7881 iter/s, 3.248s/100 iters), loss = 87.3365
I0425 15:52:30.341897  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:52:30.341900  7720 sgd_solver.cpp:105] Iteration 200, lr = 0.01
I0425 15:52:33.588219  7720 solver.cpp:219] Iteration 300 (30.8039 iter/s, 3.24634s/100 iters), loss = 87.3365
I0425 15:52:33.588248  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:52:33.588253  7720 sgd_solver.cpp:105] Iteration 300, lr = 0.01
I0425 15:52:36.826927  7720 solver.cpp:219] Iteration 400 (30.8766 iter/s, 3.2387s/100 iters), loss = 87.3365
I0425 15:52:36.826956  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:52:36.826961  7720 sgd_solver.cpp:105] Iteration 400, lr = 0.01
I0425 15:52:40.065803  7720 solver.cpp:219] Iteration 500 (30.8751 iter/s, 3.23886s/100 iters), loss = 87.3365
I0425 15:52:40.065843  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:52:40.065848  7720 sgd_solver.cpp:105] Iteration 500, lr = 0.01
I0425 15:52:43.299963  7720 solver.cpp:219] Iteration 600 (30.9201 iter/s, 3.23414s/100 iters), loss = 87.3365
I0425 15:52:43.300007  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:52:43.300012  7720 sgd_solver.cpp:105] Iteration 600, lr = 0.01
I0425 15:52:46.536787  7720 solver.cpp:219] Iteration 700 (30.8947 iter/s, 3.2368s/100 iters), loss = 87.3365
I0425 15:52:46.536852  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:52:46.536870  7720 sgd_solver.cpp:105] Iteration 700, lr = 0.01
I0425 15:52:49.035092  7727 data_layer.cpp:73] Restarting data prefetching from start.
I0425 15:52:49.784915  7720 solver.cpp:219] Iteration 800 (30.7872 iter/s, 3.2481s/100 iters), loss = 87.3365
I0425 15:52:49.784942  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:52:49.784965  7720 sgd_solver.cpp:105] Iteration 800, lr = 0.01
I0425 15:52:53.020370  7720 solver.cpp:219] Iteration 900 (30.9076 iter/s, 3.23545s/100 iters), loss = 87.3365
I0425 15:52:53.020398  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:52:53.020403  7720 sgd_solver.cpp:105] Iteration 900, lr = 0.01
I0425 15:52:56.206591  7720 solver.cpp:331] Iteration 1000, Testing net (#0)
I0425 15:52:56.431803  7720 solver.cpp:398]     Test net output #0: accuracy = 0.094
I0425 15:52:56.431844  7720 solver.cpp:398]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:52:56.466840  7720 solver.cpp:219] Iteration 1000 (29.0152 iter/s, 3.44647s/100 iters), loss = 87.3365
I0425 15:52:56.466878  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:52:56.466886  7720 sgd_solver.cpp:105] Iteration 1000, lr = 0.01
I0425 15:52:59.716724  7720 solver.cpp:219] Iteration 1100 (30.7704 iter/s, 3.24987s/100 iters), loss = 87.3365
I0425 15:52:59.716753  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:52:59.716758  7720 sgd_solver.cpp:105] Iteration 1100, lr = 0.01
I0425 15:53:02.955013  7720 solver.cpp:219] Iteration 1200 (30.8805 iter/s, 3.23829s/100 iters), loss = 87.3365
I0425 15:53:02.955040  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:53:02.955062  7720 sgd_solver.cpp:105] Iteration 1200, lr = 0.01
I0425 15:53:06.197443  7720 solver.cpp:219] Iteration 1300 (30.8411 iter/s, 3.24243s/100 iters), loss = 87.3365
I0425 15:53:06.197489  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:53:06.197494  7720 sgd_solver.cpp:105] Iteration 1300, lr = 0.01
I0425 15:53:09.439074  7720 solver.cpp:219] Iteration 1400 (30.8489 iter/s, 3.24161s/100 iters), loss = 87.3365
I0425 15:53:09.439119  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:53:09.439137  7720 sgd_solver.cpp:105] Iteration 1400, lr = 0.01
I0425 15:53:12.692270  7720 solver.cpp:219] Iteration 1500 (30.7392 iter/s, 3.25317s/100 iters), loss = 87.3365
I0425 15:53:12.692298  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:53:12.692303  7720 sgd_solver.cpp:105] Iteration 1500, lr = 0.01
I0425 15:53:14.571508  7727 data_layer.cpp:73] Restarting data prefetching from start.
I0425 15:53:15.936585  7720 solver.cpp:219] Iteration 1600 (30.8232 iter/s, 3.24431s/100 iters), loss = 87.3365
I0425 15:53:15.936630  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:53:15.936635  7720 sgd_solver.cpp:105] Iteration 1600, lr = 0.01
I0425 15:53:19.177570  7720 solver.cpp:219] Iteration 1700 (30.855 iter/s, 3.24096s/100 iters), loss = 87.3365
I0425 15:53:19.177597  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:53:19.177620  7720 sgd_solver.cpp:105] Iteration 1700, lr = 0.01
I0425 15:53:22.422940  7720 solver.cpp:219] Iteration 1800 (30.8132 iter/s, 3.24536s/100 iters), loss = 87.3365
I0425 15:53:22.422972  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:53:22.422977  7720 sgd_solver.cpp:105] Iteration 1800, lr = 0.01
I0425 15:53:25.675300  7720 solver.cpp:219] Iteration 1900 (30.747 iter/s, 3.25235s/100 iters), loss = 87.3365
I0425 15:53:25.675328  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:53:25.675333  7720 sgd_solver.cpp:105] Iteration 1900, lr = 0.01
I0425 15:53:28.861340  7720 solver.cpp:331] Iteration 2000, Testing net (#0)
I0425 15:53:29.086068  7720 solver.cpp:398]     Test net output #0: accuracy = 0.1
I0425 15:53:29.086093  7720 solver.cpp:398]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:53:29.120663  7720 solver.cpp:219] Iteration 2000 (29.0245 iter/s, 3.44536s/100 iters), loss = 87.3365
I0425 15:53:29.120685  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:53:29.120692  7720 sgd_solver.cpp:105] Iteration 2000, lr = 0.01
I0425 15:53:32.356458  7720 solver.cpp:219] Iteration 2100 (30.9043 iter/s, 3.2358s/100 iters), loss = 87.3365
I0425 15:53:32.356487  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:53:32.356509  7720 sgd_solver.cpp:105] Iteration 2100, lr = 0.01
I0425 15:53:35.592933  7720 solver.cpp:219] Iteration 2200 (30.8978 iter/s, 3.23647s/100 iters), loss = 87.3365
I0425 15:53:35.592962  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:53:35.592967  7720 sgd_solver.cpp:105] Iteration 2200, lr = 0.01
I0425 15:53:38.835633  7720 solver.cpp:219] Iteration 2300 (30.8386 iter/s, 3.24269s/100 iters), loss = 87.3365
I0425 15:53:38.835659  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:53:38.835664  7720 sgd_solver.cpp:105] Iteration 2300, lr = 0.01
I0425 15:53:40.099519  7727 data_layer.cpp:73] Restarting data prefetching from start.
I0425 15:53:42.069022  7720 solver.cpp:219] Iteration 2400 (30.9273 iter/s, 3.23339s/100 iters), loss = 87.3365
I0425 15:53:42.069051  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:53:42.069056  7720 sgd_solver.cpp:105] Iteration 2400, lr = 0.01
I0425 15:53:45.303012  7720 solver.cpp:219] Iteration 2500 (30.9216 iter/s, 3.23398s/100 iters), loss = 87.3365
I0425 15:53:45.303040  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:53:45.303045  7720 sgd_solver.cpp:105] Iteration 2500, lr = 0.01
I0425 15:53:48.539132  7720 solver.cpp:219] Iteration 2600 (30.9013 iter/s, 3.23611s/100 iters), loss = 87.3365
I0425 15:53:48.539160  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:53:48.539183  7720 sgd_solver.cpp:105] Iteration 2600, lr = 0.01
I0425 15:53:51.781410  7720 solver.cpp:219] Iteration 2700 (30.8425 iter/s, 3.24228s/100 iters), loss = 87.3365
I0425 15:53:51.781437  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:53:51.781441  7720 sgd_solver.cpp:105] Iteration 2700, lr = 0.01
I0425 15:53:55.023624  7720 solver.cpp:219] Iteration 2800 (30.8431 iter/s, 3.24221s/100 iters), loss = 87.3365
I0425 15:53:55.023651  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:53:55.023656  7720 sgd_solver.cpp:105] Iteration 2800, lr = 0.01
I0425 15:53:58.250794  7720 solver.cpp:219] Iteration 2900 (30.9869 iter/s, 3.22717s/100 iters), loss = 87.3365
I0425 15:53:58.250823  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:53:58.250828  7720 sgd_solver.cpp:105] Iteration 2900, lr = 0.01
I0425 15:54:01.452880  7720 solver.cpp:331] Iteration 3000, Testing net (#0)
I0425 15:54:01.678984  7720 solver.cpp:398]     Test net output #0: accuracy = 0.112
I0425 15:54:01.679009  7720 solver.cpp:398]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:54:01.714114  7720 solver.cpp:219] Iteration 3000 (28.8741 iter/s, 3.46332s/100 iters), loss = 87.3365
I0425 15:54:01.714195  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:54:01.714202  7720 sgd_solver.cpp:105] Iteration 3000, lr = 0.01
I0425 15:54:04.952334  7720 solver.cpp:219] Iteration 3100 (30.8815 iter/s, 3.23818s/100 iters), loss = 87.3365
I0425 15:54:04.952363  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:54:04.952368  7720 sgd_solver.cpp:105] Iteration 3100, lr = 0.01
I0425 15:54:05.604693  7727 data_layer.cpp:73] Restarting data prefetching from start.
I0425 15:54:08.192765  7720 solver.cpp:219] Iteration 3200 (30.8601 iter/s, 3.24043s/100 iters), loss = 87.3365
I0425 15:54:08.192791  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:54:08.192814  7720 sgd_solver.cpp:105] Iteration 3200, lr = 0.01
I0425 15:54:11.441422  7720 solver.cpp:219] Iteration 3300 (30.7819 iter/s, 3.24866s/100 iters), loss = 87.3365
I0425 15:54:11.441449  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:54:11.441471  7720 sgd_solver.cpp:105] Iteration 3300, lr = 0.01
I0425 15:54:14.685158  7720 solver.cpp:219] Iteration 3400 (30.8287 iter/s, 3.24374s/100 iters), loss = 87.3365
I0425 15:54:14.685185  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:54:14.685207  7720 sgd_solver.cpp:105] Iteration 3400, lr = 0.01
I0425 15:54:17.913728  7720 solver.cpp:219] Iteration 3500 (30.9735 iter/s, 3.22857s/100 iters), loss = 87.3365
I0425 15:54:17.913758  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:54:17.913763  7720 sgd_solver.cpp:105] Iteration 3500, lr = 0.01
I0425 15:54:21.154537  7720 solver.cpp:219] Iteration 3600 (30.8565 iter/s, 3.2408s/100 iters), loss = 87.3365
I0425 15:54:21.154564  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:54:21.154569  7720 sgd_solver.cpp:105] Iteration 3600, lr = 0.01
I0425 15:54:24.390427  7720 solver.cpp:219] Iteration 3700 (30.9034 iter/s, 3.23589s/100 iters), loss = 87.3365
I0425 15:54:24.390472  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:54:24.390477  7720 sgd_solver.cpp:105] Iteration 3700, lr = 0.01
I0425 15:54:27.620461  7720 solver.cpp:219] Iteration 3800 (30.9595 iter/s, 3.23002s/100 iters), loss = 87.3365
I0425 15:54:27.620491  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:54:27.620513  7720 sgd_solver.cpp:105] Iteration 3800, lr = 0.01
I0425 15:54:30.855489  7720 solver.cpp:219] Iteration 3900 (30.9117 iter/s, 3.23502s/100 iters), loss = 87.3365
I0425 15:54:30.855535  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:54:30.855540  7720 sgd_solver.cpp:105] Iteration 3900, lr = 0.01
I0425 15:54:30.928478  7727 data_layer.cpp:73] Restarting data prefetching from start.
I0425 15:54:34.050943  7720 solver.cpp:331] Iteration 4000, Testing net (#0)
I0425 15:54:34.276928  7720 solver.cpp:398]     Test net output #0: accuracy = 0.098
I0425 15:54:34.276952  7720 solver.cpp:398]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:54:34.311790  7720 solver.cpp:219] Iteration 4000 (28.9328 iter/s, 3.45629s/100 iters), loss = 87.3365
I0425 15:54:34.311815  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:54:34.311820  7720 sgd_solver.cpp:105] Iteration 4000, lr = 0.01
I0425 15:54:37.544373  7720 solver.cpp:219] Iteration 4100 (30.935 iter/s, 3.23259s/100 iters), loss = 87.3365
I0425 15:54:37.544404  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:54:37.544409  7720 sgd_solver.cpp:105] Iteration 4100, lr = 0.01
I0425 15:54:40.778465  7720 solver.cpp:219] Iteration 4200 (30.9206 iter/s, 3.23409s/100 iters), loss = 87.3365
I0425 15:54:40.778501  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:54:40.778506  7720 sgd_solver.cpp:105] Iteration 4200, lr = 0.01
I0425 15:54:44.007378  7720 solver.cpp:219] Iteration 4300 (30.9703 iter/s, 3.2289s/100 iters), loss = 87.3365
I0425 15:54:44.007413  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:54:44.007421  7720 sgd_solver.cpp:105] Iteration 4300, lr = 0.01
I0425 15:54:47.236251  7720 solver.cpp:219] Iteration 4400 (30.9707 iter/s, 3.22886s/100 iters), loss = 87.3365
I0425 15:54:47.236287  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:54:47.236294  7720 sgd_solver.cpp:105] Iteration 4400, lr = 0.01
I0425 15:54:50.466392  7720 solver.cpp:219] Iteration 4500 (30.9585 iter/s, 3.23013s/100 iters), loss = 87.3365
I0425 15:54:50.466426  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:54:50.466434  7720 sgd_solver.cpp:105] Iteration 4500, lr = 0.01
I0425 15:54:53.696331  7720 solver.cpp:219] Iteration 4600 (30.9604 iter/s, 3.22993s/100 iters), loss = 87.3365
I0425 15:54:53.696367  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:54:53.696372  7720 sgd_solver.cpp:105] Iteration 4600, lr = 0.01
I0425 15:54:56.378502  7727 data_layer.cpp:73] Restarting data prefetching from start.
I0425 15:54:56.931696  7720 solver.cpp:219] Iteration 4700 (30.9085 iter/s, 3.23535s/100 iters), loss = 87.3365
I0425 15:54:56.931732  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:54:56.931738  7720 sgd_solver.cpp:105] Iteration 4700, lr = 0.01
I0425 15:55:00.167137  7720 solver.cpp:219] Iteration 4800 (30.9078 iter/s, 3.23543s/100 iters), loss = 87.3365
I0425 15:55:00.167167  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:55:00.167171  7720 sgd_solver.cpp:105] Iteration 4800, lr = 0.01
I0425 15:55:03.408427  7720 solver.cpp:219] Iteration 4900 (30.8519 iter/s, 3.24129s/100 iters), loss = 87.3365
I0425 15:55:03.408473  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:55:03.408478  7720 sgd_solver.cpp:105] Iteration 4900, lr = 0.01
I0425 15:55:06.598245  7720 solver.cpp:331] Iteration 5000, Testing net (#0)
I0425 15:55:06.824208  7720 solver.cpp:398]     Test net output #0: accuracy = 0.092
I0425 15:55:06.824231  7720 solver.cpp:398]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:55:06.859156  7720 solver.cpp:219] Iteration 5000 (28.9795 iter/s, 3.45071s/100 iters), loss = 87.3365
I0425 15:55:06.859176  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:55:06.859200  7720 sgd_solver.cpp:105] Iteration 5000, lr = 0.01
I0425 15:55:10.104202  7720 solver.cpp:219] Iteration 5100 (30.8161 iter/s, 3.24505s/100 iters), loss = 87.3365
I0425 15:55:10.104230  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:55:10.104235  7720 sgd_solver.cpp:105] Iteration 5100, lr = 0.01
I0425 15:55:13.344877  7720 solver.cpp:219] Iteration 5200 (30.8578 iter/s, 3.24067s/100 iters), loss = 87.3365
I0425 15:55:13.344924  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:55:13.344928  7720 sgd_solver.cpp:105] Iteration 5200, lr = 0.01
I0425 15:55:16.581183  7720 solver.cpp:219] Iteration 5300 (30.8996 iter/s, 3.23629s/100 iters), loss = 87.3365
I0425 15:55:16.581212  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:55:16.581234  7720 sgd_solver.cpp:105] Iteration 5300, lr = 0.01
I0425 15:55:19.818737  7720 solver.cpp:219] Iteration 5400 (30.8875 iter/s, 3.23756s/100 iters), loss = 87.3365
I0425 15:55:19.818765  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:55:19.818771  7720 sgd_solver.cpp:105] Iteration 5400, lr = 0.01
I0425 15:55:21.895216  7727 data_layer.cpp:73] Restarting data prefetching from start.
I0425 15:55:23.063109  7720 solver.cpp:219] Iteration 5500 (30.8226 iter/s, 3.24437s/100 iters), loss = 87.3365
I0425 15:55:23.063134  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:55:23.063139  7720 sgd_solver.cpp:105] Iteration 5500, lr = 0.01
I0425 15:55:26.300832  7720 solver.cpp:219] Iteration 5600 (30.8859 iter/s, 3.23773s/100 iters), loss = 87.3365
I0425 15:55:26.300860  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:55:26.300864  7720 sgd_solver.cpp:105] Iteration 5600, lr = 0.01
I0425 15:55:29.538766  7720 solver.cpp:219] Iteration 5700 (30.8839 iter/s, 3.23794s/100 iters), loss = 87.3365
I0425 15:55:29.538794  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:55:29.538799  7720 sgd_solver.cpp:105] Iteration 5700, lr = 0.01
I0425 15:55:32.777554  7720 solver.cpp:219] Iteration 5800 (30.8757 iter/s, 3.23879s/100 iters), loss = 87.3365
I0425 15:55:32.777582  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:55:32.777587  7720 sgd_solver.cpp:105] Iteration 5800, lr = 0.01
I0425 15:55:36.015617  7720 solver.cpp:219] Iteration 5900 (30.8827 iter/s, 3.23806s/100 iters), loss = 87.3365
I0425 15:55:36.015679  7720 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:55:36.015686  7720 sgd_solver.cpp:105] Iteration 5900, lr = 0.01
I0425 15:55:39.210420  7720 solver.cpp:448] Snapshotting to binary proto file examples/cifar10_full_sigmoid_iter_6000.caffemodel
I0425 15:55:39.237201  7720 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10_full_sigmoid_iter_6000.solverstate
I0425 15:55:39.257438  7720 solver.cpp:311] Iteration 6000, loss = 87.3365
I0425 15:55:39.257460  7720 solver.cpp:331] Iteration 6000, Testing net (#0)
I0425 15:55:39.464848  7720 solver.cpp:398]     Test net output #0: accuracy = 0.088
I0425 15:55:39.464872  7720 solver.cpp:398]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:55:39.464876  7720 solver.cpp:316] Optimization Done.
I0425 15:55:39.464879  7720 caffe.cpp:259] Optimization Done.
