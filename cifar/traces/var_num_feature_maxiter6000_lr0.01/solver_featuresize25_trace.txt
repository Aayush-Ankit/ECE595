I0425 15:50:10.439019  7658 caffe.cpp:218] Using GPUs 0
I0425 15:50:10.453928  7658 caffe.cpp:223] GPU 0: GeForce GTX 950M
I0425 15:50:10.637387  7658 solver.cpp:44] Initializing solver from parameters: 
test_iter: 10
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 6000
lr_policy: "step"
gamma: 1
momentum: 0.9
stepsize: 5000
snapshot: 10000
snapshot_prefix: "examples/cifar10_full_sigmoid"
solver_mode: GPU
device_id: 0
net: "ECE595/cifar/train_test/var_num_feature_maxiter6000/cifarlenet_train_test_featuresize25.prototxt"
train_state {
  level: 0
  stage: ""
}
I0425 15:50:10.637593  7658 solver.cpp:87] Creating training net from net file: ECE595/cifar/train_test/var_num_feature_maxiter6000/cifarlenet_train_test_featuresize25.prototxt
I0425 15:50:10.637867  7658 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0425 15:50:10.637902  7658 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0425 15:50:10.638046  7658 net.cpp:51] Initializing net from parameters: 
name: "CIFARLeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0425 15:50:10.638154  7658 layer_factory.hpp:77] Creating layer cifar
I0425 15:50:10.638270  7658 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_train_lmdb
I0425 15:50:10.638314  7658 net.cpp:84] Creating Layer cifar
I0425 15:50:10.638339  7658 net.cpp:380] cifar -> data
I0425 15:50:10.638375  7658 net.cpp:380] cifar -> label
I0425 15:50:10.638387  7658 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0425 15:50:10.639551  7658 data_layer.cpp:45] output data size: 64,3,32,32
I0425 15:50:10.643533  7658 net.cpp:122] Setting up cifar
I0425 15:50:10.643553  7658 net.cpp:129] Top shape: 64 3 32 32 (196608)
I0425 15:50:10.643570  7658 net.cpp:129] Top shape: 64 (64)
I0425 15:50:10.643573  7658 net.cpp:137] Memory required for data: 786688
I0425 15:50:10.643592  7658 layer_factory.hpp:77] Creating layer conv0
I0425 15:50:10.643628  7658 net.cpp:84] Creating Layer conv0
I0425 15:50:10.643635  7658 net.cpp:406] conv0 <- data
I0425 15:50:10.643664  7658 net.cpp:380] conv0 -> conv0
I0425 15:50:10.644651  7658 net.cpp:122] Setting up conv0
I0425 15:50:10.644680  7658 net.cpp:129] Top shape: 64 25 28 28 (1254400)
I0425 15:50:10.644682  7658 net.cpp:137] Memory required for data: 5804288
I0425 15:50:10.644701  7658 layer_factory.hpp:77] Creating layer pool0
I0425 15:50:10.644742  7658 net.cpp:84] Creating Layer pool0
I0425 15:50:10.644745  7658 net.cpp:406] pool0 <- conv0
I0425 15:50:10.644769  7658 net.cpp:380] pool0 -> pool0
I0425 15:50:10.644832  7658 net.cpp:122] Setting up pool0
I0425 15:50:10.644840  7658 net.cpp:129] Top shape: 64 25 14 14 (313600)
I0425 15:50:10.644841  7658 net.cpp:137] Memory required for data: 7058688
I0425 15:50:10.644845  7658 layer_factory.hpp:77] Creating layer conv1
I0425 15:50:10.644855  7658 net.cpp:84] Creating Layer conv1
I0425 15:50:10.644858  7658 net.cpp:406] conv1 <- pool0
I0425 15:50:10.644886  7658 net.cpp:380] conv1 -> conv1
I0425 15:50:10.645872  7658 net.cpp:122] Setting up conv1
I0425 15:50:10.645884  7658 net.cpp:129] Top shape: 64 25 10 10 (160000)
I0425 15:50:10.645886  7658 net.cpp:137] Memory required for data: 7698688
I0425 15:50:10.645913  7658 layer_factory.hpp:77] Creating layer pool1
I0425 15:50:10.645933  7658 net.cpp:84] Creating Layer pool1
I0425 15:50:10.645938  7658 net.cpp:406] pool1 <- conv1
I0425 15:50:10.645946  7658 net.cpp:380] pool1 -> pool1
I0425 15:50:10.645994  7658 net.cpp:122] Setting up pool1
I0425 15:50:10.646000  7658 net.cpp:129] Top shape: 64 25 5 5 (40000)
I0425 15:50:10.646023  7658 net.cpp:137] Memory required for data: 7858688
I0425 15:50:10.646025  7658 layer_factory.hpp:77] Creating layer ip1
I0425 15:50:10.646034  7658 net.cpp:84] Creating Layer ip1
I0425 15:50:10.646037  7658 net.cpp:406] ip1 <- pool1
I0425 15:50:10.646045  7658 net.cpp:380] ip1 -> ip1
I0425 15:50:10.648741  7658 net.cpp:122] Setting up ip1
I0425 15:50:10.648749  7658 net.cpp:129] Top shape: 64 500 (32000)
I0425 15:50:10.648752  7658 net.cpp:137] Memory required for data: 7986688
I0425 15:50:10.648761  7658 layer_factory.hpp:77] Creating layer relu1
I0425 15:50:10.648764  7658 net.cpp:84] Creating Layer relu1
I0425 15:50:10.648767  7658 net.cpp:406] relu1 <- ip1
I0425 15:50:10.648771  7658 net.cpp:367] relu1 -> ip1 (in-place)
I0425 15:50:10.648775  7658 net.cpp:122] Setting up relu1
I0425 15:50:10.648778  7658 net.cpp:129] Top shape: 64 500 (32000)
I0425 15:50:10.648779  7658 net.cpp:137] Memory required for data: 8114688
I0425 15:50:10.648800  7658 layer_factory.hpp:77] Creating layer ip2
I0425 15:50:10.648805  7658 net.cpp:84] Creating Layer ip2
I0425 15:50:10.648808  7658 net.cpp:406] ip2 <- ip1
I0425 15:50:10.648830  7658 net.cpp:380] ip2 -> ip2
I0425 15:50:10.648988  7658 net.cpp:122] Setting up ip2
I0425 15:50:10.648996  7658 net.cpp:129] Top shape: 64 10 (640)
I0425 15:50:10.649018  7658 net.cpp:137] Memory required for data: 8117248
I0425 15:50:10.649022  7658 layer_factory.hpp:77] Creating layer relu2
I0425 15:50:10.649030  7658 net.cpp:84] Creating Layer relu2
I0425 15:50:10.649035  7658 net.cpp:406] relu2 <- ip2
I0425 15:50:10.649040  7658 net.cpp:367] relu2 -> ip2 (in-place)
I0425 15:50:10.649047  7658 net.cpp:122] Setting up relu2
I0425 15:50:10.649052  7658 net.cpp:129] Top shape: 64 10 (640)
I0425 15:50:10.649056  7658 net.cpp:137] Memory required for data: 8119808
I0425 15:50:10.649058  7658 layer_factory.hpp:77] Creating layer loss
I0425 15:50:10.649063  7658 net.cpp:84] Creating Layer loss
I0425 15:50:10.649067  7658 net.cpp:406] loss <- ip2
I0425 15:50:10.649072  7658 net.cpp:406] loss <- label
I0425 15:50:10.649091  7658 net.cpp:380] loss -> loss
I0425 15:50:10.649104  7658 layer_factory.hpp:77] Creating layer loss
I0425 15:50:10.649207  7658 net.cpp:122] Setting up loss
I0425 15:50:10.649214  7658 net.cpp:129] Top shape: (1)
I0425 15:50:10.649219  7658 net.cpp:132]     with loss weight 1
I0425 15:50:10.649235  7658 net.cpp:137] Memory required for data: 8119812
I0425 15:50:10.649237  7658 net.cpp:198] loss needs backward computation.
I0425 15:50:10.649260  7658 net.cpp:198] relu2 needs backward computation.
I0425 15:50:10.649263  7658 net.cpp:198] ip2 needs backward computation.
I0425 15:50:10.649279  7658 net.cpp:198] relu1 needs backward computation.
I0425 15:50:10.649282  7658 net.cpp:198] ip1 needs backward computation.
I0425 15:50:10.649287  7658 net.cpp:198] pool1 needs backward computation.
I0425 15:50:10.649291  7658 net.cpp:198] conv1 needs backward computation.
I0425 15:50:10.649294  7658 net.cpp:198] pool0 needs backward computation.
I0425 15:50:10.649299  7658 net.cpp:198] conv0 needs backward computation.
I0425 15:50:10.649310  7658 net.cpp:200] cifar does not need backward computation.
I0425 15:50:10.649317  7658 net.cpp:242] This network produces output loss
I0425 15:50:10.649344  7658 net.cpp:255] Network initialization done.
I0425 15:50:10.649543  7658 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/cifar/train_test/var_num_feature_maxiter6000/cifarlenet_train_test_featuresize25.prototxt
I0425 15:50:10.649566  7658 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0425 15:50:10.649668  7658 net.cpp:51] Initializing net from parameters: 
name: "CIFARLeNet"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 25
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0425 15:50:10.649775  7658 layer_factory.hpp:77] Creating layer cifar
I0425 15:50:10.649840  7658 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0425 15:50:10.649855  7658 net.cpp:84] Creating Layer cifar
I0425 15:50:10.649863  7658 net.cpp:380] cifar -> data
I0425 15:50:10.649886  7658 net.cpp:380] cifar -> label
I0425 15:50:10.649894  7658 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0425 15:50:10.650043  7658 data_layer.cpp:45] output data size: 100,3,32,32
I0425 15:50:10.654150  7658 net.cpp:122] Setting up cifar
I0425 15:50:10.654201  7658 net.cpp:129] Top shape: 100 3 32 32 (307200)
I0425 15:50:10.654204  7658 net.cpp:129] Top shape: 100 (100)
I0425 15:50:10.654207  7658 net.cpp:137] Memory required for data: 1229200
I0425 15:50:10.654211  7658 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0425 15:50:10.654232  7658 net.cpp:84] Creating Layer label_cifar_1_split
I0425 15:50:10.654235  7658 net.cpp:406] label_cifar_1_split <- label
I0425 15:50:10.654242  7658 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I0425 15:50:10.654263  7658 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I0425 15:50:10.654435  7658 net.cpp:122] Setting up label_cifar_1_split
I0425 15:50:10.654440  7658 net.cpp:129] Top shape: 100 (100)
I0425 15:50:10.654444  7658 net.cpp:129] Top shape: 100 (100)
I0425 15:50:10.654446  7658 net.cpp:137] Memory required for data: 1230000
I0425 15:50:10.654448  7658 layer_factory.hpp:77] Creating layer conv0
I0425 15:50:10.654472  7658 net.cpp:84] Creating Layer conv0
I0425 15:50:10.654476  7658 net.cpp:406] conv0 <- data
I0425 15:50:10.654495  7658 net.cpp:380] conv0 -> conv0
I0425 15:50:10.654721  7658 net.cpp:122] Setting up conv0
I0425 15:50:10.654727  7658 net.cpp:129] Top shape: 100 25 28 28 (1960000)
I0425 15:50:10.654729  7658 net.cpp:137] Memory required for data: 9070000
I0425 15:50:10.654750  7658 layer_factory.hpp:77] Creating layer pool0
I0425 15:50:10.654757  7658 net.cpp:84] Creating Layer pool0
I0425 15:50:10.654758  7658 net.cpp:406] pool0 <- conv0
I0425 15:50:10.654762  7658 net.cpp:380] pool0 -> pool0
I0425 15:50:10.654803  7658 net.cpp:122] Setting up pool0
I0425 15:50:10.654808  7658 net.cpp:129] Top shape: 100 25 14 14 (490000)
I0425 15:50:10.654809  7658 net.cpp:137] Memory required for data: 11030000
I0425 15:50:10.654811  7658 layer_factory.hpp:77] Creating layer conv1
I0425 15:50:10.654831  7658 net.cpp:84] Creating Layer conv1
I0425 15:50:10.654834  7658 net.cpp:406] conv1 <- pool0
I0425 15:50:10.654839  7658 net.cpp:380] conv1 -> conv1
I0425 15:50:10.655256  7658 net.cpp:122] Setting up conv1
I0425 15:50:10.655282  7658 net.cpp:129] Top shape: 100 25 10 10 (250000)
I0425 15:50:10.655285  7658 net.cpp:137] Memory required for data: 12030000
I0425 15:50:10.655308  7658 layer_factory.hpp:77] Creating layer pool1
I0425 15:50:10.655315  7658 net.cpp:84] Creating Layer pool1
I0425 15:50:10.655319  7658 net.cpp:406] pool1 <- conv1
I0425 15:50:10.655325  7658 net.cpp:380] pool1 -> pool1
I0425 15:50:10.655536  7658 net.cpp:122] Setting up pool1
I0425 15:50:10.655545  7658 net.cpp:129] Top shape: 100 25 5 5 (62500)
I0425 15:50:10.655568  7658 net.cpp:137] Memory required for data: 12280000
I0425 15:50:10.655571  7658 layer_factory.hpp:77] Creating layer ip1
I0425 15:50:10.655581  7658 net.cpp:84] Creating Layer ip1
I0425 15:50:10.655587  7658 net.cpp:406] ip1 <- pool1
I0425 15:50:10.655594  7658 net.cpp:380] ip1 -> ip1
I0425 15:50:10.658224  7658 net.cpp:122] Setting up ip1
I0425 15:50:10.658252  7658 net.cpp:129] Top shape: 100 500 (50000)
I0425 15:50:10.658255  7658 net.cpp:137] Memory required for data: 12480000
I0425 15:50:10.658267  7658 layer_factory.hpp:77] Creating layer relu1
I0425 15:50:10.658275  7658 net.cpp:84] Creating Layer relu1
I0425 15:50:10.658293  7658 net.cpp:406] relu1 <- ip1
I0425 15:50:10.658299  7658 net.cpp:367] relu1 -> ip1 (in-place)
I0425 15:50:10.658324  7658 net.cpp:122] Setting up relu1
I0425 15:50:10.658327  7658 net.cpp:129] Top shape: 100 500 (50000)
I0425 15:50:10.658330  7658 net.cpp:137] Memory required for data: 12680000
I0425 15:50:10.658334  7658 layer_factory.hpp:77] Creating layer ip2
I0425 15:50:10.658341  7658 net.cpp:84] Creating Layer ip2
I0425 15:50:10.658345  7658 net.cpp:406] ip2 <- ip1
I0425 15:50:10.658351  7658 net.cpp:380] ip2 -> ip2
I0425 15:50:10.658506  7658 net.cpp:122] Setting up ip2
I0425 15:50:10.658514  7658 net.cpp:129] Top shape: 100 10 (1000)
I0425 15:50:10.658535  7658 net.cpp:137] Memory required for data: 12684000
I0425 15:50:10.658540  7658 layer_factory.hpp:77] Creating layer relu2
I0425 15:50:10.658547  7658 net.cpp:84] Creating Layer relu2
I0425 15:50:10.658551  7658 net.cpp:406] relu2 <- ip2
I0425 15:50:10.658555  7658 net.cpp:367] relu2 -> ip2 (in-place)
I0425 15:50:10.658561  7658 net.cpp:122] Setting up relu2
I0425 15:50:10.658567  7658 net.cpp:129] Top shape: 100 10 (1000)
I0425 15:50:10.658582  7658 net.cpp:137] Memory required for data: 12688000
I0425 15:50:10.658586  7658 layer_factory.hpp:77] Creating layer ip2_relu2_0_split
I0425 15:50:10.658592  7658 net.cpp:84] Creating Layer ip2_relu2_0_split
I0425 15:50:10.658596  7658 net.cpp:406] ip2_relu2_0_split <- ip2
I0425 15:50:10.658614  7658 net.cpp:380] ip2_relu2_0_split -> ip2_relu2_0_split_0
I0425 15:50:10.658634  7658 net.cpp:380] ip2_relu2_0_split -> ip2_relu2_0_split_1
I0425 15:50:10.658699  7658 net.cpp:122] Setting up ip2_relu2_0_split
I0425 15:50:10.658718  7658 net.cpp:129] Top shape: 100 10 (1000)
I0425 15:50:10.658722  7658 net.cpp:129] Top shape: 100 10 (1000)
I0425 15:50:10.658725  7658 net.cpp:137] Memory required for data: 12696000
I0425 15:50:10.658741  7658 layer_factory.hpp:77] Creating layer accuracy
I0425 15:50:10.658748  7658 net.cpp:84] Creating Layer accuracy
I0425 15:50:10.658766  7658 net.cpp:406] accuracy <- ip2_relu2_0_split_0
I0425 15:50:10.658769  7658 net.cpp:406] accuracy <- label_cifar_1_split_0
I0425 15:50:10.658797  7658 net.cpp:380] accuracy -> accuracy
I0425 15:50:10.658823  7658 net.cpp:122] Setting up accuracy
I0425 15:50:10.658828  7658 net.cpp:129] Top shape: (1)
I0425 15:50:10.658830  7658 net.cpp:137] Memory required for data: 12696004
I0425 15:50:10.658833  7658 layer_factory.hpp:77] Creating layer loss
I0425 15:50:10.658859  7658 net.cpp:84] Creating Layer loss
I0425 15:50:10.658876  7658 net.cpp:406] loss <- ip2_relu2_0_split_1
I0425 15:50:10.658880  7658 net.cpp:406] loss <- label_cifar_1_split_1
I0425 15:50:10.658886  7658 net.cpp:380] loss -> loss
I0425 15:50:10.658895  7658 layer_factory.hpp:77] Creating layer loss
I0425 15:50:10.659034  7658 net.cpp:122] Setting up loss
I0425 15:50:10.659041  7658 net.cpp:129] Top shape: (1)
I0425 15:50:10.659045  7658 net.cpp:132]     with loss weight 1
I0425 15:50:10.659085  7658 net.cpp:137] Memory required for data: 12696008
I0425 15:50:10.659107  7658 net.cpp:198] loss needs backward computation.
I0425 15:50:10.659113  7658 net.cpp:200] accuracy does not need backward computation.
I0425 15:50:10.659117  7658 net.cpp:198] ip2_relu2_0_split needs backward computation.
I0425 15:50:10.659140  7658 net.cpp:198] relu2 needs backward computation.
I0425 15:50:10.659145  7658 net.cpp:198] ip2 needs backward computation.
I0425 15:50:10.659148  7658 net.cpp:198] relu1 needs backward computation.
I0425 15:50:10.659152  7658 net.cpp:198] ip1 needs backward computation.
I0425 15:50:10.659157  7658 net.cpp:198] pool1 needs backward computation.
I0425 15:50:10.659160  7658 net.cpp:198] conv1 needs backward computation.
I0425 15:50:10.659164  7658 net.cpp:198] pool0 needs backward computation.
I0425 15:50:10.659168  7658 net.cpp:198] conv0 needs backward computation.
I0425 15:50:10.659173  7658 net.cpp:200] label_cifar_1_split does not need backward computation.
I0425 15:50:10.659178  7658 net.cpp:200] cifar does not need backward computation.
I0425 15:50:10.659183  7658 net.cpp:242] This network produces output accuracy
I0425 15:50:10.659186  7658 net.cpp:242] This network produces output loss
I0425 15:50:10.659216  7658 net.cpp:255] Network initialization done.
I0425 15:50:10.659296  7658 solver.cpp:56] Solver scaffolding done.
I0425 15:50:10.659860  7658 caffe.cpp:248] Starting Optimization
I0425 15:50:10.659868  7658 solver.cpp:273] Solving CIFARLeNet
I0425 15:50:10.659886  7658 solver.cpp:274] Learning Rate Policy: step
I0425 15:50:10.660751  7658 solver.cpp:331] Iteration 0, Testing net (#0)
I0425 15:50:10.799584  7658 solver.cpp:398]     Test net output #0: accuracy = 0.091
I0425 15:50:10.799608  7658 solver.cpp:398]     Test net output #1: loss = 48.6677 (* 1 = 48.6677 loss)
I0425 15:50:10.821913  7658 solver.cpp:219] Iteration 0 (-3.51594e-36 iter/s, 0.162004s/100 iters), loss = 48.1996
I0425 15:50:10.821935  7658 solver.cpp:238]     Train net output #0: loss = 48.1996 (* 1 = 48.1996 loss)
I0425 15:50:10.821969  7658 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0425 15:50:13.019495  7658 solver.cpp:219] Iteration 100 (45.5049 iter/s, 2.19757s/100 iters), loss = 87.3365
I0425 15:50:13.019541  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:50:13.019546  7658 sgd_solver.cpp:105] Iteration 100, lr = 0.01
I0425 15:50:15.220847  7658 solver.cpp:219] Iteration 200 (45.4275 iter/s, 2.20131s/100 iters), loss = 87.3365
I0425 15:50:15.220875  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:50:15.220897  7658 sgd_solver.cpp:105] Iteration 200, lr = 0.01
I0425 15:50:17.420207  7658 solver.cpp:219] Iteration 300 (45.4682 iter/s, 2.19934s/100 iters), loss = 87.3365
I0425 15:50:17.420235  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:50:17.420240  7658 sgd_solver.cpp:105] Iteration 300, lr = 0.01
I0425 15:50:19.616024  7658 solver.cpp:219] Iteration 400 (45.5416 iter/s, 2.19579s/100 iters), loss = 87.3365
I0425 15:50:19.616052  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:50:19.616057  7658 sgd_solver.cpp:105] Iteration 400, lr = 0.01
I0425 15:50:21.816802  7658 solver.cpp:219] Iteration 500 (45.4389 iter/s, 2.20076s/100 iters), loss = 87.3365
I0425 15:50:21.816830  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:50:21.816835  7658 sgd_solver.cpp:105] Iteration 500, lr = 0.01
I0425 15:50:24.012254  7658 solver.cpp:219] Iteration 600 (45.5492 iter/s, 2.19543s/100 iters), loss = 87.3365
I0425 15:50:24.012282  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:50:24.012287  7658 sgd_solver.cpp:105] Iteration 600, lr = 0.01
I0425 15:50:26.199605  7658 solver.cpp:219] Iteration 700 (45.7179 iter/s, 2.18733s/100 iters), loss = 87.3365
I0425 15:50:26.199648  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:50:26.199652  7658 sgd_solver.cpp:105] Iteration 700, lr = 0.01
I0425 15:50:27.890111  7666 data_layer.cpp:73] Restarting data prefetching from start.
I0425 15:50:28.398152  7658 solver.cpp:219] Iteration 800 (45.4851 iter/s, 2.19852s/100 iters), loss = 87.3365
I0425 15:50:28.398181  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:50:28.398186  7658 sgd_solver.cpp:105] Iteration 800, lr = 0.01
I0425 15:50:30.587949  7658 solver.cpp:219] Iteration 900 (45.6668 iter/s, 2.18977s/100 iters), loss = 87.3365
I0425 15:50:30.587977  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:50:30.587999  7658 sgd_solver.cpp:105] Iteration 900, lr = 0.01
I0425 15:50:32.740975  7658 solver.cpp:331] Iteration 1000, Testing net (#0)
I0425 15:50:32.879231  7658 solver.cpp:398]     Test net output #0: accuracy = 0.094
I0425 15:50:32.879258  7658 solver.cpp:398]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:50:32.900688  7658 solver.cpp:219] Iteration 1000 (43.2391 iter/s, 2.31272s/100 iters), loss = 87.3365
I0425 15:50:32.900704  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:50:32.900710  7658 sgd_solver.cpp:105] Iteration 1000, lr = 0.01
I0425 15:50:35.087064  7658 solver.cpp:219] Iteration 1100 (45.738 iter/s, 2.18637s/100 iters), loss = 87.3365
I0425 15:50:35.087127  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:50:35.087131  7658 sgd_solver.cpp:105] Iteration 1100, lr = 0.01
I0425 15:50:37.277789  7658 solver.cpp:219] Iteration 1200 (45.6482 iter/s, 2.19067s/100 iters), loss = 87.3365
I0425 15:50:37.277850  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:50:37.277855  7658 sgd_solver.cpp:105] Iteration 1200, lr = 0.01
I0425 15:50:39.476014  7658 solver.cpp:219] Iteration 1300 (45.4924 iter/s, 2.19817s/100 iters), loss = 87.3365
I0425 15:50:39.476042  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:50:39.476047  7658 sgd_solver.cpp:105] Iteration 1300, lr = 0.01
I0425 15:50:41.663794  7658 solver.cpp:219] Iteration 1400 (45.7088 iter/s, 2.18776s/100 iters), loss = 87.3365
I0425 15:50:41.663893  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:50:41.663918  7658 sgd_solver.cpp:105] Iteration 1400, lr = 0.01
I0425 15:50:43.855624  7658 solver.cpp:219] Iteration 1500 (45.6259 iter/s, 2.19174s/100 iters), loss = 87.3365
I0425 15:50:43.855654  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:50:43.855659  7658 sgd_solver.cpp:105] Iteration 1500, lr = 0.01
I0425 15:50:45.128643  7666 data_layer.cpp:73] Restarting data prefetching from start.
I0425 15:50:46.050766  7658 solver.cpp:219] Iteration 1600 (45.5556 iter/s, 2.19512s/100 iters), loss = 87.3365
I0425 15:50:46.050809  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:50:46.050814  7658 sgd_solver.cpp:105] Iteration 1600, lr = 0.01
I0425 15:50:48.242796  7658 solver.cpp:219] Iteration 1700 (45.6205 iter/s, 2.192s/100 iters), loss = 87.3365
I0425 15:50:48.242823  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:50:48.242828  7658 sgd_solver.cpp:105] Iteration 1700, lr = 0.01
I0425 15:50:50.435205  7658 solver.cpp:219] Iteration 1800 (45.6123 iter/s, 2.19239s/100 iters), loss = 87.3365
I0425 15:50:50.435233  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:50:50.435238  7658 sgd_solver.cpp:105] Iteration 1800, lr = 0.01
I0425 15:50:52.627488  7658 solver.cpp:219] Iteration 1900 (45.6149 iter/s, 2.19226s/100 iters), loss = 87.3365
I0425 15:50:52.627533  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:50:52.627538  7658 sgd_solver.cpp:105] Iteration 1900, lr = 0.01
I0425 15:50:54.784673  7658 solver.cpp:331] Iteration 2000, Testing net (#0)
I0425 15:50:54.922492  7658 solver.cpp:398]     Test net output #0: accuracy = 0.1
I0425 15:50:54.922516  7658 solver.cpp:398]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:50:54.943955  7658 solver.cpp:219] Iteration 2000 (43.1697 iter/s, 2.31644s/100 iters), loss = 87.3365
I0425 15:50:54.943971  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:50:54.943995  7658 sgd_solver.cpp:105] Iteration 2000, lr = 0.01
I0425 15:50:57.136392  7658 solver.cpp:219] Iteration 2100 (45.6115 iter/s, 2.19243s/100 iters), loss = 87.3365
I0425 15:50:57.136420  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:50:57.136425  7658 sgd_solver.cpp:105] Iteration 2100, lr = 0.01
I0425 15:50:59.324529  7658 solver.cpp:219] Iteration 2200 (45.7013 iter/s, 2.18812s/100 iters), loss = 87.3365
I0425 15:50:59.324558  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:50:59.324563  7658 sgd_solver.cpp:105] Iteration 2200, lr = 0.01
I0425 15:51:01.517060  7658 solver.cpp:219] Iteration 2300 (45.6098 iter/s, 2.19251s/100 iters), loss = 87.3365
I0425 15:51:01.517086  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:01.517109  7658 sgd_solver.cpp:105] Iteration 2300, lr = 0.01
I0425 15:51:02.377295  7666 data_layer.cpp:73] Restarting data prefetching from start.
I0425 15:51:03.713140  7658 solver.cpp:219] Iteration 2400 (45.536 iter/s, 2.19606s/100 iters), loss = 87.3365
I0425 15:51:03.713168  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:03.713173  7658 sgd_solver.cpp:105] Iteration 2400, lr = 0.01
I0425 15:51:05.900313  7658 solver.cpp:219] Iteration 2500 (45.7216 iter/s, 2.18715s/100 iters), loss = 87.3365
I0425 15:51:05.900343  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:05.900367  7658 sgd_solver.cpp:105] Iteration 2500, lr = 0.01
I0425 15:51:08.160130  7658 solver.cpp:219] Iteration 2600 (44.2518 iter/s, 2.25979s/100 iters), loss = 87.3365
I0425 15:51:08.160157  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:08.160162  7658 sgd_solver.cpp:105] Iteration 2600, lr = 0.01
I0425 15:51:10.355870  7658 solver.cpp:219] Iteration 2700 (45.5431 iter/s, 2.19572s/100 iters), loss = 87.3365
I0425 15:51:10.355916  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:10.355934  7658 sgd_solver.cpp:105] Iteration 2700, lr = 0.01
I0425 15:51:12.549546  7658 solver.cpp:219] Iteration 2800 (45.5864 iter/s, 2.19364s/100 iters), loss = 87.3365
I0425 15:51:12.549665  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:12.549671  7658 sgd_solver.cpp:105] Iteration 2800, lr = 0.01
I0425 15:51:14.739475  7658 solver.cpp:219] Iteration 2900 (45.6659 iter/s, 2.18982s/100 iters), loss = 87.3365
I0425 15:51:14.739501  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:14.739506  7658 sgd_solver.cpp:105] Iteration 2900, lr = 0.01
I0425 15:51:16.896510  7658 solver.cpp:331] Iteration 3000, Testing net (#0)
I0425 15:51:17.034729  7658 solver.cpp:398]     Test net output #0: accuracy = 0.112
I0425 15:51:17.034751  7658 solver.cpp:398]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:17.056211  7658 solver.cpp:219] Iteration 3000 (43.1644 iter/s, 2.31672s/100 iters), loss = 87.3365
I0425 15:51:17.056232  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:17.056255  7658 sgd_solver.cpp:105] Iteration 3000, lr = 0.01
I0425 15:51:19.245846  7658 solver.cpp:219] Iteration 3100 (45.6699 iter/s, 2.18962s/100 iters), loss = 87.3365
I0425 15:51:19.245892  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:19.245896  7658 sgd_solver.cpp:105] Iteration 3100, lr = 0.01
I0425 15:51:19.691467  7666 data_layer.cpp:73] Restarting data prefetching from start.
I0425 15:51:21.450407  7658 solver.cpp:219] Iteration 3200 (45.3613 iter/s, 2.20452s/100 iters), loss = 87.3365
I0425 15:51:21.450453  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:21.450458  7658 sgd_solver.cpp:105] Iteration 3200, lr = 0.01
I0425 15:51:23.643808  7658 solver.cpp:219] Iteration 3300 (45.5921 iter/s, 2.19336s/100 iters), loss = 87.3365
I0425 15:51:23.643854  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:23.643858  7658 sgd_solver.cpp:105] Iteration 3300, lr = 0.01
I0425 15:51:25.835321  7658 solver.cpp:219] Iteration 3400 (45.6314 iter/s, 2.19147s/100 iters), loss = 87.3365
I0425 15:51:25.835350  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:25.835355  7658 sgd_solver.cpp:105] Iteration 3400, lr = 0.01
I0425 15:51:28.026018  7658 solver.cpp:219] Iteration 3500 (45.648 iter/s, 2.19068s/100 iters), loss = 87.3365
I0425 15:51:28.026046  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:28.026052  7658 sgd_solver.cpp:105] Iteration 3500, lr = 0.01
I0425 15:51:30.215819  7658 solver.cpp:219] Iteration 3600 (45.6667 iter/s, 2.18978s/100 iters), loss = 87.3365
I0425 15:51:30.215847  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:30.215872  7658 sgd_solver.cpp:105] Iteration 3600, lr = 0.01
I0425 15:51:32.405829  7658 solver.cpp:219] Iteration 3700 (45.6623 iter/s, 2.18999s/100 iters), loss = 87.3365
I0425 15:51:32.405879  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:32.405884  7658 sgd_solver.cpp:105] Iteration 3700, lr = 0.01
I0425 15:51:34.596012  7658 solver.cpp:219] Iteration 3800 (45.6591 iter/s, 2.19014s/100 iters), loss = 87.3365
I0425 15:51:34.596041  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:34.596046  7658 sgd_solver.cpp:105] Iteration 3800, lr = 0.01
I0425 15:51:36.786101  7658 solver.cpp:219] Iteration 3900 (45.6607 iter/s, 2.19007s/100 iters), loss = 87.3365
I0425 15:51:36.786129  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:36.786152  7658 sgd_solver.cpp:105] Iteration 3900, lr = 0.01
I0425 15:51:36.837395  7666 data_layer.cpp:73] Restarting data prefetching from start.
I0425 15:51:38.955374  7658 solver.cpp:331] Iteration 4000, Testing net (#0)
I0425 15:51:39.093757  7658 solver.cpp:398]     Test net output #0: accuracy = 0.098
I0425 15:51:39.093777  7658 solver.cpp:398]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:39.115283  7658 solver.cpp:219] Iteration 4000 (42.9338 iter/s, 2.32917s/100 iters), loss = 87.3365
I0425 15:51:39.115315  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:39.115322  7658 sgd_solver.cpp:105] Iteration 4000, lr = 0.01
I0425 15:51:41.304100  7658 solver.cpp:219] Iteration 4100 (45.6873 iter/s, 2.18879s/100 iters), loss = 87.3365
I0425 15:51:41.304129  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:41.304134  7658 sgd_solver.cpp:105] Iteration 4100, lr = 0.01
I0425 15:51:43.492884  7658 solver.cpp:219] Iteration 4200 (45.6878 iter/s, 2.18877s/100 iters), loss = 87.3365
I0425 15:51:43.493075  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:43.493083  7658 sgd_solver.cpp:105] Iteration 4200, lr = 0.01
I0425 15:51:45.685964  7658 solver.cpp:219] Iteration 4300 (45.6016 iter/s, 2.19291s/100 iters), loss = 87.3365
I0425 15:51:45.686027  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:45.686048  7658 sgd_solver.cpp:105] Iteration 4300, lr = 0.01
I0425 15:51:47.875594  7658 solver.cpp:219] Iteration 4400 (45.6709 iter/s, 2.18958s/100 iters), loss = 87.3365
I0425 15:51:47.875622  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:47.875627  7658 sgd_solver.cpp:105] Iteration 4400, lr = 0.01
I0425 15:51:50.064487  7658 solver.cpp:219] Iteration 4500 (45.6855 iter/s, 2.18888s/100 iters), loss = 87.3365
I0425 15:51:50.064517  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:50.064522  7658 sgd_solver.cpp:105] Iteration 4500, lr = 0.01
I0425 15:51:52.254168  7658 solver.cpp:219] Iteration 4600 (45.6692 iter/s, 2.18966s/100 iters), loss = 87.3365
I0425 15:51:52.254215  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:52.254236  7658 sgd_solver.cpp:105] Iteration 4600, lr = 0.01
I0425 15:51:54.072350  7666 data_layer.cpp:73] Restarting data prefetching from start.
I0425 15:51:54.449069  7658 solver.cpp:219] Iteration 4700 (45.5608 iter/s, 2.19487s/100 iters), loss = 87.3365
I0425 15:51:54.449096  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:54.449100  7658 sgd_solver.cpp:105] Iteration 4700, lr = 0.01
I0425 15:51:56.639889  7658 solver.cpp:219] Iteration 4800 (45.6453 iter/s, 2.1908s/100 iters), loss = 87.3365
I0425 15:51:56.639919  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:56.639941  7658 sgd_solver.cpp:105] Iteration 4800, lr = 0.01
I0425 15:51:58.831897  7658 solver.cpp:219] Iteration 4900 (45.6207 iter/s, 2.19199s/100 iters), loss = 87.3365
I0425 15:51:58.831941  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:51:58.831946  7658 sgd_solver.cpp:105] Iteration 4900, lr = 0.01
I0425 15:52:00.992223  7658 solver.cpp:331] Iteration 5000, Testing net (#0)
I0425 15:52:01.130408  7658 solver.cpp:398]     Test net output #0: accuracy = 0.092
I0425 15:52:01.130432  7658 solver.cpp:398]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:52:01.151810  7658 solver.cpp:219] Iteration 5000 (43.1057 iter/s, 2.31988s/100 iters), loss = 87.3365
I0425 15:52:01.151831  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:52:01.151836  7658 sgd_solver.cpp:105] Iteration 5000, lr = 0.01
I0425 15:52:03.344331  7658 solver.cpp:219] Iteration 5100 (45.6098 iter/s, 2.19251s/100 iters), loss = 87.3365
I0425 15:52:03.344372  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:52:03.344377  7658 sgd_solver.cpp:105] Iteration 5100, lr = 0.01
I0425 15:52:05.546102  7658 solver.cpp:219] Iteration 5200 (45.4183 iter/s, 2.20176s/100 iters), loss = 87.3365
I0425 15:52:05.546146  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:52:05.546151  7658 sgd_solver.cpp:105] Iteration 5200, lr = 0.01
I0425 15:52:07.745640  7658 solver.cpp:219] Iteration 5300 (45.4648 iter/s, 2.1995s/100 iters), loss = 87.3365
I0425 15:52:07.745685  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:52:07.745689  7658 sgd_solver.cpp:105] Iteration 5300, lr = 0.01
I0425 15:52:09.947721  7658 solver.cpp:219] Iteration 5400 (45.4123 iter/s, 2.20205s/100 iters), loss = 87.3365
I0425 15:52:09.947767  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:52:09.947772  7658 sgd_solver.cpp:105] Iteration 5400, lr = 0.01
I0425 15:52:11.364675  7666 data_layer.cpp:73] Restarting data prefetching from start.
I0425 15:52:12.158788  7658 solver.cpp:219] Iteration 5500 (45.2278 iter/s, 2.21103s/100 iters), loss = 87.3365
I0425 15:52:12.158818  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:52:12.158821  7658 sgd_solver.cpp:105] Iteration 5500, lr = 0.01
I0425 15:52:14.359109  7658 solver.cpp:219] Iteration 5600 (45.4483 iter/s, 2.2003s/100 iters), loss = 87.3365
I0425 15:52:14.359236  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:52:14.359254  7658 sgd_solver.cpp:105] Iteration 5600, lr = 0.01
I0425 15:52:16.552309  7658 solver.cpp:219] Iteration 5700 (45.5977 iter/s, 2.19309s/100 iters), loss = 87.3365
I0425 15:52:16.552338  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:52:16.552362  7658 sgd_solver.cpp:105] Iteration 5700, lr = 0.01
I0425 15:52:18.757045  7658 solver.cpp:219] Iteration 5800 (45.3573 iter/s, 2.20472s/100 iters), loss = 87.3365
I0425 15:52:18.757072  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:52:18.757076  7658 sgd_solver.cpp:105] Iteration 5800, lr = 0.01
I0425 15:52:20.960871  7658 solver.cpp:219] Iteration 5900 (45.3759 iter/s, 2.20381s/100 iters), loss = 87.3365
I0425 15:52:20.960901  7658 solver.cpp:238]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:52:20.960904  7658 sgd_solver.cpp:105] Iteration 5900, lr = 0.01
I0425 15:52:23.124114  7658 solver.cpp:448] Snapshotting to binary proto file examples/cifar10_full_sigmoid_iter_6000.caffemodel
I0425 15:52:23.141098  7658 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10_full_sigmoid_iter_6000.solverstate
I0425 15:52:23.151268  7658 solver.cpp:311] Iteration 6000, loss = 87.3365
I0425 15:52:23.151284  7658 solver.cpp:331] Iteration 6000, Testing net (#0)
I0425 15:52:23.284327  7658 solver.cpp:398]     Test net output #0: accuracy = 0.088
I0425 15:52:23.284384  7658 solver.cpp:398]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I0425 15:52:23.284406  7658 solver.cpp:316] Optimization Done.
I0425 15:52:23.284409  7658 caffe.cpp:259] Optimization Done.
