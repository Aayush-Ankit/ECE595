I0425 14:50:04.633769  6528 caffe.cpp:218] Using GPUs 0
I0425 14:50:04.655148  6528 caffe.cpp:223] GPU 0: GeForce GTX 950M
I0425 14:50:05.008059  6528 solver.cpp:44] Initializing solver from parameters: 
test_iter: 10
test_interval: 1000
base_lr: 0.001
display: 100
max_iter: 6000
lr_policy: "step"
gamma: 1
momentum: 0.9
stepsize: 5000
snapshot: 10000
snapshot_prefix: "examples/cifar10_full_sigmoid"
solver_mode: GPU
device_id: 0
net: "ECE595/cifar/train_test/var_num_feature_maxiter6000/cifarlenet_train_test_featuresize10.prototxt"
train_state {
  level: 0
  stage: ""
}
I0425 14:50:05.008332  6528 solver.cpp:87] Creating training net from net file: ECE595/cifar/train_test/var_num_feature_maxiter6000/cifarlenet_train_test_featuresize10.prototxt
I0425 14:50:05.024191  6528 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0425 14:50:05.024246  6528 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0425 14:50:05.024483  6528 net.cpp:51] Initializing net from parameters: 
name: "CIFARLeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0425 14:50:05.024621  6528 layer_factory.hpp:77] Creating layer cifar
I0425 14:50:05.024916  6528 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_train_lmdb
I0425 14:50:05.024978  6528 net.cpp:84] Creating Layer cifar
I0425 14:50:05.025002  6528 net.cpp:380] cifar -> data
I0425 14:50:05.025048  6528 net.cpp:380] cifar -> label
I0425 14:50:05.025077  6528 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0425 14:50:05.027395  6528 data_layer.cpp:45] output data size: 64,3,32,32
I0425 14:50:05.035374  6528 net.cpp:122] Setting up cifar
I0425 14:50:05.035415  6528 net.cpp:129] Top shape: 64 3 32 32 (196608)
I0425 14:50:05.035428  6528 net.cpp:129] Top shape: 64 (64)
I0425 14:50:05.035434  6528 net.cpp:137] Memory required for data: 786688
I0425 14:50:05.035451  6528 layer_factory.hpp:77] Creating layer conv0
I0425 14:50:05.035490  6528 net.cpp:84] Creating Layer conv0
I0425 14:50:05.035528  6528 net.cpp:406] conv0 <- data
I0425 14:50:05.035555  6528 net.cpp:380] conv0 -> conv0
I0425 14:50:05.037318  6528 net.cpp:122] Setting up conv0
I0425 14:50:05.037356  6528 net.cpp:129] Top shape: 64 10 28 28 (501760)
I0425 14:50:05.037364  6528 net.cpp:137] Memory required for data: 2793728
I0425 14:50:05.037405  6528 layer_factory.hpp:77] Creating layer pool0
I0425 14:50:05.037432  6528 net.cpp:84] Creating Layer pool0
I0425 14:50:05.037446  6528 net.cpp:406] pool0 <- conv0
I0425 14:50:05.037458  6528 net.cpp:380] pool0 -> pool0
I0425 14:50:05.037734  6528 net.cpp:122] Setting up pool0
I0425 14:50:05.037771  6528 net.cpp:129] Top shape: 64 10 14 14 (125440)
I0425 14:50:05.037780  6528 net.cpp:137] Memory required for data: 3295488
I0425 14:50:05.037789  6528 layer_factory.hpp:77] Creating layer conv1
I0425 14:50:05.037825  6528 net.cpp:84] Creating Layer conv1
I0425 14:50:05.037842  6528 net.cpp:406] conv1 <- pool0
I0425 14:50:05.037866  6528 net.cpp:380] conv1 -> conv1
I0425 14:50:05.038684  6528 net.cpp:122] Setting up conv1
I0425 14:50:05.038723  6528 net.cpp:129] Top shape: 64 10 10 10 (64000)
I0425 14:50:05.038735  6528 net.cpp:137] Memory required for data: 3551488
I0425 14:50:05.038764  6528 layer_factory.hpp:77] Creating layer pool1
I0425 14:50:05.038792  6528 net.cpp:84] Creating Layer pool1
I0425 14:50:05.038806  6528 net.cpp:406] pool1 <- conv1
I0425 14:50:05.038830  6528 net.cpp:380] pool1 -> pool1
I0425 14:50:05.038938  6528 net.cpp:122] Setting up pool1
I0425 14:50:05.038956  6528 net.cpp:129] Top shape: 64 10 5 5 (16000)
I0425 14:50:05.038964  6528 net.cpp:137] Memory required for data: 3615488
I0425 14:50:05.038978  6528 layer_factory.hpp:77] Creating layer ip1
I0425 14:50:05.039005  6528 net.cpp:84] Creating Layer ip1
I0425 14:50:05.039019  6528 net.cpp:406] ip1 <- pool1
I0425 14:50:05.039041  6528 net.cpp:380] ip1 -> ip1
I0425 14:50:05.043009  6528 net.cpp:122] Setting up ip1
I0425 14:50:05.043048  6528 net.cpp:129] Top shape: 64 500 (32000)
I0425 14:50:05.043056  6528 net.cpp:137] Memory required for data: 3743488
I0425 14:50:05.043094  6528 layer_factory.hpp:77] Creating layer relu1
I0425 14:50:05.043126  6528 net.cpp:84] Creating Layer relu1
I0425 14:50:05.043144  6528 net.cpp:406] relu1 <- ip1
I0425 14:50:05.043179  6528 net.cpp:367] relu1 -> ip1 (in-place)
I0425 14:50:05.043225  6528 net.cpp:122] Setting up relu1
I0425 14:50:05.043241  6528 net.cpp:129] Top shape: 64 500 (32000)
I0425 14:50:05.043289  6528 net.cpp:137] Memory required for data: 3871488
I0425 14:50:05.043300  6528 layer_factory.hpp:77] Creating layer ip2
I0425 14:50:05.043325  6528 net.cpp:84] Creating Layer ip2
I0425 14:50:05.043345  6528 net.cpp:406] ip2 <- ip1
I0425 14:50:05.043359  6528 net.cpp:380] ip2 -> ip2
I0425 14:50:05.044663  6528 net.cpp:122] Setting up ip2
I0425 14:50:05.044688  6528 net.cpp:129] Top shape: 64 10 (640)
I0425 14:50:05.044698  6528 net.cpp:137] Memory required for data: 3874048
I0425 14:50:05.044721  6528 layer_factory.hpp:77] Creating layer relu2
I0425 14:50:05.044747  6528 net.cpp:84] Creating Layer relu2
I0425 14:50:05.044764  6528 net.cpp:406] relu2 <- ip2
I0425 14:50:05.044786  6528 net.cpp:367] relu2 -> ip2 (in-place)
I0425 14:50:05.044808  6528 net.cpp:122] Setting up relu2
I0425 14:50:05.044826  6528 net.cpp:129] Top shape: 64 10 (640)
I0425 14:50:05.044836  6528 net.cpp:137] Memory required for data: 3876608
I0425 14:50:05.044848  6528 layer_factory.hpp:77] Creating layer loss
I0425 14:50:05.044867  6528 net.cpp:84] Creating Layer loss
I0425 14:50:05.044880  6528 net.cpp:406] loss <- ip2
I0425 14:50:05.044893  6528 net.cpp:406] loss <- label
I0425 14:50:05.044916  6528 net.cpp:380] loss -> loss
I0425 14:50:05.044977  6528 layer_factory.hpp:77] Creating layer loss
I0425 14:50:05.045315  6528 net.cpp:122] Setting up loss
I0425 14:50:05.045336  6528 net.cpp:129] Top shape: (1)
I0425 14:50:05.045346  6528 net.cpp:132]     with loss weight 1
I0425 14:50:05.045387  6528 net.cpp:137] Memory required for data: 3876612
I0425 14:50:05.045397  6528 net.cpp:198] loss needs backward computation.
I0425 14:50:05.045418  6528 net.cpp:198] relu2 needs backward computation.
I0425 14:50:05.045434  6528 net.cpp:198] ip2 needs backward computation.
I0425 14:50:05.045490  6528 net.cpp:198] relu1 needs backward computation.
I0425 14:50:05.045496  6528 net.cpp:198] ip1 needs backward computation.
I0425 14:50:05.045506  6528 net.cpp:198] pool1 needs backward computation.
I0425 14:50:05.045516  6528 net.cpp:198] conv1 needs backward computation.
I0425 14:50:05.045523  6528 net.cpp:198] pool0 needs backward computation.
I0425 14:50:05.045531  6528 net.cpp:198] conv0 needs backward computation.
I0425 14:50:05.045542  6528 net.cpp:200] cifar does not need backward computation.
I0425 14:50:05.045552  6528 net.cpp:242] This network produces output loss
I0425 14:50:05.045577  6528 net.cpp:255] Network initialization done.
I0425 14:50:05.045871  6528 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/cifar/train_test/var_num_feature_maxiter6000/cifarlenet_train_test_featuresize10.prototxt
I0425 14:50:05.045917  6528 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0425 14:50:05.046077  6528 net.cpp:51] Initializing net from parameters: 
name: "CIFARLeNet"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 10
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0425 14:50:05.046237  6528 layer_factory.hpp:77] Creating layer cifar
I0425 14:50:05.046345  6528 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0425 14:50:05.046371  6528 net.cpp:84] Creating Layer cifar
I0425 14:50:05.046389  6528 net.cpp:380] cifar -> data
I0425 14:50:05.046406  6528 net.cpp:380] cifar -> label
I0425 14:50:05.046419  6528 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0425 14:50:05.046701  6528 data_layer.cpp:45] output data size: 100,3,32,32
I0425 14:50:05.054020  6528 net.cpp:122] Setting up cifar
I0425 14:50:05.054051  6528 net.cpp:129] Top shape: 100 3 32 32 (307200)
I0425 14:50:05.054059  6528 net.cpp:129] Top shape: 100 (100)
I0425 14:50:05.054061  6528 net.cpp:137] Memory required for data: 1229200
I0425 14:50:05.054069  6528 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0425 14:50:05.054105  6528 net.cpp:84] Creating Layer label_cifar_1_split
I0425 14:50:05.054114  6528 net.cpp:406] label_cifar_1_split <- label
I0425 14:50:05.054124  6528 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I0425 14:50:05.054139  6528 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I0425 14:50:05.054208  6528 net.cpp:122] Setting up label_cifar_1_split
I0425 14:50:05.054217  6528 net.cpp:129] Top shape: 100 (100)
I0425 14:50:05.054222  6528 net.cpp:129] Top shape: 100 (100)
I0425 14:50:05.054226  6528 net.cpp:137] Memory required for data: 1230000
I0425 14:50:05.054230  6528 layer_factory.hpp:77] Creating layer conv0
I0425 14:50:05.054244  6528 net.cpp:84] Creating Layer conv0
I0425 14:50:05.054250  6528 net.cpp:406] conv0 <- data
I0425 14:50:05.054257  6528 net.cpp:380] conv0 -> conv0
I0425 14:50:05.054587  6528 net.cpp:122] Setting up conv0
I0425 14:50:05.054596  6528 net.cpp:129] Top shape: 100 10 28 28 (784000)
I0425 14:50:05.054600  6528 net.cpp:137] Memory required for data: 4366000
I0425 14:50:05.054613  6528 layer_factory.hpp:77] Creating layer pool0
I0425 14:50:05.054625  6528 net.cpp:84] Creating Layer pool0
I0425 14:50:05.054630  6528 net.cpp:406] pool0 <- conv0
I0425 14:50:05.054636  6528 net.cpp:380] pool0 -> pool0
I0425 14:50:05.054687  6528 net.cpp:122] Setting up pool0
I0425 14:50:05.054694  6528 net.cpp:129] Top shape: 100 10 14 14 (196000)
I0425 14:50:05.054698  6528 net.cpp:137] Memory required for data: 5150000
I0425 14:50:05.054702  6528 layer_factory.hpp:77] Creating layer conv1
I0425 14:50:05.054714  6528 net.cpp:84] Creating Layer conv1
I0425 14:50:05.054718  6528 net.cpp:406] conv1 <- pool0
I0425 14:50:05.054728  6528 net.cpp:380] conv1 -> conv1
I0425 14:50:05.055053  6528 net.cpp:122] Setting up conv1
I0425 14:50:05.055068  6528 net.cpp:129] Top shape: 100 10 10 10 (100000)
I0425 14:50:05.055074  6528 net.cpp:137] Memory required for data: 5550000
I0425 14:50:05.055088  6528 layer_factory.hpp:77] Creating layer pool1
I0425 14:50:05.055095  6528 net.cpp:84] Creating Layer pool1
I0425 14:50:05.055104  6528 net.cpp:406] pool1 <- conv1
I0425 14:50:05.055111  6528 net.cpp:380] pool1 -> pool1
I0425 14:50:05.055186  6528 net.cpp:122] Setting up pool1
I0425 14:50:05.055196  6528 net.cpp:129] Top shape: 100 10 5 5 (25000)
I0425 14:50:05.055200  6528 net.cpp:137] Memory required for data: 5650000
I0425 14:50:05.055203  6528 layer_factory.hpp:77] Creating layer ip1
I0425 14:50:05.055212  6528 net.cpp:84] Creating Layer ip1
I0425 14:50:05.055217  6528 net.cpp:406] ip1 <- pool1
I0425 14:50:05.055224  6528 net.cpp:380] ip1 -> ip1
I0425 14:50:05.056438  6528 net.cpp:122] Setting up ip1
I0425 14:50:05.056447  6528 net.cpp:129] Top shape: 100 500 (50000)
I0425 14:50:05.056450  6528 net.cpp:137] Memory required for data: 5850000
I0425 14:50:05.056460  6528 layer_factory.hpp:77] Creating layer relu1
I0425 14:50:05.056466  6528 net.cpp:84] Creating Layer relu1
I0425 14:50:05.056469  6528 net.cpp:406] relu1 <- ip1
I0425 14:50:05.056473  6528 net.cpp:367] relu1 -> ip1 (in-place)
I0425 14:50:05.056480  6528 net.cpp:122] Setting up relu1
I0425 14:50:05.056484  6528 net.cpp:129] Top shape: 100 500 (50000)
I0425 14:50:05.056486  6528 net.cpp:137] Memory required for data: 6050000
I0425 14:50:05.056489  6528 layer_factory.hpp:77] Creating layer ip2
I0425 14:50:05.056499  6528 net.cpp:84] Creating Layer ip2
I0425 14:50:05.056504  6528 net.cpp:406] ip2 <- ip1
I0425 14:50:05.056509  6528 net.cpp:380] ip2 -> ip2
I0425 14:50:05.056656  6528 net.cpp:122] Setting up ip2
I0425 14:50:05.056664  6528 net.cpp:129] Top shape: 100 10 (1000)
I0425 14:50:05.056666  6528 net.cpp:137] Memory required for data: 6054000
I0425 14:50:05.056673  6528 layer_factory.hpp:77] Creating layer relu2
I0425 14:50:05.056677  6528 net.cpp:84] Creating Layer relu2
I0425 14:50:05.056681  6528 net.cpp:406] relu2 <- ip2
I0425 14:50:05.056685  6528 net.cpp:367] relu2 -> ip2 (in-place)
I0425 14:50:05.056690  6528 net.cpp:122] Setting up relu2
I0425 14:50:05.056694  6528 net.cpp:129] Top shape: 100 10 (1000)
I0425 14:50:05.056710  6528 net.cpp:137] Memory required for data: 6058000
I0425 14:50:05.056712  6528 layer_factory.hpp:77] Creating layer ip2_relu2_0_split
I0425 14:50:05.056716  6528 net.cpp:84] Creating Layer ip2_relu2_0_split
I0425 14:50:05.056720  6528 net.cpp:406] ip2_relu2_0_split <- ip2
I0425 14:50:05.056726  6528 net.cpp:380] ip2_relu2_0_split -> ip2_relu2_0_split_0
I0425 14:50:05.056736  6528 net.cpp:380] ip2_relu2_0_split -> ip2_relu2_0_split_1
I0425 14:50:05.056771  6528 net.cpp:122] Setting up ip2_relu2_0_split
I0425 14:50:05.056777  6528 net.cpp:129] Top shape: 100 10 (1000)
I0425 14:50:05.056782  6528 net.cpp:129] Top shape: 100 10 (1000)
I0425 14:50:05.056784  6528 net.cpp:137] Memory required for data: 6066000
I0425 14:50:05.056787  6528 layer_factory.hpp:77] Creating layer accuracy
I0425 14:50:05.056795  6528 net.cpp:84] Creating Layer accuracy
I0425 14:50:05.056799  6528 net.cpp:406] accuracy <- ip2_relu2_0_split_0
I0425 14:50:05.056803  6528 net.cpp:406] accuracy <- label_cifar_1_split_0
I0425 14:50:05.056809  6528 net.cpp:380] accuracy -> accuracy
I0425 14:50:05.056818  6528 net.cpp:122] Setting up accuracy
I0425 14:50:05.056824  6528 net.cpp:129] Top shape: (1)
I0425 14:50:05.056828  6528 net.cpp:137] Memory required for data: 6066004
I0425 14:50:05.056833  6528 layer_factory.hpp:77] Creating layer loss
I0425 14:50:05.056838  6528 net.cpp:84] Creating Layer loss
I0425 14:50:05.056843  6528 net.cpp:406] loss <- ip2_relu2_0_split_1
I0425 14:50:05.056846  6528 net.cpp:406] loss <- label_cifar_1_split_1
I0425 14:50:05.056851  6528 net.cpp:380] loss -> loss
I0425 14:50:05.056860  6528 layer_factory.hpp:77] Creating layer loss
I0425 14:50:05.057179  6528 net.cpp:122] Setting up loss
I0425 14:50:05.057186  6528 net.cpp:129] Top shape: (1)
I0425 14:50:05.057189  6528 net.cpp:132]     with loss weight 1
I0425 14:50:05.057200  6528 net.cpp:137] Memory required for data: 6066008
I0425 14:50:05.057204  6528 net.cpp:198] loss needs backward computation.
I0425 14:50:05.057209  6528 net.cpp:200] accuracy does not need backward computation.
I0425 14:50:05.057214  6528 net.cpp:198] ip2_relu2_0_split needs backward computation.
I0425 14:50:05.057216  6528 net.cpp:198] relu2 needs backward computation.
I0425 14:50:05.057219  6528 net.cpp:198] ip2 needs backward computation.
I0425 14:50:05.057222  6528 net.cpp:198] relu1 needs backward computation.
I0425 14:50:05.057225  6528 net.cpp:198] ip1 needs backward computation.
I0425 14:50:05.057229  6528 net.cpp:198] pool1 needs backward computation.
I0425 14:50:05.057234  6528 net.cpp:198] conv1 needs backward computation.
I0425 14:50:05.057236  6528 net.cpp:198] pool0 needs backward computation.
I0425 14:50:05.057240  6528 net.cpp:198] conv0 needs backward computation.
I0425 14:50:05.057243  6528 net.cpp:200] label_cifar_1_split does not need backward computation.
I0425 14:50:05.057250  6528 net.cpp:200] cifar does not need backward computation.
I0425 14:50:05.057252  6528 net.cpp:242] This network produces output accuracy
I0425 14:50:05.057255  6528 net.cpp:242] This network produces output loss
I0425 14:50:05.057271  6528 net.cpp:255] Network initialization done.
I0425 14:50:05.057317  6528 solver.cpp:56] Solver scaffolding done.
I0425 14:50:05.057626  6528 caffe.cpp:248] Starting Optimization
I0425 14:50:05.057632  6528 solver.cpp:273] Solving CIFARLeNet
I0425 14:50:05.057634  6528 solver.cpp:274] Learning Rate Policy: step
I0425 14:50:05.058619  6528 solver.cpp:331] Iteration 0, Testing net (#0)
I0425 14:50:05.146841  6528 solver.cpp:398]     Test net output #0: accuracy = 0.087
I0425 14:50:05.146864  6528 solver.cpp:398]     Test net output #1: loss = 35.7179 (* 1 = 35.7179 loss)
I0425 14:50:05.161310  6528 solver.cpp:219] Iteration 0 (-5.50101e+27 iter/s, 0.103638s/100 iters), loss = 40.9127
I0425 14:50:05.161331  6528 solver.cpp:238]     Train net output #0: loss = 40.9127 (* 1 = 40.9127 loss)
I0425 14:50:05.161360  6528 sgd_solver.cpp:105] Iteration 0, lr = 0.001
I0425 14:50:06.451647  6528 solver.cpp:219] Iteration 100 (77.5064 iter/s, 1.29022s/100 iters), loss = 2.30259
I0425 14:50:06.451694  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:06.451700  6528 sgd_solver.cpp:105] Iteration 100, lr = 0.001
I0425 14:50:07.729598  6528 solver.cpp:219] Iteration 200 (78.259 iter/s, 1.27781s/100 iters), loss = 2.30259
I0425 14:50:07.729627  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:07.729632  6528 sgd_solver.cpp:105] Iteration 200, lr = 0.001
I0425 14:50:09.007068  6528 solver.cpp:219] Iteration 300 (78.2874 iter/s, 1.27735s/100 iters), loss = 2.30259
I0425 14:50:09.007098  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:09.007104  6528 sgd_solver.cpp:105] Iteration 300, lr = 0.001
I0425 14:50:10.282732  6528 solver.cpp:219] Iteration 400 (78.3982 iter/s, 1.27554s/100 iters), loss = 2.30259
I0425 14:50:10.282763  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:10.282768  6528 sgd_solver.cpp:105] Iteration 400, lr = 0.001
I0425 14:50:11.560626  6528 solver.cpp:219] Iteration 500 (78.2615 iter/s, 1.27777s/100 iters), loss = 2.30259
I0425 14:50:11.560655  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:11.560662  6528 sgd_solver.cpp:105] Iteration 500, lr = 0.001
I0425 14:50:12.840026  6528 solver.cpp:219] Iteration 600 (78.1692 iter/s, 1.27928s/100 iters), loss = 2.30259
I0425 14:50:12.840056  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:12.840061  6528 sgd_solver.cpp:105] Iteration 600, lr = 0.001
I0425 14:50:14.114754  6528 solver.cpp:219] Iteration 700 (78.4555 iter/s, 1.27461s/100 iters), loss = 2.30259
I0425 14:50:14.114784  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:14.114789  6528 sgd_solver.cpp:105] Iteration 700, lr = 0.001
I0425 14:50:15.102269  6535 data_layer.cpp:73] Restarting data prefetching from start.
I0425 14:50:15.403079  6528 solver.cpp:219] Iteration 800 (77.6277 iter/s, 1.2882s/100 iters), loss = 2.30259
I0425 14:50:15.403108  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:15.403115  6528 sgd_solver.cpp:105] Iteration 800, lr = 0.001
I0425 14:50:16.677404  6528 solver.cpp:219] Iteration 900 (78.4805 iter/s, 1.2742s/100 iters), loss = 2.30259
I0425 14:50:16.677434  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:16.677459  6528 sgd_solver.cpp:105] Iteration 900, lr = 0.001
I0425 14:50:17.932081  6528 solver.cpp:331] Iteration 1000, Testing net (#0)
I0425 14:50:18.013280  6528 solver.cpp:398]     Test net output #0: accuracy = 0.094
I0425 14:50:18.013306  6528 solver.cpp:398]     Test net output #1: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:18.025923  6528 solver.cpp:219] Iteration 1000 (74.1621 iter/s, 1.3484s/100 iters), loss = 2.30259
I0425 14:50:18.025940  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:18.025949  6528 sgd_solver.cpp:105] Iteration 1000, lr = 0.001
I0425 14:50:19.297143  6528 solver.cpp:219] Iteration 1100 (78.6713 iter/s, 1.27111s/100 iters), loss = 2.30259
I0425 14:50:19.297207  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:19.297231  6528 sgd_solver.cpp:105] Iteration 1100, lr = 0.001
I0425 14:50:20.569525  6528 solver.cpp:219] Iteration 1200 (78.6025 iter/s, 1.27222s/100 iters), loss = 2.30259
I0425 14:50:20.569556  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:20.569563  6528 sgd_solver.cpp:105] Iteration 1200, lr = 0.001
I0425 14:50:21.842628  6528 solver.cpp:219] Iteration 1300 (78.5559 iter/s, 1.27298s/100 iters), loss = 2.30259
I0425 14:50:21.842659  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:21.842664  6528 sgd_solver.cpp:105] Iteration 1300, lr = 0.001
I0425 14:50:23.114621  6528 solver.cpp:219] Iteration 1400 (78.6245 iter/s, 1.27187s/100 iters), loss = 2.30259
I0425 14:50:23.114699  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:23.114704  6528 sgd_solver.cpp:105] Iteration 1400, lr = 0.001
I0425 14:50:24.388481  6528 solver.cpp:219] Iteration 1500 (78.5118 iter/s, 1.27369s/100 iters), loss = 2.30259
I0425 14:50:24.388511  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:24.388536  6528 sgd_solver.cpp:105] Iteration 1500, lr = 0.001
I0425 14:50:25.129288  6535 data_layer.cpp:73] Restarting data prefetching from start.
I0425 14:50:25.665693  6528 solver.cpp:219] Iteration 1600 (78.3033 iter/s, 1.27709s/100 iters), loss = 2.30259
I0425 14:50:25.665724  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:25.665729  6528 sgd_solver.cpp:105] Iteration 1600, lr = 0.001
I0425 14:50:26.940460  6528 solver.cpp:219] Iteration 1700 (78.4534 iter/s, 1.27464s/100 iters), loss = 2.30259
I0425 14:50:26.940490  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:26.940515  6528 sgd_solver.cpp:105] Iteration 1700, lr = 0.001
I0425 14:50:28.212230  6528 solver.cpp:219] Iteration 1800 (78.6382 iter/s, 1.27165s/100 iters), loss = 2.30259
I0425 14:50:28.212294  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:28.212318  6528 sgd_solver.cpp:105] Iteration 1800, lr = 0.001
I0425 14:50:29.483651  6528 solver.cpp:219] Iteration 1900 (78.6618 iter/s, 1.27126s/100 iters), loss = 2.30259
I0425 14:50:29.483683  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:29.483690  6528 sgd_solver.cpp:105] Iteration 1900, lr = 0.001
I0425 14:50:30.740489  6528 solver.cpp:331] Iteration 2000, Testing net (#0)
I0425 14:50:30.822242  6528 solver.cpp:398]     Test net output #0: accuracy = 0.1
I0425 14:50:30.822266  6528 solver.cpp:398]     Test net output #1: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:30.834862  6528 solver.cpp:219] Iteration 2000 (74.0146 iter/s, 1.35108s/100 iters), loss = 2.30259
I0425 14:50:30.834879  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:30.834887  6528 sgd_solver.cpp:105] Iteration 2000, lr = 0.001
I0425 14:50:32.108767  6528 solver.cpp:219] Iteration 2100 (78.5056 iter/s, 1.27379s/100 iters), loss = 2.30259
I0425 14:50:32.108794  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:32.108799  6528 sgd_solver.cpp:105] Iteration 2100, lr = 0.001
I0425 14:50:33.381422  6528 solver.cpp:219] Iteration 2200 (78.5834 iter/s, 1.27253s/100 iters), loss = 2.30259
I0425 14:50:33.381469  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:33.381474  6528 sgd_solver.cpp:105] Iteration 2200, lr = 0.001
I0425 14:50:34.654997  6528 solver.cpp:219] Iteration 2300 (78.5279 iter/s, 1.27343s/100 iters), loss = 2.30259
I0425 14:50:34.655390  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:34.655396  6528 sgd_solver.cpp:105] Iteration 2300, lr = 0.001
I0425 14:50:35.156538  6535 data_layer.cpp:73] Restarting data prefetching from start.
I0425 14:50:35.935348  6528 solver.cpp:219] Iteration 2400 (78.133 iter/s, 1.27987s/100 iters), loss = 2.30259
I0425 14:50:35.935376  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:35.935398  6528 sgd_solver.cpp:105] Iteration 2400, lr = 0.001
I0425 14:50:37.209511  6528 solver.cpp:219] Iteration 2500 (78.4904 iter/s, 1.27404s/100 iters), loss = 2.30259
I0425 14:50:37.209558  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:37.209563  6528 sgd_solver.cpp:105] Iteration 2500, lr = 0.001
I0425 14:50:38.482599  6528 solver.cpp:219] Iteration 2600 (78.5579 iter/s, 1.27295s/100 iters), loss = 2.30259
I0425 14:50:38.482627  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:38.482631  6528 sgd_solver.cpp:105] Iteration 2600, lr = 0.001
I0425 14:50:39.755869  6528 solver.cpp:219] Iteration 2700 (78.5455 iter/s, 1.27315s/100 iters), loss = 2.30259
I0425 14:50:39.755897  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:39.755900  6528 sgd_solver.cpp:105] Iteration 2700, lr = 0.001
I0425 14:50:41.044153  6528 solver.cpp:219] Iteration 2800 (77.6297 iter/s, 1.28817s/100 iters), loss = 2.30259
I0425 14:50:41.044183  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:41.044205  6528 sgd_solver.cpp:105] Iteration 2800, lr = 0.001
I0425 14:50:42.316844  6528 solver.cpp:219] Iteration 2900 (78.5809 iter/s, 1.27257s/100 iters), loss = 2.30259
I0425 14:50:42.316874  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:42.316881  6528 sgd_solver.cpp:105] Iteration 2900, lr = 0.001
I0425 14:50:43.571095  6528 solver.cpp:331] Iteration 3000, Testing net (#0)
I0425 14:50:43.652423  6528 solver.cpp:398]     Test net output #0: accuracy = 0.112
I0425 14:50:43.652447  6528 solver.cpp:398]     Test net output #1: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:43.665190  6528 solver.cpp:219] Iteration 3000 (74.1717 iter/s, 1.34822s/100 iters), loss = 2.30259
I0425 14:50:43.665205  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:43.665210  6528 sgd_solver.cpp:105] Iteration 3000, lr = 0.001
I0425 14:50:44.936540  6528 solver.cpp:219] Iteration 3100 (78.6631 iter/s, 1.27124s/100 iters), loss = 2.30259
I0425 14:50:44.936569  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:44.936573  6528 sgd_solver.cpp:105] Iteration 3100, lr = 0.001
I0425 14:50:45.197283  6535 data_layer.cpp:73] Restarting data prefetching from start.
I0425 14:50:46.216567  6528 solver.cpp:219] Iteration 3200 (78.1309 iter/s, 1.2799s/100 iters), loss = 2.30259
I0425 14:50:46.216614  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:46.216619  6528 sgd_solver.cpp:105] Iteration 3200, lr = 0.001
I0425 14:50:47.489691  6528 solver.cpp:219] Iteration 3300 (78.5556 iter/s, 1.27298s/100 iters), loss = 2.30259
I0425 14:50:47.489737  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:47.489742  6528 sgd_solver.cpp:105] Iteration 3300, lr = 0.001
I0425 14:50:48.762435  6528 solver.cpp:219] Iteration 3400 (78.577 iter/s, 1.27264s/100 iters), loss = 2.30259
I0425 14:50:48.762477  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:48.762482  6528 sgd_solver.cpp:105] Iteration 3400, lr = 0.001
I0425 14:50:50.034459  6528 solver.cpp:219] Iteration 3500 (78.5883 iter/s, 1.27245s/100 iters), loss = 2.30259
I0425 14:50:50.034507  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:50.034510  6528 sgd_solver.cpp:105] Iteration 3500, lr = 0.001
I0425 14:50:51.306433  6528 solver.cpp:219] Iteration 3600 (78.5854 iter/s, 1.2725s/100 iters), loss = 2.30259
I0425 14:50:51.306496  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:51.306500  6528 sgd_solver.cpp:105] Iteration 3600, lr = 0.001
I0425 14:50:52.579888  6528 solver.cpp:219] Iteration 3700 (78.4953 iter/s, 1.27396s/100 iters), loss = 2.30259
I0425 14:50:52.579918  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:52.579922  6528 sgd_solver.cpp:105] Iteration 3700, lr = 0.001
I0425 14:50:53.852270  6528 solver.cpp:219] Iteration 3800 (78.5597 iter/s, 1.27292s/100 iters), loss = 2.30259
I0425 14:50:53.852299  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:53.852322  6528 sgd_solver.cpp:105] Iteration 3800, lr = 0.001
I0425 14:50:55.126524  6528 solver.cpp:219] Iteration 3900 (78.4446 iter/s, 1.27478s/100 iters), loss = 2.30259
I0425 14:50:55.126552  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:55.126556  6528 sgd_solver.cpp:105] Iteration 3900, lr = 0.001
I0425 14:50:55.152992  6535 data_layer.cpp:73] Restarting data prefetching from start.
I0425 14:50:56.388924  6528 solver.cpp:331] Iteration 4000, Testing net (#0)
I0425 14:50:56.471194  6528 solver.cpp:398]     Test net output #0: accuracy = 0.098
I0425 14:50:56.471217  6528 solver.cpp:398]     Test net output #1: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:56.483963  6528 solver.cpp:219] Iteration 4000 (73.6375 iter/s, 1.358s/100 iters), loss = 2.30259
I0425 14:50:56.483979  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:56.483984  6528 sgd_solver.cpp:105] Iteration 4000, lr = 0.001
I0425 14:50:57.755100  6528 solver.cpp:219] Iteration 4100 (78.6368 iter/s, 1.27167s/100 iters), loss = 2.30259
I0425 14:50:57.755128  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:57.755133  6528 sgd_solver.cpp:105] Iteration 4100, lr = 0.001
I0425 14:50:59.026945  6528 solver.cpp:219] Iteration 4200 (78.5942 iter/s, 1.27236s/100 iters), loss = 2.30259
I0425 14:50:59.026973  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:50:59.026978  6528 sgd_solver.cpp:105] Iteration 4200, lr = 0.001
I0425 14:51:00.304272  6528 solver.cpp:219] Iteration 4300 (78.2573 iter/s, 1.27784s/100 iters), loss = 2.30259
I0425 14:51:00.304301  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:51:00.304304  6528 sgd_solver.cpp:105] Iteration 4300, lr = 0.001
I0425 14:51:01.576520  6528 solver.cpp:219] Iteration 4400 (78.5701 iter/s, 1.27275s/100 iters), loss = 2.30259
I0425 14:51:01.576550  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:51:01.576553  6528 sgd_solver.cpp:105] Iteration 4400, lr = 0.001
I0425 14:51:02.848829  6528 solver.cpp:219] Iteration 4500 (78.5667 iter/s, 1.2728s/100 iters), loss = 2.30259
I0425 14:51:02.848860  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:51:02.848865  6528 sgd_solver.cpp:105] Iteration 4500, lr = 0.001
I0425 14:51:04.121811  6528 solver.cpp:219] Iteration 4600 (78.5255 iter/s, 1.27347s/100 iters), loss = 2.30259
I0425 14:51:04.121840  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:51:04.121843  6528 sgd_solver.cpp:105] Iteration 4600, lr = 0.001
I0425 14:51:05.179249  6535 data_layer.cpp:73] Restarting data prefetching from start.
I0425 14:51:05.400506  6528 solver.cpp:219] Iteration 4700 (78.1749 iter/s, 1.27918s/100 iters), loss = 2.30259
I0425 14:51:05.400534  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:51:05.400557  6528 sgd_solver.cpp:105] Iteration 4700, lr = 0.001
I0425 14:51:06.673336  6528 solver.cpp:219] Iteration 4800 (78.5353 iter/s, 1.27331s/100 iters), loss = 2.30259
I0425 14:51:06.673365  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:51:06.673388  6528 sgd_solver.cpp:105] Iteration 4800, lr = 0.001
I0425 14:51:07.945600  6528 solver.cpp:219] Iteration 4900 (78.5707 iter/s, 1.27274s/100 iters), loss = 2.30259
I0425 14:51:07.945631  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:51:07.945636  6528 sgd_solver.cpp:105] Iteration 4900, lr = 0.001
I0425 14:51:09.199643  6528 solver.cpp:331] Iteration 5000, Testing net (#0)
I0425 14:51:09.282472  6528 solver.cpp:398]     Test net output #0: accuracy = 0.092
I0425 14:51:09.282498  6528 solver.cpp:398]     Test net output #1: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:51:09.295125  6528 solver.cpp:219] Iteration 5000 (74.0726 iter/s, 1.35003s/100 iters), loss = 2.30259
I0425 14:51:09.295140  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:51:09.295145  6528 sgd_solver.cpp:105] Iteration 5000, lr = 0.001
I0425 14:51:10.569042  6528 solver.cpp:219] Iteration 5100 (78.4684 iter/s, 1.2744s/100 iters), loss = 2.30259
I0425 14:51:10.569069  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:51:10.569093  6528 sgd_solver.cpp:105] Iteration 5100, lr = 0.001
I0425 14:51:11.843893  6528 solver.cpp:219] Iteration 5200 (78.4121 iter/s, 1.27531s/100 iters), loss = 2.30259
I0425 14:51:11.843921  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:51:11.843945  6528 sgd_solver.cpp:105] Iteration 5200, lr = 0.001
I0425 14:51:13.116535  6528 solver.cpp:219] Iteration 5300 (78.5486 iter/s, 1.2731s/100 iters), loss = 2.30259
I0425 14:51:13.116564  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:51:13.116586  6528 sgd_solver.cpp:105] Iteration 5300, lr = 0.001
I0425 14:51:14.389344  6528 solver.cpp:219] Iteration 5400 (78.5386 iter/s, 1.27326s/100 iters), loss = 2.30259
I0425 14:51:14.389371  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:51:14.389376  6528 sgd_solver.cpp:105] Iteration 5400, lr = 0.001
I0425 14:51:15.207600  6535 data_layer.cpp:73] Restarting data prefetching from start.
I0425 14:51:15.670209  6528 solver.cpp:219] Iteration 5500 (78.0456 iter/s, 1.2813s/100 iters), loss = 2.30259
I0425 14:51:15.670238  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:51:15.670259  6528 sgd_solver.cpp:105] Iteration 5500, lr = 0.001
I0425 14:51:16.942580  6528 solver.cpp:219] Iteration 5600 (78.5662 iter/s, 1.27281s/100 iters), loss = 2.30259
I0425 14:51:16.942627  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:51:16.942632  6528 sgd_solver.cpp:105] Iteration 5600, lr = 0.001
I0425 14:51:18.212322  6528 solver.cpp:219] Iteration 5700 (78.7299 iter/s, 1.27017s/100 iters), loss = 2.30259
I0425 14:51:18.212353  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:51:18.212357  6528 sgd_solver.cpp:105] Iteration 5700, lr = 0.001
I0425 14:51:19.486125  6528 solver.cpp:219] Iteration 5800 (78.4786 iter/s, 1.27423s/100 iters), loss = 2.30259
I0425 14:51:19.486153  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:51:19.486177  6528 sgd_solver.cpp:105] Iteration 5800, lr = 0.001
I0425 14:51:20.758258  6528 solver.cpp:219] Iteration 5900 (78.5819 iter/s, 1.27256s/100 iters), loss = 2.30259
I0425 14:51:20.758306  6528 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:51:20.758311  6528 sgd_solver.cpp:105] Iteration 5900, lr = 0.001
I0425 14:51:22.011412  6528 solver.cpp:448] Snapshotting to binary proto file examples/cifar10_full_sigmoid_iter_6000.caffemodel
I0425 14:51:22.019531  6528 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10_full_sigmoid_iter_6000.solverstate
I0425 14:51:22.024715  6528 solver.cpp:311] Iteration 6000, loss = 2.30259
I0425 14:51:22.024731  6528 solver.cpp:331] Iteration 6000, Testing net (#0)
I0425 14:51:22.100437  6528 solver.cpp:398]     Test net output #0: accuracy = 0.088
I0425 14:51:22.100461  6528 solver.cpp:398]     Test net output #1: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:51:22.100464  6528 solver.cpp:316] Optimization Done.
I0425 14:51:22.100466  6528 caffe.cpp:259] Optimization Done.
