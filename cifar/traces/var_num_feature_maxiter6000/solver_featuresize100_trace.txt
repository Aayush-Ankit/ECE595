I0425 14:56:50.836154  6676 caffe.cpp:218] Using GPUs 0
I0425 14:56:50.850587  6676 caffe.cpp:223] GPU 0: GeForce GTX 950M
I0425 14:56:51.033442  6676 solver.cpp:44] Initializing solver from parameters: 
test_iter: 10
test_interval: 1000
base_lr: 0.001
display: 100
max_iter: 6000
lr_policy: "step"
gamma: 1
momentum: 0.9
stepsize: 5000
snapshot: 10000
snapshot_prefix: "examples/cifar10_full_sigmoid"
solver_mode: GPU
device_id: 0
net: "ECE595/cifar/train_test/var_num_feature_maxiter6000/cifarlenet_train_test_featuresize100.prototxt"
train_state {
  level: 0
  stage: ""
}
I0425 14:56:51.033630  6676 solver.cpp:87] Creating training net from net file: ECE595/cifar/train_test/var_num_feature_maxiter6000/cifarlenet_train_test_featuresize100.prototxt
I0425 14:56:51.033854  6676 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0425 14:56:51.033865  6676 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0425 14:56:51.033996  6676 net.cpp:51] Initializing net from parameters: 
name: "CIFARLeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0425 14:56:51.034103  6676 layer_factory.hpp:77] Creating layer cifar
I0425 14:56:51.034215  6676 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_train_lmdb
I0425 14:56:51.034260  6676 net.cpp:84] Creating Layer cifar
I0425 14:56:51.034283  6676 net.cpp:380] cifar -> data
I0425 14:56:51.034338  6676 net.cpp:380] cifar -> label
I0425 14:56:51.034368  6676 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0425 14:56:51.035203  6676 data_layer.cpp:45] output data size: 64,3,32,32
I0425 14:56:51.038638  6676 net.cpp:122] Setting up cifar
I0425 14:56:51.038674  6676 net.cpp:129] Top shape: 64 3 32 32 (196608)
I0425 14:56:51.038679  6676 net.cpp:129] Top shape: 64 (64)
I0425 14:56:51.038682  6676 net.cpp:137] Memory required for data: 786688
I0425 14:56:51.038692  6676 layer_factory.hpp:77] Creating layer conv0
I0425 14:56:51.038727  6676 net.cpp:84] Creating Layer conv0
I0425 14:56:51.038733  6676 net.cpp:406] conv0 <- data
I0425 14:56:51.038748  6676 net.cpp:380] conv0 -> conv0
I0425 14:56:51.039671  6676 net.cpp:122] Setting up conv0
I0425 14:56:51.039697  6676 net.cpp:129] Top shape: 64 100 28 28 (5017600)
I0425 14:56:51.039701  6676 net.cpp:137] Memory required for data: 20857088
I0425 14:56:51.039750  6676 layer_factory.hpp:77] Creating layer pool0
I0425 14:56:51.039760  6676 net.cpp:84] Creating Layer pool0
I0425 14:56:51.039765  6676 net.cpp:406] pool0 <- conv0
I0425 14:56:51.039770  6676 net.cpp:380] pool0 -> pool0
I0425 14:56:51.039993  6676 net.cpp:122] Setting up pool0
I0425 14:56:51.039999  6676 net.cpp:129] Top shape: 64 100 14 14 (1254400)
I0425 14:56:51.040002  6676 net.cpp:137] Memory required for data: 25874688
I0425 14:56:51.040005  6676 layer_factory.hpp:77] Creating layer conv1
I0425 14:56:51.040014  6676 net.cpp:84] Creating Layer conv1
I0425 14:56:51.040019  6676 net.cpp:406] conv1 <- pool0
I0425 14:56:51.040026  6676 net.cpp:380] conv1 -> conv1
I0425 14:56:51.041788  6676 net.cpp:122] Setting up conv1
I0425 14:56:51.041798  6676 net.cpp:129] Top shape: 64 100 10 10 (640000)
I0425 14:56:51.041802  6676 net.cpp:137] Memory required for data: 28434688
I0425 14:56:51.041812  6676 layer_factory.hpp:77] Creating layer pool1
I0425 14:56:51.041821  6676 net.cpp:84] Creating Layer pool1
I0425 14:56:51.041826  6676 net.cpp:406] pool1 <- conv1
I0425 14:56:51.041833  6676 net.cpp:380] pool1 -> pool1
I0425 14:56:51.041862  6676 net.cpp:122] Setting up pool1
I0425 14:56:51.041868  6676 net.cpp:129] Top shape: 64 100 5 5 (160000)
I0425 14:56:51.041872  6676 net.cpp:137] Memory required for data: 29074688
I0425 14:56:51.041875  6676 layer_factory.hpp:77] Creating layer ip1
I0425 14:56:51.041903  6676 net.cpp:84] Creating Layer ip1
I0425 14:56:51.041923  6676 net.cpp:406] ip1 <- pool1
I0425 14:56:51.041929  6676 net.cpp:380] ip1 -> ip1
I0425 14:56:51.049386  6676 net.cpp:122] Setting up ip1
I0425 14:56:51.049401  6676 net.cpp:129] Top shape: 64 500 (32000)
I0425 14:56:51.049404  6676 net.cpp:137] Memory required for data: 29202688
I0425 14:56:51.049453  6676 layer_factory.hpp:77] Creating layer relu1
I0425 14:56:51.049482  6676 net.cpp:84] Creating Layer relu1
I0425 14:56:51.049486  6676 net.cpp:406] relu1 <- ip1
I0425 14:56:51.049509  6676 net.cpp:367] relu1 -> ip1 (in-place)
I0425 14:56:51.049536  6676 net.cpp:122] Setting up relu1
I0425 14:56:51.049556  6676 net.cpp:129] Top shape: 64 500 (32000)
I0425 14:56:51.049558  6676 net.cpp:137] Memory required for data: 29330688
I0425 14:56:51.049561  6676 layer_factory.hpp:77] Creating layer ip2
I0425 14:56:51.049569  6676 net.cpp:84] Creating Layer ip2
I0425 14:56:51.049590  6676 net.cpp:406] ip2 <- ip1
I0425 14:56:51.049597  6676 net.cpp:380] ip2 -> ip2
I0425 14:56:51.049731  6676 net.cpp:122] Setting up ip2
I0425 14:56:51.049737  6676 net.cpp:129] Top shape: 64 10 (640)
I0425 14:56:51.049741  6676 net.cpp:137] Memory required for data: 29333248
I0425 14:56:51.049746  6676 layer_factory.hpp:77] Creating layer relu2
I0425 14:56:51.049757  6676 net.cpp:84] Creating Layer relu2
I0425 14:56:51.049760  6676 net.cpp:406] relu2 <- ip2
I0425 14:56:51.049765  6676 net.cpp:367] relu2 -> ip2 (in-place)
I0425 14:56:51.049772  6676 net.cpp:122] Setting up relu2
I0425 14:56:51.049777  6676 net.cpp:129] Top shape: 64 10 (640)
I0425 14:56:51.049779  6676 net.cpp:137] Memory required for data: 29335808
I0425 14:56:51.049783  6676 layer_factory.hpp:77] Creating layer loss
I0425 14:56:51.049788  6676 net.cpp:84] Creating Layer loss
I0425 14:56:51.049793  6676 net.cpp:406] loss <- ip2
I0425 14:56:51.049796  6676 net.cpp:406] loss <- label
I0425 14:56:51.049806  6676 net.cpp:380] loss -> loss
I0425 14:56:51.049818  6676 layer_factory.hpp:77] Creating layer loss
I0425 14:56:51.049885  6676 net.cpp:122] Setting up loss
I0425 14:56:51.049890  6676 net.cpp:129] Top shape: (1)
I0425 14:56:51.049895  6676 net.cpp:132]     with loss weight 1
I0425 14:56:51.049909  6676 net.cpp:137] Memory required for data: 29335812
I0425 14:56:51.049913  6676 net.cpp:198] loss needs backward computation.
I0425 14:56:51.049921  6676 net.cpp:198] relu2 needs backward computation.
I0425 14:56:51.049938  6676 net.cpp:198] ip2 needs backward computation.
I0425 14:56:51.049942  6676 net.cpp:198] relu1 needs backward computation.
I0425 14:56:51.049945  6676 net.cpp:198] ip1 needs backward computation.
I0425 14:56:51.049948  6676 net.cpp:198] pool1 needs backward computation.
I0425 14:56:51.049953  6676 net.cpp:198] conv1 needs backward computation.
I0425 14:56:51.049957  6676 net.cpp:198] pool0 needs backward computation.
I0425 14:56:51.049959  6676 net.cpp:198] conv0 needs backward computation.
I0425 14:56:51.049963  6676 net.cpp:200] cifar does not need backward computation.
I0425 14:56:51.049967  6676 net.cpp:242] This network produces output loss
I0425 14:56:51.049978  6676 net.cpp:255] Network initialization done.
I0425 14:56:51.050110  6676 solver.cpp:173] Creating test net (#0) specified by net file: ECE595/cifar/train_test/var_num_feature_maxiter6000/cifarlenet_train_test_featuresize100.prototxt
I0425 14:56:51.050129  6676 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0425 14:56:51.050249  6676 net.cpp:51] Initializing net from parameters: 
name: "CIFARLeNet"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv0"
  type: "Convolution"
  bottom: "data"
  top: "conv0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool0"
  type: "Pooling"
  bottom: "conv0"
  top: "pool0"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pool0"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 100
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0425 14:56:51.050410  6676 layer_factory.hpp:77] Creating layer cifar
I0425 14:56:51.050508  6676 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0425 14:56:51.050520  6676 net.cpp:84] Creating Layer cifar
I0425 14:56:51.050526  6676 net.cpp:380] cifar -> data
I0425 14:56:51.050554  6676 net.cpp:380] cifar -> label
I0425 14:56:51.050578  6676 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0425 14:56:51.050745  6676 data_layer.cpp:45] output data size: 100,3,32,32
I0425 14:56:51.054502  6676 net.cpp:122] Setting up cifar
I0425 14:56:51.054533  6676 net.cpp:129] Top shape: 100 3 32 32 (307200)
I0425 14:56:51.054536  6676 net.cpp:129] Top shape: 100 (100)
I0425 14:56:51.054539  6676 net.cpp:137] Memory required for data: 1229200
I0425 14:56:51.054574  6676 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0425 14:56:51.054601  6676 net.cpp:84] Creating Layer label_cifar_1_split
I0425 14:56:51.054625  6676 net.cpp:406] label_cifar_1_split <- label
I0425 14:56:51.054633  6676 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I0425 14:56:51.054661  6676 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I0425 14:56:51.054735  6676 net.cpp:122] Setting up label_cifar_1_split
I0425 14:56:51.054741  6676 net.cpp:129] Top shape: 100 (100)
I0425 14:56:51.054744  6676 net.cpp:129] Top shape: 100 (100)
I0425 14:56:51.054749  6676 net.cpp:137] Memory required for data: 1230000
I0425 14:56:51.054751  6676 layer_factory.hpp:77] Creating layer conv0
I0425 14:56:51.054792  6676 net.cpp:84] Creating Layer conv0
I0425 14:56:51.054796  6676 net.cpp:406] conv0 <- data
I0425 14:56:51.054822  6676 net.cpp:380] conv0 -> conv0
I0425 14:56:51.055094  6676 net.cpp:122] Setting up conv0
I0425 14:56:51.055102  6676 net.cpp:129] Top shape: 100 100 28 28 (7840000)
I0425 14:56:51.055106  6676 net.cpp:137] Memory required for data: 32590000
I0425 14:56:51.055136  6676 layer_factory.hpp:77] Creating layer pool0
I0425 14:56:51.055143  6676 net.cpp:84] Creating Layer pool0
I0425 14:56:51.055164  6676 net.cpp:406] pool0 <- conv0
I0425 14:56:51.055169  6676 net.cpp:380] pool0 -> pool0
I0425 14:56:51.055235  6676 net.cpp:122] Setting up pool0
I0425 14:56:51.055241  6676 net.cpp:129] Top shape: 100 100 14 14 (1960000)
I0425 14:56:51.055249  6676 net.cpp:137] Memory required for data: 40430000
I0425 14:56:51.055253  6676 layer_factory.hpp:77] Creating layer conv1
I0425 14:56:51.055280  6676 net.cpp:84] Creating Layer conv1
I0425 14:56:51.055287  6676 net.cpp:406] conv1 <- pool0
I0425 14:56:51.055294  6676 net.cpp:380] conv1 -> conv1
I0425 14:56:51.057229  6676 net.cpp:122] Setting up conv1
I0425 14:56:51.057238  6676 net.cpp:129] Top shape: 100 100 10 10 (1000000)
I0425 14:56:51.057242  6676 net.cpp:137] Memory required for data: 44430000
I0425 14:56:51.057265  6676 layer_factory.hpp:77] Creating layer pool1
I0425 14:56:51.057271  6676 net.cpp:84] Creating Layer pool1
I0425 14:56:51.057276  6676 net.cpp:406] pool1 <- conv1
I0425 14:56:51.057296  6676 net.cpp:380] pool1 -> pool1
I0425 14:56:51.057579  6676 net.cpp:122] Setting up pool1
I0425 14:56:51.057585  6676 net.cpp:129] Top shape: 100 100 5 5 (250000)
I0425 14:56:51.057602  6676 net.cpp:137] Memory required for data: 45430000
I0425 14:56:51.057605  6676 layer_factory.hpp:77] Creating layer ip1
I0425 14:56:51.057615  6676 net.cpp:84] Creating Layer ip1
I0425 14:56:51.057618  6676 net.cpp:406] ip1 <- pool1
I0425 14:56:51.057638  6676 net.cpp:380] ip1 -> ip1
I0425 14:56:51.064877  6676 net.cpp:122] Setting up ip1
I0425 14:56:51.064898  6676 net.cpp:129] Top shape: 100 500 (50000)
I0425 14:56:51.064901  6676 net.cpp:137] Memory required for data: 45630000
I0425 14:56:51.064936  6676 layer_factory.hpp:77] Creating layer relu1
I0425 14:56:51.064946  6676 net.cpp:84] Creating Layer relu1
I0425 14:56:51.064951  6676 net.cpp:406] relu1 <- ip1
I0425 14:56:51.064960  6676 net.cpp:367] relu1 -> ip1 (in-place)
I0425 14:56:51.064986  6676 net.cpp:122] Setting up relu1
I0425 14:56:51.064991  6676 net.cpp:129] Top shape: 100 500 (50000)
I0425 14:56:51.065011  6676 net.cpp:137] Memory required for data: 45830000
I0425 14:56:51.065014  6676 layer_factory.hpp:77] Creating layer ip2
I0425 14:56:51.065043  6676 net.cpp:84] Creating Layer ip2
I0425 14:56:51.065062  6676 net.cpp:406] ip2 <- ip1
I0425 14:56:51.065083  6676 net.cpp:380] ip2 -> ip2
I0425 14:56:51.065261  6676 net.cpp:122] Setting up ip2
I0425 14:56:51.065268  6676 net.cpp:129] Top shape: 100 10 (1000)
I0425 14:56:51.065270  6676 net.cpp:137] Memory required for data: 45834000
I0425 14:56:51.065276  6676 layer_factory.hpp:77] Creating layer relu2
I0425 14:56:51.065301  6676 net.cpp:84] Creating Layer relu2
I0425 14:56:51.065306  6676 net.cpp:406] relu2 <- ip2
I0425 14:56:51.065311  6676 net.cpp:367] relu2 -> ip2 (in-place)
I0425 14:56:51.065318  6676 net.cpp:122] Setting up relu2
I0425 14:56:51.065333  6676 net.cpp:129] Top shape: 100 10 (1000)
I0425 14:56:51.065354  6676 net.cpp:137] Memory required for data: 45838000
I0425 14:56:51.065357  6676 layer_factory.hpp:77] Creating layer ip2_relu2_0_split
I0425 14:56:51.065384  6676 net.cpp:84] Creating Layer ip2_relu2_0_split
I0425 14:56:51.065388  6676 net.cpp:406] ip2_relu2_0_split <- ip2
I0425 14:56:51.065393  6676 net.cpp:380] ip2_relu2_0_split -> ip2_relu2_0_split_0
I0425 14:56:51.065418  6676 net.cpp:380] ip2_relu2_0_split -> ip2_relu2_0_split_1
I0425 14:56:51.065485  6676 net.cpp:122] Setting up ip2_relu2_0_split
I0425 14:56:51.065490  6676 net.cpp:129] Top shape: 100 10 (1000)
I0425 14:56:51.065495  6676 net.cpp:129] Top shape: 100 10 (1000)
I0425 14:56:51.065497  6676 net.cpp:137] Memory required for data: 45846000
I0425 14:56:51.065521  6676 layer_factory.hpp:77] Creating layer accuracy
I0425 14:56:51.065528  6676 net.cpp:84] Creating Layer accuracy
I0425 14:56:51.065532  6676 net.cpp:406] accuracy <- ip2_relu2_0_split_0
I0425 14:56:51.065537  6676 net.cpp:406] accuracy <- label_cifar_1_split_0
I0425 14:56:51.065562  6676 net.cpp:380] accuracy -> accuracy
I0425 14:56:51.065589  6676 net.cpp:122] Setting up accuracy
I0425 14:56:51.065594  6676 net.cpp:129] Top shape: (1)
I0425 14:56:51.065599  6676 net.cpp:137] Memory required for data: 45846004
I0425 14:56:51.065620  6676 layer_factory.hpp:77] Creating layer loss
I0425 14:56:51.065644  6676 net.cpp:84] Creating Layer loss
I0425 14:56:51.065647  6676 net.cpp:406] loss <- ip2_relu2_0_split_1
I0425 14:56:51.065651  6676 net.cpp:406] loss <- label_cifar_1_split_1
I0425 14:56:51.065671  6676 net.cpp:380] loss -> loss
I0425 14:56:51.065680  6676 layer_factory.hpp:77] Creating layer loss
I0425 14:56:51.065763  6676 net.cpp:122] Setting up loss
I0425 14:56:51.065770  6676 net.cpp:129] Top shape: (1)
I0425 14:56:51.065774  6676 net.cpp:132]     with loss weight 1
I0425 14:56:51.065798  6676 net.cpp:137] Memory required for data: 45846008
I0425 14:56:51.065801  6676 net.cpp:198] loss needs backward computation.
I0425 14:56:51.065820  6676 net.cpp:200] accuracy does not need backward computation.
I0425 14:56:51.065826  6676 net.cpp:198] ip2_relu2_0_split needs backward computation.
I0425 14:56:51.065830  6676 net.cpp:198] relu2 needs backward computation.
I0425 14:56:51.065834  6676 net.cpp:198] ip2 needs backward computation.
I0425 14:56:51.065836  6676 net.cpp:198] relu1 needs backward computation.
I0425 14:56:51.065840  6676 net.cpp:198] ip1 needs backward computation.
I0425 14:56:51.065843  6676 net.cpp:198] pool1 needs backward computation.
I0425 14:56:51.065847  6676 net.cpp:198] conv1 needs backward computation.
I0425 14:56:51.065850  6676 net.cpp:198] pool0 needs backward computation.
I0425 14:56:51.065855  6676 net.cpp:198] conv0 needs backward computation.
I0425 14:56:51.065860  6676 net.cpp:200] label_cifar_1_split does not need backward computation.
I0425 14:56:51.065865  6676 net.cpp:200] cifar does not need backward computation.
I0425 14:56:51.065868  6676 net.cpp:242] This network produces output accuracy
I0425 14:56:51.065872  6676 net.cpp:242] This network produces output loss
I0425 14:56:51.065886  6676 net.cpp:255] Network initialization done.
I0425 14:56:51.065922  6676 solver.cpp:56] Solver scaffolding done.
I0425 14:56:51.066164  6676 caffe.cpp:248] Starting Optimization
I0425 14:56:51.066169  6676 solver.cpp:273] Solving CIFARLeNet
I0425 14:56:51.066171  6676 solver.cpp:274] Learning Rate Policy: step
I0425 14:56:51.067020  6676 solver.cpp:331] Iteration 0, Testing net (#0)
I0425 14:56:51.472434  6676 solver.cpp:398]     Test net output #0: accuracy = 0.089
I0425 14:56:51.472460  6676 solver.cpp:398]     Test net output #1: loss = 42.8238 (* 1 = 42.8238 loss)
I0425 14:56:51.544976  6676 solver.cpp:219] Iteration 0 (0 iter/s, 0.478768s/100 iters), loss = 38.4776
I0425 14:56:51.545007  6676 solver.cpp:238]     Train net output #0: loss = 38.4776 (* 1 = 38.4776 loss)
I0425 14:56:51.545078  6676 sgd_solver.cpp:105] Iteration 0, lr = 0.001
I0425 14:56:58.047212  6676 solver.cpp:219] Iteration 100 (15.379 iter/s, 6.50237s/100 iters), loss = 2.30259
I0425 14:56:58.047264  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:56:58.047289  6676 sgd_solver.cpp:105] Iteration 100, lr = 0.001
I0425 14:57:04.551543  6676 solver.cpp:219] Iteration 200 (15.3741 iter/s, 6.50444s/100 iters), loss = 2.30259
I0425 14:57:04.551573  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:57:04.551596  6676 sgd_solver.cpp:105] Iteration 200, lr = 0.001
I0425 14:57:11.053135  6676 solver.cpp:219] Iteration 300 (15.3806 iter/s, 6.50171s/100 iters), loss = 2.30259
I0425 14:57:11.053164  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:57:11.053170  6676 sgd_solver.cpp:105] Iteration 300, lr = 0.001
I0425 14:57:17.548828  6676 solver.cpp:219] Iteration 400 (15.3945 iter/s, 6.49581s/100 iters), loss = 2.30259
I0425 14:57:17.548861  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:57:17.548867  6676 sgd_solver.cpp:105] Iteration 400, lr = 0.001
I0425 14:57:24.044059  6676 solver.cpp:219] Iteration 500 (15.3957 iter/s, 6.49534s/100 iters), loss = 2.30259
I0425 14:57:24.044148  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:57:24.044155  6676 sgd_solver.cpp:105] Iteration 500, lr = 0.001
I0425 14:57:30.542843  6676 solver.cpp:219] Iteration 600 (15.3874 iter/s, 6.49883s/100 iters), loss = 2.30259
I0425 14:57:30.542873  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:57:30.542896  6676 sgd_solver.cpp:105] Iteration 600, lr = 0.001
I0425 14:57:37.036300  6676 solver.cpp:219] Iteration 700 (15.3999 iter/s, 6.49355s/100 iters), loss = 2.30259
I0425 14:57:37.036329  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:57:37.036335  6676 sgd_solver.cpp:105] Iteration 700, lr = 0.001
I0425 14:57:42.037880  6684 data_layer.cpp:73] Restarting data prefetching from start.
I0425 14:57:43.533259  6676 solver.cpp:219] Iteration 800 (15.3916 iter/s, 6.49705s/100 iters), loss = 2.30259
I0425 14:57:43.533288  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:57:43.533295  6676 sgd_solver.cpp:105] Iteration 800, lr = 0.001
I0425 14:57:50.026170  6676 solver.cpp:219] Iteration 900 (15.4012 iter/s, 6.49299s/100 iters), loss = 2.30259
I0425 14:57:50.026201  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:57:50.026207  6676 sgd_solver.cpp:105] Iteration 900, lr = 0.001
I0425 14:57:56.469672  6676 solver.cpp:331] Iteration 1000, Testing net (#0)
I0425 14:57:56.901710  6676 solver.cpp:398]     Test net output #0: accuracy = 0.094
I0425 14:57:56.901733  6676 solver.cpp:398]     Test net output #1: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:57:56.970800  6676 solver.cpp:219] Iteration 1000 (14.3995 iter/s, 6.94469s/100 iters), loss = 2.30259
I0425 14:57:56.970829  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:57:56.970836  6676 sgd_solver.cpp:105] Iteration 1000, lr = 0.001
I0425 14:58:03.478587  6676 solver.cpp:219] Iteration 1100 (15.366 iter/s, 6.50786s/100 iters), loss = 2.30259
I0425 14:58:03.478618  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:58:03.478624  6676 sgd_solver.cpp:105] Iteration 1100, lr = 0.001
I0425 14:58:09.984572  6676 solver.cpp:219] Iteration 1200 (15.3703 iter/s, 6.50605s/100 iters), loss = 2.30259
I0425 14:58:09.984601  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:58:09.984607  6676 sgd_solver.cpp:105] Iteration 1200, lr = 0.001
I0425 14:58:16.490031  6676 solver.cpp:219] Iteration 1300 (15.3716 iter/s, 6.50552s/100 iters), loss = 2.30259
I0425 14:58:16.490061  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:58:16.490085  6676 sgd_solver.cpp:105] Iteration 1300, lr = 0.001
I0425 14:58:22.979979  6676 solver.cpp:219] Iteration 1400 (15.4083 iter/s, 6.49001s/100 iters), loss = 2.30259
I0425 14:58:22.980007  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:58:22.980013  6676 sgd_solver.cpp:105] Iteration 1400, lr = 0.001
I0425 14:58:29.478410  6676 solver.cpp:219] Iteration 1500 (15.3882 iter/s, 6.49849s/100 iters), loss = 2.30259
I0425 14:58:29.478615  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:58:29.478626  6676 sgd_solver.cpp:105] Iteration 1500, lr = 0.001
I0425 14:58:33.251873  6684 data_layer.cpp:73] Restarting data prefetching from start.
I0425 14:58:35.980541  6676 solver.cpp:219] Iteration 1600 (15.3799 iter/s, 6.50201s/100 iters), loss = 2.30259
I0425 14:58:35.980574  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:58:35.980582  6676 sgd_solver.cpp:105] Iteration 1600, lr = 0.001
I0425 14:58:42.472852  6676 solver.cpp:219] Iteration 1700 (15.4027 iter/s, 6.49235s/100 iters), loss = 2.30259
I0425 14:58:42.472882  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:58:42.472888  6676 sgd_solver.cpp:105] Iteration 1700, lr = 0.001
I0425 14:58:48.965317  6676 solver.cpp:219] Iteration 1800 (15.4024 iter/s, 6.49251s/100 iters), loss = 2.30259
I0425 14:58:48.965378  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:58:48.965384  6676 sgd_solver.cpp:105] Iteration 1800, lr = 0.001
I0425 14:58:55.456444  6676 solver.cpp:219] Iteration 1900 (15.4056 iter/s, 6.49113s/100 iters), loss = 2.30259
I0425 14:58:55.456475  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:58:55.456480  6676 sgd_solver.cpp:105] Iteration 1900, lr = 0.001
I0425 14:59:01.850057  6676 solver.cpp:331] Iteration 2000, Testing net (#0)
I0425 14:59:02.279891  6676 solver.cpp:398]     Test net output #0: accuracy = 0.1
I0425 14:59:02.279917  6676 solver.cpp:398]     Test net output #1: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:59:02.348852  6676 solver.cpp:219] Iteration 2000 (14.5087 iter/s, 6.89244s/100 iters), loss = 2.30259
I0425 14:59:02.348887  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:59:02.348896  6676 sgd_solver.cpp:105] Iteration 2000, lr = 0.001
I0425 14:59:08.845360  6676 solver.cpp:219] Iteration 2100 (15.3928 iter/s, 6.49653s/100 iters), loss = 2.30259
I0425 14:59:08.845391  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:59:08.845397  6676 sgd_solver.cpp:105] Iteration 2100, lr = 0.001
I0425 14:59:15.341176  6676 solver.cpp:219] Iteration 2200 (15.3945 iter/s, 6.49584s/100 iters), loss = 2.30259
I0425 14:59:15.341207  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:59:15.341212  6676 sgd_solver.cpp:105] Iteration 2200, lr = 0.001
I0425 14:59:21.834157  6676 solver.cpp:219] Iteration 2300 (15.4012 iter/s, 6.49301s/100 iters), loss = 2.30259
I0425 14:59:21.834188  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:59:21.834195  6676 sgd_solver.cpp:105] Iteration 2300, lr = 0.001
I0425 14:59:24.371168  6684 data_layer.cpp:73] Restarting data prefetching from start.
I0425 14:59:28.334838  6676 solver.cpp:219] Iteration 2400 (15.383 iter/s, 6.5007s/100 iters), loss = 2.30259
I0425 14:59:28.334867  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:59:28.334892  6676 sgd_solver.cpp:105] Iteration 2400, lr = 0.001
I0425 14:59:34.831270  6676 solver.cpp:219] Iteration 2500 (15.393 iter/s, 6.49646s/100 iters), loss = 2.30259
I0425 14:59:34.831401  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:59:34.831410  6676 sgd_solver.cpp:105] Iteration 2500, lr = 0.001
I0425 14:59:41.325425  6676 solver.cpp:219] Iteration 2600 (15.3986 iter/s, 6.49408s/100 iters), loss = 2.30259
I0425 14:59:41.325458  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:59:41.325464  6676 sgd_solver.cpp:105] Iteration 2600, lr = 0.001
I0425 14:59:47.818222  6676 solver.cpp:219] Iteration 2700 (15.4017 iter/s, 6.49281s/100 iters), loss = 2.30259
I0425 14:59:47.818253  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:59:47.818259  6676 sgd_solver.cpp:105] Iteration 2700, lr = 0.001
I0425 14:59:54.313413  6676 solver.cpp:219] Iteration 2800 (15.396 iter/s, 6.4952s/100 iters), loss = 2.30259
I0425 14:59:54.313477  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 14:59:54.313483  6676 sgd_solver.cpp:105] Iteration 2800, lr = 0.001
I0425 15:00:00.818071  6676 solver.cpp:219] Iteration 2900 (15.3736 iter/s, 6.50464s/100 iters), loss = 2.30259
I0425 15:00:00.818119  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:00:00.818123  6676 sgd_solver.cpp:105] Iteration 2900, lr = 0.001
I0425 15:00:07.210477  6676 solver.cpp:331] Iteration 3000, Testing net (#0)
I0425 15:00:07.642127  6676 solver.cpp:398]     Test net output #0: accuracy = 0.112
I0425 15:00:07.642150  6676 solver.cpp:398]     Test net output #1: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:00:07.711318  6676 solver.cpp:219] Iteration 3000 (14.507 iter/s, 6.89324s/100 iters), loss = 2.30259
I0425 15:00:07.711350  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:00:07.711356  6676 sgd_solver.cpp:105] Iteration 3000, lr = 0.001
I0425 15:00:14.206255  6676 solver.cpp:219] Iteration 3100 (15.3966 iter/s, 6.49494s/100 iters), loss = 2.30259
I0425 15:00:14.206284  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:00:14.206290  6676 sgd_solver.cpp:105] Iteration 3100, lr = 0.001
I0425 15:00:15.511551  6684 data_layer.cpp:73] Restarting data prefetching from start.
I0425 15:00:20.706936  6676 solver.cpp:219] Iteration 3200 (15.383 iter/s, 6.50069s/100 iters), loss = 2.30259
I0425 15:00:20.706967  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:00:20.706974  6676 sgd_solver.cpp:105] Iteration 3200, lr = 0.001
I0425 15:00:27.195430  6676 solver.cpp:219] Iteration 3300 (15.412 iter/s, 6.48846s/100 iters), loss = 2.30259
I0425 15:00:27.195459  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:00:27.195466  6676 sgd_solver.cpp:105] Iteration 3300, lr = 0.001
I0425 15:00:33.691803  6676 solver.cpp:219] Iteration 3400 (15.3932 iter/s, 6.49638s/100 iters), loss = 2.30259
I0425 15:00:33.691835  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:00:33.691843  6676 sgd_solver.cpp:105] Iteration 3400, lr = 0.001
I0425 15:00:40.183349  6676 solver.cpp:219] Iteration 3500 (15.4047 iter/s, 6.49154s/100 iters), loss = 2.30259
I0425 15:00:40.183531  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:00:40.183542  6676 sgd_solver.cpp:105] Iteration 3500, lr = 0.001
I0425 15:00:46.678656  6676 solver.cpp:219] Iteration 3600 (15.3961 iter/s, 6.49516s/100 iters), loss = 2.30259
I0425 15:00:46.678686  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:00:46.678692  6676 sgd_solver.cpp:105] Iteration 3600, lr = 0.001
I0425 15:00:53.168686  6676 solver.cpp:219] Iteration 3700 (15.4082 iter/s, 6.49003s/100 iters), loss = 2.30259
I0425 15:00:53.168716  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:00:53.168722  6676 sgd_solver.cpp:105] Iteration 3700, lr = 0.001
I0425 15:00:59.669211  6676 solver.cpp:219] Iteration 3800 (15.3834 iter/s, 6.50053s/100 iters), loss = 2.30259
I0425 15:00:59.669240  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:00:59.669246  6676 sgd_solver.cpp:105] Iteration 3800, lr = 0.001
I0425 15:01:06.170553  6676 solver.cpp:219] Iteration 3900 (15.3814 iter/s, 6.50134s/100 iters), loss = 2.30259
I0425 15:01:06.170583  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:01:06.170590  6676 sgd_solver.cpp:105] Iteration 3900, lr = 0.001
I0425 15:01:06.308676  6684 data_layer.cpp:73] Restarting data prefetching from start.
I0425 15:01:12.568398  6676 solver.cpp:331] Iteration 4000, Testing net (#0)
I0425 15:01:12.999213  6676 solver.cpp:398]     Test net output #0: accuracy = 0.098
I0425 15:01:12.999238  6676 solver.cpp:398]     Test net output #1: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:01:13.068254  6676 solver.cpp:219] Iteration 4000 (14.4976 iter/s, 6.8977s/100 iters), loss = 2.30259
I0425 15:01:13.068289  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:01:13.068297  6676 sgd_solver.cpp:105] Iteration 4000, lr = 0.001
I0425 15:01:19.560477  6676 solver.cpp:219] Iteration 4100 (15.4031 iter/s, 6.49221s/100 iters), loss = 2.30259
I0425 15:01:19.560513  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:01:19.560521  6676 sgd_solver.cpp:105] Iteration 4100, lr = 0.001
I0425 15:01:26.048398  6676 solver.cpp:219] Iteration 4200 (15.4133 iter/s, 6.48791s/100 iters), loss = 2.30259
I0425 15:01:26.048431  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:01:26.048439  6676 sgd_solver.cpp:105] Iteration 4200, lr = 0.001
I0425 15:01:32.540052  6676 solver.cpp:219] Iteration 4300 (15.4044 iter/s, 6.49164s/100 iters), loss = 2.30259
I0425 15:01:32.540086  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:01:32.540093  6676 sgd_solver.cpp:105] Iteration 4300, lr = 0.001
I0425 15:01:39.026808  6676 solver.cpp:219] Iteration 4400 (15.4161 iter/s, 6.48674s/100 iters), loss = 2.30259
I0425 15:01:39.026841  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:01:39.026849  6676 sgd_solver.cpp:105] Iteration 4400, lr = 0.001
I0425 15:01:45.532029  6676 solver.cpp:219] Iteration 4500 (15.3723 iter/s, 6.50521s/100 iters), loss = 2.30259
I0425 15:01:45.532220  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:01:45.532229  6676 sgd_solver.cpp:105] Iteration 4500, lr = 0.001
I0425 15:01:52.032802  6676 solver.cpp:219] Iteration 4600 (15.3832 iter/s, 6.5006s/100 iters), loss = 2.30259
I0425 15:01:52.032831  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:01:52.032838  6676 sgd_solver.cpp:105] Iteration 4600, lr = 0.001
I0425 15:01:57.432967  6684 data_layer.cpp:73] Restarting data prefetching from start.
I0425 15:01:58.541040  6676 solver.cpp:219] Iteration 4700 (15.3652 iter/s, 6.50822s/100 iters), loss = 2.30259
I0425 15:01:58.541136  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:01:58.541164  6676 sgd_solver.cpp:105] Iteration 4700, lr = 0.001
I0425 15:02:05.061827  6676 solver.cpp:219] Iteration 4800 (15.3357 iter/s, 6.52072s/100 iters), loss = 2.30259
I0425 15:02:05.061856  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:02:05.061880  6676 sgd_solver.cpp:105] Iteration 4800, lr = 0.001
I0425 15:02:11.563463  6676 solver.cpp:219] Iteration 4900 (15.3808 iter/s, 6.50162s/100 iters), loss = 2.30259
I0425 15:02:11.563493  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:02:11.563499  6676 sgd_solver.cpp:105] Iteration 4900, lr = 0.001
I0425 15:02:17.961809  6676 solver.cpp:331] Iteration 5000, Testing net (#0)
I0425 15:02:18.393040  6676 solver.cpp:398]     Test net output #0: accuracy = 0.092
I0425 15:02:18.393065  6676 solver.cpp:398]     Test net output #1: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:02:18.461741  6676 solver.cpp:219] Iteration 5000 (14.4964 iter/s, 6.89827s/100 iters), loss = 2.30259
I0425 15:02:18.461771  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:02:18.461777  6676 sgd_solver.cpp:105] Iteration 5000, lr = 0.001
I0425 15:02:24.957756  6676 solver.cpp:219] Iteration 5100 (15.3941 iter/s, 6.496s/100 iters), loss = 2.30259
I0425 15:02:24.957787  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:02:24.957794  6676 sgd_solver.cpp:105] Iteration 5100, lr = 0.001
I0425 15:02:31.455546  6676 solver.cpp:219] Iteration 5200 (15.3899 iter/s, 6.49777s/100 iters), loss = 2.30259
I0425 15:02:31.455576  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:02:31.455582  6676 sgd_solver.cpp:105] Iteration 5200, lr = 0.001
I0425 15:02:37.946903  6676 solver.cpp:219] Iteration 5300 (15.4051 iter/s, 6.49134s/100 iters), loss = 2.30259
I0425 15:02:37.946940  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:02:37.946949  6676 sgd_solver.cpp:105] Iteration 5300, lr = 0.001
I0425 15:02:44.433985  6676 solver.cpp:219] Iteration 5400 (15.4153 iter/s, 6.48706s/100 iters), loss = 2.30259
I0425 15:02:44.434016  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:02:44.434023  6676 sgd_solver.cpp:105] Iteration 5400, lr = 0.001
I0425 15:02:48.592743  6684 data_layer.cpp:73] Restarting data prefetching from start.
I0425 15:02:50.932488  6676 solver.cpp:219] Iteration 5500 (15.3882 iter/s, 6.49849s/100 iters), loss = 2.30259
I0425 15:02:50.932518  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:02:50.932525  6676 sgd_solver.cpp:105] Iteration 5500, lr = 0.001
I0425 15:02:57.423660  6676 solver.cpp:219] Iteration 5600 (15.4056 iter/s, 6.49115s/100 iters), loss = 2.30259
I0425 15:02:57.423697  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:02:57.423705  6676 sgd_solver.cpp:105] Iteration 5600, lr = 0.001
I0425 15:03:03.916426  6676 solver.cpp:219] Iteration 5700 (15.4018 iter/s, 6.49274s/100 iters), loss = 2.30259
I0425 15:03:03.916458  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:03:03.916465  6676 sgd_solver.cpp:105] Iteration 5700, lr = 0.001
I0425 15:03:10.409226  6676 solver.cpp:219] Iteration 5800 (15.4017 iter/s, 6.49278s/100 iters), loss = 2.30259
I0425 15:03:10.409255  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:03:10.409261  6676 sgd_solver.cpp:105] Iteration 5800, lr = 0.001
I0425 15:03:16.905498  6676 solver.cpp:219] Iteration 5900 (15.3935 iter/s, 6.49625s/100 iters), loss = 2.30259
I0425 15:03:16.905529  6676 solver.cpp:238]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:03:16.905536  6676 sgd_solver.cpp:105] Iteration 5900, lr = 0.001
I0425 15:03:23.297847  6676 solver.cpp:448] Snapshotting to binary proto file examples/cifar10_full_sigmoid_iter_6000.caffemodel
I0425 15:03:23.361603  6676 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar10_full_sigmoid_iter_6000.solverstate
I0425 15:03:23.400182  6676 solver.cpp:311] Iteration 6000, loss = 2.30259
I0425 15:03:23.400205  6676 solver.cpp:331] Iteration 6000, Testing net (#0)
I0425 15:03:23.793052  6676 solver.cpp:398]     Test net output #0: accuracy = 0.088
I0425 15:03:23.793081  6676 solver.cpp:398]     Test net output #1: loss = 2.30259 (* 1 = 2.30259 loss)
I0425 15:03:23.793085  6676 solver.cpp:316] Optimization Done.
I0425 15:03:23.793089  6676 caffe.cpp:259] Optimization Done.
